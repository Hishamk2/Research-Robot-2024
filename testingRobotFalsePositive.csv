,Unnamed: 0,questionId,questionTitle,Tags,questionBody,questionCreationDate,AcceptedAnswerId,questionViewCount,AnswerCount,CommentCount,questionScore,questionFavoriteCount,questionUserId,questionUserLocation,questionUserCreationDate,questionUserViews,answerId,answerBody,answerUserId,answerScore,answerCommentCount,commentId,commentText,,
0,127,166742,Robot simulation environments,|simulation|environment|robotics|,"<p>I would like to make a list of remarkable robot simulation environments including advantages and disadvantages of them. Some examples I know of are <a href=""http://www.cyberbotics.com/"" rel=""noreferrer"">Webots</a> and <a href=""http://playerstage.sourceforge.net/"" rel=""noreferrer"">Player/Stage</a>.</p>
",10/3/2008 13:07,166799,5688,12,2,15,0,21047,"Budapest, Hungary",9/23/2008 11:27,704,166799,"<p>This made me remember the <a href=""http://www.spiderland.org/node/2598"" rel=""nofollow noreferrer"">breve</a> project.</p>

<blockquote>
  <p><em>breve is a free, open-source software package which makes it easy to build 3D simulations of multi-agent systems and artificial life.</em></p>
</blockquote>

<p>There is also a wikipage listing <a href=""http://en.wikipedia.org/wiki/Robotics_simulator"" rel=""nofollow noreferrer"">Robotics simulators</a></p>
",842,4,0,16833762,We have several good answers: I prefer those that include a whole list like epatel's and Ezu's answers or is elaborated as Prometheus.one's answer.,,
1,100,106684,How to create a new type of entity in Microsoft Robotics Studio 2.0?,|artificial-intelligence|robotics|robotics-studio|,"<p>What I'm trying to do with MRS is to teach myself some basic AI; what I want to do is to make a rocket entity, with things such as vectored exhaust, and staging. Anyone have an idea on how to make an entity that can fly? Or do I just need to constantly apply a force upwards?</p>
",9/20/2008 1:16,110338,477,2,0,6,0,18658,"Brisbane, Australia",9/19/2008 8:49,1055,110338,"<p>Hey TraumaPony, your question looked lonely :)</p>

<p>I took a look at an MSDN article about MRS 2.0 <a href=""http://msdn.microsoft.com/en-us/magazine/cc546547.aspx"" rel=""nofollow noreferrer"">here</a> I believe you'll actually need to create a Rocket entity of some kind and then a Thruster entity that it can use. In the article they were able to reuse a DifferentialDrive entity to propel their bot forward. I hope that helps. I'm more or less shooting in the dark since no else has tried to help ya out yet. Cheers! :)</p>
",9401,3,0,,,,
4,260,1011602,How can you add a camera to a robot in the Breve Simulator?,|python|simulation|robotics|,"<p>I've created a two wheeled robot based on the braitenberg vehicle. Our robots have two wheels and a PolygonDisk body(Much like kepera and e-puck robots). I would like to add a camera to the front of the robot. The problem then becomes how to control the camera and how to keep pointing it in the right direction(same direction as the robot). How can you make the camera point in the same direction as the robot ?</p>
",6/18/2009 9:01,,362,1,0,1,,109730,,5/20/2009 5:01,13,1027059,"<p>After much trying and failing I finally made it work.
So here is how I did it:</p>

<p>The general idea is to have an link or object linked to the vehicle and then measuring 
its rotation and location in order to find out in which direction the camera should be aimed.</p>

<p>1) Add an object that is linked to the robot:</p>

<pre><code>def addVisualCam(self):
    joint = None
    cam = breve.createInstances(breve.Link,1)
    cam.setShape(breve.createInstances(breve.PolygonCone, 1).initWith(10,0.08,0.08))
    joint = breve.createInstances(breve.FixedJoint,1)
    # So ad-hoc it hurts. oh well...
    joint.setRelativeRotation(breve.vector(0,1,0), -3.14/2)
    joint.link(breve.vector(0,1.05,0), breve.vector(0,0,0), cam, self.vehicle.bodyLink, 0)
    joint.setDoubleSpring(300, 1.01000, -1.01000)
    self.vehicle.addDependency(joint)
    self.vehicle.addDependency(cam)
    cam.setColor(breve.vector(0,0,0))
    self.cam = cam
</code></pre>

<p>2) Add this postIterate:</p>

<pre><code>def postIterate(self):
    look_at = self.cam.getLocation() + (self.cam.getRotation() * breve.vector(0,0,1))
    look_from = -(self.cam.getRotation()*breve.vector(0,0,1))
    self.vision.look(look_at, look_from)
</code></pre>
",109730,1,0,,,,
14,466,2329934,Robot Simulation in Java,|java|simulation|robotics|,"<p>I am doing a project concerning robot simulation and i need help. I have to simulate the activities of a robot in a warehouse. I am using mindstorm robots and lego's for the warehouse. The point here is i have to simulate all the activities of the robot on a Java GUI. That is whenever the robot is moving, users have to see it on the GUI a moving object which represents the robot.</p>

<p>When the roads/rails/crossings of the warehouse changes it must also be changed on the screen. The whole project is i have to simulate whatever the robot is doing in the warehouse in real-time. Everything must happen in real-time  </p>

<p>I am asking which libraries in Java i can use to do this simulations in real-time and if someone can also point me to any site for good information. Am asking for libraries in Java that i can use to visualize the simulation in real-time.</p>
",2/24/2010 22:00,,3773,3,1,6,0,,,,,2329960,"<p>Perhaps the easiest (if not best) place to start is the ""Java2D"" API: <a href=""http://java.sun.com/products/java-media/2D/index.jsp"" rel=""nofollow noreferrer"">http://java.sun.com/products/java-media/2D/index.jsp</a></p>
",65977,3,0,2304974,"Note that ""real-time"" has a special meaning in software. http://en.wikipedia.org/wiki/Real-time_computing",,
20,1259,7861132,iterative lengthening pseudo code for traversal of all the nodes,|algorithm|artificial-intelligence|robotics|,"<p>What would the algorithm, pseudo code or actual code to traverse all the nodes in a graph using an iterative lengthening depth-first approach?</p>
",10/22/2011 17:00,,3006,1,0,1,0,452944,Stockholm,9/16/2010 8:23,371,7887942,"<p>I give you first the depth-first pseudo-code for graph</p>

<pre><code>DLS(node, goal, depth, visited) 
{
  if ( depth &gt;= 0 ) 
    {
    if ( node == goal )
      return node

    visited.insert(node)

    for each child in expand(node)
      if (child is not in visited)
          DLS(child, goal, depth-1, visited)
  }
}
</code></pre>

<p>and the iterative DLS is</p>

<pre><code>IDDFS(start, goal)
{
  depth = 0
  while(no solution)
  {
    visited = [] // &lt;-- Empty List
    solution = DLS(start, goal, depth,visited)
    depth = depth + 1
  }
  return solution
}
</code></pre>

<p>You can always transform a graph in a tree by removing graph loop with a <em>visited list</em>. :)</p>
",173787,1,3,,,,
28,1282,8513998,Panda3d Robotics,|c++|python|ruby|robotics|panda3d|,"<p>The title makes it obvious, is this a good idea? I've been looking for a robotics simulator in languages i know (i know ruby best, then c++, then python -- want to strengthen here--, forget about javascript, but i know it). </p>

<p></p>

<p>i found something called pyro, but it probably doesn't fit my needs (listed below). </p>

<p></p>

<p>In my last university term i learned c++, then they took me to RobotC (which was only about 2 months of the term). Pyro seems similar but now i want something different.</p>

<p></p>

<p>I need something that allows to import graphics, allows 3d environments, allows to easily modify actions robot can perform. Also provides other things necessary for robot programming, like a sensor.</p>
",12/15/2011 1:24,,953,2,1,2,0,989635,,10/11/2011 13:44,61,10025027,"<p>Panda 3D is a good language to write your own robot system in.  It's written by CMU people, so it's very clean and makes a lot of sense.  It allows you to import very complex models from Maya or Blender.  It supports 3D environments.  Although it has its own scripting language for running actions (animations) imported from your modeling package, I prefer to write my own robot driver.  It supports three different physics engines, including its own basic version, Open Dynamics Engine (ODE), and most recently Bullet.  Although it supports collision detection, which allows triggering, it is an animation and graphic rendering system, not a robotics system per se, and so you'll have to craft your own sensor simulations beside or on top of it.  All in all, though, it is quite satisfactory.  Good luck.</p>
",841457,0,3,10651737,"Have a look here - [http://stackoverflow.com/questions/2533321/robotics-simulator][1]


  [1]: http://stackoverflow.com/questions/2533321/robotics-simulator",,
29,949,4749693,Can I get Erlang OTP behaviors in C Nodes?,|event-handling|erlang|robotics|erlang-otp|erl-interface|,"<p>For example, right now I have a C Node (call it <strong>CN</strong>) which connects to an erlang node (call it <strong>EN</strong>) and uses a RPC to use OTP behaviors. Hence, to send an event from <strong>CN</strong> to an event manager on <strong>EN</strong> I connect <strong>CN</strong> to <strong>EN</strong> and do</p>

<pre><code>args = erl_format(""[data_man, {~f, ~f}]"", ch.at(0), ch.at(1));
erl_rpc_to(fd, ""gen_event"", ""notify"", args);
</code></pre>

<p>But, then, my C Node really isn't behaving as a node (i.e. why create a node that only uses remote procedure calls?).</p>

<p>Is there a way to directly use OTP behaviors within a C Node?</p>

<p>If there isn't, should I look under the hood at the message formats being used by OTP and send messages using that format (i.e. can I spoof OTP behaviors?)? <em>I don't like this idea, I'll have to watch for changes in the implementation of OTP etc.</em></p>

<p>I have hard latency limits in my requirements, how does this effect my choice of communication between a C process and Erlang (are RPCs going to bog me down? etc.)?</p>
",1/20/2011 16:24,,320,1,0,3,0,79168,,3/17/2009 19:19,823,4771331,"<p>There is no way to directly use OTP behaviours from C. I also don't think you should mimic the OTP behaviours to use them directly.</p>

<p>You should just first use RPC and then test your code against your performance requirements. If needed, you could always send a simple message to your gen_event process to make it notify itself through its handle_info/2 method.</p>
",493099,4,0,,,,
34,1406,11424438,detecting lump region in a 2D array,|matlab|image-processing|computer-vision|computational-geometry|robotics|,"<p>in the 2D array plotted below, we are interested in finding the ""lump"" region. As you can see it is not a continuous graph. Also, we know the approximate dimension of the ""lump"" region. A set of data are given below. First column contains the y values and the second contains the x values. Any suggestion as to how to detect lump regions like this?</p>

<p><img src=""https://i.stack.imgur.com/eQDZt.png"" alt=""enter image description here""></p>

<pre><code>   21048        -980
   21044        -956
   21040        -928
   21036        -904
   21028        -880
   21016        -856
   21016        -832
   21016        -808
   21004        -784
   21004        -760
   20996        -736
   20996        -712
   20992        -684
   20984        -660
   20980        -636
   20968        -612
   20968        -588
   20964        -564
   20956        -540
   20956        -516
   20952        -492
   20948        -468
   20940        -440
   20936        -416
   20932        -392
   20928        -368
   20924        -344
   20920        -320
   20912        -296
   20912        -272
   20908        -248
   20904        -224
   20900        -200
   20900        -176
   20896        -152
   20888        -128
   20888        -104
   20884         -80
   20872         -52
   20864         -28
   20856          -4
   20836          16
   20812          40
   20780          64
   20748          88
   20744         112
   20736         136
   20736         160
   20732         184
   20724         208
   20724         232
   20724         256
   20720         280
   20720         304
   20720         328
   20724         352
   20724         376
   20732         400
   20732         424
   20736         448
   20736         472
   20740         496
   20740         520
   20748         544
   20740         568
   20736         592
   20736         616
   20736         640
   20740         664
   20740         688
   20736         712
   20736         736
   20744         760
   20748         788
   20760         812
   20796         836
   20836         860
   20852         888
   20852         912
   20844         936
   20836         960
   20828         984
   20820        1008
   20816        1032
   20820        1056
   20852        1080
   20900        1108
   20936        1132
   20956        1156
   20968        1184
   20980        1208
   20996        1232
   21004        1256
   21012        1280
   21016        1308
   21024        1332
   21024        1356
   21028        1380
   21024        1404
   21020        1428
   21016        1452
   21008        1476
   21004        1500
   20992        1524
   20980        1548
   20956        1572
   20944        1596
   20920        1616
   20896        1640
   20872        1664
   20848        1684
   20812        1708
   20752        1728
   20664        1744
   20640        1768
   20628        1792
   20628        1816
   20620        1836
   20616        1860
   20612        1884
   20604        1908
   20596        1932
   20588        1956
   20584        1980
   20580        2004
   20572        2024
   20564        2048
   20552        2072
   20548        2096
   20536        2120
   20536        2144
   20524        2164
   20516        2188
   20512        2212
   20508        2236
   20500        2260
   20488        2280
   20476        2304
   20472        2328
   20476        2352
   20460        2376
   20456        2396
   20452        2420
   20452        2444
   20436        2468
   20432        2492
   20432        2516
   20424        2536
   20420        2560
   20408        2584
   20396        2608
   20388        2628
   20380        2652
   20364        2676
   20364        2700
   20360        2724
   20352        2744
   20344        2768
   20336        2792
   20332        2812
   20328        2836
   20332        2860
   20340        2888
   20356        2912
   20380        2940
   20428        2968
   20452        2996
   20496        3024
   20532        3052
   20568        3080
   20628        3112
   20652        3140
   20728        3172
   20772        3200
   20868        3260
   20864        3284
   20864        3308
   20868        3332
   20860        3356
   20884        3384
   20884        3408
   20912        3436
   20944        3464
   20948        3488
   20948        3512
   20932        3536
   20940        3564
</code></pre>
",7/11/2012 1:47,11424758,404,1,1,3,0,1284853,,3/22/2012 2:18,316,11424758,"<p>It may be just a coincidence, but the lump you show looks fairly parabolic.  It's not completely clear what you mean by ""know the approximate dimension of the lump region"" but if you mean that you know approximately how wide it is (i.e. how much of the x-axis it takes up), you could simply slide a window of that width along the x-axis and do a parabolic fit (a.k.a. polyfit with degree 2) to all data that fits into the window at each point.  Then, compute r^2 goodness-of-fit values at each point and the point with the r^2 closest to 1.0 would be the best fit.  You'd probably need a threshold value and to throw out those where the x^2 coefficient was positive (to find lumps rather than dips) for sanity, but this might be a workable approach.</p>

<p>Even if the parabolic look is a coincidence, I think this would ba a reasonable approach--a downward pointing parabola is a pretty good description of a general ""lump"" by any definition I can think of.</p>

<p><strong>Edit:  Attempted Implementation Below</strong></p>

<p>I got curious and went ahead and implemented my proposed solution (with slight modifications).  First, here's the code (ugly but functional):</p>

<pre><code>function [x, p] = find_lump(data, width)

  n = size(data, 1);

  f = plot(data(:,1),data(:,2), 'bx-');
  hold on;

  bestX = -inf;
  bestP = [];
  bestMSE = inf;
  bestXdat = [];
  bestYfit = [];

  spanStart = 0;
  spanStop = 1;
  spanWidth = 0;

  while (spanStop &lt; n)
    if (spanStart &gt; 0)
      % Drop first segment from window (since we'll advance x):
      spanWidth = spanWidth - (data(spanStart + 1, 1) - x);
    end

    spanStart = spanStart + 1;
    x = data(spanStart, 1);

    % Advance spanStop index to maintain window width:
    while ((spanStop &lt; n) &amp;&amp; (spanWidth &lt;= width))
      spanStop = spanStop + 1;
      spanWidth = data(spanStop, 1) - x;
    end

    % Correct for overshoot:
    if (spanWidth &gt; width) 
      spanStop = spanStop - 1;
      spanWidth = data(spanStop, 1) - x;
    end

    % Fit parabola to data in the current window:
    xdat = data(spanStart:spanStop, 1);
    ydat = data(spanStart:spanStop, 2);
    p = polyfit(xdat, ydat, 2);

    % Compute fit quality (mean squared error):
    yfit = polyval(p,xdat);
    r = yfit - ydat;
    mse = (r' * r) / size(xdat,1);

    if ((p(1) &lt; -0.002) &amp;&amp; (mse &lt; bestMSE))
      bestMSE = mse;
      bestX = x;
      bestP = p;
      bestXdat = xdat;
      bestYfit = yfit;
    end
  end

  x = bestX;
  p = bestP;

  plot(bestXdat,bestYfit,'r-');
</code></pre>

<p>...and here's a result using the given data (I swapped the columns so column 1 is x values and column 2 is y values) with a window width parameter of 750:</p>

<p><img src=""https://i.stack.imgur.com/J57sR.png"" alt=""Data plot with lump fit drawn in""></p>

<p><strong>Comments:</strong></p>

<p>I opted to use mean squared error between the fit parabola and the original data within each window as the quality metric, rather than correlation coefficient (r^2 value) due to laziness more than anything else.  I don't think the results would be much different the other way.</p>

<p>The output is heavily dependent on the threshold value chosen for the quadratic coefficient (see the bestMSE condition at the end of the loop).  Truth be told, I cheated here by outputing the fit coefficients at each point, then selected the threshold based on the known lump shape.  This is equivalent to using a lump template as suggested by @chaohuang and may not be very robust depending on the expected variance in the data.</p>

<p>Note that some sort of shape control parameter seems to be necessary if this approach is used.  The reason is that any random (smooth) run of data can be fit nicely to some parabola, but not necessarily around the maximum value.  Here's a result where I set the threshold to zero and thus only restricted the fit to parabolas pointing downwards:</p>

<p><img src=""https://i.stack.imgur.com/N4rbz.png"" alt=""Best fit is not a &quot;lump&quot;""> </p>

<p>An improvement would be to add a check that the fit parabola at least has a maximum within the window interval (that is, check that the first derivative goes to zero within the window so we at least find local maxima along the curve).  This alone is not sufficient as you still might have a tiny little lump that fits a parabola better than an ""obvious"" big lump as seen in the given data set.</p>
",23934,7,0,15070721,"Since the approximate dimension of the lump region is known, maybe you can use some 'average' or 'standard' lump to perform the cross-correlation to detect it.",,
35,1321,9321402,Associate file format with my program (Java),|java|file|associations|robotics|,"<p>I am making Scouting Software (in Java) for my <a href=""http://www.usfirst.org/roboticsprograms/frc"" rel=""nofollow"" title=""First Robotics Competition Official Website"">FRC</a> robotics team. Scouting is like collecting data on other teams' robots during competition. It is absolutely critical that my program makes that process as simple and easy as possible. My program can save its data in two ways, one of which is by writing a .scout file to the user's hard drive. All this is working well, but as a finishing touch i would like to implement a way to associate .scout files with my program so that .scout files are opened with my program. It's like .docx for Microsoft Word. It associates .doc/.docx/...etc to itself such that when the user clicks on a file with those extensions, Word opens itself up and then opens the file the user clicked on. I want something like this for my application. Keep in mind, it is written in Java and meant to work on different operating systems (Windows, OSX, Ubuntu Linux, etc). </p>
",2/17/2012 0:46,9322949,1868,1,2,2,,901880,"Bellevue, WA",8/19/2011 6:08,509,9322949,"<p>Does the program have a GUI?  If so, launch it with <a href=""https://stackoverflow.com/tags/java-web-start/info"">Java Web Start</a>.  </p>

<p>JWS can associate a file-type with an application on Windows, OS X &amp; *nix.  Here is a <a href=""http://pscode.org/jws/api.html#fs"" rel=""nofollow noreferrer"">demo. of the JNLP API file service</a> that associates the <code>.zzz</code> file type with the demo. app.</p>
",418556,2,1,11760975,"It is not provided through standard Java library in Java; it is native OS-specific feature. Under Windows, this can be done through Windows registry http://stackoverflow.com/questions/1387769/create-registry-entry-to-associate-file-extension-with-application-in-c. You can use installer builder like NSIS, InnoSetup, etc. to set a registry entry. Java Web Start (JWS) probably has a this feature to support cross-platform.",,
38,1367,10550874,How to calc a cyclic arc through 3 points and parameterize it 0..1 in 3d,|c#|math|robotics|,"<p>How can i calculate an arc through 3 points A, B, C in 3d. from A to C passing B (order is taken care of).</p>

<p>Most robot arms have this kind of move command. I need to simulate it and apply different speed dynamics to it and need therefore a parameter 0..1 which moves a position from A to C.</p>

<p>EDIT:</p>

<p>what i have is radius and center of the arc, but how can i parameterize the circle in 3d if i know the start and end angle?</p>

<p>EDIT2:</p>

<p>getting closer. if i have two unit length perpendicular vectors v1 and v2 on the plane in which the circle lies, i can do a parameterization like: <strong>x</strong>(t) = <strong>c</strong> + r * cos(t) * <strong>v1</strong> + r * sin(t) * <strong>v2</strong></p>

<p>so i take v1 = a-c and i only need to find v2 now. any ideas?</p>
",5/11/2012 11:57,16641390,3612,3,10,5,0,355485,"Berlin, Germany",6/1/2010 14:00,343,10557037,"<p>Martin Doms recently wrote <a href=""http://blog.martindoms.com/2012/04/25/splines-and-curves-part-i-bezier-curves/"" rel=""nofollow"">a blog entry about splines and bezier curves</a> that you might find useful.  </p>

<p>Part of his post describes how to get a 2D curve defined by the three control points P<sub>0</sub>, P<sub>1</sub>, and P<sub>2</sub>.  The curve is parameterized by a value <code>t</code> that ranges from 0 to 1:</p>

<p>F(t) = (1-t)<sup>2</sup> P<sub>0</sub> + 2t (1-t) P<sub>1</sub> + t<sup>2</sup> P<sub>2</sub></p>

<p>It seems likely that you could adapt that to 3D with a little thought.  (Of course, bezier curves don't necessarily go through the control points.  This may not work if that's a deal-breaker for you.)</p>

<p>As an aside, Jason Davies put together <a href=""http://www.jasondavies.com/animated-bezier/"" rel=""nofollow"">a nice little animation of curve interpolation</a>.</p>
",98654,3,3,23933819,"I found a solution after all, check my answer.",,
45,1408,11656945,Computer Vision/Image Processing frameworks,|image-processing|computer-vision|augmented-reality|robotics|,"<p>I'm curious to know if there are any image processing/computer vision frameworks out there that allow you to create a filter pipeline by dynamically creating chains of filters/filter blocks (similar to simulink blocks in MATLAB). </p>

<p>The idea is mostly inspired by <a href=""http://www.roborealm.com/"" rel=""nofollow"">RoboRealm</a>, but I'd like to implement this mostly in C/C++ with the ability to graphically build image processing pipelines. I'm familiar with one such framework, <a href=""https://code.google.com/p/camunits/"" rel=""nofollow"">Camunits</a>, which I shall use as a foundation to build this graphical filter framework, but please do let me know if you are aware of any. CamUnits integrates well with LCM (Lightweight Communications and Marshalling) which handles most of the marshalling and networking needs that I'd like to avoid for now. Furthermore, CamUnits also integrates well with the logging framework within LCM, and has a bunch of tools for image acquisition (firewire cameras, automatic gain/exposure correction, fast de-bayering etc). </p>

<p>In short, I'd like to have the functionality to build a graphical interface that lets you dynamically create image processing pipelines (threaded if-needed), which would in turn help in rapid prototyping of image processing/computer vision algorithms. I'm also curious to know if there'd be any interest in this type of framework (modular, and quickly/highly reconfigurable). </p>
",7/25/2012 19:15,,3727,3,2,-1,0,768319,"Cambridge, MA",5/24/2011 18:41,13,11670155,"<p>This is (almost) the oldest idea in the zoo of image processing applications: the ""kitchen sink"" GUI app where filters are boxes, images are input to the left, data flow through boxes, images come out to the right.</p>

<p>The oldest I remember using firsthand was <a href=""http://www.agocg.ac.uk/reports/visual/vissyst/dogbo_45.htm"" rel=""nofollow"">Khoros</a> (and that may tell you how old I am), but am almost positive that the people at Xerox had something similar way earlier than that.
More recently, a host of image compositing apps have used a similar UI approach, most notably <a href=""http://en.wikipedia.org/wiki/Apple_Shake"" rel=""nofollow"">Shake</a>.</p>

<p>In my experience, they are quite useful for algorithm exploration, but I have never seen one where the GUI didn't get in the way of getting things done when the problems started getting complicated. ""Visual computing"" is appealing for getting the rough outline of a solution, but there is a reason why harder problems are best reasoned upon and communicated using equations - it's a more concise notation that dispenses with hundres of useless bubbles and lines drawn upon a screen.</p>

<p>In production practice, the usefulness of these apps ends up being tied to their output scripting capabilities: mouse-dragging gets quickly tiresome when you do find a solution to your problem, and you want to apply it to a truckload of images. Then the app better have a way to output code implementing the image transformation in a way that's easy to interface with the rest of your codebase.   </p>
",1435240,3,0,15478942,"As others have said here, there are many frameworks that do this (in fact, most of the image processing ones I've seen can be rigged up in this fashion). Apple's Core Image framework on Mac and iOS is built around this structure, and its Quartz Composer tool even lets you do the graphical drag and drop connection of filters, inputs, and outputs. I wrote my own open source iOS framework along these lines, with modular filters or processing operations that you chain together and can swap out as needed. I even know someone who has build a GUI for rapid prototyping of filter chains from this.",,
50,1623,14453201,Posterior probability after robot movement,|probability|robot|,"<p>Suppose robot moves in cells and there are 5 cells. Their dist. is as follows:
|   1/9  |   1/3  |   1/3  |   1/9  |  1/9  |</p>

<p>The robot moves one cells towards right. And the world is cyclic. When it moves to the most right cell, it return back to the most left one. </p>

<p>And the posterior probability after one cell movement is as follows:
|  1/9   |   1/9  |   1/3  |   1/3  |   1/9 |</p>

<p>The following diagram is a good illustration. 
<img src=""https://i.stack.imgur.com/IdJqV.png"" alt=""enter image description here""></p>

<p>Can any guy tell me why the posterior probability shifts to the right one cell?
Thanks in advance!</p>
",1/22/2013 7:00,14477186,449,2,0,1,,1418947,,5/26/2012 11:26,58,14477186,"<p>Think about the probability of the robot being in any cell A at Time t in terms of its probability of being or not being in cell A-1 at Time t-1:</p>

<p>Break event up into mutually exclusive joint events:</p>

<p>-->  <strong>P(Robot loc @ T = A ) = P(Robot loc @ T=A, Robot loc @ T-1 = A-1) + P(Robot loc @ T=A, Robot loc @ T-1 &lt;> A-1)</strong></p>

<p>Use conditional probability to break those joint events up into independent events:</p>

<p>-->  <strong>P(Robot loc @ T= A ) = P(Robot loc @ T=A | Robot loc @ T-1 = A-1) . P(Robot loc @ T-1 = A-1) + P(Robot loc @ T=A | Robot loc @ T-1 &lt;> A-1) . P(Robot loc @ T-1 &lt;> A-1)</strong></p>

<p>And that allows us to use the fact that the robot is moving to the right (the event that the robot has moved to the right has prob 1, any other possibility has prob 0).</p>

<p>-->  <strong>P(Robot loc @ T= A ) = 1 . P(Robot loc @ T-1 = A-1) + 0 . P(Robot loc @ T-1 &lt;> A-1)</strong></p>

<p>Simplify, and get the answer you wanted.</p>

<p>-->  <strong>P(Robot loc @ T= A ) = P(Robot loc @ T-1 = A-1)</strong></p>
",1616231,1,2,,,,
51,1649,14862810,How to put and use two different values in buffer in C?,|c|robotics|,"<p>The following is part of a C code to make a robot move in its simulator.</p>

<pre><code>while (1)
{
    sprintf(buf, ""M LR 100 100\n"");    //motor left and right moves with speed 100 each.
    write(sock, buf, strlen(buf));     //sends the buffer to the socket (simulator)
        int lme, rme;                  //lme and rme are right and left motor encoder values, the other value I need to send to buffer.
        sprintf(buf, ""S MELR\n"");      //sensor command to find ME values
        sscanf(buf, ""S MELR %i %i\n"", &amp;lme, &amp;rme);       //sending the actual ME values, that need to be sent to a(nother?) buffer.
        printf(buf, ""lme , rme"");      //the values of MEncoders.
    memset(buf, 0, 80);                //clear the buffer, set buffer value to 0
    read(sock, buf, 80);               //read from socket to get results.        
}
</code></pre>

<p>This does not work, as although the robot moves with speed 100, the terminal just shows S MELR and no motor encoder values, but it shows the value when the M LR command is removed so I think it has something to do with the MELR values not being sent to the buffer. How can this be improved or how can I set a new buffer for the MELR values?</p>
",2/13/2013 20:47,,320,3,4,1,,1950436,,1/5/2013 5:27,81,14863096,"<p>You dont need to read buf once more? What do you want to do with lme and rme?      </p>

<pre><code>while (1)
{
    sprintf(buf, ""M LR 100 100\n"");    //motor left and right moves with speed 100 each.
    write(sock, buf, strlen(buf));     //sends the buffer to the socket (simulator)
        int lme, rme;                  //lme and rme are right and left motor encoder values, the other value I need to send to buffer.
        sprintf(buf, ""S MELR\n"");      //sensor command to find ME values
    write(sock, buf, strlen(buf));     //sends the buffer to the socket 
    read(sock, buf, 80);               //read from socket to get results.    
        sscanf(buf, ""S MELR %i %i\n"", &amp;lme, &amp;rme);       //sending the actual ME values, that need to be sent to a(nother?) buffer.
        // ???? printf(buf, ""lme , rme"");      //the values of MEncoders.
    memset(buf, 0, 80);                //clear the buffer, set buffer value to 0
    read(sock, buf, 80);  //???        //read from socket to get results.        
}
</code></pre>

<p>How you can “<em>compensate for the difference between the voltage you commanded and the speed you actually achieved</em>”??. I don’t have any idea about robots… but you can try : ? </p>

<pre><code>int lme=100, rme=100;     //lme and rme are right and left motor encoder values, the other value I need to send to buffer.             
while (1)
{
    sprintf(buf, ""M LR %i %i\n"", lme, rme);    //motor left and right moves with speed 100 each.
    write(sock, buf, strlen(buf));     //sends the buffer to the socket (simulator)
        sprintf(buf, ""S MELR\n"");      //sensor command to find ME values
    write(sock, buf, strlen(buf));     //sends the buffer to the socket 
    read(sock, buf, 80);               //read from socket to get results.    
        sscanf(buf, ""S MELR %i %i\n"", &amp;lme, &amp;rme);       
    lme=100+(100-lme) ; rem=100+(100-rme);  // compensate  ????
}
</code></pre>
",1458030,0,1,20835249,"As I can understand the task should be: 1. send a motion command to simulator via socket, 2. send a ""get encoder values"" command to simulator via socket, 3. read response from socket, 4. parse response extracting encoders values, 5. print encoder values. Is that correct? If answer is ""yes"" then, how many times do you send commands in your code? I see only 1. And I can not see that you read response with encoders values for parsing.",,
53,1614,14405659,"Describing nonlinear transformation between two images, using homography",|image-processing|computer-vision|robotics|camera-calibration|projective-geometry|,"<p>A one to one point matching has already been established 
between the blue dots on the two images. 
The image2  is the distorted version of the image1. The distortion model seems to be
eyefish lens distortion. The question is:
Is there any way to compute a transformation matrix which describes this transition.
In fact a matrix which transforms the blue 
dots on the first image to their corresponding blue dots on the second image?
The problem here is that we don’t know the focal length(means images are uncalibrated), however we do have
perfect matching between around 200 points on the two images.
<img src=""https://i.stack.imgur.com/EFL1b.png"" alt=""image1(original)"">
the distorted image:
<img src=""https://i.stack.imgur.com/wEKYT.png"" alt=""eimage2""></p>
",1/18/2013 18:53,14460154,3241,1,7,8,0,699559,,4/9/2011 1:47,978,14460154,"<p>I think what you're trying to do can be treated as a distortion correction problem, without the need of the rest of a classic camera calibration.</p>

<p>A matrix transformation is a linear one and linear transformations map always straight lines into straight lines (<a href=""http://en.wikipedia.org/wiki/Linear_map"" rel=""noreferrer"">http://en.wikipedia.org/wiki/Linear_map</a>). It is apparent from the picture that the transformation is nonlinear so you cannot describe it with a matrix operation.</p>

<p>That said, you can use a lens distortion model like the one used by OpenCV (<a href=""http://docs.opencv.org/doc/tutorials/calib3d/camera_calibration/camera_calibration.html"" rel=""noreferrer"">http://docs.opencv.org/doc/tutorials/calib3d/camera_calibration/camera_calibration.html</a>) and obtaining the coefficients shouldn't be very difficult. Here is what you can do in Matlab:</p>

<p>Call (x, y) the coordinates of an original point (top picture) and (xp, yp) the coordinates of a distorted point (bottom picture), both shifted to the center of the image and divided by a scaling factor (same for x and y) so they lie more or less in the [-1, 1] interval. The distortion model is:</p>

<pre><code>x = ( xp*(1 + k1*r^2 + k2*r^4 + k3*r^6) + 2*p1*xp*yp + p2*(r^2 + 2*xp^2));
y = ( yp*(1 + k1*r^2 + k2*r^4 + k3*r^6) + 2*p2*xp*yp + p1*(r^2 + 2*yp^2));
</code></pre>

<p>Where</p>

<pre><code>r = sqrt(x^2 + y^2);
</code></pre>

<p>You have 5 parameters: k1, k2, k3, p1, p2 for radial and tangential distortion and 200 pairs of points, so you can solve the nonlinear system.</p>

<p>Be sure the x, y, xp and yp arrays exist in the workspace and declare them global:</p>

<pre><code>global x y xp yp
</code></pre>

<p>Write a function to evaluate the mean square error given a set of arbitrary distortion coefficients, say it's called 'dist':</p>

<pre><code>function val = dist(var)

global x y xp yp

val = zeros(size(xp));

k1 = var(1);
k2 = var(2);
k3 = var(3);
p1 = var(4);
p2 = var(5);

r = sqrt(xp.*xp + yp.*yp);
temp1 = x - ( xp.*(1 + k1*r.^2 + k2*r.^4 + k3*r.^6) + 2*p1*xp.*yp + p2*(r.^2 + 2*xp.^2));
temp2 = y - ( yp.*(1 + k1*r.^2 + k2*r.^4 + k3*r.^6) + 2*p2*xp.*yp + p1*(r.^2 + 2*yp.^2));
val = sqrt(temp1.*temp1 + temp2.*temp2);
</code></pre>

<p>Solve the system with 'fsolve"":</p>

<pre><code>[coef, fval] = fsolve(@dist, zeros(5,1));
</code></pre>

<p>The values in 'coef' are the distortion coefficients you're looking for. To correct the distortion of new points (xp, yp) not present in the original set, use the equations:</p>

<pre><code>r = sqrt(xp.*xp + yp.*yp);
x_corr = xp.*(1 + k1*r.^2 + k2*r.^4 + k3*r.^6) + 2*p1*xp.*yp + p2*(r.^2 + 2*xp.^2);
y_corr = yp.*(1 + k1*r.^2 + k2*r.^4 + k3*r.^6) + 2*p2*xp.*yp + p1*(r.^2 + 2*yp.^2);
</code></pre>

<p>Results will be shifted to the center of the image and scaled by the factor you used above.</p>

<p>Notes:</p>

<ul>
<li>Coordinates must be shifted to the center of the image as the distortion is symmetric with respect to it. </li>
<li>It should't be necessary to normalize to the interval [-1, 1] but it is comon to do it so the distortion coefficients obtained are more or less of the same order of magnitude (working with powers 2, 4 and 6 of pixel coordinates would need very small coefficients).</li>
<li>This method doesn't require the points in the image to be in an uniform grid.</li>
</ul>
",1755482,6,0,20045612,Can you tell what is the difference between this and image registration ?,,
56,1728,16507542,Movement of a surgical robot's arm OpenGL,|opengl|graphics|robotics|,"<p>I have a question concerning surgical robot arm's movements in OpenGL. </p>

<p>Our arm consists of 7 pieces that suppose to be the arm's joints and they are responsible for bending and twisting the arm. We draw the arm this way: first we create the element which is responsible for moving the shoulder like ""up and down"" and then we ""move"" using Translatef to the point in which we draw the next element, responsible for twisting the shoulder (we control the movement using Rotatef) and so on with the next joints (elbow, wrist). </p>

<p>The point is to create an arm that can make human-like movements. Our mechanism works, but now our tutor wants us to draw a line strip with the end of the arm. We put the code responsible for drawing and moving an arm between push and pop matrix, so it works like in real, I mean when we move the soulder, any other elements in arm also moves. </p>

<p>There is a lot of elements moving, rotating, we have a couple of rotate matrices that are attached to different elements which we can control and now we have no idea how to precisely find a new location of the end of an arm in space to be able to add a new point to a line strip. Anyone can help?</p>

<pre><code>    glGetFloatv(GL_MODELVIEW_MATRIX,mvm2);

     x=mvm2[12];
     y=mvm2[13];
     z=mvm2[14];

     glPointSize(5.0f);
     glColor3f(1.0f, 0.0f, 0.0f);

     glBegin(GL_POINTS);
     glVertex3f(x,y,z);
     glEnd();
</code></pre>

<p>When I checked using watch what are the x,y,z values, I got (0,-1.16-12e,17.222222), what can't be true, as my arm has length about 9.0 (on z-axis). I think only the last column of modelview matrix is important and I don't have to muliply it by local coordinates of the vertex, as the they are (0,0,0) since I finish my drawning here.</p>
",5/12/2013 12:49,16528168,1210,2,2,0,,,,,,16510129,"<blockquote>
  <p>we have no idea how to precisely find a new location of the end of an arm in space to be able to add a new point to a line strip.</p>
</blockquote>

<p>You do this by performing the matrix math and transformations yourself.</p>

<p>(from comment)</p>

<blockquote>
  <p>To do this we are suppose to multiply the matrices and get some information out of glGetFloatv</p>
</blockquote>

<p>Please don't do this. Especially not if you're supposed to build a pretransformed line strip geometry yourself. OpenGL is not a matrix math library and there's absolutely no benefit to use OpenGL's fixed function pipeline matrix functions. But it has a lot of drawbacks. Better use a real matrix math library.</p>

<p>Your robot arm technically consists of a number of connected segments where each segment is transformed by the composition of transformations of the segments upward in the transformation hierachy.</p>

<pre><code>M_i = M_{i-1} · (R_i · T_i)
</code></pre>

<p>where R_i and T_i are the respective rotation and translation of each segment. So for each segment you need the individual transform matrix to retrieve the point of the line segment. </p>

<p>Since you'll place each segment's origin at the tip of the previous segment you'd transform the homogenous point (0,0,0,1) with the segment's transformation matrix, which has the nice property of being just the 4th column of the transformation matrix.</p>

<p>This leaves you with the task of creating the transformation matrix chain. Doing this with OpenGL is tedious. Use a real math library for this. If your tutor insists on you using the OpenGL fixed function pipeline please ask him to show you the reference for the functions in the specicifications of a current OpenGL version (OpenGL-3 and later); he won't find them because all the matrix math functions have been removed entirely from modern OpenGL.</p>

<p>For math libraries I can recommend <a href=""http://glm.g-truc.net/"" rel=""nofollow"">GLM</a>, <a href=""http://eigen.tuxfamily.org/"" rel=""nofollow"">Eigen</a> (with the OpenGL extra module) and <a href=""https://github.com/datenwolf/linmath.h"" rel=""nofollow"">linmath.h</a> (self advertisement). With each of these libraries building transformation hierachies is simple, because you can create copies of each intermediary matrix without much effort.</p>
",524368,3,2,23698922,"We are using only basic OpenGL in our project, no shaders. To do this we are suppose to multiply the matrices and get some information out of glGetFloatv.",,
75,2142,27648625,Is it possible to measure depth of an image(JPEG/PNG),|image|image-processing|robotics|camera-calibration|point-cloud-library|,"<p>I am wondering there must be a way to get a depth of image. Certainly some portions can be extruded so that we get 3d version of 2d image. Any sources that will help in this out.</p>

<p>FYI: I would like to get point cloud from 2d image.
Thank you in advance..</p>
",12/25/2014 15:42,,877,2,3,0,0,4024298,,9/9/2014 19:44,2,27651998,"<p>Reconstruction from single 2D image is not possible. As mentioned by others, there is loads of literature to refer to. Multi-View Geometry by Hartely and Zisserman can be a good start. An example tutorial to start with: <a href=""http://vgl-ait.org/cvwiki/doku.php?id=matlab:tutorial:3d_reconstruction_with_calibrated_image_sequences"" rel=""nofollow"">Reconstruction</a>. you can also refer to computer vision toolbox from matlab/opencv</p>
",2577219,0,0,43715022,This is an extremely broad topic and out-of-scope for StackOverflow. This site is about answering specific questions to programming issues.,,
76,2075,26007202,Calculate new point offset based on angle of rotation?,|c#|.net|math|trigonometry|robotics|,"<p>I am working on an application for the past few weeks which involves some trigonometry and am currently stuck.  As shown in the diagram below, I have a circular item (green circle at position #1) which I know the center point (let's call that X1,Y1).  The circle has another point (orange circle) that is off-centered a bit - midway between two other marks (blue circles).  These marks can move around.  The coordinates of the orange point are calculated (let's call it X2, Y2) and the angle of the blue line is calculated (call it Angle) in relation to the horizontal of the circle.</p>

<p><img src=""https://i.stack.imgur.com/JEs4k.png"" alt=""Diagram""></p>

<p>I can calculate the difference between the center of the circle and the point by:</p>

<p>deltaX = X2-X1</p>

<p>deltaY = Y2-Y1</p>

<p>I need to move and rotate the green circle (either CW or CCW - whichever is shorter) from it's start location (position 1) over to position 2.  This means the angle could be negative or positive.  The blue line must end up vertical and the orange dot at the center of position 2 (red square).  I know the coordinates for the center of position 2 (let's call this point X3,Y3). Position #1 and position #2 are exactly 90 degrees from each other.</p>

<p>I thought I could use some trig identity formulas that calculate the rotation of a point, as such:</p>

<p>offsetX = deltaX * cos(90-Angle) - deltaY * sin(90-Angle)</p>

<p>offsetY = deltaX * sin(90-Angle) + deltaY * cos(90-Angle)</p>

<p>I was hoping these offsets would be what I need to adjust the circle to it's new center when it moves/rotates over to position 2.</p>

<p>X3 = X3 + offsetX</p>

<p>Y3 = Y3 + offsetY</p>

<p>However, when I try use this math, it's not placing the orange mark of the circle in the center of the square.  Not sure if my equations and calculations are correct based on the angle of rotation (positive or negative, CW or CCW) or if I'm using the angle correctly (where I subtract the known angle from 90 degrees).  How do I correctly calculate the final point/position?  Any help and examples would be greatly appreciated!</p>

<p>Thank you very much for your time!</p>
",9/24/2014 1:38,,11171,2,0,7,0,1218207,,2/18/2012 15:14,71,26007927,"<p>So you need to rotate your circle by <code>90 - Angle</code> and then move orange point to (X3, Y3)?<br />
First you need to find orange point coordinate after rotation:</p>

<pre><code>newX = X2 * cos(90 - Angle) - Y2 * sin(90 - Angle);
newY = X2 * sin(90 - Angle) + Y2 * cos(90 - Angle);
</code></pre>

<p><code>newX</code> and <code>newY</code> are orange point coordinates after rotation. To find move transformation simply substract:</p>

<pre><code>moveX = X3 - newX;
moveY = Y3 - newY;
</code></pre>

<p>Now if you rotate circle by <code>90 - Angle</code> and move it by (moveX, moveY) orange point will move to (X3, Y3). That is if you rotate circle around (0, 0) point. If you rotating around some (X, Y) point, you first need to substract X from X2, Y from Y2 and then add X to newX, Y to newY. That substraction 'moves' your rotation base point to (0, 0), so after rotation you need to move it back:</p>

<pre><code>newX = (X2 - X) * cos(90 - Angle) - (Y2 - Y) * sin(90 - Angle) + X;
newY = (X2 - X) * sin(90 - Angle) + (Y2 - Y) * cos(90 - Angle) + Y;
</code></pre>
",800613,1,3,,,,
78,1970,23731077,How to write a .sdf file in Gazebo 3D simulator for a robot arm with 3 revolute joints?,|simulation|robotics|,"<p>Something similar to the CRS arm robot with 3 revolute joints. I am having trouble writing the sdf(Simulator Description Format) for the arm in Gazebo.</p>
",5/19/2014 6:33,24164926,823,1,0,0,,3125386,,12/21/2013 13:57,27,24164926,"<p>In case you havent already read it, this is the sdf manual with description of each tag: <a href=""http://gazebosim.org/sdf/1.5.html"" rel=""nofollow"">http://gazebosim.org/sdf/1.5.html</a>.</p>

<p>If you don't succeed in writing the sdf from scratch, maybe you can find a description file written in other format (eg urdf) and convert it with some tools.</p>
",1248675,0,0,,,,
82,2257,30881607,Camera Calibration with OpenCV: Using the distortion and rotation-translation matrix,|c++|opencv|computer-vision|robotics|,"<p>I am reading the following documentation: <a href=""http://docs.opencv.org/doc/tutorials/calib3d/camera_calibration/camera_calibration.html"" rel=""nofollow noreferrer"">http://docs.opencv.org/doc/tutorials/calib3d/camera_calibration/camera_calibration.html</a></p>

<p>I have managed to successfully calibrate the camera obtaining the camera matrix and the distortion matrix. </p>

<p>I had two sub-questions:</p>

<p>1) How do I use the distortion matrix as I don't know 'r'? </p>

<p><img src=""https://i.stack.imgur.com/3NPRy.png"" alt=""Formula to get corrected coordinates""> </p>

<p>2) For all the views I have the rotation and translation vectors which transform the object points (given in the model coordinate space) to the image points (given in the world coordinate space). So a total of 6 coordinates per image(3 rotational, 3 translational). How do I make use of this information to obtain the rotational-translational matrix?</p>

<p>Any help would be appreciated. Thanks!</p>
",6/17/2015 3:02,30939087,920,1,0,1,0,3572768,,4/25/2014 11:40,75,30939087,"<p>Answers in order:</p>

<p>1) ""r"" is the pixel's radius with respect to the distortion center. That is: </p>

<pre><code>r = sqrt((x - x_c)^2 + (y - y_c)^2)
</code></pre>

<p>where (x_c, y_c) is the center of the nonlinear distortion (i.e. the point in the image that has zero nonlinear distortion. This is usually (and approximately) identified with the principal point, i.e. the intersection of the camera focal axis with the image plane. The coordinates of the principal point are in the 3rd column of the matrix of the camera intrinsic paramers.</p>

<p>2) Use <a href=""https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula"" rel=""nofollow"">Rodrigues's formula</a> to convert between rotation vectors and rotation matrices.</p>
",1435240,1,0,,,,
87,2214,29385348,2-D Multi-Robots Avoiding Obstacles to Goal,|java|a-star|robot|,"<p>I am thinking of making and a three robots connecting together and forming a triangle (not in a physical way) and try to avoiding static obstacles on their way to the goal location in Java applet. Besides, I am focusing on A* algorithm for the path finding and choosing the center of system as reference point for the heuristic value. But I have found out that even the A* has generated a path based on the center of the system, the multi-robots might still bump into the obstacles while traveling to the goal. Is there any good way to solve this?</p>
",4/1/2015 7:40,,339,1,5,2,,4737091,Los Angeles,4/1/2015 7:31,2,30063618,"<p>If you are using an occupancy map for navigation make each tile at least the size of the robot network. Then if a cell is ""not occupied"" you know with certainty your entire network can fit in it without bumping any obstacles. The robots themselves could use a subdivided map. So you can imagine that one cell in the occupancy map represents a 3x3 cell to the bots. So the bots could still have precision in their formation using their subdivided cell while the path planning algorithm can ensure no matter how the bots are arranged they will be safe</p>

<p>I have included a toy example. The dark lines represent the cell for the occupancy map, this is what your path planning algorithm sees. The bots (the small yellow squares) subdivide the occupancy map internally (the planning algorithm doesn't need to know this goes on) and can also change their own configuration within a single occupancy cell. </p>

<p>The dark pink cells show where a physical block may be, but we consider the entire region (large cell) occupied. Even if the network could potentially navigate past the physical obstruction. By simply saying the region near it is occupied we reduce the odds of getting in tricky situations.</p>

<p><img src=""https://i.stack.imgur.com/iMXxl.png"" alt=""enter image description here""></p>

<p>A* can only see the light pink and the light green cells. The robots can further subdivide the larger blocks internally</p>
",2705382,0,0,46969886,"@user902383 Sorry for making this question unclear. All the implementation is simulating in Java applet and robots will be moving around and it can spread inward and outward but connecting as a triangle shape all the time. If I consider it as a single point, how can I count the heuristic value? or is there other good algorithm to solve coordinated moving robots other than A*?",,
92,2252,30499463,Render arbitrary CSG solid given boolean function?,|c#|robotics|csg|,"<p>I'm looking to implement my own CSG classes for a Robotics project, and I'm thinking to implement each solid as a function that returns a boolean value, given a 3D point; this function will return true if the 3D point is contained within the solid. I figured by doing things this way, I can easily perform union, intersection and subtraction of solids.</p>

<p>This will be sufficient for performing collision detection .etc. by itself, but I'll want to actually render the solids, so my question is this; are there any methods of rendering a solid given its boolean function as described above? I'm more than happy to implement this myself as I want to ideally know exactly what's going on so I can streamline and add to the code as required. I'm also open to suggestions for representing solids in a different way if it will make things easier!</p>

<p>An interesting thing to note is that it would be useful if I could derive things such as the center-of-mass of a solid.</p>

<p>Thanks in advance!
Lee.</p>
",5/28/2015 7:06,30558601,1262,2,0,1,,3767259,,6/23/2014 11:41,47,30558601,"<p>Constructive Solid Geometry looks simple, but like most geometry problems, there enough subtleties that you really don't want to implement this for yourself unless it's the core of your work / research. </p>

<p>My suggestion is that you instead look for a high quality computational geometry library (ideally supported by good academic pedigree and published as open source). <a href=""http://www.cgal.org/"" rel=""nofollow"">CGAL</a> is a good option.</p>

<p>If speed isn't a huge priority (ie/ you can solve the problems offline in a separate tool), the problem is common enough problem that other people have done a lot of the hard work already. Check out <a href=""http://www.openscad.org/"" rel=""nofollow"">OpenSCAD</a> ""The Programmers Solid 3D CAD Modeller"", which uses CGAL to do the boolean operations.</p>

<p>If you need a compromise between the very low, and very high level interfaces, <a href=""https://github.com/SolidCode/SolidPython"" rel=""nofollow"">SolidCode</a> might not be a bad intermediate api that lets you call OpenSCAD like commands from Python code.</p>

<p>For rendering, I would advise that you consider treating everything as a (triangulated) surface mesh or volumetric mesh as appropriate, and using CSG or any other tool (eg/ blender) as just a mechanism for data entry. You may also find that collision libraries are readily available for triangulated meshes - and while you may gain some runtime performance improvement for using CSG, it's likely to take longer to develop the whole project.</p>
",2246,1,0,,,,
100,2461,35771915,Find closest value in a 2d grid c#,|c#|multidimensional-array|robotics|a-star|,"<p>I have created an c# console application that is used to simulate a robot application.
I have created a 2D grid for the robot to move around:</p>
<pre><code>List&lt;List&lt;int&gt; Map;
</code></pre>
<p>The map is a 25x25 grid (to start with) and filled with the following values:</p>
<pre><code>0 = Unexplored space,
1 = Explored space,
2 = Wall, 
3 = Obstacle, 
9 = Robot
</code></pre>
<p>The robot begins in position (12,12).
I would like to be able to search this grid for the nearest Unexplored space and return that position so that I can then feed that position and the robots position to an A* search algorithm for planning.</p>
<p>What would be the most efficient method to search through the Map for said value?</p>
",3/3/2016 12:13,35772727,1525,1,2,-1,,1379704,"Hull, UK",5/7/2012 12:14,192,35772727,"<p>wrote this on a notepad so i havent tested it, but you should get an idea.
Basically get all unexplored places and sort them by the distance from current and get the first value in the list.
CalculateDistance method should implement the formula Nikola.Lukovic mentioned.</p>

<pre><code>public KeyValuePair&lt;int, int&gt; GetClosestUnexploredPosition(List&lt;List&lt;int&gt;&gt; map, int currentX, int currentY)
{
    Dictionary&lt;KeyValuePair&lt;int, int&gt;, double&gt; unexploredPlaces = new Dictionary&lt;KeyValuePair&lt;int, int&gt;, double&gt;();

    foreach (List&lt;int&gt; valueList in map)
    {
        foreach (int value in valueList)
        {
            if (value == 0)
            {
                int x = map.IndexOf(valueList);
                int y = valueList.IndexOf(value));
                if (x != currentX &amp;&amp; y != currentY)
                {
                    unexploredPlaces.Add(new KeyValuePair(x, y), CalculateDistance(currentX, currentY, x, y));
                }
            }
        }
    }

    return unexploredPlaces.OrderBy(x =&gt; x.Value).FirstOrDefault();
}
</code></pre>
",6013031,2,0,59215948,"The problem is I do not know the location of the other position. I need to search the grid for the nearest position first. 
I could use this distance formula on every '0' in the grid but I do not think that would be the most efficient way to do it.",,
108,2527,37303953,Robotics Maze Representation in C,|c|robotics|,"<p>So I'd like to represent a rectangular maze of say dimensions 5x4 (rows x columns) using a 2D array in C language. However I am having trouble specifying what actually needs to be put into the 2D array.</p>

<pre><code>int a[5][4] = {
    {},
    {},
    {},
    {},
    {}, 
};
</code></pre>

<p>Here is the skeleton of the 2D array, in each row there will be 4 values, I assume that each of these values is a single integer that tells us the properties of a cell in the maze. My problem is, is that really enough? How does a single value tell a robot weather there are 3 walls, 2 walls etc</p>

<p>Someone please enlighten me D:</p>

<p><a href=""https://i.stack.imgur.com/EojXM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EojXM.png"" alt=""Maze""></a></p>
",5/18/2016 15:25,,146,2,7,2,,6262982,,4/27/2016 17:54,10,37304048,"<p>use specific bits for specific properties of the room</p>

<pre><code>#define ROOM_WALL_ABOVE (1 &lt;&lt; 0)
#define ROOM_WALL_LEFT  (1 &lt;&lt; 1)
#define ROOM_WALL_BELOW (1 &lt;&lt; 2)
#define ROOM_WALL_RIGHT (1 &lt;&lt; 3)
#define ROOM_DOOR       (1 &lt;&lt; 4)

int a[5][4] = {0};
a[0][0] = ROOM_WALL_ABOVE | ROOM_WALL_LEFT;

if (a[x][y] &amp; ROOM_WALL_RIGHT) printf(""Cannot walk right.\n"");
</code></pre>
",25324,7,5,62130226,"@EugeneSh. For sure, as pmg already answered.",,
110,2546,37739705,BAM(Bidirectional Associative Memory),|neural-network|artificial-intelligence|robotics|recurrent-neural-network|biological-neural-network|,"<p>Suppose [1 0 0 1 0 1] &lt;--> [0 0 0 1], this is a association then in implementing BAM, why we convert 0 to -1 and then calculate weight matrix.</p>
",6/10/2016 4:05,,781,2,0,0,,4520795,"Bengaluru, India",2/2/2015 16:33,61,39056391,"<p>The fundamental reason why 0 are unsuitable for BAM storage is that 0's in binary patterns are ignored when added, but -1's in bipolar patterns are not: 1+0=1 but 1+(-1)=0. If the numbers are matrix entries that represent synaptic strengths, then multiplying and ass in binary quantities can only produce excitatory connections. </p>

<p>Meanwhile multiplying and add in bipolar quantities produces excitatory and inhibitory connections. The connections strengths represent the frequency of excitatory and inhibitory connections int the individual correlation matrices.</p>

<p>Refer:</p>

<ul>
<li><a href=""http://sipi.usc.edu/~kosko/BAM.pdf"" rel=""nofollow"">http://sipi.usc.edu/~kosko/BAM.pdf</a> ""Bidirectional Associative Memory, Bart Kosko""</li>
</ul>
",1600172,1,0,,,,
117,2507,36925245,Exporting Nao robot Simulator in Unity3D,|unity-game-engine|robot|,"<p>I'm using the Coregraphe 2.1.4 software as simulator, I'd like to export this simulator into Unity 3D in order to make a videogame where people can interact and play with Nao.</p>

<p>I've successfully exported the model of the Robot in Unity but i can not animate it.</p>

<p>Is it possible to export the Nao robot simulator in Unity3D? Or, are there other way to run the simulator in Unity3D?</p>

<p>Thank :)</p>
",4/28/2016 20:53,36925445,624,1,1,1,,6268708,,4/28/2016 20:39,14,36925445,"<blockquote>
  <p>I've successfully exported the model of the Robot in Unity but i can
  not animate it.</p>
</blockquote>

<p>Export it to a 3D software such as Maya or Blender, then animate it there and import the 3D model and the animation back to Unity. Blender is free. You just need to learn animation. </p>

<blockquote>
  <p>Is it possible to export the Nao robot simulator in Unity3D?</p>
</blockquote>

<p>No. You can't export a software to Unity. If the company that made it produced a DLL plugin for Unity then you can. They don't have plugin for Unity, so you can't.</p>

<blockquote>
  <p>are there other way to run the simulator in Unity3D?</p>
</blockquote>

<p>No. All the answer to your questions are No, No and No. Although, you can animate your 3D model in a 3D software then import into Unity.</p>
",3785314,0,13,61415836,"To be honest it sounds like you've stolen the ""Nao"" 3D model from their software! That's just a commercial product isn't it? It's not in the slightest available for open source, or anything like that - unless I'm mistaken.",,
123,2968,47727674,How to connect keyboard arrows with robotino sim and view?,|c#|mqtt|robotics|,"<p>So I have a project to do, which is to connect a Robotino view with a Robotino Sim, and also connect to a C# client program to <strong>move the Robotino in the View with the arrows on the keyboard</strong>. </p>

<p>The C# client is supposed to send the data about pressing the arrow buttons 4 times per second through cloud MQTT.</p>

<p>So far I connected the View with the Sim, added some basic navigator ( on which you click and the robot moves), but I have no idea how to do the rest.</p>

<p>I also found this cloud MQTT .Net thingy, but it just doesn't speak to me.
<a href=""https://www.cloudmqtt.com/docs-dotnet.html"" rel=""nofollow noreferrer"">https://www.cloudmqtt.com/docs-dotnet.html</a></p>

<p>I would really appreciate if someone could help me with this issue, since I found nothing really useful, only this video (basically that is what I want to do ), which has no description but the project is working for him.
<a href=""https://www.youtube.com/watch?v=gmKyFNLWv_A&amp;t=11s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=gmKyFNLWv_A&amp;t=11s</a></p>
",12/9/2017 10:40,,116,0,2,0,,8934727,"Dunaújváros, Hungary",11/13/2017 19:06,6,,,,,,82437476,"Stack Overflow doesn't work this way. You have to try yourself, then when you get stuck, you show us what you have done, explain how it doesn't work and somebody will help you fix it.",,
129,2900,46398319,Simultaneous Localization and Mapping(SLAM) simulation,|artificial-intelligence|robotics|,"<p>I am planning to do a project on Simultaneous Localization and Mapping(SLAM) using simulation since I am completely new to robotics I have no idea where to start and how to proceed.Please do help me to begin my work and I need some references for the tutorial. </p>
",9/25/2017 5:45,46425576,383,1,0,0,,6813937,,9/9/2016 14:16,15,46425576,"<p>I would suggest you to use <a href=""http://www.ros.org"" rel=""nofollow noreferrer"">Robot Operating System (ROS)</a>. It has good development tools and the algorithms for SLAM and planning already available. You can start with <a href=""http://wiki.ros.org/kinetic/Installation"" rel=""nofollow noreferrer"">installing ROS</a> followed by <a href=""http://wiki.ros.org/ROS/Tutorials"" rel=""nofollow noreferrer"">ROS tutorials</a>.</p>
",1595504,2,0,,,,
141,2802,44620509,Robot Motion - Dynamic Programming,|c++|dynamic-programming|robotics|,"<p>Given a 1D world of infinite length (x),
and available moves (y) of, for example [1, 2, 3, -1, -2, -3],
and a destination (d) (ie 15), write a function that returns
the smallest number of moves (result) needed to reach d.</p>

<p>For example if d = 15, result = 5
since the most optimal move is 3, and it can be done 5 times.</p>

<p>This problem is very similar to this: <a href=""https://www.youtube.com/watch?v=Y0ZqKpToTic"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=Y0ZqKpToTic</a> 
except that negative values are allowed.</p>

<p>I have the code below that only works for positive number. Any ideas to make it work for mixed positive and negative values?</p>

<pre><code>class Solution {
public:
    int Robotmotion(vector&lt;int&gt; &amp;moves, int &amp;d) {
        if (d == 0) return 0;
        if (d &lt; 0) {
            d = -d;
            for (auto &amp;move : moves) move *= -1;
        }

        sort(moves.begin(), moves.end());

        vector&lt;int&gt; dp(d + 1, d + 1);

        dp[0] = 0;

        for (int i = 1; i &lt;= d; i++) {
            for (int j = 0; j &lt; moves.size(); j++) {
                if (moves[j] &lt;= i) {
                    dp[i] = min(dp[i], dp[i - moves[j]] + 1);
                }
            }
        }

        return dp[d] == d + 1 ? -1 : dp[d];
    }
};



int main() {

    Solution s;
    vector&lt;int&gt; moves = {1,2,3};
    int d = 15;
    int min_steps = s.Robotmotion(moves, d);
    cout &lt;&lt; ""Mim steps:"" &lt;&lt; endl &lt;&lt; min_steps &lt;&lt; endl;
    return 0;
}
</code></pre>
",6/18/2017 23:35,,277,1,6,0,,1527457,,7/15/2012 20:59,59,44627727,"<p>I don't think dynamic programming can solve the problem. Instead, you should view the number as vertices in a graph and use BFS to solve the problem. You can even use a bidirectional BFS to speed up the process.</p>
",4017815,0,0,76228645,Is it legal to overshoot and backtrack? As in if you can move 10 and then -1 instead of having to got 3 three times?,,
143,3116,50060927,"how to use ""robotlocomotion drake"" codes in my project, for example ""qp_inverse_dynamics""",|installation|open-source|robotics|drake|,"<p>i want to implement drake in my project, but i found it almost impossible. 
Although there are notebooks and course explain robotics theory and how drake works, 
<a href=""http://underactuated.csail.mit.edu/underactuated.html?chapter=drake"" rel=""nofollow noreferrer"">http://underactuated.csail.mit.edu/underactuated.html?chapter=drake</a>
<a href=""https://www.edx.org/course/underactuated-robotics-mitx-6-832x-0"" rel=""nofollow noreferrer"">https://www.edx.org/course/underactuated-robotics-mitx-6-832x-0</a></p>

<p>But how could i use the codes in drake in my project. For example, i want to simulate a 6-Dof arm using V-rep and ROS, and i want to inplement force control to the arm using ""qp_inverse_dynamics"" in drake, do i need to include all files that ""qp_inverse_dynamics"" used, and construct the build system? There are tons of files.</p>

<p>I have made a quadruped robot using position control and PID controller, and have a little bit experience of using open source convex quadratic programs solver(osqp)。
And now, after build and tested drake using bazel, what coule I do to use codes in drake in my project? Or should I just write my own codes using the method in Underactuated Robotics notebook?</p>

<p>thanks a lot.</p>
",4/27/2018 10:56,50075307,398,1,0,1,,9708755,,4/27/2018 6:35,34,50075307,"<p>This repository is our working example of how to use drake in your own project:<br>
<a href=""https://github.com/RobotLocomotion/drake-shambhala"" rel=""nofollow noreferrer"">https://github.com/RobotLocomotion/drake-shambhala</a></p>

<p>We do support OSQP as one of many solver backends.  There is a chance that you will find that you want some feature in drake that is not yet exposed in the binary installation, in which case please make a request on github.  But I suspect it should work well for you.</p>

<p>N.B.  The lectures you've pointed to on edX are a few years old now.  The current version of the course is running right now, with streamed/recorded lectures available at <a href=""http://underactuated.csail.mit.edu/Spring2018/index.html#textbook/assignments/videos"" rel=""nofollow noreferrer"">http://underactuated.csail.mit.edu/Spring2018/index.html#textbook/assignments/videos</a></p>
",9510020,2,1,,,,
146,3001,48302877,My inverse compositional homograhy image align cannot converge?,|c++|computer-vision|robotics|,"<p>I implement the algorithm based on <a href=""https://github.com/albertoCrive/homographyTrackingDemo"" rel=""nofollow noreferrer"">homographyTrackingDemo</a> and <a href=""http://www.ncorr.com/download/publications/bakerunify.pdf"" rel=""nofollow noreferrer"">LK-20-years</a>. It's adaption of inverse compositional LK algorithm for estimating affine transformation. There are several steps I have tried: </p>

<ol>
<li><p>Derive the derivative of template image;</p></li>
<li><p>Derive the derivative of homography transformation on template control points at identity transformation</p></li>
<li>Compute the Steepest Gradient Matrix and Hessian matrix</li>
</ol>

<p>These steps are precomputed before stepping into iteration and we iterate:</p>

<ol>
<li>Warp target image by homography</li>
<li>Compute the difference between warped target image and template image</li>
<li>Solve the linear equations and then update homography parameters</li>
</ol>

<p>I hope some guy can give me a hint about whether my understanding about this algorithm is wrong. The c++ code is as follows:</p>

<pre><code>AlignmentResults ICA::GaussNewtonMinimization(const StructOfArray2di &amp; pixelsOnTemplate, const vector&lt;Mat&gt; &amp; images, const vector&lt;Mat&gt; &amp; templates, const OptimizationParameters optParam, vector&lt;float&gt; &amp; parameters)
{
    AlignmentResults alignmentResults;
    alignmentResults.nIter = 0;
    alignmentResults.exitFlag = 1e6;

    //parameters must contain the initial guess. It is updated during optimization
    uint nChannels(images.size());
    vector&lt;vector&lt;float&gt; &gt; templatePixelIntensities(nChannels,vector&lt;float&gt;(pixelsOnTemplate.size()));//, templatePixelDx(nChannels,vector&lt;float&gt;(pixelsOnTemplate.size())), templatePixelDy(nChannels,vector&lt;float&gt;(pixelsOnTemplate.size()));
    vector&lt;Mat&gt; imageDx(nChannels), imageDy(nChannels);
    uint nParam = parameters.size();

    //vector&lt;Eigen::Matrix&lt;float, 2, N_PARAM&gt;, Eigen::aligned_allocator&lt;Eigen::Matrix&lt;float, 2, N_PARAM&gt; &gt; &gt; warpJacobians(pixelsOnTemplate.size());

    Eigen::MatrixXf sdImages(pixelsOnTemplate.size(), nParam);
    vector&lt;float&gt;  errorImage(pixelsOnTemplate.size(), 0.0);

    StructOfArray2di warpedPixels;
    Eigen::Matrix&lt;float, N_PARAM, N_PARAM&gt; hessian;
    Eigen::Matrix&lt;float, N_PARAM, N_PARAM&gt; hessianInv;
    hessian.setZero();


    std::vector&lt;Eigen::Matrix&lt;float, N_PARAM, N_PARAM&gt;, Eigen::aligned_allocator&lt;Eigen::Matrix&lt;float, N_PARAM, N_PARAM&gt;&gt;&gt; hessians(nChannels);
    std::vector&lt;Eigen::MatrixXf&gt; JacosT(nChannels);
    Eigen::Matrix&lt;float, N_PARAM, 1&gt; rhs;
    Eigen::Matrix&lt;float, N_PARAM, 1&gt; deltaParam;

    std::vector&lt;float&gt; zeroPara(8, 0.);
#pragma omp parallel for
    for(int iChannel = 0; iChannel&lt;nChannels; ++iChannel)
    {
        ComputeImageDerivatives(templates[iChannel], imageDx[iChannel], imageDy[iChannel]);
        hessians[iChannel].setZero();
        JacosT[iChannel] = Eigen::MatrixXf(N_PARAM, pixelsOnTemplate.size());

        for(int iPoint = 0; iPoint &lt; pixelsOnTemplate.size(); ++iPoint)
        {
            int pos = templates[iChannel].cols * pixelsOnTemplate.y[iPoint] + pixelsOnTemplate.x[iPoint];
            templatePixelIntensities[iChannel][iPoint] = ((float*)templates[iChannel].data)[pos];

            float Dx = ((float*)imageDx[iChannel].data)[pos];
            float Dy = ((float*)imageDy[iChannel].data)[pos];
            //printf(""%f %f\n"", Dx, Dy);
            Eigen::Matrix&lt;float, 2, N_PARAM&gt; warpJ;
            Homography::ComputeWarpJacobian(pixelsOnTemplate.x[iPoint], pixelsOnTemplate.y[iPoint], zeroPara, warpJ);
            //std::cout &lt;&lt; warpJ &lt;&lt; std::endl;
            Eigen::Matrix&lt;float, 1, N_PARAM&gt; J2H;
            Eigen::Matrix&lt;float, 1, 2&gt; ID;
            ID[0] = Dx;
            ID[1] = Dy;
            J2H = ID*warpJ;

            JacosT[iChannel].col(iPoint) = J2H.transpose();
            hessians[iChannel] += JacosT[iChannel].col(iPoint) * J2H;
        }
        hessian += hessians[iChannel];
    }

    while (alignmentResults.exitFlag == 1e6)
    {
        rhs.setZero();

        ComputeWarpedPixels(pixelsOnTemplate, parameters, warpedPixels);

#pragma omp parallel for
        for(int iChannel = 0; iChannel&lt;images.size(); ++iChannel)
        {
            ComputeResiduals(images[iChannel], templatePixelIntensities[iChannel], warpedPixels, errorImage);
            for (int i = 0; i&lt;nParam; ++i)
            {
                for(uint iPoint(0); iPoint&lt;pixelsOnTemplate.size(); ++iPoint)
                {
                    float val = (errorImage[iPoint] == std::numeric_limits&lt;float&gt;::infinity() ? 0: errorImage[iPoint]); 
                    rhs(i,0) -= (JacosT[iChannel])(i, iPoint) * val;
                }
            }
        }


        deltaParam = hessian.fullPivLu().solve(rhs);

        std::vector&lt;float&gt; vDelta(8, 0.), invDelta(8, 0.);
        for(size_t i = 0; i&lt;N_PARAM; ++i)
        {
            vDelta[i] = deltaParamd(i,0);
        }

        Homography::InverseWarpParameters(vDelta, invDelta);
        parameters = Homography::ParametersUpdateCompositional(parameters, invDelta);


        alignmentResults.poseIntermediateGuess.push_back(parameters);
        alignmentResults.residualNorm.push_back(ComputeResidualNorm(errorImage));
        if(alignmentResults.nIter &gt; 0)
            alignmentResults.exitFlag = CheckConvergenceOptimization(deltaParam.norm(), alignmentResults.nIter, abs(alignmentResults.residualNorm[alignmentResults.nIter] - alignmentResults.residualNorm[alignmentResults.nIter-1]), optParam);
        alignmentResults.nIter++;

    return alignmentResults;
}
</code></pre>
",1/17/2018 13:57,,150,0,1,1,,6409227,,6/1/2016 11:01,3,,,,,,83599710,"You should post your images, and visually show the initial solution and the final solution. This is an iterative optimization that can easily get stuck in a local minimum if your initial solution is far from the true solution.",,
151,3135,50750013,Jackal simulation in ROS kinetic - getting the error: bash: cd: jackal_ws: No such file or directory,|bash|simulation|ros|robotics|,"<p>I'm trying to simulate the Jackal via ROS Kinetic on Gazebo following this tutorial, <a href=""https://gist.github.com/vfdev-5/57a0171d8f5697831dc8d374839bca12"" rel=""nofollow noreferrer"">https://gist.github.com/vfdev-5/57a0171d8f5697831dc8d374839bca12</a></p>

<p>I have done the following steps:</p>

<pre><code>sudo apt-get install ros-kinetic-robot-localization ros-kinetic-controller-manager ros-kinetic-joint-state-controller ros-kinetic-diff-drive-controller ros-kinetic-gazebo-ros ros-kinetic-gazebo-ros-control ros-kinetic-gazebo-plugins             ros-kinetic-lms1xx ros-kinetic-pointgrey-camera-description ros-kinetic-roslint ros-kinetic-amcl ros-kinetic-gmapping      ros-kinetic-map-server ros-kinetic-move-base ros-kinetic-urdf ros-kinetic-xacro ros-kinetic-message-runtime ros-kinetic-topic-tools ros-kinetic-teleop-twist-joy    
</code></pre>

<p>and</p>

<pre><code>mkdir -p jackal_ws/src; cd jackal_ws/src; catkin_init_workspace
git clone https://github.com/jackal/jackal.git
git clone https://github.com/jackal/jackal_simulator.git
git clone https://github.com/jackal/jackal_desktop.git
git clone https://github.com/ros-visualization/interactive_marker_twist_server.git
</code></pre>

<p>After the above step, I got: </p>

<pre><code>bash: cd: jackal_ws: No such file or directory
</code></pre>

<p>Then, I did the next step below:</p>

<pre><code>cd jackal_ws; catkin_make; source devel/setup.bash
</code></pre>

<p>After the above step, I got: </p>

<pre><code>bash: cd: jackal_ws: No such file or directory  
Base path: /home/USER/jackal_ws/src  
The specified source space ""/home/USER/jackal_ws/src/src"" does not exist   
bash: devel/setup.bash: No such file or directory
</code></pre>

<p>What have I done wrong?</p>

<hr>

<p>I redid the steps separately, and here is what I get in the terminal: </p>

<p>user@user:~$ sudo apt-get install ros-kinetic-robot-localization ros-kinetic-controller-manager ros-kinetic-joint-state-controller ros-kinetic-diff-drive-controller ros-kinetic-gazebo-ros ros-kinetic-gazebo-ros-control ros-kinetic-gazebo-plugins             ros-kinetic-lms1xx ros-kinetic-pointgrey-camera-description ros-kinetic-roslint ros-kinetic-amcl ros-kinetic-gmapping      ros-kinetic-map-server ros-kinetic-move-base ros-kinetic-urdf ros-kinetic-xacro ros-kinetic-message-runtime ros-kinetic-topic-tools ros-kinetic-teleop-twist-joy</p>

<p>[sudo] password for user: </p>

<p>E: Could not get lock /var/lib/dpkg/lock - open (11: Resource temporarily unavailable)</p>

<p>E: Unable to lock the administration directory (/var/lib/dpkg/), is another process using it?</p>

<p>user@user:~$ sudo rm /var/lib/dpkg/lock</p>

<p>user@user:~$ sudo dpkg --configure -a</p>

<p>dpkg: error: dpkg status database is locked by another process</p>

<p>user@user:~$ sudo apt-get install ros-kinetic-robot-localization ros-kinetic-controller-manager ros-kinetic-joint-state-controller ros-kinetic-diff-drive-controller ros-kinetic-gazebo-ros ros-kinetic-gazebo-ros-control ros-kinetic-gazebo-plugins             ros-kinetic-lms1xx ros-kinetic-pointgrey-camera-description ros-kinetic-roslint ros-kinetic-amcl ros-kinetic-gmapping      ros-kinetic-map-server ros-kinetic-move-base ros-kinetic-urdf ros-kinetic-xacro ros-kinetic-message-runtime ros-kinetic-topic-tools ros-kinetic-teleop-twist-joy</p>

<p>E: Could not get lock /var/lib/dpkg/lock - open (11: Resource temporarily unavailable)</p>

<p>E: Unable to lock the administration directory (/var/lib/dpkg/), is another process using it?</p>

<p>user@user:~$ mkdir -p jackal_ws/src</p>

<p>user@user:~$ cd jackal_ws/src</p>

<p>user@user:~/jackal_ws/src$ catkin_init_workspace</p>

<p>File ""/home/user/jackal_ws/src/CMakeLists.txt"" already </p>

<p>existsuser@user:~/jackal_ws/src$ git clone </p>

<p><a href=""https://github.com/jackal/jackal"" rel=""nofollow noreferrer"">https://github.com/jackal/jackal</a>. </p>

<p>fatal: destination path 'jackal' already exists and is not an empty directory.</p>

<p>user@user:~/jackal_ws/src$ git clone </p>

<p><a href=""https://github.com/jackal/jackal_simulator.git"" rel=""nofollow noreferrer"">https://github.com/jackal/jackal_simulator.git</a></p>

<p>fatal: destination path 'jackal_simulator' already exists and is not an empty directory.</p>

<p>user@user:~/jackal_ws/src$ git clone </p>

<p><a href=""https://github.com/jackal/jackal_desktop.git"" rel=""nofollow noreferrer"">https://github.com/jackal/jackal_desktop.git</a></p>

<p>fatal: destination path 'jackal_desktop' already exists and is not an empty directory.</p>

<p>user@user:~/jackal_ws/src$ git clone <a href=""https://github.com/ros-visualization/interactive_marker_twist_server.git"" rel=""nofollow noreferrer"">https://github.com/ros-visualization/interactive_marker_twist_server.git</a></p>

<p>fatal: destination path 'interactive_marker_twist_server' already exists and is not an empty directory.</p>

<p>user@user:~/jackal_ws/src$ cd jackal_ws</p>

<p>bash: cd: jackal_ws: No such file or directory</p>
",6/7/2018 20:54,,467,0,7,0,,9841724,,5/24/2018 14:49,8,,,,,,88533311,"@thatotherguy, I redid the steps, but after the line (cd jackal_ws), I get the error: bash: cd: jackal_ws: No such file or directory.",,
160,3072,49674179,Understanding Inverse Kinematics pybullet,|robotics|bulletphysics|inverse-kinematics|,"<p>I'm trying to do <strong>cartesian control</strong> with a simulated PR2 robot in <strong>pybullet</strong>.
In pybullet, the function <em>calculateInverseKinematics(...)</em> optionally takes joint lower limits, upper limits, joint ranges and rest poses in order to do null space control. </p>

<p>First of all, what practical benefit do you get using null space control instead of ""regular"" inverse kinematics? </p>

<p>Secondly, why do you need to specify joint ranges, isn't that fully determined by the lower and upper limits? What is the range of a continuous joint? </p>

<p>What exactly are rest poses? Is it just the initial pose before the robot starts to do a task?</p>
",4/5/2018 13:55,,6633,1,0,6,0,3925668,,8/9/2014 18:44,8,49769256,"<p>There are often many solutions to the Inverse Kinematics problem. Using the null space allows you to influence the IK solution, for example closer to a rest pose.</p>

<p>By default, the PyBullet IK doesn't use the limits from the URDF file, hence you can explicitly specify the desired ranges for the IK solution. A continuous joint has the full 360 degree range.</p>

<p>Check the <a href=""https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA/edit#"" rel=""nofollow noreferrer"">PyBullet</a> user manual and there are several examples how to use inverse kinematics with PyBullet:</p>

<p><a href=""https://github.com/bulletphysics/bullet3/tree/master/examples/pybullet/examples"" rel=""nofollow noreferrer"">https://github.com/bulletphysics/bullet3/tree/master/examples/pybullet/examples</a>
(just use git checkout <a href=""https://github.com/bulletphysics/bullet3"" rel=""nofollow noreferrer"">https://github.com/bulletphysics/bullet3</a> and go to examples/pybullet/examples)</p>

<p>There is also an additional PyBullet IK example for the Sawyer robot here:
<a href=""https://github.com/erwincoumans/pybullet_robots"" rel=""nofollow noreferrer"">https://github.com/erwincoumans/pybullet_robots</a></p>
",295157,1,1,,,,
163,3108,49880973,Room coverage in Answer Set Programming,|robotics|answer-set-programming|clingo|,"<p>I'm currently developing an Answer Set Programming problem, consisting in a robot that is needed to cover a room avoiding obstacles and reach a Goal point when all the room is covered.
My idea was to transform the room map into asp predicates,in the form of room/3, being the parameters:</p>

<ul>
<li>X:x coord</li>
<li>Y:y coord</li>
<li>V:Value of the point in the room, being 0(initial point),1(point to cover),2(Obstacle),3(Goal point)</li>
</ul>

<p>One of the criteria that the program must meet is to cover every point with a value of 1,which can be achieved with a constraint, but I do not know how to model the robot movement. My idea was to use a predicate of the form move/1,with up,down,left or right.</p>

<p>Can anybody help me guiding me in how to model this problem?</p>

<pre><code>    void map_to_asp(std::ofstream&amp; file,std::vector&lt;std::vector&lt;char&gt;&gt;&amp; room)
{
  std::cout &lt;&lt; room.size() &lt;&lt; "","" &lt;&lt; room[0].size() &lt;&lt; std::endl;
  for(int i = 0; i &lt; room.size(); i++)
  {
    for(int j = 0;j &lt; room[0].size(); j++)
    {
      switch(room[i][j])
      {
        case '@':
        file &lt;&lt; ""initial("" &lt;&lt; i+1 &lt;&lt; "","" &lt;&lt; j+1 &lt;&lt; "").\n"";
        break;
        case '.':
        file &lt;&lt; ""toClean("" &lt;&lt; i+1 &lt;&lt; "","" &lt;&lt; j+1 &lt;&lt; "").\n"";
        break;
        case '#':
        file &lt;&lt; ""obstacle("" &lt;&lt; i+1 &lt;&lt; "","" &lt;&lt; j+1 &lt;&lt; "").\n"";
        break;
        case 'X':
        file &lt;&lt; ""goal("" &lt;&lt; i+1 &lt;&lt; "","" &lt;&lt; j+1 &lt;&lt; "").\n"";
        break;
      }
    }
  }
}
</code></pre>

<p>Thank you in advance.</p>
",4/17/2018 14:37,,227,1,2,1,,9659283,,4/17/2018 14:25,2,50087368,"<p>If your goal is to have a model for each possible path, a simple way to go is to make an iterative progression in the graph.</p>

<p>We need first to define all positions we can move in (we are actually building a graph before solving any problem):</p>

<pre><code>position(X,Y,S):- room(X,Y,S) ; not S=2.
</code></pre>

<p>Now we decide where we can go from any position (edges of the graph):</p>

<pre><code>edge((X,Y),(I,J)):- position(X,Y,_) ; position(I,J,_) ; |X-I|=0..1 ; |Y-J|=0..1 ; |X-I|+|Y-J|=1..2 .
</code></pre>

<p>Note that we consider that the graph is undirected (not necessarily true, if there is a slide in your room for instance).
Let's define some constants:</p>

<pre><code>#const start_pos=(1,1).
#const goal=(5,5).
#const path_maxlen=100.
</code></pre>

<p>We obviously start at the starting point:</p>

<pre><code>path(1,start_pos).
</code></pre>

<p>And now, we recursively indicate that there is a next way to go, with a limit to avoid too useless solutions.</p>

<pre><code>0{path(N+1,E): path(N,S), edge(S,E), S!=goal}1:- path(N,_) ; N&lt;path_maxlen.
</code></pre>

<p>We have to avoid all useless paths.</p>

<pre><code>% a path that do not join the end is illegal.
:- path(N,E) ; not path(N+1,_) ; not E=goal.

% a path must go by all milestone (example of milestone: milestone(2,14)).
:- not path(_,(X,Y)): milestone(X,Y).
</code></pre>

<p>We want the shortest path:</p>

<pre><code>last_step(N):- path(N,_) ; not path(N+1,_).
#minimize{N: last_step(N)}.
</code></pre>

<p>The full code is available <a href=""https://github.com/Aluriak/learning-ASP/blob/master/path-search.lp"" rel=""nofollow noreferrer"">here</a>.</p>

<hr>

<p>As a side-notes:</p>

<ul>
<li>since we don't use them, you could (should) take rid of all room/3 that describe an obstacle.</li>
<li>you could also make you goal point artificial (out of the room, but the real goad is linked to it) in order to allow your path to pass by the real goal, without stopping. Using that, you can achieve support for multiple goal.</li>
</ul>
",3077939,0,0,86778187,Do you have an example of what you have done so far?,,
165,3010,48598961,Explaining environments in Roboschool Half-Cheetah,|environment|robotics|reinforcement-learning|openai-gym|,"<p>I have some questions regarding the roboschool Half-Cheetah.</p>

<ol>
<li><p>I see that the observation space for Half-Cheetah is 26. Can anyone tell me what is each value for?- I only counted 18. (also, some of the values seem to remain 0 for all timesteps)</p></li>
<li><p>In the half_cheetah.xml under roboschool/mujoco_assets, there is the following comment:</p>

<p>Cheetah Model</p></li>
</ol>

<p>The state space is populated with joints in the order that they are
defined in this file. The actuators also operate on joints.</p>

<pre><code>State-Space (name/joint/parameter):
    - rootx     slider      position (m)
    - rootz     slider      position (m)
    - rooty     hinge       angle (rad)
    - bthigh    hinge       angle (rad)
    - bshin     hinge       angle (rad)
    - bfoot     hinge       angle (rad)
    - fthigh    hinge       angle (rad)
    - fshin     hinge       angle (rad)
    - ffoot     hinge       angle (rad)
    - rootx     slider      velocity (m/s)
    - rootz     slider      velocity (m/s)
    - rooty     hinge       angular velocity (rad/s)
    - bthigh    hinge       angular velocity (rad/s)
    - bshin     hinge       angular velocity (rad/s)
    - bfoot     hinge       angular velocity (rad/s)
    - fthigh    hinge       angular velocity (rad/s)
    - fshin     hinge       angular velocity (rad/s)
    - ffoot     hinge       angular velocity (rad/s)


Actuators (name/actuator/parameter):
    - bthigh    hinge       torque (N m)
    - bshin     hinge       torque (N m)
    - bfoot     hinge       torque (N m)
    - fthigh    hinge       torque (N m)
    - fshin     hinge       torque (N m)
    - ffoot     hinge       torque (N m)
</code></pre>

<p>Could you please confirm to me if the order presented here is the same with the order they appear in the observation matrix? If so, should I take the values that are always 0 into account?</p>

<p>Thank you.</p>
",2/3/2018 15:27,,1239,0,2,4,0,7151522,,11/13/2016 1:01,3,,,,,,92680869,"Looks like you got some answers on Github, maybe you can create an answer with your findings so others know: https://github.com/openai/roboschool/issues/142",,
167,3611,59101547,How to pass a function as an argument to other function? My code is given below,|python|function|class|object|robotics|,"<p>this is the code for RRT algorithm </p>

<p>import sys
import pygame
import random, math
from math import sqrt, atan2, cos, sin
from pygame.locals import *</p>

<p>class RRT(object):</p>

<pre><code>x = 0
y = 0
X_dimension = 0
Y_dimension = 0
Window_size = 0
EPS         = 0
Max_nodes   = 0 
nodes       = list()
K_ESCAPE    = True
KEYUP       = True
QUIT        = True

def __init__(self,x,y):
    self.x = x
    self.y = y

#parameters
    self.X_dimension = 1280                                #length of the window
    self.Y_dimension = 1280                                  #breadth of the window
    self.Window_size = [self.X_dimension, self.Y_dimension]  #Window size
    self.EPS         =  7000                                #EPSILON or Incremental Distance 
    self.Max_nodes   = 100                                  #maximum number of nodes
    self.nodes       = list()
    self.QUIT        = QUIT
    self.KEYUP       = KEYUP
    self.K_ESCAPE    = K_ESCAPE


#function for calculating euclidean distance
def Calculate_Distance(self,x,y):

    x = [10,20]
    y = [15,30]  
    return sqrt((x[0]-y[0])*(x[0]-y[0])+(x[1]-y[1])*(x[1]-y[1]))

    pass 

def Initiate_Sampling(self,x,y):

    self.EPS = 7000 

    if Calculate_Distance(x,y) &lt; 7000:
        return y
    else:
        theta = atan2(y[1]-x[1], y[0]-x[0])
        return x[0] + self.EPS*cos(theta), x[1] + self.EPS*sin(theta)

#Function for displaying the output
def Start_The_Game(self):

    pygame.init()

    screen = pygame.display.set_node(Window_size)

    caption = pygame.display.set_caption(""performing RRT"")

    white = 255, 240, 200
    black = 20, 20, 40
    screen.fill(black)

#Main Function
def Node_Generation(self, nodes):
    self.nodes      = []
    self.QUIT       = QUIT
    self.KEYUP      = KEYUP
    self.K_ESCAPE   = K_ESCAPE

    #nodes.append(X_dimension/2.0, Y_dimension/2.0)

    nodes.append(0.0, 0.0)
    pygame.init()

for i in range(Max_nodes):
    rand = random.random()*640.0, random.random()*480.0
    nn = nodes[0]

for p in nodes:
    if dist(p,rand) &lt; dist(nn,rand):
        nn = p
        newnode = step_from_to(nn,rand)
        nodes.append(newnode)
        pygame.draw.line(screen,white,nn,newnode)
        pygame.display.update()
        print (i, ""  "", nodes)

for j in pygame.event.get():
    if j.type == QUIT or (j.type == KEYUP and j.key == K_ESCAPE):
        pygame.quit()
        sys.exit(""GAME OVER"")
</code></pre>

<p>path = RRT(10,15)</p>

<p>path.Calculate_Distance(10,15)</p>

<p>path.Initiate_Sampling(path.Calculate_Distance(10,15))</p>

<p>path.Start_The_Game()</p>

<p>path.Node_Generation()        </p>

#

<p>My query - I want to pass Calculate_Distance function as an argument to the Initiate_Sampling function to compare it with EPS.</p>
",11/29/2019 8:52,59101588,35,1,0,0,,12455645,,11/29/2019 8:47,7,59101588,"<p>You can access member function via <code>self</code> the same way you would access other class members within the class. i.e. </p>

<pre><code>self.Calculate_Distance
</code></pre>
",9006027,0,0,,,,
168,3285,54606363,Simple rotation of a rectangle along a curve in matlab,|matlab|matlab-figure|robotics|,"<p>I'm trying to rotate a rectangle(polyshape) in matlab to orient itself along the curve(a set of points). So far this is my code.</p>

<pre><code>l=2;w=1;xc=-1;yc=2;
xvs= [xc+w/2 xc+w/2 xc-w/2 xc-w/2];
yvs= [yc-l/2 yc+l/2 yc+l/2 yc-l/2];
ax = gca;
polyin = polyshape(xvs,yvs); % %w/2,h/2% polyin = rectangle('Position',[-0.1 -0.1 0.2, 0.4]);
k=2;
% t = acos((y(k-1)*x(k-1)+y(k)*x(k))/(norm([x(k-1) y(k-1)])*norm([x(k) y(k)])));
t = atan(y(k)/x(k));
polyout = translate(polyin,[-xc -yc]);
polyout = translate(polyout,[x(1) y(1)]);
polyout = rotate(polyout, t, [x(2) y(2)]);
plot(polyout);
axis([-4 4 0 50]);
prev_t = atan(y(2)/x(2));
for k=2:length(x)
    cla();
    t = atan((y(k)-y(k-1))/(x(k)-x(k-1)));
%     t = acos((y(k-1)*x(k-1)+y(k)*x(k))/(norm([x(k-1) y(k-1)])*norm([x(k) y(k)])));
%     t = atan((y(k-1))/(x(k-1)));
    polyout=translate(polyout,x(k)-x(k-1),y(k)-y(k-1));
    [t3,t4]=centroid(polyout);
    t-prev_t
    polyout=rotate(polyout,rad2deg(t-prev_t) ,[t3, t4] );%, );
    prev_t = t;
    plot(polyout);
    hold on;
    plot(x(1:k),y(1:k));
    hold on; 
    quiver(x(k), y(k), 1, 1);
    axis([-4 4 0 50]);
    drawnow;
end
</code></pre>

<p>x,y is listed below</p>

<pre><code>x = [-1 -1.00972272933410   -1.01870478051530   -1.02695805761983   -1.03449438259277   -1.04132549543428   -1.04746305438566   -1.05291863611549   -1.05770373590578   -1.06182976783809   -1.06530806497965   -1.06814987956949   -1.07036638320459   -1.07196866702597   -1.07296774190485   -1.07337453862878   -1.07319990808774   -1.07245462146029   -1.07114937039970   -1.06929476722009   -1.06690134508251   -1.06397955818112   -1.06053978192930   -1.05659231314578   -1.05214737024076   -1.04721509340205   -1.04180554478121   -1.03592870867963   -1.02959449173473   -1.02281272310602   -1.01559315466128   -1.00794546116266   -0.999879240452816  -0.991404013641053  -0.982529225289427  -0.973264243598892  -0.963618360595424  -0.953600792316151  -0.943220678995479  -0.932487085251223  -0.921409000270737  -0.909995337997038  -0.898254937314939  -0.886196562237176  -0.873828902090537  -0.861160571701991  -0.848200111584816  -0.834955988124727  -0.821436593766009  -0.807650247197639  -0.793605193539421  -0.779309604528111  -0.764771578703544  -0.749999141594771  -0.735000245906179  -0.719782771703621  -0.704354526600550  -0.688723245944142  -0.672896593001429  -0.656882159145424  -0.640687464041251  -0.624319955832276  -0.607787011326233  -0.591095936181355  -0.574253965092497  -0.557268261977274  -0.540145920162182  -0.522893962568731  -0.505519341899570  -0.488028940824620  -0.470429572167200  -0.452727979090155  -0.434930835281988  -0.417044745142986  -0.399076243971348  -0.381031798149319  -0.362917805329311  -0.344740594620036  -0.326506426772637  -0.308221494366812  -0.289891921996946  -0.271523766458236  -0.253123016932825  -0.234695595175927  -0.216247355701956  -0.197784085970655  -0.179311506573228  -0.160835271418462  -0.142360967918861  -0.123894117176775  -0.105440174170524  -0.0870045279405308 -0.0685925017754495 -0.0502093533982922 -0.0318602751525592 -0.0135503941883670 0.00471522735142232 0.0229315921450729  0.0410937675058457  0.0591968851958715  0.0772361412400196  0.0952067957397721  0.113104172687091   0.130923659778296   0.148660708227926   0.166310832582623   0.183869610534992   0.201332682737479   0.218695752616237   0.235954586185006   0.253105011858973   0.270142920268653   0.287064264073754   0.303865057777052   0.320541377538260   0.337089360987901   0.353505207041178   0.369785175711844   0.385925587926079   0.401922825336353   0.417773330135305   0.433473604869608   0.449020212253846   0.464409774984379   0.479638975553221   0.494704556061906   0.509603318035363   0.524332122235782   0.538887888476493   0.553267595435832   0.567468280471012   0.581487039431996   0.595321026475369   0.608967453878208   0.622423591851952   0.635686768356277   0.648754368912964   0.661623836419772   0.674292670964308   0.686758429637899   0.699018726349463   0.711071231639382   0.722913672493370   0.734543832156346   0.745959549946308   0.757158721068200   0.768139296427783   0.778899282445511   0.789436740870399   0.799749788593894   0.809836597463747   0.819695394097884   0.829324459698281   0.838722129864827   0.847886794409204   0.856816897168752   0.865510935820345   0.873967461694257   0.882185079588040   0.890162447580389   0.897898276845016   0.905391331464524   0.912640428244272   0.919644436526252   0.926402278002956   0.932912926531251   0.939175407946249   0.945188799875176   0.950952231551246   0.956464883627534   0.961725987990839   0.966734827575568   0.971490736177594   0.975993098268136   0.980241348807631   0.984234973059598   0.987973506404514   0.991456534153687   0.994683691363123   0.997654662647400   1.00036918199354    1.00282703257487    1.00502804656492    1.00697210495126    1.00865913734939    1.01008912181662    1.01126208466593    1.01217810027981    1.01283729092421    1.01323982656233    1.01338592466854    1.01327585004223    1.01290991462170    1.01228847729801    1.01141194372885    1.01028076615244    1.00889544320138    1.00725651971651    1.00536458656082    1.00322028043328    1.00082428368272    0.998177324121733   0.995280174840507   0.992133654020716   0.988738624749389   0.985095994832775   0.981206716610224   0.977071786768050   0.972692246153409   0.968069179588162   0.963203715682755   0.958097026650084   0.952750328119371   0.947164878950031   0.941341981045545   0.935282979167332   0.928989260748622   0.922462255708321   0.915703436264890   0.908714316750211   0.901496453423459   0.894051444284976   0.886380928890139   0.878486588163234   0.870370144211326   0.862033360138129   0.853478039857879   0.844706027909207   0.835719209269005   0.826519509166304   0.817108892896138   0.807489365633420   0.797662972246815   0.787631797112606   0.777397963928569   0.766963635527843   0.756331013692799   0.745502338968920   0.734479890478659   0.723265985735320   0.711862980456928   0.700273268380098   0.688499281073906   0.676543487753763   0.664408395095284   0.652096547048159   0.639610524650029   0.626952945840348   0.614126465274265   0.601133774136485   0.587977599955151   0.574660706415706   0.561185893174769   0.547555995674006   0.533773884953999   0.519842467468121   0.505764684896401   0.491543513959404   0.477181966232097   0.462683087957718   0.448049959861653   0.433285696965303   0.418393448399959   0.403376397220668   0.388237760220109   0.372980787742464   0.357608763497285   0.342125004373370   0.326532860252634   0.310835713823975   0.295036980397154   0.279140107716656   0.263148575775570   0.247065896629457   0.230895614210219   0.214641304139976   0.198306573544930   0.181895060869242   0.165410435688903   0.148856398525599   0.132236680660593   0.115555043948583   0.0988152806315876  0.0820212131528061  0.0651766939704940  0.0482856053718364  0.0313518592868137  0.0143793971020798  -0.00262781052517255    -0.0196657638533383 -0.0367304342425269 -0.0538177643406945 -0.0709236682697698 -0.0880440318117838 -0.105174712595000  -0.122311540280039  -0.139450316746014  -0.156586816276652  -0.173716785746427  -0.190835944806689  -0.207939986071788  -0.225024575305211  -0.242085351605702  -0.259117927593395  -0.276117889595944  -0.293080797834649  -0.310002186610585  -0.326877564490733  -0.343702414494106  -0.360472194277880  -0.377182336323518  -0.393828248122908  -0.410405312364482  -0.426908887119349  -0.443334306027426  -0.459676878483560  -0.475931889823665  -0.492094601510844  -0.508160251321521  -0.524124053531569  -0.539981199102440  -0.555726855867290  -0.571356168717112  -0.586864259786862  -0.602246228641589  -0.617497152462564  -0.632612086233408  -0.647586062926218  -0.662414093687702  -0.677091168025301  -0.691612253993325  -0.705972298379074  -0.720166226888972  -0.734188944334692  -0.748035334819290  -0.761700261923330  -0.775178568891010  -0.788465078816297  -0.801554594829052  -0.814441900281157  -0.827121758932650  -0.839588915137848  -0.851838094031477  -0.863864001714800  -0.875661325441750  -0.887224733805055  -0.898548876922365  -0.909628386622385  -0.920457876631001  -0.931031942757411  -0.941345163080250  -0.951392098133724  -0.961167291093732  -0.970665267964000  -0.979880537762209  -0.988807592706122  -0.997440908399714  -1.00577494401930   -1.01380414249966   -1.02152293072018   -1.02892571969097   -1.03600690473899   -1.04276086569418   -1.04918196707561   -1.05526455827758   -1.06100297375575   -1.06639153321331   -1.07142454178703   -1.07609629023348   -1.08040105511509   -1.08433309898632   -1.08788667057976   -1.09105600499228   -1.09383532387113   -1.09621883560011   -1.09820073548567   -1.09977520594304   -1.10093641668239   -1.10167852489489   -1.10199567543892   -1.10188200102614   -1.10133162240765   -1.10033864856010   -1.09889717687183   -1.09700129332900   -1.09464507270172   -1.09182257873014   -1.08852786431066   -1.08475497168197   -1.08049793261124   -1.07575076858022   -1.07050749097139   -1.06476210125405   -1.05850859117050   -1.05174094292213   -1.04445312935557   -1.03663911414879   -1.02829285199726   -1.01940828880009   -1.00997936184609   -1]    

y = [2  2.26499544506307    2.52345285165919    2.77551012741366    3.02130356526704    3.26096785177083    3.49463607538307    3.72243973476395    3.94450874707147    4.16097145625704    4.37195464136115    4.57758352480892    4.77798178070582    4.97327154313322    5.16357341444407    5.34900647355850    5.52968828425946    5.70573490348834    5.87726088964058    6.04437931086135    6.20720175334112    6.36583832961133    6.52039768683998    6.67098701512728    6.81771205580129    6.96067710971352    7.09998504553457    7.23573730804975    7.36803392645471    7.49697352265110    7.62265331954214    7.74516914932828    7.86461546180284    7.98108533264758    8.09467047172843    8.20546123139099    8.31354661475628    8.41901428401626    8.52195056872954    8.62244047411698    8.72056768935728    8.81641459588266    8.91006227567448    9.00159051955883    9.09107783550219    9.17860145690705    9.26423735090754    9.34806022666506    9.43014354366389    9.51055952000682    9.58937914071082    9.66667216600261    9.74250713961429    9.81695139707904    9.89007107402665    9.96193111447923    10.0325952791468    10.1021261537228    10.1705851571801    10.2380325500660    10.3045274427986    10.3701278039618    10.4348904686013    10.4988711465199    10.5621244305736    10.6247038049668    10.6866616535481    10.7480492681059    10.8089168566642    10.8693135517778    10.9292874188284    10.9888854643200    11.0481536441744    11.1071368720271    11.1658790275228    11.2244229646110    11.2828105198417    11.3410825206610    11.3992787937066    11.4574381731036    11.5155985087602    11.5737966746629    11.6320685771728    11.6904491633205    11.7489724291023    11.8076714277756    11.8665782781543    11.9257241729050    11.9851393868420    12.0448532852234    12.1048943320464    12.1652900983432    12.2260672704765    12.2872516584350    12.3488682041292    12.4109409896870    12.4734932457494    12.5365473597658    12.6001248842901    12.6642465452759    12.7289322503726    12.7942010972203    12.8600713817463    12.9265606064601    12.9936854887492    13.0614619691748    13.1299052197674    13.1990296523224    13.2688489266957    13.3393759590994    13.4106229303974    13.4826012944010    13.5553217861645    13.6287944302809    13.7030285491776    13.7780327714119    13.8538150399664    13.9303826205452    14.0077421098691    14.0858994439713    14.1648599064931    14.2446281369795    14.3252081391748    14.4066032893182    14.4888163444396    14.5718494506550    14.6557041514622    14.7403813960366    14.8258815475266    14.9122043913492    14.9993491434859    15.0873144587782    15.1760984392230    15.2656986422686    15.3561120891100    15.4473352729848    15.5393641674687    15.6321942347710    15.7258204340304    15.8202372296108    15.9154385993964    16.0114180430878    16.1081685904975    16.2056828098453    16.3039528160545    16.4029702790468    16.5027264320383    16.6032120798353    16.7044176071297    16.8063329867944    16.9089477881795    17.0122511854074    17.1162319656689    17.2208785375182    17.3261789391692    17.4321208467907    17.5386915828022    17.6458781241694    17.7536671107000    17.8620448533392    17.9709973424653    18.0805102561854    18.1905689686311    18.3011585582540    18.4122638161214    18.5238692542118    18.6359591137108    18.7485173733063    18.8615277574846    18.9749737448257    19.0888385762990    19.2031052635592    19.3177565972414    19.4327751552571    19.5481433110899    19.6638432420907    19.7798569377739    19.8961662081124    20.0127526918339    20.1295978647159    20.2466830478817    20.3639894160961    20.4814980060606    20.5991897247094    20.7170453575050    20.8350455767337    20.9531709498011    21.0714019475281    21.1897189524464    21.3081022670938    21.4265321223103    21.5449886855333    21.6634520690937    21.7819023385110    21.9003195207895    22.0186836127132    22.1369745891423    22.2551724113081    22.3732570351088    22.4912084194057    22.6090065343178    22.7266313695183    22.8440629425299    22.9612813070203    23.0782665610982    23.1949988556084    23.3114584024278    23.4276254827613    23.5434804554365    23.6590037652004    23.7741759510141    23.8889776543494    24.0033896274833    24.1173927417946    24.2309679960590    24.3440965247448    24.4567596063089    24.5689386714917    24.6806153116135    24.7917712868696    24.9023885346262    25.0124491777157    25.1219355327329    25.2308301183300    25.3391156635127    25.4467751159356    25.5537916501978    25.6601486761387    25.7658298471334    25.8708190683886    25.9751005052380    26.0786585914379    26.1814780374631    26.2835438388023    26.3848412842538    26.4853559642211    26.5850737790085    26.6839809471169    26.7820640135392    26.8793098580560    26.9757057035313    27.0712391242082    27.1658980540041    27.2596707948069    27.3525460247703    27.4445128066094    27.5355605958967    27.6256792493570    27.7148590331637    27.8030906312344    27.8903651535260    27.9766741443309    28.0620095905721    28.1463639300994    28.2297300599846    28.3121013448172    28.3934716250003    28.4738352250458    28.5531869618704    28.6315221530909    28.7088366253202    28.7851267224627    28.8603893140098    28.9346218033358    29.0078221359933    29.0799888080092    29.1511208741798    29.2212179563668    29.2902802517928    29.3583085413370    29.4253041978307    29.4912691943530    29.5562061125265    29.6201181508128    29.6830091328082    29.7448835155393    29.8057463977587    29.8656035282405    29.9244613140760    29.9823268289694    30.0392078215333    30.0951127235842    30.1500506584387    30.2040314492083    30.2570656270958    30.3091644396903    30.3603398592634    30.4106045910644    30.4599720816159    30.5084565270100    30.5560728812031    30.6028368643122    30.6487649709102    30.6938744783217    30.7381834549183    30.7817107684148    30.8244760941642    30.8664999234537    30.9078035718003    30.9484091872463    30.9883397586550    31.0276191240064    31.0662719786928    31.1043238838141    31.1418012744739    31.1787314680751    31.2151426726150    31.2510639949816    31.2865254492487    31.3215579649719    31.3561933954840    31.3904645261906    31.4244050828661    31.4580497399487    31.4914341288367    31.5245948461837    31.5575694621942    31.5903965289195    31.6231155885532    31.6557671817267    31.6883928558052    31.7210351731827    31.7537377195783    31.7865451123314    31.8195030086975    31.8526581141437    31.8860581906445    31.9197520649774    31.9537896370183    31.9882218880374    32.0231008889946    32.0584798088355    32.0944129227864    32.1309556206506    32.1681644151037    32.2060969499891    32.2448120086139    32.2843695220444    32.3248305774017    32.3662574261575    32.4087134924294    32.4522633812768    32.4969728869964    32.5429090014181    32.5901399222001    32.6387350611250    32.6887650523952    32.7403017609286    32.7934182906543    32.8481889928080    32.9046894742278    32.9629966056499    33.0231885300040    33.0853446707091    33.1495457399690    33.2158737470682    33.2844120066671    33.3552451470980    33.4284591186606    33.5041412019175    33.5823800159901    33.6632655268539    33.7468890556344    33.8333432869025    33.9227222769705    34.0151214621873    34.1106376672341    34.2093691134204    34.3114154269792    34.4168776473628    34.5258582355384    34.6384610822839    34.7547915164830    34.8749563134217    34.9990637030830    35.1272233784432    35.2595465037671    35.3961457229040    35.5371351675831    35.6826304657089    35.8327487496574    35.9876086645714    36.1473303766559    36.3120355814743    36.4818475122433    36.6568909481293    36.8372922225435    37.0231792314376    37.2146814415997    37.4119298989495    37.6150572368345    37.8241976843249    38.0394870745098    38.2610628527928    38.4890640851872    38.7236314666120    38.9649073291876    39.2130356505310    39.4681620620517    39.7304338572476    40]
</code></pre>

<p>However the rectangle doesn't orient itself exactly towards the curve in question, it still has a offset.Any help is appreciated. Thanks. </p>
",2/9/2019 12:48,54647022,55,1,2,0,,961682,,9/23/2011 17:34,134,54647022,"<p>Hi i'm posting the solution to the question here. Thanks to anyone who took time to take a look at it. </p>

<pre><code>      for k=2:length(x)
        %%%Plot road
        rl=-2; rr=0; w=4; l=100;
        yroad = 0:l;
        xroad = repmat(rl+w/2,l+1);
        plot(xroad, yroad, 'w--','LineWidth',1.5);
        hold on;
        rectangle('Position',[rl,rr,w,l],'FaceColor',[0 0 0 0.9]);
        axis([-4 4 0 50]);
        hold on;
        %%%%%
        set(gca,'children',flipud(get(gca,'children')))
        %%%time annotation
        str = strcat('t=',string(t));
        % annotation('textbox',[0.8 0.8 .5 .5],'String',str,'FitBoxToText','on');
        text(1,47,str,'Color','w')
        ratio = diff(get(gca, 'YLim'))/diff(get(gca, 'XLim'));
        phi = atan2(y(k) - y(k - 1), (x(k) - x(k - 1))*ratio);
        [x_rect_rot, y_rect_rot]=get_rectangle(phi, x(k), y(k));
        plot(polyshape(x_rect_rot, y_rect_rot),'FaceColor','c','FaceAlpha',0.85);
        hold on;
        plot(x(1:k), y(1:k));
        hold on;  
        drawnow;
      end

    function[xa,ya] =get_rectangle(phi,xc,yc,varargin)
        if length(varargin)&lt;2
            h = 5;
            w = 3;
        else 
            h = varargin{4};
            w = varargin{5};
        end
        x_rect = [-h, h, h, -h]/2;
        y_rect = [-w, -w, w, w]/2;
        % Consider aspect ratio of the axis
        ratio = diff(get(gca, 'YLim'))/diff(get(gca, 'XLim'));            

        % Calculate rotated rectangle
        x_rect_rot = x_rect*cos(phi) - y_rect*sin(phi);
        y_rect_rot = x_rect*sin(phi) + y_rect*cos(phi);

        % Incorporate ratio
        x_rect_rot = x_rect_rot/ratio;

        % Calculate offset
        xa = x_rect_rot + xc;
        ya = y_rect_rot + yc;

    end
</code></pre>
",961682,0,0,96085079,"I wanted to indicate the direction of next point to move to, which i wrote wrong here in the question, this question is answered in a discord chatroom, i'll put the answer here, once again thank you very much for your time ViG :)",,
170,3577,58739227,roblem with ROS control and Gazebo,|ros|robotics|gazebo-simu|,"<p>I have a problem with controlling a URDF that I exported from SolidWorks. (Ubuntu 16.04 , Kinetic, Gazebo 7.x) I followed this tutorial and I wanted to implemented on my robot. All the controllers are starting correctly so as the Gazebo simulation also the Node publish the data correctly I have checked it with echo-ing the topic and with different values for the data. Is there a chance not working because the PID values ?</p>

<p>All the transmissions look like this :</p>

<pre class=""lang-xml prettyprint-override""><code>&lt;transmission name=""tran1""&gt;
    &lt;type&gt;transmission_interface/SimpleTransmission&lt;/type&gt;
    &lt;joint name=""Joint_1""&gt;
      &lt;hardwareInterface&gt;hardware_interface/EffortJointInterface&lt;/hardwareInterface&gt;
    &lt;/joint&gt;
    &lt;actuator name=""motor1""&gt;
      &lt;hardwareInterface&gt;hardware_interface/EffortJointInterface&lt;/hardwareInterface&gt;
      &lt;mechanicalReduction&gt;1&lt;/mechanicalReduction&gt;
    &lt;/actuator&gt;
&lt;/transmission&gt;  
</code></pre>

<p>The controller is like this (for all joints) :</p>

<pre><code>joint_state_controller:
    type: joint_state_controller/JointStateController
    publish_rate: 50
joint1_position_controller:
    type: effort_controllers/JointPositionController
    joint: Joint_1
    pid: {p: 100.0, i: 0.01, d: 10.0}
</code></pre>

<p>And I have this node:</p>

<pre class=""lang-py prettyprint-override""><code>    rospy.init_node('ArmMovement')
    pub1=rospy.Publisher(""/rrbot/joint1_position_controller/command"",Float64,queue_size=10 )
    rate = rospy.Rate(50)
    ArmCor1= Float64()
    ArmCor1.data=0
    while not rospy.is_shutdown():
      pub1.publish(ArmCor1)
      rate.sleep()
</code></pre>

<p>Part of URDF for the Joint_1:</p>

<pre class=""lang-xml prettyprint-override""><code>&lt;joint name=""Joint_1"" type=""revolute""&gt;
    &lt;origin
      xyz=""0 0 -0.008""
      rpy=""1.5708 0 0"" /&gt;
    &lt;parent link=""base_link"" /&gt;
    &lt;child link=""Link_1"" /&gt;
    &lt;axis
      xyz=""0 1 0"" /&gt;
    &lt;limit
      lower=""0""
      upper=""3.14""
      effort=""0""
      velocity=""0"" /&gt;
&lt;/joint&gt;
</code></pre>
",11/6/2019 22:10,58827131,825,1,1,1,,12334653,,11/6/2019 22:03,12,58827131,"<p>Thank you guys for your help, it actually work after your comments. 
This was my Joint_1 :</p>

<pre><code> &lt;joint name=""Joint_1"" type=""revolute""&gt;
 &lt;origin
 xyz=""0 0 -0.008""
 rpy=""1.5708 0 0"" /&gt;
&lt;parent link=""base_link"" /&gt;
&lt;child link=""Link_1"" /&gt;
&lt;axis
xyz=""0 1 0"" /&gt;
&lt;limit
 lower=""0""
 upper=""3.14""
 effort=""0""
 velocity=""0"" /&gt;
&lt;/joint&gt;
</code></pre>

<p>I changed the limit section to this :</p>

<pre><code>&lt;limit
lower=""0""
upper=""3.14""
effort=""2""
velocity=""2.0"" /&gt;
</code></pre>

<p>I have other problems like a very annoying shiver (that comes from effort value I think) but It is not for this topic.</p>
",12334653,0,0,103770883,What does the [rostopic list](http://wiki.ros.org/rostopic#rostopic_list) console command return?,,
173,3363,55352630,Robot Model drops off the floor after running the simulation,|gravity|robot|physics-engine|webots|,"<p>I imported a model in webots simulation from URDFs in ROS. The robot is a tricycle drive with 3 castor wheels. I have followed the wheel style as in the webots style guide and changed accordingly.
My problem is that when I run the simulation the robot's wheels drops off the floor and is not able to move. Just chassis is on the floor and the wheels hangs down.</p>
",3/26/2019 8:22,55352846,498,1,0,3,,5230316,"Bonn, Germany",8/15/2015 13:40,114,55352846,"<p>Your robot is probably too heavy for the physics configuration.</p>

<p>You can fix this by changing the fields values of the WorldInfo node (<a href=""https://www.cyberbotics.com/doc/reference/worldinfo"" rel=""nofollow noreferrer"">https://www.cyberbotics.com/doc/reference/worldinfo</a>).
Here are the important fields:</p>

<ul>
<li><strong>ERP</strong>: you probably want to increase the default value which works fine for small and light objects (try setting it to ~0.6)</li>
<li><strong>basicTimeStep</strong>: here you might decrease the default value to 16 (or even 8) this will make the simulation runs slightly slower because it computes more steps but much more stable.</li>
<li><strong>contactProperties</strong>: You probably need to add a contact property defining the properties of the contact between the floor and your robot's wheels (to increase the friction, and decrease the spongyness of the contact).</li>
</ul>

<p>Here is an example of contact properties:</p>

<pre><code>ContactProperties {
  material2 ""MyRobotWheelContactMaterial""
  coulombFriction [
    8
  ]
  softCFM 1e-5
}
</code></pre>
",8427891,2,2,,,,
177,3446,57123050,Spatial toolbox robot model,|matlab|simulation|spatial|robotics|,"<p>I want to model these D-H parameters in spatial toolbox:</p>

<pre><code>Link:          alpha,      a,        theta,      d
Link 1 :        -90        0        theta1*      d1
Link 2 :          0        a2       theta2*      0
Link 3 :          0        a3       theta3*      0
</code></pre>

<p>This is the code I have tried:</p>

<pre><code>n=3;
rob.NB = n;
rob.parent = [0:n-1];
rob.jtype = { 'R', 'R', 'R' }

l2=0.28;    %link length
l3=0.2;     %link length
d1=0.05;    %link offset


rob.Xtree{1} = rotx(pi)*xlt([0,0,0]);
rob.Xtree{2} = rotz(0)*xlt([l2 0 0]);
rob.Xtree{3} = xlt([l3 0 0]);

ax1=0.03; ay1=0.03; az1=0.03;
ax2=0.28; ay2=0.05; az2=0.05;
ax3=0.2; ay3=0.05; az3=0.05;

rob.I{1} = mcI( 1, [0 0 -0.02], 1/12*[ay1^2+az1^2 0 0; 0 ax1^2+az1^2 0; 0 0 ax1^2+ay1^2] )
rob.I{2} = mcI( 4, [0.14 0 0], 4/12*[ay2^2+az2^2 0 0; 0 ax2^2+az2^2 0; 0 0 ax2^2+ay2^2] )
rob.I{3} = mcI( 3, [0.1 0 0], 3/12*[ay3^2+az3^2 0 0; 0 ax3^2+az3^2 0; 0 0 ax3^2+ay3^2] )

rob.appearance.base = ...
  { 'box', [-0.2 -0.3 -0.2; 0.2 0.3 -0.07] };


rob.appearance.body{1} = ...
    { 'box', [0 -0.07 -0.04; 0.05 0.07 0.04], ...
      'cyl', [0 -0.07 0; 0 0.07 0], 0.06 };

rob.appearance.body{2} = ...
    { 'box', [0 -0.07 -0.04; 0.28 0.07 0.04], ...
      'cyl', [0 0 -0.07; 0 0 0.07], 0.06 };

rob.appearance.body{3} = ...
    { 'box', [0 -0.07 -0.04; 0.2 0.07 0.04], ...
      'cyl', [0 0 -0.07; 0 0 0.07], 0.06 };

showmotion(rob)
</code></pre>

<p>But this is what I get, with motion only along 1 axis: </p>

<p>showmotion robot model</p>

<p><a href=""https://i.stack.imgur.com/R5GVv.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/R5GVv.jpg"" alt=""""></a></p>

<p>how to get the model right?</p>
",7/20/2019 8:32,57141051,45,1,0,0,,11713385,,6/28/2019 11:48,15,57141051,"<p>I changed my code and now it works properly.</p>

<pre><code>rob.Xtree{1} = rotx(1.57) * xlt([0 0 0]);
rob.Xtree{2} = roty(1.57) * xlt([0.15,0,0]);
rob.Xtree{3} = xlt([0.34 0 0]);
</code></pre>
",11713385,0,0,,,,
178,3587,58836294,Create graph from GVD (edges and vertices),|c++|opencv|computer-vision|graph-theory|robotics|,"<p>Given a binary image of a map with obstacles (black pixels), the objective is to create a Voronoi diagram graph. So far, this is done using the brushfire algorithm, from which the GVD can be extracted using the Laplacian operator (i.e. the gradient); the process is demonstrated below and the C++ code can be found in <a href=""https://github.com/martinandrovich/rb-pro5/blob/45be4b23db1b910f8ac6b0facc864d5b0ca3ea95/src/modules/geometry.h#L365"" rel=""nofollow noreferrer"">this file</a>.</p>

<p><a href=""https://i.stack.imgur.com/5pwfE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5pwfE.png"" alt=""gvd process""></a></p>

<p>The goal is now to use the GVD (image 3) to create an adjacency graph, being basically a graph-like data structure with the Voronoi vertices as nodes and edges being the paths between nodes.</p>

<p>My question is, what is the most optimal way of doing this? So far, I have detected vertices of the GVD using the <code>cv::findContours</code> method as shown in the last image, and the idea is to now remove any near-duplicate vertices and try connecting the nodes using the black pixels and a line iterator.</p>

<p>Is there a better way?</p>
",11/13/2019 11:39,,270,0,0,1,,1658105,Denmark,9/9/2012 12:39,77,,,,,,,,,
180,3447,57132401,How to run a robot's controller in multi processes or multi threads in webots?,|robotics|webots|,"<p>I want to have a controller that somehow runs 3 processes to run the robot's code.</p>

<p>I am trying to simulate a humanoid soccer robot in webots . To run our robot's code, we run 3 processes. One for the servomotors' power management , another one for image processing and communications and the last one for motion control.</p>

<p>Now I want to have a controller making me somehow able to simulate something like this or at least similar to it. Does anyone have any idea how I can do this?</p>
",7/21/2019 10:41,57140469,748,1,0,1,,7892779,,4/20/2017 0:43,6,57140469,"<p>Good news: the Webots API is thread safe :-)</p>

<p>Generally speaking, I would not recommend to use multi-threads, because programming threads is a big source of issues. So, if you have any possibility to merge your threads into a single-threaded application, it's the way to go!</p>

<p>If you would like to go in this direction, the best solution is certainly to create a single controller running your 3 threads, and synchronize them with the main thread (thread 0).</p>

<p>The tricky part is to deal correctly with the time management and the simulation steps. A solution could be to set the <a href=""https://cyberbotics.com/doc/reference/robot"" rel=""nofollow noreferrer""><code>Robot.synchronization</code></a> field to FALSE and to use the main thread to call the <code>wb_robot_step(duration)</code> function every <code>duration</code> time (real time).</p>
",2210777,3,4,,,,
181,3612,59170947,Getting the pygame window but no output,|python|algorithm|pygame|robotics|,"<p>I am trying to write an RRT path planning algorithm in python. Although the code executes without error, the result is a plain pygame window with no output. </p>

<pre><code>import sys, random, math, pygame
from math import sqrt,cos,sin,atan2
from pygame.locals import *
pygame.init()

class RRT(object):

X_dimension = 0
Y_dimension = 0
Window_size = 0
EPS         = 0
Max_nodes   = 0 
nodes       = list()
K_ESCAPE    = True
KEYUP       = True
QUIT        = True

def __init__(self,x,y):
    self.x = [10,20,40,50,60,0]
    self.y = [15,30,0,54,75,68]  

#parameters
    self.X_dimension = 1280                                  #length of the window
    self.Y_dimension = 1280                                  #breadth of the window
    self.Window_size = [self.X_dimension, self.Y_dimension]  #Window size
    self.EPS         =  7000                                 #EPSILON or Incremental Distance 
    self.Max_nodes   = 100                                   #maximum number of nodes
    self.QUIT        = QUIT
    self.KEYUP       = KEYUP
    self.K_ESCAPE    = K_ESCAPE


#function for calculating euclidean distance
def Calculate_Distance(self):
    x= self.x
    y=self.y

    return sqrt((x[5]-y[5])*(x[5]-y[5])+(x[4]-y[4])*(x[4]-y[4]))

#Function for calculating all the possible points
def Initiate_Sampling(self):

    self.EPS = 7000 
    y = self.y
    x = self.x

    if self.Calculate_Distance() &lt; 70:
        return y, x
    else:
        theta = atan2(y[1]-x[1], y[0]-x[0])
        return x[0] + self.EPS*cos(theta), x[1] + self.EPS*sin(theta)

#Function for displaying the output
def Start_The_Game(self):

    pygame.init()

    screen = pygame.display.set_mode(self.Window_size)
    caption = pygame.display.set_caption(""performing RRT"")
    white = 255, 240, 200
    black = 20, 20, 40
    screen.fill(black) 

    return('GAME BEGINS')   


#Main Function
def Node_Generation(self, nodes=True):
    self.nodes      = nodes
    self.QUIT       = QUIT
    self.KEYUP      = KEYUP
    self.K_ESCAPE   = K_ESCAPE

    nodes = [(5.0,5.25),(7.0,7.25),(8,8.25)]

    #nodes.append(X_dimension/2.0, Y_dimension/2.0)        
    nodes.append((0.0,0.0))

for i in range(Max_nodes):
    rand = random.random()*640.0, random.random()*480.0
    nn = self.nodes[0]

for p in nodes:
    if self.Calculate_Distance(p,rand) &lt; self.Calculate_Distance(nn,rand):
        nn = p
        newnode = step_from_to(nn,rand)
        nodes.append(newnode)
        pygame.draw.line(screen,white,nn,newnode)
        pygame.display.update()
        #print (i, ""  "", nodes)

for event in pygame.event.get():
    if event.type == QUIT or (event.type == KEYUP and event.key == K_ESCAPE):
        pygame.quit()
        sys.exit(""GAME OVER"")



path = RRT(0,0)#write the starting nodes
path.Calculate_Distance()
path.Initiate_Sampling()
path.Start_The_Game()
path.Node_Generation()    
</code></pre>

<p>The above results in a plain pygame window but no output.</p>
",12/4/2019 7:30,,93,0,3,0,,,,,,,,,,,104565837,I will fix the indentation. Also the node is a list object. Apart from the indentation is there any logical or syntactical error?,,
183,3415,56867596,Is it possible to use G1ANT Studio to automate outlook?,|robotics|rpa|g1ant|,"<p>Greetings fellow software engineers,</p>

<p>I've extremely new to RPA and I'm looking forward to using some of the best tools. The first task that I want to robotize/automate involves heavy use of outlook. I've seen that G1ANT gives the biggest opportunities to use programming languages (C#) and I want to know is it possible to use it to outlook automation or is there any templates/solutions to do it?</p>

<p>Thanks a lot!</p>
",7/3/2019 10:02,56871204,152,2,0,1,,11733698,"Warsaw, Poland",7/3/2019 9:54,46,56871204,"<p>@NeedHelpAsap, G1ANT offers a number of commands for working with Outlook. If you have already installed the latest G1ANT developer version, open it and look on the left side of the workspace for the ""Addons"" window (if you don't see it, go to the ""View"" menu at the top and click on ""Addons"". You should see this, or similar:</p>

<p><a href=""https://i.stack.imgur.com/lXxh3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lXxh3.png"" alt=""Addon window""></a></p>

<p>Check the ""msoffice"" box as shown. You'll see a list of commands in the window below. The Outlook-related commands portion should look very much like this:</p>

<p><a href=""https://i.stack.imgur.com/SMmtC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SMmtC.png"" alt=""List of commands""></a></p>

<p>Double-clicking any of these commands will bring up its help. In the help, in addition to a description of the command and its arguments, there is a tab for the Manual page for that command. You can also view the <a href=""https://manual.g1ant.com/"" rel=""nofollow noreferrer"">G1ANT.Robot Manual page</a> online. </p>

<p>Hope this gets you started.</p>

<p>Regards,
burque505</p>
",6457212,3,0,,,,
184,3293,54790336,Control Webots from external Python IDE,|python|robotics|webots|,"<p>Is it possible to control Webots from external Python IDE (like pyCharm) ? I would appreciate if there is an example showing how to do so and location of modules to be added. Thanks</p>
",2/20/2019 15:52,,667,1,6,4,,2974823,"6th of October City, Egypt",11/9/2013 20:25,89,57497084,"<p>Yes, there is some new documentation on how to run Webots robot controllers within PyCharm <a href=""https://cyberbotics.com/doc/guide/using-pycharm-with-webots"" rel=""nofollow noreferrer"">here</a>.</p>
",810268,1,0,96412579,You are correct @FabienRohrer. I am working on Windows and I hope to see a short example of external python script for moving a model and if possible getting images from the scene. Thanks a lot,,
188,3412,56806576,Forward and Inverse Kinematics for robot MATLAB,|matlab|simulink|robotics|inverse-kinematics|kinematics|,"<p>Hope you are doing well.</p>

<p>I am verifying the output of my forward kinematics through inverse kinematics and the results are not as desired. As the output of my inverse kinematics is not coming out to be the same as the input of forward kinematics.</p>

<p>The D-H parameters of manipulator is given as: </p>

<p>Link: alpha, a, theta, d</p>

<p>Link 1 :         -90            0        theta1*        d1</p>

<p>Link 2 :           0            a2       theta2*         0</p>

<p>Link 3 :           0            a3       theta3*         0</p>

<p>Functions used are:</p>

<p>Inverse kinematics</p>

<pre><code>function q = inv_kinematics(ph)
%input in radians: [ph1 ph2 ph3] = [0.1 0.2 0.4]
l1 = 0.05;
l2 = 0.28;
l3 = 0.2;
d1 = 0.03;
ph1 = ph(1);
ph2 = ph(2);
ph3 = ph(3);
r=sqrt(ph1^2+(ph3-d1)^2);
alpha=acos((r^2+l2^2-l3^2)/(2*r*l2))
q = zeros(3,1);
q1=atan2(ph2,ph1);
q2=atan2(ph1,ph3-d1)-alpha;
q3=atan2(ph1-l2*sin(q2),ph3-d1-l2*cos(q2))-q2;
q=[q1;q2;q3];
%output: q = [1.107 -0.265 1.314]
end
</code></pre>

<p>Forward kinematics:</p>

<pre><code>function ph  = forward_kinematics(q)
%input: [q1 q2 q3] = [1.107 -0.265 1.314]
l2 = 0.28;
l3 = 0.2;
d1 = 0.03;
q1 = q(1);
q2 = q(2);
q3 = q(3);
ph = zeros(3,1);
ph1 = l2*cos(q1)*cos(q2)+l3*cos(q1)*cos(q2+q3);
ph2 = l2*sin(q1)*cos(q2)+l3*sin(q1)*cos(q2+q3);
ph3 = d1-l2*sin(q2)-l3*sin(q2+q3);
ph=[ph1;ph2;ph3];
%output: p = [0.1655 0.3308 -0.07005] 
end
</code></pre>
",6/28/2019 12:00,,1195,0,3,0,,11713385,,6/28/2019 11:48,15,,,,,,100238852,"Very few (if any) people here are going to download a random model from someone they don't know.  As part of your question please provide a value for `q` (presumably a 3-element numeric vector) showing what your first function gives for `ph`, and then what your second function gives for `q`.",,
191,3928,65206902,How to use the iot lab platform and how to choose the right nodes,|iot|robotics|riot-os|,"<p>I want to create an Iot-project where I am gonna have to create a watering system.
I am going to use Riot-OS on an ESP32 or ESP8266 that will interact with a water pump, n-mosfet, power supply and a humidity sensor.
At the same time I will have a Linux webserver running accepting the data from this controller.</p>
<p>Until my hardware arrives I want to test the code using iot-lab. However, I do not know what nodes I have to use and why. How do I choose the correct ones?
Do I also need an .elf file for the sensor, like in this tutorial <a href=""https://www.iot-lab.info/earn/"" rel=""nofollow noreferrer"">https://www.iot-lab.info/earn/</a> is being used for the lamp ?</p>
<p>Also, regarding if I use the ssh keygen command and then the copy paste and the connect to the experiment is the only thing that I need to do in order to run my code properly??</p>
<p>Is there a good tutorial that explains those things because I did not find any ?</p>
",12/8/2020 21:06,,109,1,5,0,,,,,,65343909,"<p>If you are going to use IoT lab, you only can use the <code>Pycom FiPy</code>.</p>
<blockquote>
<p>Until my hardware arrives I want to test the code using iot-lab.
However, I do not know what nodes I have to use and why. How do I
choose the correct ones? Do I also need an .elf file for the sensor,
like in this tutorial <a href=""https://www.iot-lab.info/earn/"" rel=""nofollow noreferrer"">https://www.iot-lab.info/earn/</a> is being used for
the lamp ?</p>
</blockquote>
<p>Just follow the <a href=""https://doc.riot-os.org/getting-started.html"" rel=""nofollow noreferrer"">getting started guide.</a> That should do the job.
<code>make flash</code> is generating the elf and flashs it. I recommend the target <code>native</code> for development. <code>BOARD=native make flash term</code> in order to also see the serial output. It has a proper IP stack (when configured with <code>tapsetup</code>) and works well enough for development. At least for the networking part. When you need to interact with the hardware, you have to use the actual hardware, of course. IoTLab is only useful, if you have big routing simulations etc. IoTLab also doesn't have the water pump etc. you need.</p>
<blockquote>
<p>Is there a good tutorial that explains those things because I did not find any ?</p>
</blockquote>
<p>Checkout the <a href=""https://github.com/RIOT-OS/RIOT/tree/master/examples"" rel=""nofollow noreferrer"">RIOT OS examples</a>.
<code>gnrc networking</code> and <code>saul</code> are interesting for you, I guess.
You should also read the <a href=""https://riot-os.org/api/group__drivers__saul.html"" rel=""nofollow noreferrer"">SAUL documentation</a>.</p>
",4094489,0,0,115321406,"Are you sure they didn't mean that you're supposed to program riot in Linux, and then flash it on your esp32? With a Linux server running alongside of it?",,
203,3706,61711549,Solving kidnapped robot problem using particle or kalman filter,|ros|robotics|,"<p>I'm doing some research on navigation algorithms in ROS and I want to test kidnapped robot problem in gazebo. Looking on internet I saw the two solutions are particle and kalman filter. I know that amcl already implements particle filter and you can use kalman filter with this <a href=""http://wiki.ros.org/robot_pose_ekf"" rel=""nofollow noreferrer"">package</a>, but the problem with them is that amcl needs robot's initial position. So my question is does amcl realy solve the kidnapped robot problem and are there any other methods for solving this issue? </p>
",5/10/2020 11:56,61726423,928,1,0,0,,13381031,"Zagreb, Hrvatska",4/22/2020 12:23,26,61726423,"<p>AMCL doesn't need initial pose. When the initial pose is not given, it will initialize the particles uniformly across the map. After moving the robot enough distance, particle filter will converge to correct pose.</p>

<p>AMCL solves kidnapped robot problem by adding random particles. When the robot is kidnapped, number of random particles added will increase. Of the random particles, which are near the actual pose of the robot get highest weight and upon resampling, more particle will be added near the correct pose. After few sensor updates and resampling, pf will converge to actual pose of the robot.</p>

<p>There are many solutions proposed for kidnapped robot problem in research. Most of them use additional setup or additional sensors.</p>
",1595504,0,0,,,,
205,3639,60183911,Blue Prism - Not able to spy elements - Browser Firefox,|automation|robotics|blueprism|rpa|,"<p>I have an issue with spy Browser mode in Firefox.
I have designed a new RPA process on Dev machine where I have spy in Browser mode (in firefox) web page.
On my Dev machine the Browser mode is working good and the process runs very good.
The issue is on production machine where the modeler don't see the elements which have been spied on dev machine.</p>

<p>Actions so far done.
We have set all settings regarding Firefox to the same like on dev machine using the BP guide,
We have installed the same Firefox extension on prod like on dev machine.
We have set the same internet options.</p>

<p>Non of the actions have helped us to be able to spy elements in Firefox on prod machine.</p>

<p>My questions to the experts community:)
What else could have impact on the BP - Browser mode which is stopping BP to see elements?
What virtual machine settings need to be set/or what to check?</p>

<p>We have BP Version 6:
Application Manager 6.4.2.10610
.Net Framework 4.7
Firefox version 72.0.2
Blue Prism Browser Extension version 6.4.2.10610vycoxormiz (updated 30. Jan. 2020)</p>

<p>Thank you for your help!</p>
",2/12/2020 8:20,60274369,3299,1,0,2,,5739124,"Berlin, Deutschland",1/2/2016 21:31,34,60274369,"<p>I found the solution.</p>

<p>During investigation I came on this link</p>

<p>[<a href=""https://superuser.com/questions/719875/google-chrome-always-says-google-chrome-was-not-shut-down-properly][1]"">https://superuser.com/questions/719875/google-chrome-always-says-google-chrome-was-not-shut-down-properly][1]</a></p>

<p>In the location ""%UserProfile%\AppData\Local\Google\Chrome\User Data\Default\Preferences</p>

<p>I have changed the ""exit_type"": ""normal"" to ""exit_type"": ""standard"" and immediate the </p>

<p>Browser mode on production machine was working and we could run the process.</p>

<p>It seems that the chrome when it wasn't shut down properly has impact on Blue Prism.</p>

<p>Adding update.
I have noticed that each time Chrome opens it changes in the Preferences file the status to Normal.
I have fixed the issue in that way, that I have added additional logic to my solution:
1. I have copied the Preferences file to a different location and changed in the file the status to Standard.
2. I have added additional logic to my process where bot copy the file from the new location and replace the file in the Chrome location each time he runs.</p>

<p>Hope this will help others with similar issue.</p>

<p>Regards! </p>
",5739124,1,0,,,,
206,3728,62592629,What is this error ? It's an digital assistant,|python|stream|pyaudio|robotics|errno|,"<p>This is my code:</p>
<pre><code>import speech_recognition
import pyttsx3
from datetime import date, datetime

robot_ear = speech_recognition.Recognizer()
robot_mouth = pyttsx3.init()
robot_brain = &quot;&quot;

while True:
    with speech_recognition.Microphone() as mic:
        print(&quot;Robot: I'm Listening&quot;)
        audio = robot_ear.listen(mic)

    print(&quot;Robot:...&quot;)

    try:
        you = robot_ear.recognize_google(audio)
    except:
        you = &quot;&quot;
    print (&quot;You: &quot; + you)

    you = &quot;hello&quot;

    if you == &quot;&quot;:
        robot_brain = &quot;I can't hear you, try again!&quot;
    elif&quot;hello&quot; in you:
        robot_brain = &quot;Hello Huan&quot;
    elif &quot;today&quot; in you:
        today = date.today()
        robot_brain = today.strftime(&quot;%B %d, %Y&quot;)
    elif &quot;time&quot; in you:
        now = datetime.now()
        robot_brain = now.strftime(&quot;%H hours %M minutes %S seconds&quot;)
    elif &quot;president&quot; in you:
        robot_brain = &quot;Donald Trump&quot;
    elif &quot;bye&quot; in you:
        robot_brain = &quot;Bye Duong Gia Huan&quot;
        print(&quot;Robot: &quot; + robot_brain)
        robot_mouth.say(robot_brain)
        robot_mouth.runAndWait()
        break
    else:
        robot_brain = &quot;I'm fine, Thank you, and you ?&quot;

    print(&quot;Robot: &quot; + robot_brain)
    robot_mouth.say(robot_brain)
    robot_mouth.runAndWait()
</code></pre>
<p>when I run it, It's have an error like this :</p>
<pre><code>Robot: I'm Listening
Traceback (most recent call last):
  File &quot;trolyao.py&quot;, line 12, in &lt;module&gt;
    audio = robot_ear.listen(mic)
  File &quot;C:\Users\huana\AppData\Local\Programs\Python\Python38\lib\site-packages\speech_recognition\__init__.py&quot;, line 652, in listen
    buffer = source.stream.read(source.CHUNK)
  File &quot;C:\Users\huana\AppData\Local\Programs\Python\Python38\lib\site-packages\speech_recognition\__init__.py&quot;, line 161, in read
    return self.pyaudio_stream.read(size, exception_on_overflow=False)
  File &quot;C:\Users\huana\AppData\Local\Programs\Python\Python38\lib\site-packages\pyaudio.py&quot;, line 608, in read
    return pa.read_stream(self._stream, num_frames, exception_on_overflow)
OSError: [Errno -9999] Unanticipated host error

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;trolyao.py&quot;, line 12, in &lt;module&gt;
    audio = robot_ear.listen(mic)
  File &quot;C:\Users\huana\AppData\Local\Programs\Python\Python38\lib\site-packages\speech_recognition\__init__.py&quot;, line 151, in __exit__
    self.stream.close()
  File &quot;C:\Users\huana\AppData\Local\Programs\Python\Python38\lib\site-packages\speech_recognition\__init__.py&quot;, line 166, in close
    if not self.pyaudio_stream.is_stopped():
  File &quot;C:\Users\huana\AppData\Local\Programs\Python\Python38\lib\site-packages\pyaudio.py&quot;, line 543, in is_stopped
    return pa.is_stream_stopped(self._stream)
OSError: [Errno -9988] Stream closed
</code></pre>
<p>Help me fix it !!!I'm a begginer of Python !
<strong>Help me fix it, pls</strong></p>
<blockquote>
<p>Can you help me ?
I don't know what it is ?
I think we should fix the audio with the microphone, Right ?
<em>ntroduction to Python Programming language. Python is developed by Guido van Rossum. Guido van Rossum started implementing Python in 1989. Python is a very simple programming language so even if you are new to programming, you can learn python without facing any issues.</em></p>
</blockquote>
",6/26/2020 10:17,,381,1,0,-2,0,13795538,,6/23/2020 0:02,4,62592770,"<pre><code>r = sr.Recognizer()
with sr.Microphone() as source:
    r.pause_threshold = 1
    r.adjust_for_ambient_noise(source, duration=1)
    audio = r.listen(source)
try:
    print(&quot;Recognizing...&quot;)
    query = r.recognize_google(audio, language='en-us')
except Exception as e:
    print(&quot;Say that again please...&quot;)
    return &quot;None&quot;
return query
</code></pre>
<p>I don't know the reason behind the error  but i think you need add the above code. I guess it is not recognizing audio clearly so try to adjust the ambient noise. I hope it helps. I am a beginner too. Peace</p>
",13754401,0,0,,,,
207,3854,63766021,Callback fuction is not getting called,|c++|c++14|ros|robotics|gazebo-simu|,"<p>I am trying to implement ROS GotoGoal in c++, here is the code</p>
<pre class=""lang-cpp prettyprint-override""><code>#include &quot;ros/ros.h&quot;
#include &quot;geometry_msgs/Twist.h&quot;
#include &quot;geometry_msgs/Pose2D.h&quot;
#include &quot;turtlesim/Pose.h&quot;

class Turtle {

public :
  Turtle(int argc,char** argv){
    ros::init(argc,argv,&quot;mover&quot;);
    ros::NodeHandle n;
    pose = turtlesim::Pose();
    pub = n.advertise&lt;geometry_msgs::Twist&gt;(&quot;/turtle1/cmd_vel&quot;, 100);
    sub = n.subscribe(&quot;/turtle1/pose&quot;, 100, &amp;Turtle::Update, this);
  }

  void Update(const turtlesim::Pose::ConstPtr&amp; msg){
    ROS_INFO(&quot;Pose recieved : x = %f y = %f\n&quot;, msg-&gt;x, msg-&gt;y );
    pose = *msg;
  }

  void move2goal(){
    turtlesim::Pose goalPose= turtlesim::Pose() ;
    
    std::cout&lt;&lt;&quot;Enter goal x : &quot;&lt;&lt;&quot; &quot;;
    std::cin&gt;&gt;goalPose.x ;
    std::cout&lt;&lt;&quot;Enter goal y : &quot;&lt;&lt;&quot; &quot;;
    std::cin&gt;&gt;goalPose.y ;
    
    float d ;
    std::cout&lt;&lt;&quot;Enter distance tolerance d : &quot;&lt;&lt;&quot; &quot;;
    std::cin&gt;&gt;d ;
    
    auto vel_msg = geometry_msgs::Twist() ;
    ros::Rate loop_rate(2.0);
    while(distance(goalPose)&gt;=d &amp;&amp; ros::ok()){
      vel_msg.linear.x = linear_velocity(goalPose,1.5); 
      vel_msg.linear.y =0 ;
      vel_msg.linear.z = 0 ;
      
      vel_msg.angular.x = 0 ;
      vel_msg.angular.y= 0 ;
      vel_msg.angular.z = angular_velocity(goalPose,6) ;
      
      pub.publish(vel_msg) ;
      loop_rate.sleep() ; 
 ROS_INFO(&quot;current : %f %f\n&quot;,pose.x,pose.y) ;  
 }
    vel_msg.angular.z=0 ;
    vel_msg.linear.x =0 ;
    pub.publish(vel_msg) ;
    ros::spin();
  }

  ros::Publisher pub;
  ros::Subscriber sub;
  turtlesim::Pose pose;
  int ch = 0;
};

int main(int argc, char** argv) {
  Turtle turtle = Turtle(argc,argv);
  turtle.move2goal();
  return 0;
}
</code></pre>
<p>But the <strong>Update</strong> callback function is not getting called and the turtle is moving in a circle as pose is not getting updated. I tried using ROS_INFO for debugging the issue but nothing worked.
What am I doing wrong here?
<strong>Note</strong>: implementations of few functions have been removed from the code snippet due to stackoverflow's policy.</p>
<p>[Output][1]
[1]:https://i.stack.imgur.com/eL9Sr.png</p>
",9/6/2020 15:48,63769117,1334,1,5,0,,14133727,,8/19/2020 20:47,4,63769117,"<p>I think you misunderstood what the sleep does. Unlike <code>spin</code> it doesn't actually perform all the ROS communication events. It's just a convenience for accurate sleep. See <a href=""https://stackoverflow.com/questions/23227024/difference-between-spin-and-rate-sleep-in-ros"">Difference between spin and rate.sleep in ROS</a>.</p>
<p>Fortunately the fix is really easy, just add a <code>spinOnce</code>:</p>
<pre><code>while( distance(goalPose) &gt;= d &amp;&amp; ros::ok()) {
  // (..)
  ros::spinOnce();
  loop_rate.sleep();
}
</code></pre>
",1087119,1,2,112772470,"cin statements are there for getting the goal location (x,y) from the console. I added a debug statement in Update callback function and one at the end of the loop",,
208,3821,63271230,Can you dynamically create variables based on rows in an Excel file in C#?,|c#|dynamic|robotics|,"<p>I am trying to make a C# program that takes coordinates from an excel file in the sense that each column is x,y,z and r, respectively, and each row is a different point. I would like to be able to create variables in the format point0, point1, etc. depending on how many rows there are.</p>
<p>As of right now I am reading each cell into an Array, then manually creating points from that array. In this case there are 4 rows and 4 points (points 0 to 3). This works for now but I have to imagine there is a much easier way of doing this or at least something more dynamic. 4 points is not a big deal but there could be many more.</p>
<pre><code>        int rows = 4;

        for(int i = 0; i &lt; 4; i++)
        {
           for(int j = 0; j &lt; rows; j++)
           {
               points[i,j] = excel.ReadCell(i, j);
           }
        }
      
        for(int i = 0; i &lt; 4; i++)
        {
            point0[0, i] = points[0, i];
        }

        for (int i = 0; i &lt; 4; i++)
        {
            point1[1, i] = points[1, i];
        }

        for (int i = 0; i &lt; 4; i++)
        {
            point2[2, i] = points[2, i];
        }

        for (int i = 0; i &lt; 4; i++)
        {
            point3[3, i] = points[3, i];
        }
</code></pre>
<p>Even condensing the set of loops where the points are manually created would save time, I am just not sure if there is a way to say something such as</p>
<pre><code>       for(int i = 0; i &lt; rows; i++)
       {
           for(int j = 0; j &lt; cols; j++)
           {
               point+&quot;i&quot;[i,j] = points[i,j]
           }
       }
</code></pre>
<p>Where the ith iteration is concatenated to the variable name.</p>
<p>Any help would be greatly appreciated, and I am open to all recommendations (I am pretty new to C# if you can't tell)</p>
",8/5/2020 18:16,63272484,381,1,2,0,,14055717,,8/5/2020 17:56,2,63272484,"<p>I would define a class of what the combination of the four points mean.  Not based on the row or column that they're stored on but what they actually represent.</p>
<pre><code>public class Shape
{
    public int Start { get; set; }
    public int End { get; set; }
    public int Mean { get; set; }
    public int Median { get; set; }
}
</code></pre>
<p>Then as you loop through each row you can Create and add all four points at the same time.  Some thing like this.</p>
<p>public List GetShapesFromExcel()
{
var list = new List();</p>
<pre><code>        int StartColumn = 'x';
        int EndColumn = 'y';
        int MeanColumn = 'z';
        int MedianColumn = 'r';

        foreach (var row in workSheet)
        {
            var shape = new Shape()
            {
                Start = excel.ReadCell(row, StartColumn);
                End = excel.ReadCell(row, EndColumn);
                Mean = excel.ReadCell(row, MeanColumn);
                Median = excel.ReadCell(row, MedianColumn);
            };
        
        }
    return list;
    }
</code></pre>
<p>I'm taking a wild stab in the dark on what you're data actually represents but I would take the time to go ahead and spin that up into a real object so that it's easier to reason about as you're writing the code.</p>
<p>variables like ‘I’ &amp; ‘J’  Don't save enough time in this case to be useful.</p>
<p>last suggestion is to go ahead and check out the package EPPlus. that package has the ability to turn rows into structured classes</p>
<p>check this out to get started.
<a href=""https://stackoverflow.com/questions/33436525/how-to-parse-excel-rows-back-to-types-using-epplus"">How to parse excel rows back to types using EPPlus</a></p>
",3808982,0,0,111882898,"Rather than store these in variables whose names get larger numbers added, you can just store them in a list.",,
213,3626,59645243,simulating a 6 DOF robot in matlab robotic toolbox,|matlab|robotics|,"<p>I am new with matlab and its robotic toolbox. I am trying to simulate a simple model of a 6 DOF manipulator in matlab but i got this error. Here is my code:</p>

<pre><code>clc

startup_rvc;  
syms th1 th2 th3 th4 th5 
%//////////robot  d-h////////////
%%%%L= Link([ th d a alpha 'joint type'])%%%%
L(1) = Link([th1 0.1519 0 -pi/2 ]);
L(2) = Link([th2 0.1198 0.24365 0 ]);
L(3) = Link([th3 -0.0925 0.21325 0 ]);
L(4) = Link([th4 0.08505 0 -pi/2 ]);
L(5)= Link([th5 0.08535 0 pi/2]);
L(6)= Link([th6 0 0 0]);

robot = SerialLink(L,'name','surgicalarm');
q=[0 0 0 0 0];
robot.plot(q)
robot.teach();
</code></pre>

<p>but when I run this code I got this error:</p>

<pre><code>Error using SerialLink/plot (line 205)
Insufficient columns in q

Error in surgicalarm (line 16)
robot.plot(q)
</code></pre>

<p>Can anyone help me to fix this? Thanks.</p>
",1/8/2020 11:49,,796,1,0,0,,10546023,,10/23/2018 11:50,3,59800548,"<p>Since your robot has 6 DOF, I would expect <code>q</code> also have 6 columns instead of 5. </p>

<p>Try with <code>q = [0 0 0 0 0 0]</code> in your code.</p>
",6018272,2,0,,,,
219,4185,70326276,How to concatenate matrices with Math.Net. How to call for a particular row or column with Math.Net?,|c#|matrix|concatenation|robotics|mathnet-numerics|,"<p>How can I call for a particular row or column?</p>
<p>Lets say I have this 8 x 6 matrix and want to call only one row or one column and assign that to a new variable, How to go about this in c#.</p>
<p>Here is a piece of the code:</p>
<pre><code>//The Eight Solutions as one matrix
            Matrix&lt;double&gt; eightsols = DenseMatrix.OfArray(new double[,]
            {
            {theta1_1 * Degrees, theta2_1 * Degrees,  theta3_1 * Degrees, theta4_1 * Degrees, theta5_1 * Degrees, theta6_1 * Degrees},
            {theta1_1 * Degrees, theta2_2 * Degrees,  theta3_2 * Degrees, theta4_2 * Degrees, theta5_2 * Degrees, theta6_2 * Degrees},
            {theta1_2 * Degrees, theta2_3 * Degrees,  theta3_1 * Degrees, theta4_3 * Degrees, theta5_3 * Degrees, theta6_3 * Degrees},
            {theta1_2 * Degrees, theta2_4 * Degrees,  theta3_2 * Degrees, theta4_4 * Degrees, theta5_4 * Degrees, theta6_4 * Degrees},
            {theta1_1 * Degrees, theta2_1 * Degrees,  theta3_1 * Degrees, (theta4_1*Degrees) + Math.PI, -theta5_1 * Degrees, (theta6_1*Degrees) + Math.PI},
            {theta1_1 * Degrees, theta2_2 * Degrees,  theta3_2 * Degrees, (theta4_2*Degrees) + Math.PI, -theta5_2 * Degrees, (theta6_2*Degrees) + Math.PI},
            {theta1_2 * Degrees, theta2_3 * Degrees,  theta3_1 * Degrees, (theta4_3*Degrees) + Math.PI, -theta5_3 * Degrees, (theta6_3*Degrees) + Math.PI},
            {theta1_2 * Degrees, theta2_4 * Degrees,  theta3_2 * Degrees, (theta4_4*Degrees) + Math.PI, -theta5_4 * Degrees, (theta6_4*Degrees) + Math.PI}
            });
            Console.WriteLine(&quot;eightsols: &quot; + eightsols);
</code></pre>
<p><strong>Now, how do I get one of these Rows or column and assign to a variable?</strong></p>
<p>Secondly, Lets say I coded it differently and want to combine or concatenate a set of 1x6 matrix as an one 8x6, how can I do such in c#? I know how to do it in MATLAB, but getting a lot of errors when trying to rewrite my program in c#. <strong>Does anyone knows where to find a good documentation or book for MathNet.Numerics other than their website?</strong></p>
<p>Here is a potion of the code:</p>
<pre><code>//Solutions 1 to 4
            Matrix&lt;double&gt; Sol1 = DenseMatrix.OfArray(new double[,]
             {
             {theta1_1 * Degrees, theta2_1 * Degrees,  theta3_1 * Degrees, theta4_1 * Degrees, theta5_1 * Degrees, theta6_1 * Degrees }
             });
            Console.WriteLine(&quot;\nSol1: &quot; + Sol1);

            Matrix&lt;double&gt; Sol2 = DenseMatrix.OfArray(new double[,]
             {
             {theta1_1 * Degrees, theta2_2 * Degrees,  theta3_2 * Degrees, theta4_2 * Degrees, theta5_2 * Degrees, theta6_2 * Degrees }
             });
            Console.WriteLine(&quot;\nSol2: &quot; + Sol2);
</code></pre>
",12/12/2021 18:01,,235,1,0,1,,16920008,,9/15/2021 14:58,6,70363091,"<p>I decided to stick with the 1 x 6 matrices as is then place the equations in an 8 x 6 matrix.</p>
<pre class=""lang-cs prettyprint-override""><code>//Inverse kinematics solutions test 
//Solutions 1 to 4
Matrix&lt;double&gt; Sol1 = DenseMatrix.OfArray(new double[,]
{
    {theta1_1 * Degrees, theta2_1 * Degrees,  theta3_1 * Degrees, theta4_1 * Degrees, theta5_1 * Degrees, theta6_1 * Degrees }
});
Console.WriteLine(&quot;\nSol1: &quot; + Sol1);

Matrix&lt;double&gt; Sol2 = DenseMatrix.OfArray(new double[,]
{
    {theta1_1 * Degrees, theta2_2 * Degrees,  theta3_2 * Degrees, theta4_2 * Degrees, theta5_2 * Degrees, theta6_2 * Degrees }
});
Console.WriteLine(&quot;\nSol2: &quot; + Sol2);

Matrix&lt;double&gt; Sol3 = DenseMatrix.OfArray(new double[,]
{
    {theta1_2 * Degrees, theta2_3 * Degrees,  theta3_1 * Degrees, theta4_3 * Degrees, theta5_3 * Degrees, theta6_3 * Degrees }
});
Console.WriteLine(&quot;\nSol3: &quot; + Sol3);

Matrix&lt;double&gt; Sol4 = DenseMatrix.OfArray(new double[,]
{
    {theta1_2 * Degrees, theta2_4 * Degrees,  theta3_2 * Degrees, theta4_4 * Degrees, theta5_4 * Degrees, theta6_4 * Degrees }
});
Console.WriteLine(&quot;\nSol4: &quot; + Sol4);

// Solutions 5 to 8
Matrix&lt;double&gt; Sol5 = DenseMatrix.OfArray(new double[,]
{
    {theta1_1 * Degrees, theta2_1 * Degrees,  theta3_1 * Degrees, (theta4_1*Degrees) + Math.PI, -theta5_1 * Degrees, (theta6_1*Degrees) + Math.PI}
});
Console.WriteLine(&quot;\nSol5: &quot; + Sol5);

Matrix&lt;double&gt; Sol6 = DenseMatrix.OfArray(new double[,]
{
    {theta1_1 * Degrees, theta2_2 * Degrees,  theta3_2 * Degrees, (theta4_2*Degrees) + Math.PI, -theta5_2 * Degrees, (theta6_2*Degrees) + Math.PI}
});
Console.WriteLine(&quot;\nSol6: &quot; + Sol6);

Matrix&lt;double&gt; Sol7 = DenseMatrix.OfArray(new double[,]
{
    {theta1_2 * Degrees, theta2_3 * Degrees,  theta3_1 * Degrees, (theta4_3*Degrees) + Math.PI, -theta5_3 * Degrees, (theta6_3*Degrees) + Math.PI}
});
Console.WriteLine(&quot;\nSol7: &quot; + Sol7);

Matrix&lt;double&gt; Sol8 = DenseMatrix.OfArray(new double[,]
{
    {theta1_2 * Degrees, theta2_4 * Degrees,  theta3_2 * Degrees, (theta4_4*Degrees) + Math.PI, -theta5_4 * Degrees, (theta6_4*Degrees) + Math.PI}
});
Console.WriteLine(&quot;\nSol8: &quot; + Sol8);

//The Eight Solutions as one matrix
Matrix&lt;double&gt; eightsols = DenseMatrix.OfArray(new double[,]
{
    {theta1_1 * Degrees, theta2_1 * Degrees,  theta3_1 * Degrees, theta4_1 * Degrees, theta5_1 * Degrees, theta6_1 * Degrees},
    {theta1_1 * Degrees, theta2_2 * Degrees,  theta3_2 * Degrees, theta4_2 * Degrees, theta5_2 * Degrees, theta6_2 * Degrees},
    {theta1_2 * Degrees, theta2_3 * Degrees,  theta3_1 * Degrees, theta4_3 * Degrees, theta5_3 * Degrees, theta6_3 * Degrees},
    {theta1_2 * Degrees, theta2_4 * Degrees,  theta3_2 * Degrees, theta4_4 * Degrees, theta5_4 * Degrees, theta6_4 * Degrees},
    {theta1_1 * Degrees, theta2_1 * Degrees,  theta3_1 * Degrees, (theta4_1*Degrees) + Math.PI, -theta5_1 * Degrees, (theta6_1*Degrees) + Math.PI},
    {theta1_1 * Degrees, theta2_2 * Degrees,  theta3_2 * Degrees, (theta4_2*Degrees) + Math.PI, -theta5_2 * Degrees, (theta6_2*Degrees) + Math.PI},
    {theta1_2 * Degrees, theta2_3 * Degrees,  theta3_1 * Degrees, (theta4_3*Degrees) + Math.PI, -theta5_3 * Degrees, (theta6_3*Degrees) + Math.PI},
    {theta1_2 * Degrees, theta2_4 * Degrees,  theta3_2 * Degrees, (theta4_4*Degrees) + Math.PI, -theta5_4 * Degrees, (theta6_4*Degrees) + Math.PI}
});
Console.WriteLine(&quot;eightsols: &quot; + eightsols);

Console.ReadLine();
</code></pre>
<p><strong>Results (Got what I needed, most importantly) see photo</strong>
<a href=""https://i.stack.imgur.com/bCXxp.png"" rel=""nofollow noreferrer"">Click to view Results</a></p>
",16920008,0,0,,,,
221,3950,65759206,Modeling segway robot in simscape multibody,|matlab|simulink|robotics|simscape|,"<p>Hello guys I am trying to model a self balancing segway robot in Simscape multibody, but there is a problem that I can't see the effect of gravity on my model as I run it. I have checked the direction of the gravity and the mass of my bodies but it still does not work. The inputs of the system are the revolute joints torques which are going to be connected to a controller.</p>
<p><a href=""https://i.stack.imgur.com/m9P3L.png"" rel=""nofollow noreferrer"">The simscape model</a></p>
<p><a href=""https://i.stack.imgur.com/cKPK6.png"" rel=""nofollow noreferrer"">The robot configuration</a></p>
",1/17/2021 9:50,,213,0,2,0,,15023096,,1/17/2021 9:38,4,,,,,,116266251,"I am not 100% sure, since I do not know the contents of the 'wheels connecting rod' block, but I suspect the issue lies with the prismatic joint. This does not allow for rotation of the connector rod, so the pendulum cannot rotate. Is that indeed the case?",,
222,4166,70197548,How can I find angle between two turtles(agents) in a network in netlogo simulator?,|netlogo|simulator|angle|robotics|agent-based-modeling|,"<p>In a formation robots are linked with eachother,number of robots in a neighbourhood may vary. If one robot have 5 neighbours how can I find the angle of that one robot with its other neighbour?</p>
",12/2/2021 10:09,70198741,174,1,0,0,,16331927,"Paris, France",6/28/2021 7:28,10,70198741,"<p><em>(Following a comment, I replaced the sequence of</em> &lt;<code>face</code> <em>+ read</em> <code>heading</code>&gt; <em>with just using</em> <code>towards</code>, <em>wich I had overlooked as an option. For some reason the comment I am referring to has been deleted quickly so I don't know who gave the suggestion, but I read enough of it from the cell notification)</em></p>
<p>In NetLogo it is often possible to use turtles' <code>heading</code> to know degrees.</p>
<p>Since your agents are linked, a first thought could be to use <code>link-heading</code>, which directly <a href=""https://ccl.northwestern.edu/netlogo/docs/dictionary.html#link-heading"" rel=""nofollow noreferrer"">reports the heading in degrees from <em>end1</em> to <em>end2</em></a>.</p>
<p>However note that this might not be ideal: using <code>link-heading</code> will work spotlessly only if you are interested in knowing the heading from <em>end1</em> to <em>end2</em>, which means:</p>
<ul>
<li>If your links are <strong>directed</strong>, it reports the heading from the source to the target;</li>
<li>If your links are <strong>undirected</strong>, it reports the heading <a href=""https://ccl.northwestern.edu/netlogo/docs/programming.html#links"" rel=""nofollow noreferrer"">from the older turtle to the younger turtle</a>.</li>
</ul>
<p>If that's something that you are interested in, fine. But it might not be so! For example, if you have undirected links and are interested in knowing the angle from <code>turtle 1</code> to <code>turtle 0</code>, using <code>link-heading</code> will give you the wrong value:</p>
<pre><code>to setup
  clear-all

  create-turtles 2 [
   setxy random-xcor random-ycor
   set color black
   set label who
  ]

  ask turtle 0 [
   create-link-with turtle 1 
  ]
end


to go
  ask link 0 1 [
   show link-heading 
  ]
end
</code></pre>
<p><a href=""https://i.stack.imgur.com/azD0e.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/azD0e.png"" alt=""enter image description here"" /></a></p>
<p>... while we know, by looking at the two turtles' positions, that the degrees from <code>turtle 1</code> to <code>turtle 0</code> must be in the vicinity of 45.</p>
<p>An approach that better fits all possible cases is to directly look into the <code>heading</code> of the turtle you are interested in, regardless of the nature or direction of the link. You can let your reference turtle <code>face</code> the target turtle, and then read <code>heading</code> of the reference turtle. Or better: you can directly use <code>towards</code>, which reports just the same information but without having to make turtles actually change their heading. Copy and run the code below to see how this approach always gives the right answer!</p>
<pre><code>to setup
  clear-all

  create-turtles 5 [
   setxy random-xcor random-ycor
   set color black
   set label who
  ]
end


to go
  let reference sort turtles
  foreach reference [
   r -&gt;
   ask r [
     print &quot;---------------------------------------------------------------------------------------------------------------------------------------------&quot;
     let targets sort other turtles
     foreach targets [
       t -&gt;
       let direction (towards t)
       type &quot;I am &quot; type self type &quot;. The NetLogo angle between me and &quot; type t type &quot; is &quot; type direction type &quot;, while the normal mathematical angle is &quot; print heading-to-angle direction  
      ]
     print &quot;---------------------------------------------------------------------------------------------------------------------------------------------&quot;
    ] 
  ]
end


to-report heading-to-angle [ h ]
  report (90 - h) mod 360
end
</code></pre>
<p>In your case, the target group (that I have set just as <code>other turtles</code> in my brief example above) could be based on the actual links and so be constructed as <code>(list link-neighbors)</code> or <code>sort link-neighbors</code> (because if you want to use <code>foreach</code>, the agentset must be passed as a list - <a href=""https://ccl.northwestern.edu/netlogo/docs/dictionary.html#foreach"" rel=""nofollow noreferrer"">see here</a>).</p>
<p><strong>Update</strong>: I actually ended up also making a toy model that represents your case more closely, i.e. with links and using <code>link-neighbors</code>. See below:</p>
<pre><code>to setup
  clear-all

  create-turtles 100 [
   move-to one-of patches with [not any? turtles-here]
   set color black
   set label who
  ]

  ask n-of 2 turtles [
   create-links-with n-of 5 other turtles 
  ]
end


to go
  let reference-turtles sort turtles with [count my-links &gt; 2]
  foreach reference-turtles [
   r -&gt;
   ask r [
     print &quot;-----------------------------------------------------------------------------------------------------------------------------------------------&quot;
     let targets sort link-neighbors
     foreach targets [
       t -&gt;
       let direction (towards t)
       type &quot;I am &quot; type self type &quot;. The NetLogo angle between me and &quot; type t type &quot; is &quot; type direction type &quot;, while the normal mathematical angle is &quot; print heading-to-angle direction
      ]
     print &quot;-----------------------------------------------------------------------------------------------------------------------------------------------&quot;
    ] 
  ]
end


to-report heading-to-angle [ h ]
  report (90 - h) mod 360
end
</code></pre>
<p>Final note: you surely noticed the <code>heading-to-angle</code> procedure, taken directly from the <code>atan</code> entry <a href=""https://ccl.northwestern.edu/netlogo/docs/dictionary.html#atan"" rel=""nofollow noreferrer"">here</a>. It is a useful way to convert degrees expressed in the NetLogo geometry (where North is 0 and East is 90) to degrees expressed in the usual mathematical way (where North is 90 and East is 0). I don't know what degrees you're interested in, so it's worth to leave this hint here.</p>
",12391423,4,0,,,,
227,3960,65929472,How to use Drake with deep reinforcement learning,|machine-learning|deep-learning|simulation|robotics|drake|,"<p>Does drake have a pipeline/platform with which I can implement deep reinforcement learning algorithms?</p>
",1/28/2021 0:43,,195,1,0,0,,15095362,,1/28/2021 0:39,3,65930488,"<p>Drake is frequently used as an environment in open-source deep RL frameworks; it should plug in nicely.  You can find some examples by searching the web, and would welcome more public examples.</p>
<p>(We will likely make a tutorial for it on the main drake site once we move the tutorials to their own repo so that they can have additional dependencies).</p>
",9510020,0,0,,,,
229,4037,67972978,Is there an easy way to find out which of two frames is closer to the root in a Multibody plant?,|collision|robotics|drake|motion-planning|,"<p>I am working on recovering from collision. I have the names of bodies in collision and the frames associated with them and now I want to move the body/frame that is closer to the end effector to get out of collision, but I couldn't find a straightforward way to get this information from a <code>MultiBodyPlant</code>. I could construct another representation of the graph and search through it, but I was wondering if it is possible to maybe get this from <code>drake</code> instead?</p>
<p>The problem is that sometimes the robot ends up in collision with itself or the environment and I want to make a plan to recover it.
From the <code>QueryObject</code>, I am able to get a <code>vector&lt;SignedDistancePair&gt;</code> that gives me the geometry IDs of object instances collision, and unit vector pointing in the direction of fastest increase in collision depth
Then I use a <code>SceneGraphInspector</code> to get the corresponding frame IDs and then use the frame IDs to get the bodies in collision
For now I make the assumption that only two bodies are in collision
Now that I have the two bodies in collision, I want to find the one that is closer to the end effector and is therefore easier to move out of collision</p>
",6/14/2021 15:20,67973688,76,1,1,0,,3138875,"Zürich, Switzerland",12/27/2013 8:42,46,67973688,"<p>Wow.  I think you're right that we don't make this one easy (but we should).</p>
<p>For now, I would think you can call <code>MultibodyPlant::GetJointIndices()</code> and then loop the joints via <code>MultibodyPlant::get_joint()</code> to find the joint <code>Joint::child_body() == collision_body</code>, and then use <code>Joint::parent_body()</code>.  And we can open an issue if avoiding that (small?) linear search becomes important?</p>
",9510020,1,2,120168627,"Workflow question: Why are you doing this via indexing rather than using distances and gradients? (is it b/c distances / gradients are ill-conditioned when trying to recover from ""in-collision""?)",,
231,4015,67265287,Distance covered by wheels and the angle it turns by (physics and robotics),|c|rotation|physics|robotics|webots|,"<p>Let me try and describe what I'm working on.</p>
<p>I'm trying to code the controller for a vacuum cleaning robot. The entire project is done within the Webots platform. How it works is that when the robot hits a wall, it turns by a random angle and continues cleaning the house.</p>
<p>The issue: what I found was that the amount it turns by is inaccurate. If I specify for it to turn by 90 degrees, it only turns by 60. If I specify for it to turn by 360 degrees, it only turns by 240 or so (visual guesstimation).</p>
<p>I found a workaround - a temporary workaround. I found that multiplying the angle by a certain amount (1.2275) will turn the angle very close to the real angle specified in the question. So if I specify the angle to turn as 180, and multiply that with 1.2275, I get a value that is probably 175 or 177 (again from visual guesstimation).</p>
<p>The problem is that this error stacks up over time. So initially it might be an offset of just 2-3 degrees, which is nothing, but over 30 minutes of running the simulation it might build to relatively high numbers. This is unacceptable.</p>
<p>I asked a friend and he suggested that I scale the offset as the simulation progresses so that the offset scales with the error, but I feel this is a temporary workaround.</p>
<p>I would appreciate some help with the physics of it all as I don't really understand or know much about the mechanical field it works in. Any help would be appreciated.</p>
<p>Provided below is the code for the turn function.</p>
<pre><code>static void turn(double angle) {
    double l_offset = wb_position_sensor_get_value(left_position_sensor);
    double r_offset = wb_position_sensor_get_value(right_position_sensor);
    double orient;

    stop();

    angle = angle * ANGLE_OFFSET;   //the important part
    double neg = (angle &lt; 0.0) ? -1.0 : 1.0;

    step();

    wb_motor_set_velocity(left_motor,   neg * HALF_SPEED);
    wb_motor_set_velocity(right_motor, -neg * HALF_SPEED);

    do {
        double l  = wb_position_sensor_get_value(left_position_sensor) - l_offset;
        double r  = wb_position_sensor_get_value(right_position_sensor) - r_offset;
        double dl = l * WHEEL_RADIUS;  // distance covered by left wheel in meter
        double dr = r * WHEEL_RADIUS;  // distance covered by right wheel in meter

        orient = neg * (dl - dr) / AXLE_LENGTH; // delta orientation in radian

        step();
    } while (orient &lt; neg * angle);

    stop();
    step();
}
</code></pre>
<p>Let me provide some explanation for the code. <code>angle</code> obviously specifies the angle to turn the robot by.  The second line shows the workaround that I'm currently using. Reading through the next few lines you can see that the robot turns by moving the left wheel backwards, and the right wheel forwards. It continuously gets the distance covered by the wheels and checks for a condition. When that condition is true, it stops turning.</p>
<p>My guess is that there might be something wrong in <code>orient = neg * (dl - dr) / AXLE_LENGTH; // delta orientation in radian</code> this line. Any tips? Maybe someone who can verify if the physics/mechanics aspect of it is correct?</p>
",4/26/2021 10:50,,227,1,4,0,,12775244,"Chennai, Tamil Nadu, India",1/24/2020 12:21,4,67269337,"<p>Can you segment the cause of the discrepancy between the desired angle and the actual (visual) angle?  I seems diving in and fixing that would be a better use of time than patching a patch.</p>
",15762880,0,3,118896613,is your sample period infinite?,,
235,3953,65831042,Computed Torque Control with a Simscape Manipulator,|matlab|simulink|robotics|manipulators|simscape|,"<p>I'm trying to realize a Vertical Two-Arm Planar Manipulator (Double Pendulum like) using Simscape Multibody. I'm kinda newbie in Robotics field and I'm not even sure if I did well the following creation.
I realized this Manipulator using 2 Spherical Joints (Actuation: Torque Provided by input, abilitating Velocity Sensors) e 2 arms (1 meter-long Brick Blocks).
I don't exactly know how to realize the Computed Torque Control: I know the theory behind it but I couldn't find any examples on how to implement it in Simulink through Matlab Functions, especially in a case like this. How can I realize a PD controller from a System created on Simscape Multibody?</p>
<p><a href=""https://i.stack.imgur.com/dY9NH.png"" rel=""nofollow noreferrer"">Simscape Model I created</a></p>
<p><a href=""https://i.stack.imgur.com/eyxy3.png"" rel=""nofollow noreferrer"">The Results</a></p>
",1/21/2021 15:55,,340,0,6,1,,15053293,,1/21/2021 15:33,4,,,,,,116442170,"For Computed Torque control you have modeled the robot, and calculated what torque you have to put on each joint. Realizing this torque value can be done using a PD controller, as said previously. There are multiple ways to obtain the values for the P and D gain. A common method is trial-and-error in which the values are tweaked such that an acceptable performance is reached. Optimization is another method. The Ackermann method is different, since it is based on the pole-placement principle.",,
241,4445,74280132,How to get the transformation between two body frames in PyDrake,|python|transformation|robotics|drake|pose-estimation|,"<p>I have two frames at different joint locations of IIWA arm, using
<code>f1 = plant.GetFrameByName(&quot;iiwa_link_0&quot;, kuka_model) f2 = plant.GetFrameByName(&quot;iiwa_link_2&quot;, kuka_model)</code></p>
<p>I want to find the transformation between these two body frames(f1, f2).</p>
<p>Getting error when using f1.CalcPoseInBodyFrame() :</p>
<p><code>TypeError: CalcPoseInBodyFrame(): incompatible function arguments. The following argument types are supported: 1. (self: pydrake.multibody.tree.Frame_[float], context: pydrake.systems.framework.Context_[float]) -&gt; pydrake.math.RigidTransform_[float]</code></p>
<p>What is the correct way to approach this problem?</p>
<p>Thanks,
Sarvesh</p>
",11/1/2022 18:13,74280393,160,1,0,1,,4329907,"Irvine, California",12/5/2014 18:30,38,74280393,"<p>It would be helpful if you post the actual code giving you the error.</p>
<p>Worst case, you can do this:</p>
<pre><code>context = ... # assuming you have a context where things are posed.

f1 = plant.GetFrameByName(&quot;iiwa_link_0&quot;, kuka_model)
f2 = plant.GetFrameByName(&quot;iiwa_link_2&quot;, kuka_model)
X_F2F1 = f1.CalcPose(context, f2)
</code></pre>
",7686256,4,2,,,,
242,4453,74339346,DiagramBuilder: Cannot operate on ports of System plant until it has been registered using AddSystem,|robotics|drake|,"<p>I have an issue working with <code>DiagramBuilder</code> and <code>ManipulationStation</code> classes.
It appears to me, that c++ API and the python bindings work differently in my case.
C++ API behaves as expected, while the python bindings result in the runtime error:</p>
<p><code>DiagramBuilder: Cannot operate on ports of System plant until it has been registered using AddSystem</code></p>
<h2>How I use C++ API</h2>
<ul>
<li>In one of the <code>ManipulationStation::Setup...()</code> methods I inject a block of code, that adds an extra manipuland</li>
</ul>
<pre><code>const std::string sdf_path = FindResourceOrThrow(&quot;drake/examples/manipulation_station/models/bolt_n_nut.sdf&quot;);
RigidTransform&lt;double&gt; X_WC(RotationMatrix&lt;double&gt;::Identity(), Vector3d(0.0, -0.3, 0.1));
bolt_n_nut_ = internal::AddAndWeldModelFrom(sdf_path, &quot;nut_and_bolt&quot;, lant_-&gt;world_frame(), &quot;bolt&quot;, X_WC, plant_);
</code></pre>
<ul>
<li>I inject another block of code into the method <code>ManipulationStation::Finalize</code>:</li>
</ul>
<pre><code>auto zero_torque = builder.template AddSystem&lt;systems::ConstantVectorSource&lt;double&gt;&gt;(Eigen::VectorXd::Zero(plant_-&gt;num_velocities(bolt_n_nut_)));
builder.Connect(zero_torque-&gt;get_output_port(), plant_-&gt;get_actuation_input_port(bolt_n_nut_));
</code></pre>
<p>With these changes, the simulation runs as expected.</p>
<h2>How I use python bindings</h2>
<pre><code>plant = station.get_multibody_plant()
manipuland_path = get_manipuland_resource_path()
bolt_with_nut = Parser(plant=plant).AddModelFromFile(manipuland_path)
X_WC = RigidTransform(RotationMatrix.Identity(), [0.0, -0.3, 0.1])
plant.WeldFrames(plant.world_frame(), plant.GetFrameByName('bolt', bolt_with_nut), X_WC)
</code></pre>
<p>...</p>
<pre><code>station.Finalize()
zero_torque = builder.AddSystem(ConstantValueSource(AbstractValue.Make([0.])))
builder.Connect(zero_torque.get_output_port(), plant.get_actuation_input_port(bolt_with_nut_model))
</code></pre>
<p>This triggers a <code>RuntimeError</code> with a message as above; The port, which causes this error is <code>nut_and_bolt_actuation</code>.</p>
<p>My vague understanding of the problem is the (in) visibility of <code>nut_and_bolt</code> System, due to having two distinct <code>DiagramBuilder</code>s in a process: 1) a one is inside <code>ManipulationStation</code> 2) another is in the python code, that instantiates this <code>ManipulationStation</code> object.</p>
<p>Using <code>ManipulationStation</code> via python bindings is a preference for me, because that way I would've avoided depending on a custom build of drake library.</p>
<p>Thanks for your insight!</p>
",11/6/2022 20:06,74339555,75,1,0,2,,1912514,,12/18/2012 10:14,72,74339555,"<p>I agree with your assessment: you have two different <code>DiagramBuilder</code> objects here.  This does not have anything to due with C++ or Python; the <code>ManipulationStation</code> is itself a <code>Diagram</code> (created using its own <code>DiagramBuilder</code>), and you have a second <code>DiagramBuilder</code> (in either c++ or python) that is connecting the <code>ManipulationStation</code> together with other elements. You are trying to connect a system that is in the external diagram to a port that is in the internal diagram, but is not exposed.</p>
<p>The solution would be to have the <code>ManipulationStation</code> diagram expose the extra nut and bolt actuation port so that you can connect to it from the second builder.</p>
<p>If you prefer Python, I've switched my course to using a <a href=""https://github.com/RussTedrake/manipulation/blob/d1c4056e1029fb8fe451828a88cdf5b2868d7bcf/manipulation/scenarios.py#L476"" rel=""nofollow noreferrer"">completely python version of the manipulation station</a>. I find this version is much easier to adapt to different student projects.  (To be clear, the setup is in python, but at simulation time all of the elements are c++ and it doesn't call back to python; so the performance is almost identical.)</p>
",9510020,3,0,,,,
243,4444,74279453,Getting joint location(3D pose) w.r.t world of Kuka Iiwa arm in Drake,|python|transformation|robotics|drake|,"<p>I have a Kuka Iiwa 7 arm in the scene and I am interested in getting the current(real-time) 3D coordinates of the arm joints w.r.t world (or w.r.t robot base). I am able to get the current joint 1 DOF positions(thetas) but not the exact 3D coordinate representing the position of the joint in some frame(world or base).</p>
<p>I tried adding Triad for each link and tried get_pose_in_world() method for RigidBody_[float] type object of the frame, but received an error that the method does not exist for RigidBody_[float] class.</p>
<p>Note: I am using PyDrake for the project.</p>
<p>What is the correct way to approach this problem?</p>
",11/1/2022 17:12,74281479,149,1,0,1,,4329907,"Irvine, California",12/5/2014 18:30,38,74281479,"<p>When the parser creates a joint, it'll also crate a &quot;parent&quot; frame and a &quot;child&quot; frame. These two frames are then constrained by the joint such that in the &quot;zero configuration&quot; (say zero angle) the parent and child frames are coincident.</p>
<p>If you do happen to have the frame object, then you can call <code>Frame::CalcPoseInWorld()</code>. Now, to retrieve the frame, you need to know its name. For that you will call <code>GetFrameByName()</code>.</p>
<p>Say you want the pose of the parent frame. Here is the convention used to name frames:</p>
<ul>
<li>If the pose of the parent frame in the body frame is the identity (either a default or explicitly provided) then the parent frame IS the body frame and you can retrieve it with the body's name.</li>
<li>If the pose is not the identity, then a new frame is created (offset by that pose) and its name is &quot;parent_[body_name]&quot;, where <code>body_name</code> is the name of the parent body.</li>
</ul>
",1889975,1,0,,,,
247,4416,73880798,Drake: Integrate Mass Matrix and Bias Term in Optimization Problem,|c++|robotics|nonlinear-optimization|drake|dynamicparameters|,"<p>I am trying to implement Non Linear MPC for a 7-DOF manipulator in drake. To do this, in my constraints, I need to have dynamic parameters like the Mass matrix M(q) and the bias term C(q,q_dot)*q_dot, but those depend on the decision variables q, q_dot.</p>
<p>I tried the following</p>
<pre><code>    // finalize plant
    // create builder, diagram, context, plant context
    ...

    // formulate optimazation problem
    drake::solvers::MathematicalProgram prog;

    // create decision variables
    ...
    std::vector&lt;drake::solvers::VectorXDecisionVariable&gt; q_v;
    std::vector&lt;drake::solvers::VectorXDecisionVariable&gt; q_ddot;

    for (int i = 0; i &lt; H; i++) {
        q_v.push_back(prog.NewContinuousVariables&lt;14&gt;(state_var_name));
        q_ddot.push_back(prog.NewContinuousVariables&lt;7&gt;(input_var_name));
    }

    // add cost
    ...

    // add constraints
    ...
    for (int i = 0; i &lt; H; i++) {
        plant.SetPositionsAndVelocities(*plant_context, q_v[i]);
        plant.CalcMassMatrix(*plant_context, M);
        plant.CalcBiasTerm(*plant_context, C_q_dot);
    }

    ...
    
    for (int i = 0; i &lt; H; i++) {
        prog.AddConstraint( M * q_ddot[i] + C_q_dot + G &gt;= lb );
        prog.AddConstraint( M * q_ddot[i] + C_q_dot + G &lt;= ub );
    }

    // solve prog
    ...
    
</code></pre>
<p>The above code will not work, because <code>plant.SetPositionsAndVelocities(.)</code> doesn't accept symbolic variables.</p>
<p>Is there any way to integrate M,C in my ocp constraints ?</p>
",9/28/2022 11:41,73884081,85,1,0,2,,20109781,,9/28/2022 10:39,3,73884081,"<p>I think you want to impose the following nonlinear nonconvex constraint</p>
<pre><code>lb &lt;= M * qddot + C(q, v) + g(q) &lt;= ub
</code></pre>
<p>This constraint is non-convex. We will need to solve it through nonlinear optimization, and evaluate the constraint in every iteration of the nonlinear optimization. We can't do this evaluation using symbolic computation (it would be horribly slow with symbolic computation).</p>
<p>So you will need a constraint evaluator, something like this</p>
<pre class=""lang-cc prettyprint-override""><code>// This constraint takes [q;v;vdot] and evaluate
// M * vdot + C(q, v) + g(q)
class MyConstraint : public solvers::Constraint {
 public:
  MyConstraint(const MultibodyPlant&lt;AutoDiffXd&gt;&amp; plant, systems::Context&lt;AutoDiffXd&gt;* context, const Eigen::Ref&lt;const Eigen::VectorXd&gt;&amp; lb, const Eigen::Ref&lt;const Eigen::VectorXd&gt;&amp; ub) : solvers::Constraint(plant.num_velocitiex(), plant.num_positions() + 2 * plant.num_velocities(), lb, ub), plant_{plant}, context_{context} {
  ...
  }

 private:
  void DoEval(const Eigen::Ref&lt;const AutoDiffVecXd&gt;&amp; x, AutoDiffVecXd* y) const {
    ...
  }
  
  MultibodyPlant&lt;AutoDiffXd&gt; plant_;
  systems::Context&lt;AutoDiffXd&gt;* context_;
};

int main() {
...
// Construct the constraint and add it to every time instances
std::vector&lt;std::unique_ptr&lt;systems::Context&lt;AutoDiffXd&gt;&gt;&gt; plant_contexts;
for (int i = 0; i &lt; H; ++i) {
 
 plant_contexts.push_back(plant.CreateDefaultContext());
  prog.AddConstraint(std::make_shared&lt;MyConstraint&gt;(plant, plant_context[i], lb, ub), {q_v[i], qddot[i]});
}
}
</code></pre>
<p>You could refer to the class <a href=""https://github.com/RobotLocomotion/drake/blob/e20e8e60763a3989e241af5f11a36f93f80022b1/multibody/optimization/centroidal_momentum_constraint.h#L11-L21"" rel=""nofollow noreferrer"">CentroidalMomentumConstraint</a> on how to construct your own <code>MyConstraint</code> class.</p>
",1973861,2,3,,,,
251,4225,71324192,"How to get an array into a 28 rows x 28 columns grid which can be used as an (x,y) coordinate system?",|python|c++|coordinates|simulation|robotics|,"<p>I am inexperienced at this level of Python but need to complete it for a team project.</p>
<p>I currently have 28 arrays consisting of 28 data points inside each array which will look similar to this but way longer:</p>
<pre><code>grid =[[&quot;c&quot;,&quot;c&quot;,&quot;c&quot;,&quot;c&quot;,&quot;c&quot;,&quot;c&quot;,&quot;o&quot;,&quot;o&quot;,&quot;-&quot;,&quot;-&quot;,&quot;o&quot;,&quot;-&quot;,&quot;-&quot;,&quot;o&quot;,&quot;-&quot;,&quot;o&quot;,&quot;o&quot;,&quot;-&quot;,&quot;o&quot;,&quot;o&quot;,&quot;o&quot;,&quot;-&quot;,&quot;-&quot;,&quot;-&quot;,&quot;o&quot;,&quot;o&quot;,&quot;o&quot;,&quot;o&quot;]]
</code></pre>
<p>My goal is to get this into a grid-looking format and then use each data point like an (x,y) coordinate system so I can move an object through them using vector equations. Basically simulating the motion I want a robot to follow.</p>
<p>Grid visual example:</p>
<pre><code>[ . . . . . . . . . . .] 
[ . . . . . . . . . . . ]
continues...
</code></pre>
<p>Any guidance will be much appreciated, please!</p>
<p>I welcome results in Python and C++.</p>
<p>Thank you!</p>
",3/2/2022 14:16,,130,2,3,-1,,18354441,,3/2/2022 13:39,11,71324585,"<p>Here is an example</p>
<pre><code>#include &lt;array&gt;
#include &lt;iostream&gt;

// create a reuable alias for an array of an array of values
// for this example it will be a 5x5 grid.
using grid_t = std::array&lt;std::array&lt;char, 5&gt;, 5&gt;;

// pass grid by const reference
// So C++ will not copy the grid (pass by value)
// and the const means show_grid can't modify the content.
void show_grid(const grid_t&amp; grid)
{
    // use a range based for loop to loop over the rows in the grid
    for (const auto&amp; row : grid)
    {
        // use another to loop over the characters in a row
        for (const auto c : row) std::cout &lt;&lt; c;
        std::cout &lt;&lt; &quot;\n&quot;;
    }
}

int main()
{
    // setup a grid
    grid_t grid
    { {
        { 'a', '-' ,'o', 'a', 'a' },
        { '-', 'o' ,'o', 'a', 'a' },
        { 'o', 'a' ,'-', 'a', 'o' },
        { 'o', 'a' ,'-', '-', 'o' },
        { 'a', '-' ,'o', '-', 'a' }
    } };

    show_grid(grid);
    return 0;
}
</code></pre>
",16649550,1,0,126072618,Search example - `python print grid site:stackoverflow.com`,,
253,4333,72386899,Arc length of curve from data points in Python,|python|numpy|scipy|sympy|robotics|,"<p>I'm working on a robot simulation and trying to calculate the robot's distance from the goal along some planned trajectory. The trajectory curves to avoid obstacles, and it is given by a list of coordinates. To find progress, I need to find the arc length from the current position to the goal. I'm familiar with the equation for arc length of a function: <a href=""https://i.stack.imgur.com/8VUtL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8VUtL.png"" alt=""enter image description here"" /></a></p>
<p>The approach I was planning to use was creating a polynomial approximation of the function of the trajectory from the data points using NumPy's polynomial.polyfit, then finding its derivative, squaring that, adding 1, taking the square root, and finally integrating. However, the square root of a polynomial doesn't always exist, so this method wouldn't always work.</p>
<p>Is there some better way to approach this? I'm familiar with numerical integration, but not sure if/how it can be applied to this problem.</p>
<p>EDIT: Figured out how to do this numerically, which is much faster. Compute numerical derivative using numpy.gradient/numpy.diff, plug each element in that derivative into sqrt(1 + (dy/dx)^2), then use numpy.trapz/scipy.integrate.simpson to compute integral numerically.</p>
",5/26/2022 5:02,,2143,1,1,0,,15918774,,5/13/2021 17:31,15,72388886,"<p>How will your robot move from one point to another?</p>
<p>If it is a straight line it suffices to do</p>
<p><code>np.sum(np.sqrt(np.diff(x)**2 + np.diff(y)**2))</code></p>
<p>If not you should first figure out what the path your robot will follow.
Then having those equations you can integrate analytically, or sampling points in the curve. For smooth paths the error on the size tends to be <code>O(1/n^2)</code> where n is the number of points you use in your interpolation.</p>
",12750353,1,3,127878550,[This](https://stackoverflow.com/questions/44962794/how-to-integrate-arc-lengths-using-python-numpy-and-scipy) Appears Relevant~,,
254,4451,74295544,Communicating using receiver node in webots,|robotics|receiver|webots|emitter|,"<p>I want to implement an emitter robot and a receiver robot in webots. I have written following code.</p>
<pre><code>#include &lt;webots/robot.h&gt;
#include &lt;webots/receiver.h&gt;

#include &lt;stdio.h&gt;
#include &lt;math.h&gt;

#define TIME_STEP 64

int main(int argc, char **argv) {
  /* necessary to initialize webots stuff */
  wb_robot_init();
  WbDeviceTag rx = wb_robot_get_device(&quot;receiver&quot;);
  wb_receiver_enable(rx, 64);
  printf(&quot;Receiver sampling period: %d&quot;,wb_receiver_get_sampling_period(rx));
  
  while (wb_robot_step(TIME_STEP) != -1) {
    
    if (wb_receiver_get_queue_length(rx) &gt; 0) {
      const char *message = wb_receiver_get_data(rx);
      const double *dir = wb_receiver_get_emitter_direction(rx);
      double signal = wb_receiver_get_signal_strength(rx);
      printf(&quot;received: %s (signal=%g, dir=[%g %g %g])\n&quot;,
             message, signal, dir[0], dir[1], dir[2]);
      wb_receiver_next_packet(rx);
    }
  };

  /* Enter your cleanup code here */

  /* This is necessary to cleanup webots resources */
  wb_robot_cleanup();

  return 0;
}

</code></pre>
<p>It compiles successfully. But when I execute it, it generate following result,</p>
<p>Error: wb_receiver_enable(): invalid device tag.
Error: wb_receiver_get_sampling_period(): invalid device tag.</p>
<p>How can I fix this error?</p>
<p>I want to receive the message emitted by emitter</p>
",11/2/2022 20:55,74306154,162,1,0,1,,14958157,"Kandy, Sri Lanka",1/7/2021 10:48,19,74306154,"<p>This error shows up because there is no device named <code>&quot;receiver&quot;</code> in your <code>Robot</code> node. What kind of robot are using? If it is a <code>Robot</code> node defined in the world file (<code>.wbt</code>), you should add to the <code>children</code> list of your <code>Robot</code> node a <code>Receiver</code> node named <code>&quot;receiver&quot;</code> (which is the default name). If you are using a PROTO-based robot, like <code>E-Puck</code> or <code>Nao</code>, you should check whether the proto file already contains a <code>Receiver</code> node and use its <code>name</code> instead of <code>&quot;receiver&quot;</code>. If it doesn't contain any <code>Receiver</code> node, you should be able to add a <code>Receiver</code> node in some <code>extensionSlot</code> of the robot.</p>
<p>The very same principle applies to the <code>Emitter</code> node as well.</p>
",810268,0,0,,,,
261,4322,72271060,"how to create an arc path from 3 points(x, y, z) in plane?",|matlab|geometry|controls|tracking|robotics|,"<p>I want to create an arc trajectory cross over n=3 points P(n)=(x, y, z), I decided to draw a circle over 3 points in plane. so I have center, radius, theta (angle in x, y plane) and phi(angle around z axis), and I know the position of 3 points (x, y, z), How can I extract an arc between p1 , p2 and p3 from this circle? I implemented this program in MATLAB..
Thanks a lot.</p>
",5/17/2022 8:58,72271694,599,1,3,1,,17172183,,10/17/2021 4:51,11,72271694,"<p>This answer on math.stackexchange gives a nice simple formulation for finding the circle centre (and therefore the radius)</p>
<p><a href=""https://math.stackexchange.com/a/2755842/283393"">3D coordinates of circle center given three point on the circle. (@Sergio G.)</a></p>
<p>From this other helpful math.stackexchange answer we can define any point on that circle in terms of the centre and two (non-colinear) points from the original 3.</p>
<p><a href=""https://math.stackexchange.com/a/2375120/283393"">Parametric equation of a circle in 3D given center and two points on the circle? (@milbrandt)</a></p>
<p>Finally we need the 3 angles of your 3 points to define the arcs, which can be done with <code>atan2</code> and the component vectors created in the other steps.</p>
<p>The full commented code is below, which yields this plot, and functions to compute the circle angle for any 3D point, then the value on the circumference for any angle.</p>
<p><a href=""https://i.stack.imgur.com/4IZwI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4IZwI.png"" alt=""circle plot 3D"" /></a></p>
<pre><code>% Original points
p1 = [1;0;2];
p2 = [0;0;0];
p3 = [1;2;2];
P = [p1,p2,p3];

% Get circle definition and 3D planar normal to it
p0 = getCentre(p1,p2,p3);
r = norm( p0 - p1 );
[n0,idx] = getNormal(p0,p1,p2,p3);
% Vectors to describe the plane
q1 = P(:,idx(1));
q2 = p0 + cross(n0,(p1-p0).').';
% Function to compute circle point at given angle
fc = @(a) p0 + cos(a).*(q1-p0) + sin(a).*(q2-p0);
% Get angles of the original points for the circle formula
a1 = angleFromPoint(p0,p1,q1,q2);
a2 = angleFromPoint(p0,p2,q1,q2);
a3 = angleFromPoint(p0,p3,q1,q2);
% Plot
figure(1); clf; hold on;
args = {'markersize',20,'displayname'};
plot3( P(1,:), P(2,:), P(3,:), '.', args{:}, 'Original Points' ); 
plot3( p0(1), p0(2), p0(3), '.k', args{:}, 'Centre' );   
plotArc(fc,a1,a2); % plot arc from p1 to p2
plotArc(fc,a2,a3); % plot arc from p2 to p3
plotArc(fc,a3,a1); % plot arc from p3 to p1
grid on; legend show; view(-50,40);

function ang = angleFromPoint(p0,p,q1,q2)
    % Get the circle angle for point 'p'
    comp = @(a,b) dot(a,b)/norm(b);
    ang = atan2( comp(p-p0,q2-p0), comp(p-p0,q1-p0) );
end
function plotArc(fc,a,b)
    % Plot circle arc between angles 'a' and 'b' for circle function 'fc'
    while a &gt; b
        a = a - 2*pi; % ensure we always go from a to b
    end
    aa = linspace( a, b, 100 );
    c = fc(aa);
    plot3( c(1,:), c(2,:), c(3,:), '.r', 'markersize', 5, 'handlevisibility', 'off' );
end
function p0 = getCentre(p1,p2,p3)
    % Get centre of circle defined by 3D points 'p1','p2','p3'
    v1 = p2 - p1;
    v2 = p3 - p1;

    v11 = dot( v1.', v1 );
    v22 = dot( v2.', v2 );
    v12 = dot( v1.', v2 );

    b = 1/(2*(v11*v22-v12^2));
    k1 = b * v22 * (v11-v12);
    k2 = b * v11 * (v22-v12);

    p0 = p1 + k1*v1 + k2*v2;
end
function [n0,idx] = getNormal(p0,p1,p2,p3)
    % compute all 3 normals in case two points are colinear with centre
    n12 = cross((p1 - p0),(p2 - p0));
    n23 = cross((p3 - p0),(p2 - p0));
    n13 = cross((p3 - p0),(p1 - p0));

    n = [n12,n23,n13];
    n = n./sign(n(1,:));
    idx = find(~all(isnan(n)),2);
    n = n(:,idx(1));
    n0 = n / norm(n);
end
</code></pre>
",3978545,2,3,127682453,Could you maybe show what you already tried in matlab? Or what you have so far? Also what kind of output are you looking for? A function? A custom class? A vector of points on your arc? A graph?,,
262,4576,76081863,Docker Image with cuda and ROS2 on Ubuntu 22.04,|docker|ubuntu|dockerfile|robotics|ros2|,"<p>I am trying desperately to setup a Docker image based on nvidia/cudagl image and add ROS2 humble distro on it.  The reason is theat I need this ENV for future Gazebo simulations with the NVIDIA GPU Capabilities.</p>
<p><strong>The problem is that the official nvidia/cudagl image is based on ubuntu 20.04 and ROS2 Humble requires ubuntu 22.04 so I can't build the image properly</strong></p>
<p>anyone knows **how can I make a docker image that is based on cudagl-ubuntu 20.04  and modify it to be a layer on top of ubuntu 22.04 ** so I will get something like this in my image layers</p>
<p><a href=""https://i.stack.imgur.com/wfkiy.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>i basically followed this article its just that's iit a bit outdated <a href=""https://roboticseabass.com/2021/04/21/docker-and-ros/"" rel=""nofollow noreferrer"">text</a></p>
<p>If you have other suggestions  or can guide me to source code of simillar <strong>dockerfile</strong> it would be great
tnxxxx :))</p>
<p>I tried using this offcial Nvidia docker file and as my base image</p>
<pre><code>FROM nvidia/cudagl:11.4.2-base-ubuntu20.04
</code></pre>
<p>and adding ROS2 humble in source installation on ubuntu 22.04 (my machine)</p>
<p>but it doesnt work  also tried building cudagl image from bae of ubutu 22.04 image just to serve as base for further adding a layer of ros2 - but this also cann't be built</p>
<pre><code># Use Ubuntu 22.04 as the base image
FROM ubuntu:22.04 as base

FROM base as base-amd64

ENV NVARCH x86_64

ENV NVIDIA_REQUIRE_CUDA &quot;cuda&gt;=11.4 brand=tesla,driver&gt;=418,driver&lt;419 brand=tesla,driver&gt;=450,driver&lt;451&quot;
ENV NV_CUDA_CUDART_VERSION 11.4.108-1
ENV NV_CUDA_COMPAT_PACKAGE cuda-compat-11-4

FROM base as base-arm64

ENV NVARCH sbsa
ENV NVIDIA_REQUIRE_CUDA &quot;cuda&gt;=11.4&quot;
ENV NV_CUDA_CUDART_VERSION 11.4.108-1

FROM base-${TARGETARCH}

ARG TARGETARCH

LABEL maintainer &quot;NVIDIA CORPORATION &lt;cudatools@nvidia.com&gt;&quot;

RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \
    gnupg2 curl ca-certificates &amp;&amp; \
    curl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/${NVARCH}/3bf863cc.pub | apt-key add - &amp;&amp; \
    echo &quot;deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/${NVARCH} /&quot; &gt; /etc/apt/sources.list.d/cuda.list &amp;&amp; \
    apt-get purge --autoremove -y curl \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

ENV CUDA_VERSION 11.4.2

# For libraries in the cuda-compat-* package: https://docs.nvidia.com/cuda/eula/index.html#attachment-a
RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \
    cuda-cudart-11-4=${NV_CUDA_CUDART_VERSION} \
    ${NV_CUDA_COMPAT_PACKAGE} \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

# Required for nvidia-docker v1
RUN echo &quot;/usr/local/nvidia/lib&quot; &gt;&gt; /etc/ld.so.conf.d/nvidia.conf \
    &amp;&amp; echo &quot;/usr/local/nvidia/lib64&quot; &gt;&gt; /etc/ld.so.conf.d/nvidia.conf

ENV PATH /usr/local/nvidia/bin:/usr/local/cuda/bin:${PATH}
ENV LD_LIBRARY_PATH /usr/local/nvidia/lib:/usr/local/nvidia/lib64

# Install OpenGL packages
RUN dpkg --add-architecture i386 \
    &amp;&amp; apt-get update &amp;&amp; apt-get install -y --no-install-recommends \
        pkg-config \
        libglvnd-dev libglvnd-dev:i386 \
        libgl1-mesa-dev libgl1-mesa-dev:i386 \
        libegl1-mesa-dev libegl1-mesa-dev:i386 \
        libgles2-mesa-dev libgles2-mesa-dev:i386 \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

COPY NGC-DL-CONTAINER-LICENSE /

# nvidia-container-runtime
ENV NVIDIA_VISIBLE_DEVICES all
ENV NVIDIA_DRIVER_CAPABILITIES compute,utility

</code></pre>
",4/22/2023 20:47,,2275,1,1,2,,21709575,,4/22/2023 20:27,6,76339964,"<p>I have run into the same issue. The following solution is working under the WSL2 system with Ubuntu22.04 running Docker Desktop 4.19.0 (106363). I have not tested on real Ubuntu but I should have fewer problems than I encountered in finding this solution.</p>
<p>From <a href=""https://github.com/microsoft/WSL/issues/7507#issuecomment-950235017"" rel=""nofollow noreferrer"">this issue answer on Github</a> that redirects to <a href=""https://github.com/microsoft/wslg/blob/main/samples/container/Containers.md"" rel=""nofollow noreferrer"">this official MS guide here</a>, the following base Dockefile that I slightly modified to add the latest Cuda capabilities is capable of rendering OpenGL apps:</p>
<pre><code>FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04 as runtime

ARG DEBIAN_FRONTEND=noninteractive

# Uncomment the lines below to use a 3rd party repository
# to get the latest (unstable from mesa/main) mesa library version
RUN apt-get update &amp;&amp; apt install -y software-properties-common
RUN add-apt-repository ppa:oibaf/graphics-drivers -y

RUN apt update &amp;&amp; apt install -y \
    vainfo \
    mesa-va-drivers \
    mesa-utils

ENV LIBVA_DRIVER_NAME=d3d12
ENV LD_LIBRARY_PATH=/usr/lib/wsl/lib
CMD vainfo --display drm --device /dev/dri/card0
</code></pre>
<p>Then, you can install ROS2 from apt.
Regarding additional env vars:
<code>NVIDIA_VISIBLE_DEVICES</code> should be already set to all and <code>NVIDIA_DRIVER_CAPABILITIES</code> to <code>compute,utility</code>.</p>
<p>You may want to set:</p>
<pre><code>ENV NVIDIA_DRIVER_CAPABILITIES \
${NVIDIA_DRIVER_CAPABILITIES:+$NVIDIA_DRIVER_CAPABILITIES,}graphics,video
</code></pre>
<p>or</p>
<pre><code>ENV NVIDIA_DRIVER_CAPABILITIES all
</code></pre>
<p>Then with the following, you control the device for the hw acceleration:
Nvidia card (similarly for AMD):</p>
<pre><code>ENV MESA_D3D12_DEFAULT_ADAPTER_NAME=NVIDIA
</code></pre>
<p>or integrated Intel (if supported):</p>
<pre><code>ENV MESA_D3D12_DEFAULT_ADAPTER_NAME=Intel
</code></pre>
<p>or software, which actually for me gives faster FPS probably due to WSL2 (my system) not being optimized:</p>
<pre><code>ENV LIBGL_ALWAYS_SOFTWARE=1
</code></pre>
",5128680,1,0,134254448,"What system are you doing this on?  If Jetson then https://github.com/dusty-nv/jetson-containers/blob/master/Dockerfile.ros.humble has the code to build ros2 humble on an Ubuntu 20.04 desktop or base image.  You could merge that with cudagl or just use their image. It gets complicated though when you add other requirements (gazebo, navigation2 etc) for which there are no binaries.",,
264,4618,76477450,Mobile Robot doesn't reach nav goal within goal tolerances in nav2,|robotics|ros2|,"<p>I have found no resources on this online (or I'm just that bad at searching) so I thought I could get this answered here.</p>
<p>I have a differential drive robot that I am simulating in nvidia's Isaac sim. I'm using the navigation2 stack in order for the mobile robot to navigate its surroundings and pick up objects within the environment. Everything works fine except for one minor detail.</p>
<p>I noticed in testing that the mobile robot does not completely reach the navigation goals that are set for it (i.e. the controller claims to have reached its goal when the base_frame of the robot fails to reach the set goal point).</p>
<p>Obviously this is generally fine and understandable since the controller must prevent the robot from oscillating around the goal point, but after checking positions several times I realized that the robot doesn't even reach the goal within the specified tolerance of the goal checker (currently set at 1mm).</p>
<p>Here is the navigation parameters file:</p>
<pre><code>amcl:
  ros__parameters:
    use_sim_time: True
    alpha1: 0.2
    alpha2: 0.2
    alpha3: 0.2
    alpha4: 0.2
    alpha5: 0.2
    base_frame_id: &quot;base_link&quot;
    beam_skip_distance: 0.5
    beam_skip_error_threshold: 0.9
    beam_skip_threshold: 0.3
    do_beamskip: false
    global_frame_id: &quot;map&quot;
    lambda_short: 0.1
    laser_likelihood_max_dist: 2.0
    laser_max_range: 100.0
    laser_min_range: 0.4
    laser_model_type: &quot;likelihood_field&quot;
    max_beams: 60
    max_particles: 2000
    min_particles: 500
    odom_frame_id: &quot;odom&quot;
    pf_err: 0.05
    pf_z: 0.99
    recovery_alpha_fast: 0.0
    recovery_alpha_slow: 0.0
    resample_interval: 1
    robot_model_type: &quot;nav2_amcl::DifferentialMotionModel&quot;
    save_pose_rate: 0.5
    sigma_hit: 0.2
    tf_broadcast: true
    transform_tolerance: 1.0
    update_min_a: 0.2
    update_min_d: 0.25
    z_hit: 0.5
    z_max: 0.05
    z_rand: 0.5
    z_short: 0.05
    scan_topic: scan
    map_topic: map
    set_initial_pose: true
    always_reset_initial_pose: false
    first_map_only: false
    initial_pose:
      x: 0.0
      y: 0.0
      z: 0.0
      yaw: 0.0

amcl_map_client:
  ros__parameters:
    use_sim_time: True

amcl_rclcpp_node:
  ros__parameters:
    use_sim_time: True
bt_navigator:
  ros__parameters:
    use_sim_time: True
    global_frame: map
    robot_base_frame: base_link
    odom_topic: /odom
    bt_loop_duration: 20
    default_server_timeout: 40
    # 'default_nav_through_poses_bt_xml' and 'default_nav_to_pose_bt_xml' are use defaults:
    # nav2_bt_navigator/navigate_to_pose_w_replanning_and_recovery.xml
    # nav2_bt_navigator/navigate_through_poses_w_replanning_and_recovery.xml
    # They can be set here or via a RewrittenYaml remap from a parent launch file to Nav2.
    plugin_lib_names:
    - nav2_compute_path_to_pose_action_bt_node
    - nav2_compute_path_through_poses_action_bt_node
    - nav2_smooth_path_action_bt_node
    - nav2_follow_path_action_bt_node
    - nav2_spin_action_bt_node
    - nav2_wait_action_bt_node
    - nav2_back_up_action_bt_node
    - nav2_drive_on_heading_bt_node
    - nav2_clear_costmap_service_bt_node
    - nav2_is_stuck_condition_bt_node
    - nav2_goal_reached_condition_bt_node
    - nav2_goal_updated_condition_bt_node
    - nav2_globally_updated_goal_condition_bt_node
    - nav2_is_path_valid_condition_bt_node
    - nav2_initial_pose_received_condition_bt_node
    - nav2_reinitialize_global_localization_service_bt_node
    - nav2_rate_controller_bt_node
    - nav2_distance_controller_bt_node
    - nav2_speed_controller_bt_node
    - nav2_truncate_path_action_bt_node
    - nav2_truncate_path_local_action_bt_node
    - nav2_goal_updater_node_bt_node
    - nav2_recovery_node_bt_node
    - nav2_pipeline_sequence_bt_node
    - nav2_round_robin_node_bt_node
    - nav2_transform_available_condition_bt_node
    - nav2_time_expired_condition_bt_node
    - nav2_path_expiring_timer_condition
    - nav2_distance_traveled_condition_bt_node
    - nav2_single_trigger_bt_node
    - nav2_is_battery_low_condition_bt_node
    - nav2_navigate_through_poses_action_bt_node
    - nav2_navigate_to_pose_action_bt_node
    - nav2_remove_passed_goals_action_bt_node
    - nav2_planner_selector_bt_node
    - nav2_controller_selector_bt_node
    - nav2_goal_checker_selector_bt_node
    - nav2_controller_cancel_bt_node
    - nav2_path_longer_on_approach_bt_node
    - nav2_wait_cancel_bt_node
    - nav2_spin_cancel_bt_node
    - nav2_back_up_cancel_bt_node
    - nav2_drive_on_heading_cancel_bt_node

bt_navigator_rclcpp_node:
  ros__parameters:
    use_sim_time: True

controller_server:
  ros__parameters:
    use_sim_time: True
    controller_frequency: 20.0
    min_x_velocity_threshold: 0.001
    min_y_velocity_threshold: 0.5
    min_theta_velocity_threshold: 0.001
    failure_tolerance: 0.3
    progress_checker_plugin: &quot;progress_checker&quot;
    goal_checker_plugin: [&quot;general_goal_checker&quot;]
    controller_plugins: [&quot;FollowPath&quot;]

    # Progress checker parameters
    progress_checker:
      plugin: &quot;nav2_controller::SimpleProgressChecker&quot;
      required_movement_radius: 0.5
      movement_time_allowance: 10.0
    # Goal checker parameters
    general_goal_checker:
      plugin: &quot;nav2_controller::SimpleGoalChecker&quot;
      xy_goal_tolerance: 0.001
      yaw_goal_tolerance: 0.001
      stateful: true
    FollowPath:
      # Rotation Shim Controller parameters
      plugin: &quot;nav2_rotation_shim_controller::RotationShimController&quot;
      primary_controller: &quot;nav2_mppi_controller::MPPIController&quot;
      angular_dist_threshold: 0.785
      forward_sampling_distance: 2.0
      rotate_to_heading_angular_vel: 1.8
      max_angular_accel: 3.2
      simulate_ahead_time: 1.0
      # DWB parameters
      time_steps: 56
      model_dt: 0.05
      batch_size: 2000
      vx_std: 0.2
      vy_std: 0.2
      wz_std: 0.4
      vx_max: 0.5
      vx_min: -0.35
      vy_max: 0.5
      wz_max: 1.9
      iteration_count: 1
      prune_distance: 1.7
      transform_tolerance: 0.1
      temperature: 0.3
      gamma: 0.015
      motion_model: &quot;DiffDrive&quot;
      visualize: false
      reset_period: 1.0 # (only in Humble)
      TrajectoryVisualizer:
        trajectory_step: 5
        time_step: 3
      AckermannConstrains:
        min_turning_r: 0.2
      critics: [&quot;ConstraintCritic&quot;, &quot;ObstaclesCritic&quot;, &quot;GoalCritic&quot;, &quot;GoalAngleCritic&quot;, &quot;PathAlignCritic&quot;, &quot;PathFollowCritic&quot;, &quot;PathAngleCritic&quot;, &quot;PreferForwardCritic&quot;]
      ConstraintCritic:
        enabled: true
        cost_power: 1
        cost_weight: 4.0
      GoalCritic:
        enabled: true
        cost_power: 1
        cost_weight: 5.0
        threshold_to_consider: 1.0
      GoalAngleCritic:
        enabled: true
        cost_power: 1
        cost_weight: 3.0
        threshold_to_consider: 0.4
      PreferForwardCritic:
        enabled: true
        cost_power: 1
        cost_weight: 5.0
        threshold_to_consider: 0.4
      ObstaclesCritic:
        enabled: true
        cost_power: 1
        repulsion_weight: 1.5
        critical_weight: 20.0
        consider_footprint: false
        collision_cost: 10000.0
        collision_margin_distance: 0.1
        near_goal_distance: 0.5
        inflation_radius: 0.55 # (only in Humble)
        cost_scaling_factor: 10.0 # (only in Humble)
      PathAlignCritic:
        enabled: true
        cost_power: 1
        cost_weight: 14.0
        max_path_occupancy_ratio: 0.05
        trajectory_point_step: 3
        threshold_to_consider: 0.40
        offset_from_furthest: 20
      PathFollowCritic:
        enabled: true
        cost_power: 1
        cost_weight: 5.0
        offset_from_furthest: 5
        threshold_to_consider: 0.6
      PathAngleCritic:
        enabled: true
        cost_power: 1
        cost_weight: 2.0
        offset_from_furthest: 4
        threshold_to_consider: 0.40
        max_angle_to_furthest: 1.0

controller_server_rclcpp_node:
  ros__parameters:
    use_sim_time: True

local_costmap:
  local_costmap:
    ros__parameters:
      update_frequency: 10.0
      publish_frequency: 10.0
      global_frame: odom
      robot_base_frame: base_link
      use_sim_time: True
      rolling_window: True
      width: 10
      height: 10
      resolution: 0.05
      robot_radius: 0.5
      plugins: [&quot;obstacle_layer&quot;, &quot;inflation_layer&quot;, &quot;js_layer&quot;]
      inflation_layer:
        plugin: &quot;nav2_costmap_2d::InflationLayer&quot;
        cost_scaling_factor: 0.4
        inflation_radius: 0.25
      obstacle_layer:
        plugin: &quot;nav2_costmap_2d::ObstacleLayer&quot;
        enabled: True
        observation_sources: scan
        scan:
          topic: /scan
          max_obstacle_height: 2.0
          clearing: True
          marking: True
          data_type: &quot;LaserScan&quot;
      js_layer:
        plugin: &quot;jackstand_objective_costmap_plugin::JSLayer&quot;
        enabled: true
        costmap_resolution: 0.05
        target_clear_radius: 0.5
      always_send_full_costmap: True
  local_costmap_client:
    ros__parameters:
      use_sim_time: True
  local_costmap_rclcpp_node:
    ros__parameters:
      use_sim_time: True

global_costmap:
  global_costmap:
    ros__parameters:
      update_frequency: 10.0
      publish_frequency: 10.0
      global_frame: map
      robot_base_frame: base_link
      use_sim_time: True
      rolling_window: True
      width: 200
      height: 200
      robot_radius: 0.5
      resolution: 0.05
      # origin_x: -100.0
      # origin_y: -100.0
      plugins: [&quot;static_layer&quot;, &quot;obstacle_layer&quot;, &quot;inflation_layer&quot;, &quot;js_layer&quot;]
      obstacle_layer:
        plugin: &quot;nav2_costmap_2d::ObstacleLayer&quot;
        enabled: True
        observation_sources: scan
        scan:
          topic: /scan
          max_obstacle_height: 2.0
          clearing: True
          marking: True
          data_type: &quot;LaserScan&quot;
          raytrace_max_range: 10.0
          raytrace_min_range: 0.0
          obstacle_max_range: 10.0
          obstacle_min_range: 0.0
      static_layer:
        plugin: &quot;nav2_costmap_2d::StaticLayer&quot;
        map_subscribe_transient_local: True
      inflation_layer:
        plugin: &quot;nav2_costmap_2d::InflationLayer&quot;
        cost_scaling_factor: 3.0
        inflation_radius: 0.55
      js_layer:
        plugin: &quot;jackstand_objective_costmap_plugin::JSLayer&quot;
        enabled: true
        costmap_resolution: 0.05
        target_clear_radius: 0.5
      always_send_full_costmap: True
  global_costmap_client:
    ros__parameters:
      use_sim_time: True
  global_costmap_rclcpp_node:
    ros__parameters:
      use_sim_time: True

map_server:
  ros__parameters:
    use_sim_time: True
    yaml_filename: &quot;test_map_occupancy_params.yaml&quot;

map_saver:
  ros__parameters:
    use_sim_time: True
    save_map_timeout: 5000
    free_thresh_default: 0.25
    occupied_thresh_default: 0.65
    map_subscribe_transient_local: True

planner_server:
  ros__parameters:
    expected_planner_frequency: 20.0
    use_sim_time: True
    planner_plugins: [&quot;GridBased&quot;]
    GridBased:
      plugin: &quot;nav2_theta_star_planner/ThetaStarPlanner&quot;
      how_many_corners: 8
      w_euc_cost: 1.0
      w_traversal_cost: 2.0
      w_heuristic_cost: 1.0

planner_server_rclcpp_node:
  ros__parameters:
    use_sim_time: True

smoother_server:
  ros__parameters:
    use_sim_time: True
    smoother_plugins: [&quot;simple_smoother&quot;]
    simple_smoother:
      plugin: &quot;nav2_smoother::SimpleSmoother&quot;
      tolerance: 1.0e-10
      max_its: 1000
      do_refinement: True

behavior_server:
  ros__parameters:
    costmap_topic: local_costmap/costmap_raw
    footprint_topic: local_costmap/published_footprint
    cycle_frequency: 5.0
    behavior_plugins: [&quot;spin&quot;, &quot;backup&quot;, &quot;drive_on_heading&quot;, &quot;wait&quot;]
    spin:
      plugin: &quot;nav2_behaviors/Spin&quot;
    backup:
      plugin: &quot;nav2_behaviors/BackUp&quot;
    drive_on_heading:
      plugin: &quot;nav2_behaviors/DriveOnHeading&quot;
    wait:
      plugin: &quot;nav2_behaviors/Wait&quot;
    global_frame: odom
    robot_base_frame: base_link
    transform_tolerance: 0.2
    use_sim_time: true
    simulate_ahead_time: 2.0
    max_rotational_vel: 1.0
    min_rotational_vel: 0.4
    rotational_acc_lim: 3.2

robot_state_publisher:
  ros__parameters:
    use_sim_time: True

waypoint_follower:
  ros__parameters:
    loop_rate: 20
    stop_on_failure: false
    waypoint_task_executor_plugin: &quot;wait_at_waypoint&quot;
    wait_at_waypoint:
      plugin: &quot;nav2_waypoint_follower::WaitAtWaypoint&quot;
      enabled: True
      waypoint_pause_duration: 200
</code></pre>
<p>I have messed with the settings for hours now and have seen no change in the behavior of the mobile robot. Setting the goal_tolerance lower than the default doesn't change anything with respect to how far the robot stops away from its goal.</p>
<p>I figured that setting the tolerances in the goal_checker would make it so that the robot would need to move closer to the navigation goal before being satisfied but that isn't the case.</p>
<p>Does anyone know what I'm missing? Or know how I can get the robot to move it's base_link closer to the navigation goal?</p>
<hr />
<p>Update:</p>
<p>So it looks like nav2 isn't even pulling any of the goal_checker_plugin parameters. I set the xy_goal_tolerance much higher than the default (1 meter) and it had no effect either. Any idea why it's not using these?</p>
",6/14/2023 20:51,,399,0,0,0,,21591458,,4/7/2023 19:17,8,,,,,,,,,
267,4590,76258775,OpenAI-Gym Mojoco Walker2d-v4 model global cordinates error,|python|reinforcement-learning|robotics|openai-gym|mujoco|,"<p>I get the error</p>
<pre><code>ValueError: XML Error: global coordinates no longer supported. To convert existing models, load and save them in MuJoCo 2.3.3 or older
</code></pre>
<p>When i try to load the <code>walker2d-v4</code> model even with <code>**kwargs</code>.</p>
<pre><code>env = gym.make(
    'Walker2d-v4',
    # xml_file=&quot;walker2d.xml&quot;,
    forward_reward_weight=1.0,
    ctrl_cost_weight=1e-3,
    healthy_reward=1.0,
    terminate_when_unhealthy=True,
    healthy_z_range=(0.8, 2),
    healthy_angle_range=(-1, 1),
    reset_noise_scale=5e-3,
    exclude_current_positions_from_observation=True
)
</code></pre>
<p>Anyone face similar issues?</p>
<hr />
<p>Here's the solution to this, thanks to @yuval for pointing it out.</p>
<p>you need to load the <code>walker2d.xml</code> with this error into a model. For this you need to install mujoco 2.3.3 and then run the following code,</p>
<pre><code>model = mujoco.MjModel.from_xml_path('walker2d.xml')
mujoco.viewer.launch(model)
</code></pre>
<p>This should open up a mujoco viewer window that has an option to &quot;save XML&quot; - this should save the updated xml file as <code>mjmodel.xml</code> in the current dir. just copy paste that into <code>walker2d.xml</code> and your gym code should now be working.</p>
<h3>Walker2D XML that worked for me</h3>
<pre class=""lang-xml prettyprint-override""><code>&lt;mujoco model=&quot;walker2d&quot;&gt;
  &lt;compiler angle=&quot;radian&quot; autolimits=&quot;true&quot;/&gt;
  &lt;option integrator=&quot;RK4&quot;/&gt;
  &lt;default class=&quot;main&quot;&gt;
    &lt;joint limited=&quot;true&quot; armature=&quot;0.01&quot; damping=&quot;0.1&quot;/&gt;
    &lt;geom conaffinity=&quot;0&quot; friction=&quot;0.7 0.1 0.1&quot; rgba=&quot;0.8 0.6 0.4 1&quot;/&gt;
  &lt;/default&gt;
  &lt;asset&gt;
    &lt;texture type=&quot;skybox&quot; builtin=&quot;gradient&quot; rgb1=&quot;0.4 0.5 0.6&quot; rgb2=&quot;0 0 0&quot; width=&quot;100&quot; height=&quot;600&quot;/&gt;
    &lt;texture type=&quot;cube&quot; name=&quot;texgeom&quot; builtin=&quot;flat&quot; mark=&quot;cross&quot; rgb1=&quot;0.8 0.6 0.4&quot; rgb2=&quot;0.8 0.6 0.4&quot; markrgb=&quot;1 1 1&quot; width=&quot;127&quot; height=&quot;762&quot;/&gt;
    &lt;texture type=&quot;2d&quot; name=&quot;texplane&quot; builtin=&quot;checker&quot; rgb1=&quot;0 0 0&quot; rgb2=&quot;0.8 0.8 0.8&quot; width=&quot;100&quot; height=&quot;100&quot;/&gt;
    &lt;material name=&quot;MatPlane&quot; texture=&quot;texplane&quot; texrepeat=&quot;60 60&quot; specular=&quot;1&quot; shininess=&quot;1&quot; reflectance=&quot;0.5&quot;/&gt;
    &lt;material name=&quot;geom&quot; texture=&quot;texgeom&quot; texuniform=&quot;true&quot;/&gt;
  &lt;/asset&gt;
  &lt;worldbody&gt;
    &lt;geom name=&quot;floor&quot; size=&quot;40 40 40&quot; type=&quot;plane&quot; conaffinity=&quot;1&quot; material=&quot;MatPlane&quot; rgba=&quot;0.8 0.9 0.8 1&quot;/&gt;
    &lt;light pos=&quot;0 0 1.3&quot; dir=&quot;0 0 -1&quot; directional=&quot;true&quot; cutoff=&quot;100&quot; exponent=&quot;1&quot; diffuse=&quot;1 1 1&quot; specular=&quot;0.1 0.1 0.1&quot;/&gt;
    &lt;body name=&quot;torso&quot; pos=&quot;0 0 1.25&quot; gravcomp=&quot;0&quot;&gt;
      &lt;joint name=&quot;rootx&quot; pos=&quot;0 0 -1.25&quot; axis=&quot;1 0 0&quot; limited=&quot;false&quot; type=&quot;slide&quot; armature=&quot;0&quot; damping=&quot;0&quot;/&gt;
      &lt;joint name=&quot;rootz&quot; pos=&quot;0 0 -1.25&quot; axis=&quot;0 0 1&quot; limited=&quot;false&quot; type=&quot;slide&quot; ref=&quot;1.25&quot; armature=&quot;0&quot; damping=&quot;0&quot;/&gt;
      &lt;joint name=&quot;rooty&quot; pos=&quot;0 0 0&quot; axis=&quot;0 1 0&quot; limited=&quot;false&quot; armature=&quot;0&quot; damping=&quot;0&quot;/&gt;
      &lt;geom name=&quot;torso_geom&quot; size=&quot;0.05 0.2&quot; type=&quot;capsule&quot; friction=&quot;0.9 0.1 0.1&quot;/&gt;
      &lt;camera name=&quot;track&quot; pos=&quot;0 -3 -0.25&quot; quat=&quot;0.707107 0.707107 0 0&quot; mode=&quot;trackcom&quot;/&gt;
      &lt;body name=&quot;thigh&quot; pos=&quot;0 0 -0.2&quot; gravcomp=&quot;0&quot;&gt;
        &lt;joint name=&quot;thigh_joint&quot; pos=&quot;0 0 0&quot; axis=&quot;0 -1 0&quot; range=&quot;-2.61799 0&quot;/&gt;
        &lt;geom name=&quot;thigh_geom&quot; size=&quot;0.05 0.225&quot; pos=&quot;0 0 -0.225&quot; type=&quot;capsule&quot; friction=&quot;0.9 0.1 0.1&quot;/&gt;
        &lt;body name=&quot;leg&quot; pos=&quot;0 0 -0.7&quot; gravcomp=&quot;0&quot;&gt;
          &lt;joint name=&quot;leg_joint&quot; pos=&quot;0 0 0.25&quot; axis=&quot;0 -1 0&quot; range=&quot;-2.61799 0&quot;/&gt;
          &lt;geom name=&quot;leg_geom&quot; size=&quot;0.04 0.25&quot; type=&quot;capsule&quot; friction=&quot;0.9 0.1 0.1&quot;/&gt;
          &lt;body name=&quot;foot&quot; pos=&quot;0.2 0 -0.35&quot; gravcomp=&quot;0&quot;&gt;
            &lt;joint name=&quot;foot_joint&quot; pos=&quot;-0.2 0 0.1&quot; axis=&quot;0 -1 0&quot; range=&quot;-0.785398 0.785398&quot;/&gt;
            &lt;geom name=&quot;foot_geom&quot; size=&quot;0.06 0.1&quot; pos=&quot;-0.1 0 0.1&quot; quat=&quot;0.707107 0 -0.707107 0&quot; type=&quot;capsule&quot; friction=&quot;0.9 0.1 0.1&quot;/&gt;
          &lt;/body&gt;
        &lt;/body&gt;
      &lt;/body&gt;
      &lt;body name=&quot;thigh_left&quot; pos=&quot;0 0 -0.2&quot; gravcomp=&quot;0&quot;&gt;
        &lt;joint name=&quot;thigh_left_joint&quot; pos=&quot;0 0 0&quot; axis=&quot;0 -1 0&quot; range=&quot;-2.61799 0&quot;/&gt;
        &lt;geom name=&quot;thigh_left_geom&quot; size=&quot;0.05 0.225&quot; pos=&quot;0 0 -0.225&quot; type=&quot;capsule&quot; friction=&quot;0.9 0.1 0.1&quot; rgba=&quot;0.7 0.3 0.6 1&quot;/&gt;
        &lt;body name=&quot;leg_left&quot; pos=&quot;0 0 -0.7&quot; gravcomp=&quot;0&quot;&gt;
          &lt;joint name=&quot;leg_left_joint&quot; pos=&quot;0 0 0.25&quot; axis=&quot;0 -1 0&quot; range=&quot;-2.61799 0&quot;/&gt;
          &lt;geom name=&quot;leg_left_geom&quot; size=&quot;0.04 0.25&quot; type=&quot;capsule&quot; friction=&quot;0.9 0.1 0.1&quot; rgba=&quot;0.7 0.3 0.6 1&quot;/&gt;
          &lt;body name=&quot;foot_left&quot; pos=&quot;0.2 0 -0.35&quot; gravcomp=&quot;0&quot;&gt;
            &lt;joint name=&quot;foot_left_joint&quot; pos=&quot;-0.2 0 0.1&quot; axis=&quot;0 -1 0&quot; range=&quot;-0.785398 0.785398&quot;/&gt;
            &lt;geom name=&quot;foot_left_geom&quot; size=&quot;0.06 0.1&quot; pos=&quot;-0.1 0 0.1&quot; quat=&quot;0.707107 0 -0.707107 0&quot; type=&quot;capsule&quot; friction=&quot;1.9 0.1 0.1&quot; rgba=&quot;0.7 0.3 0.6 1&quot;/&gt;
          &lt;/body&gt;
        &lt;/body&gt;
      &lt;/body&gt;
    &lt;/body&gt;
  &lt;/worldbody&gt;
  &lt;actuator&gt;
    &lt;general joint=&quot;thigh_joint&quot; ctrlrange=&quot;-1 1&quot; gear=&quot;100 0 0 0 0 0&quot; actdim=&quot;0&quot;/&gt;
    &lt;general joint=&quot;leg_joint&quot; ctrlrange=&quot;-1 1&quot; gear=&quot;100 0 0 0 0 0&quot; actdim=&quot;0&quot;/&gt;
    &lt;general joint=&quot;foot_joint&quot; ctrlrange=&quot;-1 1&quot; gear=&quot;100 0 0 0 0 0&quot; actdim=&quot;0&quot;/&gt;
    &lt;general joint=&quot;thigh_left_joint&quot; ctrlrange=&quot;-1 1&quot; gear=&quot;100 0 0 0 0 0&quot; actdim=&quot;0&quot;/&gt;
    &lt;general joint=&quot;leg_left_joint&quot; ctrlrange=&quot;-1 1&quot; gear=&quot;100 0 0 0 0 0&quot; actdim=&quot;0&quot;/&gt;
    &lt;general joint=&quot;foot_left_joint&quot; ctrlrange=&quot;-1 1&quot; gear=&quot;100 0 0 0 0 0&quot; actdim=&quot;0&quot;/&gt;
  &lt;/actuator&gt;
&lt;/mujoco&gt;
</code></pre>
",5/16/2023 1:06,76262403,666,2,0,1,,11045673,,2/11/2019 14:11,3,76262403,"<p>See <a href=""https://github.com/deepmind/mujoco/issues/833"" rel=""nofollow noreferrer"">this issue</a> and links therein.</p>
",8593855,2,1,,,,
272,4670,77213531,"In Unity, make an object rotate between two angles that are greater than 180 degrees",|c#|unity-game-engine|quaternions|robotics|,"<p>I am trying to simulate a robotic arm in Unity. I am avoiding super expensive addons (500+ euro) that implement these features by default, so I got to somehow manage on my own or with a little bit of help. Currently I am using an addon called Hybrid IK, not that it matters too much, code is just code. So to the problem at hand:</p>
<p>I have a robot with some axes, each one rotating on either X, Y or Z. I am using quaternions to do the rotations, however quaternions have a problem they can only rotate from -180 to 180. The problem is my robot joints rotate more than that. For example my base rotates from -210 to 210. I tried several solutions but nothing worked.</p>
<p>I tried to introduce a method that reads which way the angle moves after passing from point 0 (initial position) and determining the way the angle rotates that way. So a movement going positive for a 210 angle, would mean, limit the angle from 0 to 180 and then once you flip, from -180 to -150. It didn't work.</p>
<p>Then I tried to just force Unity to go the long way, so a limit of 30, would mean that the angle would have to rotate the other way to reach that 30 (from 0 to -180 and then from 180 to 30) hence mimicking a 330 limit angle. That did not work either.</p>
<p>I am kind of stuck and I would really like to figure this out. I will share the code responsible for this function, but since this is a paid asset, I would rather not share too much.</p>
<pre><code>        public Quaternion LimitHinge(Quaternion rotation)
        {
            Quaternion offsetRot = Quaternion.Euler(limitsOffset);

            Vector3 offsetSwingAxis = (offsetRot * this.mainAxis);
            Vector3 offsetSecondaryAxis = (offsetRot * this.cross);
            Vector3.OrthoNormalize(ref offsetSwingAxis, ref offsetSecondaryAxis);
            Vector3 offsetCrossAxis = Vector3.Cross(offsetSwingAxis, offsetSecondaryAxis);

            Quaternion hingeOffset = Quaternion.AngleAxis(hingeAngleOffset, offsetSwingAxis);
            Quaternion minRotation = Quaternion.AngleAxis(-limitAngle, offsetSwingAxis) * hingeOffset;
            Quaternion maxRotation = Quaternion.AngleAxis(limitAngle, offsetSwingAxis) * hingeOffset;
            // Calculate the target quaternion for a free 1-degree-of-freedom rotation.
            Quaternion free1DOFTarget = Quaternion.FromToRotation(rotation * offsetSwingAxis, offsetSwingAxis) * rotation;

            // Check if the limit angle is greater than or equal to 180 degrees,
            // if so, no further processing is needed, return the target.
            if (limitAngle &gt;= 180)
                return free1DOFTarget;

            // Calculate the middle limit angle. If the limitAngle is greater than or equal to 90 degrees,
            // the middle limit is 180 degrees minus the limitAngle, otherwise, it's the same as limitAngle.

            // Check if the limitAngle is greater than or equal to 90 degrees.
            float midLimit;
            if (limitAngle &gt;= 90f)
            {
                // If it is, calculate midLimit as 180 degrees minus the limitAngle.
                midLimit = 180 - limitAngle;
            }
            else
            {
                // If not, midLimit is the same as limitAngle.
                midLimit = limitAngle;
            }

            // Calculate the quaternion for the middle limit rotation.
            Quaternion free1DOFMid = Quaternion.RotateTowards(minRotation, maxRotation, midLimit);

            // If the limitAngle is greater than or equal to 90 degrees, apply a 180-degree flip to the middle rotation.
            if (limitAngle &gt;= 90f)
            {
                Quaternion flip180 = Quaternion.AngleAxis(180, offsetSwingAxis);
                free1DOFMid *= flip180;
            }

            // Store the original target quaternion for comparison.
            Quaternion lastRotation = free1DOFTarget;

            // Calculate the angle between the target and middle limit rotations.
            float angle = Quaternion.Angle(free1DOFTarget, free1DOFMid);

            // Clamp the middle limit rotation towards the target by the specified limitAngle.
            Quaternion clampedFree1DOF = Quaternion.RotateTowards(free1DOFMid, free1DOFTarget, limitAngle);

            // Determine if the rotation has been clamped to the limit.
            isClampedToLimit = angle &gt;= limitAngle;

            // Return the clamped rotation.
            return clampedFree1DOF;
        }
</code></pre>
<p>P.S. I did try to contact the creator of the addon, but I couldn't reach him in anyway, seems like he is inactive since 2020 or 2021.</p>
",10/2/2023 6:16,,256,2,3,2,,17817710,"Athens, Greece",1/2/2022 21:21,7,77229417,"<p>You can try to put your angel from 0 to 1 at first and then multiply and add/subtract it in specific way to get your angles.
so for example you have from -180 to 180, then you need to add to your degrees 180, to get value from 0 to 180 * 2. the next step is to divide degrees, divide by 180 *2 then we got value from 0 to 1. after that you can multiply and add values in the sane way to get value in your custom degrees.</p>
",22553709,0,0,136121421,"Interpolate the angle instead, and use `Quaternion.AngleAxis` to construct the rotation from the angle. If you interpolate quaternions it will always rotate the ""short way"".",,
280,4689,77358482,How to create a bounding box for all the scene in CoppeliaSim?,|geometry|simulator|robotics|bounding-box|,"<p>I am trying to create a bounding box around all the elements in my scene.</p>
<p>I tried to start with a grouping, to get the bounds of all the objects and then calculate the bounding box. That is ok, but it still has issues.</p>
<p>First, the bounding box is not precise. It is not positioned properly. For example, for the object at the bottom the bounding box is not touching it, it is still under. For the object the top, the bounding box is cutting it because it is not up enough. The bounding box is lower than what it should. Sometimes it is half the object of error, or a little bit, or almost the whole object is out. This happens for the 3 axis.</p>
<p>I already positioned the bounding box at 0, 0, 0. Nothing changes, the bounding box was already at the center. But it is still not matching accurately with the objects of the scene.</p>
<p>My main problem, many problems with grouping. To restore everything, I ungroup. That ungroup is not working as expected. After ungrouping, the icons in the scene hierarchy change, I guess the properties of the objects have also changed. In addition, it is not only ungrouping my group, but everything, including robots. Finally, when the simulation ends, it destroys everything, including the floor.</p>
<p>I also thought about getting all the objects and their coordinates with a for loop. But I am reluctant to this solution because it is very time consuming. I am just looking for a reliable way to generate a bounding boy around all the scene, so you do not have to stick to the code I provided, I am open to other solutions. '</p>
<p>Thank you.</p>
<pre><code># python
from coppeliasim_zmqremoteapi_client import RemoteAPIClient
import time

'''
@dev: This function gets the x, y, and z lenghts of the bounding box of a given object
@param: the handle of the object to calculate its bounding box
@returns: 3 values. The x length, the y length and the z length
@author: Andres Masis
'''
def getObjectBoundingBoxSize(handle):
    # Gets the x lenght
    r, m = sim.getObjectFloatParameter(handle, sim.objfloatparam_objbbox_max_x)
    r, n = sim.getObjectFloatParameter(handle, sim.objfloatparam_objbbox_min_x)
    x = m - n

    # Gets the y lenght
    r, m = sim.getObjectFloatParameter(handle, sim.objfloatparam_objbbox_max_y)
    r, n = sim.getObjectFloatParameter(handle, sim.objfloatparam_objbbox_min_y)
    y = m - n

    # Gets the z lenght
    r, m = sim.getObjectFloatParameter(handle, sim.objfloatparam_objbbox_max_z)
    r, n = sim.getObjectFloatParameter(handle, sim.objfloatparam_objbbox_min_z)
    z = m - n

# Returns all the values
return x, y, z

# Access to the CoppeliaSim client
client = RemoteAPIClient()
sim = client.getObject('sim')

# Makes sure that the idle loop runs at full speed for this program:
defaultIdleFps = sim.getInt32Param(sim.intparam_idle_fps)
sim.setInt32Param(sim.intparam_idle_fps, 0)

# Run a simulation in stepping mode:
client.setStepping(True)
sim.startSimulation()

# Get all of the elements of the scene.
scene_objects = sim.getObjectsInTree(sim.handle_scene, sim.handle_all, 0)
client.step() # triggers next simulation step

# Create group of objects
groupHandle = sim.groupShapes(scene_objects, False)
client.step() # triggers next simulation step

x, y, z = getObjectBoundingBoxSize(groupHandle)

sim.ungroupShape(groupHandle)
client.step() # triggers next simulation step

boxHandle = sim.createPrimitiveShape(sim.primitiveshape_cuboid,[x, y, z], 0)
client.step() # triggers next simulation step

# Color
sim.setShapeColor(boxHandle, None, sim.colorcomponent_emission, [0, 255, 0])
client.step() # triggers next simulation step

# Transparency
sim.setShapeColor(boxHandle, None, sim.colorcomponent_transparency, [0.5])
client.step() # triggers next simulation step

# Position
sim.setObjectPosition(boxHandle, sim.handle_parent, [0.255, 0.225, 0.225])
client.step() # triggers next simulation step

time.sleep(10)

sim.stopSimulation()

# Restore the original idle loop frequency:
sim.setInt32Param(sim.intparam_idle_fps, defaultIdleFps)

print('Program ended')
</code></pre>
<p><a href=""https://i.stack.imgur.com/kCFnS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kCFnS.png"" alt=""My bounding box example"" /></a></p>
<p><strong>Tried:</strong></p>
<ul>
<li>Grouping solution</li>
<li>For loop solution to get all the coordinates</li>
<li>Ask in the CoppeliaSim forum</li>
</ul>
<p><strong>Expected:</strong></p>
<ul>
<li>Any reliable solution with an acceptable time performance to generate a bounding box</li>
</ul>
",10/25/2023 10:00,,35,0,0,0,,22694257,,10/6/2023 8:42,4,,,,,,,,,
282,4666,77141683,Simulation tools to validate the maths models of an unmanned surface vehicle (USV) with a manipulator,|validation|model|simulation|robotics|manipulators|,"<p>I'm working on a custom unmanned surface vehicle (USV) with a waterproof manipulator attached to it.</p>
<p>I'm looking for a simulator where I can validate the maths models of the robot.</p>
<p>The ideal case would be where</p>
<ul>
<li>I do not need to put equations to calculate the hydrodynamics in the simulator,</li>
<li>but I can just put the geometry of my robot, force and torque inputs, and any other necessary parameters</li>
<li>and see that both the USV and the attached manipulator are affected by corresponding hydrodynamics (buoyancy, drag, lift, and added mass)</li>
<li>and the forces and torques (wrenches) propagated between the manipulator and the USV are also taken into account</li>
<li>and the combined system (the USV and the manipulator) moves/behaves accordingly and accurately.</li>
</ul>
<p>What would be the best simulation tool for this? Any other suggestions and advice would be appreciated as well.</p>
<p>Thank you very much.</p>
",9/20/2023 10:53,,16,0,0,0,,22599536,,9/20/2023 10:37,0,,,,,,,,,
289,4690,77368485,How to put a label to an object in CoppeliaSim,|simulator|robotics|,"<p>I want to add a label to a shape in Coppelia.</p>
<p>I read about using a simSetObjectName(), but it does not appear in the RegularAPI documentation.
I read also about banners, but I do not find neither documentation about it.</p>
<p>Can you tell me please what to use in 2023.</p>
<p>For what I need, a Banner or something similar will work better than a renaming.</p>
<p>Thanks.</p>
<p>I already tried searching in the RegularAPI documentation. Please do not use simx functions because I am using a newer client. :)</p>
<p>I want to have like a little post-it or label attached to the object.</p>
",10/26/2023 15:41,,22,0,0,0,,22694257,,10/6/2023 8:42,4,,,,,,,,,
291,4538,75603072,How to activate the exact mesh of a link for contact simulation on Drake?,|python|simulation|robotics|drake|urdf|,"<p>We are trying to simulate the contact of a two-link brachiating robot with unactuated(hook-shaped) grippers on the support bar of a horizontal ladder. The following image(img4.png) is the .obj file of one of the links, opened in MeshLab. More details may be found at: <a href=""https://github.com/dfki-ric-underactuated-lab/acromonk"" rel=""nofollow noreferrer"">https://github.com/dfki-ric-underactuated-lab/acromonk</a></p>
<p>To simplify our task, we are first trying to simulate the hooking motion when the robot falls(due to gravity) over the support bar. The robot is given an initial configuration such that the gripper is exactly above the support bar. Theoretically, as the robot falls, the gripper clings onto the support bar and starts oscillating.</p>
<p>The problem is that gripper does not cling onto the support bar(as shown in the video, hi.gif, hell.gif, and images, img1.png, img2.png, &amp; img3.png). We believe that the simulator applies a collision model such that gripper hook is completely enveloped, and the cavity of the hook is disregarded(img5.png). This is happening inspite of including the .obj file shown above as the geometry mesh in the collision tag of the robot URDF. How do we correct this, and make the simulator consider the mesh file of the link as the collison model?</p>
<p>P.S. The other parts of the horizontal ladder don't have their collision model yet. Only the collision models of the robot and the support bar are active.</p>
<p><a href=""https://i.stack.imgur.com/mZeda.png"" rel=""nofollow noreferrer"">img4.png</a>
[img1.png[img2.png[img3.png<a href=""https://i.stack.imgur.com/7YF04.png"" rel=""nofollow noreferrer"">img5.png</a>](<a href=""https://i.stack.imgur.com/DNYTq.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/DNYTq.png</a>)](<a href=""https://i.stack.imgur.com/jmgnd.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/jmgnd.png</a>)](<a href=""https://i.stack.imgur.com/wU7WN.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/wU7WN.png</a>)</p>
<p><a href=""https://i.stack.imgur.com/VVQUt.gif"" rel=""nofollow noreferrer"">hi.gif</a></p>
<p>Thanks in advance,
Regards.</p>
",3/1/2023 11:27,75604333,136,2,0,1,,21310962,Bremen,3/1/2023 10:55,15,75604333,"<p>That's a great robot. Let's see if we can't get you set up properly.</p>
<p>tl;dr: Use <a href=""https://drake.mit.edu/doxygen_cxx/group__hydroelastic__user__guide.html"" rel=""nofollow noreferrer"">hydroelastic contact</a>; make the gripper rigid and make the cylindrical bars compliant.</p>
<p><strong>Details</strong></p>
<p>I'm assuming that you're using the default configuration for simulation. This means that you're using <em>point</em> contact to model contact between objects. That means that you're using <a href=""https://drake.mit.edu/doxygen_cxx/classdrake_1_1geometry_1_1_query_object.html#af3b16adde799eb804619dc86f3703f4f"" rel=""nofollow noreferrer"">this query</a>. This query has the property that arbitrary mesh objects are represented by their contact hulls. Exactly what you're seeing.</p>
<p>Drake has another, novel method for evaluating contact between bodies: <a href=""https://drake.mit.edu/doxygen_cxx/group__hydroelastic__user__guide.html"" rel=""nofollow noreferrer"">hydroelastic contact</a>. The linked guide should walk you through everything you need to do to tweak your simulation so that your hook is nicely interacting with the bars. The steps are along the lines of:</p>
<ol>
<li>Tweak your URDF to introduce the important Drake-specific hydroelastic properties. (<code>MultibodyPlant</code> is configured to make use of that information by default.)</li>
<li>Add flags to the hook to make it have a <em>rigid</em> hydroelastic representation (i.e., <code>&lt;drake:compliant_hydroelastic/&gt;</code>).</li>
<li>Replace the <em>mesh</em> cylinders with <code>&lt;cylinder&gt;</code> declarations that you can specifically declare to be <code>&lt;drake:compliant_hydroelastic/&gt;</code>.</li>
</ol>
<p>The rigid hook will interact with the compliant rods without any recourse to the convex hull.</p>
<p>I'd also recommend configuring the <code>MultibodyPlant</code> in <a href=""https://drake.mit.edu/doxygen_cxx/classdrake_1_1multibody_1_1_multibody_plant.html#a3aeb03d90d213da4702f2dc5e484fc86"" rel=""nofollow noreferrer""><em>discrete</em> mode</a> and also consider setting the <a href=""https://drake.mit.edu/doxygen_cxx/classdrake_1_1multibody_1_1_multibody_plant.html#a217d8aa9218c9d981b7bc2e8a983c4be"" rel=""nofollow noreferrer"">contact solver to SAP</a>.</p>
<hr />
<p>If hydroelastic contact proves problematic, you'll have to switch to a convex decomposition of your hook. Some Drake users have had success using <a href=""https://github.com/kmammou/v-hacd"" rel=""nofollow noreferrer"">v-hacd</a> to do the offline decomposition and using the resultant <em>family</em> of geometries in the URDF as the set of collision geometries.</p>
",7686256,1,2,,,,
297,4760,78046244,Obtaining rotation and translation matrix from homogeneous transformation matrix,|c++|eigen|robotics|eigen3|,"<p>I have following piece of code:</p>
<pre><code>Eigen::Matrix4f transformation = myMethod(); // homogenous transformation matrix
std::cout &lt;&lt; &quot;transformation = &quot; &lt;&lt; std::endl &lt;&lt; transformation &lt;&lt;  std::endl &lt;&lt; std::endl;
Eigen::Isometry3f estimate = Eigen::Isometry3f(transformation);
r = estimate.rotation(); // rotation matrix
t = estimate.translation(); // translation matrix
std::cout&lt;&lt;&quot;r = &quot; &lt;&lt; std::endl &lt;&lt; r &lt;&lt; std::endl &lt;&lt; std::endl;
std::cout&lt;&lt;&quot;t = &quot; &lt;&lt; std::endl &lt;&lt; t &lt;&lt; std::endl &lt;&lt; std::endl;
</code></pre>
<p>It prints:</p>
<pre><code>transformation = 
     1.0000002384185791  4.9265406687482027e-07 -4.5500800638365035e-07  -2.384185791015625e-07
-5.2062489430682035e-07      1.0000003576278687  5.4357258250092855e-07 -3.5762786865234375e-07
 4.9866633844430908e-07 -4.6970637868071208e-07      1.0000002384185791  -2.384185791015625e-07
                      0                       0                       0                       1

r = 
     1.0000001192092896  7.2572328235764871e-07 -3.9811814644963306e-07
-6.7004600623477018e-07     0.99999994039535522  4.5586514829665248e-07
  5.422648428066168e-07 -4.4537293319990567e-07     0.99999994039535522

t = 
 -2.384185791015625e-07
-3.5762786865234375e-07
 -2.384185791015625e-07
</code></pre>
<p>Here, I am trying to obtain rotation and translation matrices from homogeneous transformation matrix. I guess translation matrix is the first three elements of the last column of homogeneous matrix and rotation matrix is top left 3x3 matrix. Translation matrix is returned correctly above. But rotation matrix is not exactly the top left 3x3 matrix of the homogeneous matrix.</p>
<p>I guess I am missing some basic concept here. What am doing wrong?</p>
",2/23/2024 9:04,,85,1,7,1,,6357916,,5/19/2016 19:14,480,78047046,"<p>The documentation of <a href=""https://eigen.tuxfamily.org/dox/classEigen_1_1Transform.html"" rel=""nofollow noreferrer""><code>Transform</code></a> describes the different storage modes:</p>
<ul>
<li><strong>Affine</strong>: the transformation is stored as a (Dim+1)^2 matrix, where the last row is assumed to be [0 ... 0 1].</li>
<li><strong>AffineCompact</strong>: the transformation is stored as a (Dim)x(Dim+1) matrix.</li>
<li><strong>Projective</strong>: the transformation is stored as a (Dim+1)^2 matrix without any assumption.</li>
<li><strong>Isometry</strong>: same as <strong>Affine</strong> with the additional assumption that the linear part represents a rotation. This assumption is exploited to speed up some functions such as <code>inverse()</code> and <code>rotation()</code>.</li>
</ul>
<p>And this actually <a href=""https://gitlab.com/libeigen/eigen/-/blob/a6dc930d16be86cffba1f2926dea196abab0fe71/Eigen/src/Geometry/Transform.h#L1015"" rel=""nofollow noreferrer"">happens</a>, <code>Isometry</code> simply returns what was stored in the constructor:</p>
<pre><code>template &lt;typename Scalar, int Dim, int Mode, int Options&gt;
EIGEN_DEVICE_FUNC typename Transform&lt;Scalar, Dim, Mode, Options&gt;::RotationReturnType
Transform&lt;Scalar, Dim, Mode, Options&gt;::rotation() const {
  return internal::transform_rotation_impl&lt;Mode&gt;::run(*this);
}

template &lt;int Mode&gt;
struct transform_rotation_impl {
  template &lt;typename TransformType&gt;
  EIGEN_DEVICE_FUNC static inline const typename TransformType::LinearMatrixType run(const TransformType&amp; t) {
    typedef typename TransformType::LinearMatrixType LinearMatrixType;
    LinearMatrixType result;
    t.computeRotationScaling(&amp;result, (LinearMatrixType*)0);
    return result;
  }
};
template &lt;&gt;
struct transform_rotation_impl&lt;Isometry&gt; {
  template &lt;typename TransformType&gt;
  EIGEN_DEVICE_FUNC static inline typename TransformType::ConstLinearPart run(const TransformType&amp; t) {
    return t.linear();                     &lt;===================== here
  }
};
</code></pre>
<p>But, this is version <strong>v3.4</strong>. As @chtz points out in the comment, this description and the optimization itself are not present in <strong>v3.3</strong>, <a href=""https://gitlab.com/libeigen/eigen/-/blob/3.3/Eigen/src/Geometry/Transform.h?ref_type=heads#L1049"" rel=""nofollow noreferrer""><code>rotation</code></a> uses no &quot;switchboard&quot;, but directly calls a fixed function:</p>
<pre><code>template&lt;typename Scalar, int Dim, int Mode, int Options&gt;
EIGEN_DEVICE_FUNC const typename Transform&lt;Scalar,Dim,Mode,Options&gt;::LinearMatrixType
Transform&lt;Scalar,Dim,Mode,Options&gt;::rotation() const
{
  LinearMatrixType result;
  computeRotationScaling(&amp;result, (LinearMatrixType*)0);
  return result;
}
</code></pre>
<p><code>computeRotationScaling</code> (the method directly below the linked one) then calculates SVD, and that's where numerical inaccuracies can get introduced:</p>
<pre><code>template&lt;typename Scalar, int Dim, int Mode, int Options&gt;
template&lt;typename RotationMatrixType, typename ScalingMatrixType&gt;
EIGEN_DEVICE_FUNC void Transform&lt;Scalar,Dim,Mode,Options&gt;::computeRotationScaling(RotationMatrixType *rotation, ScalingMatrixType *scaling) const
{
  JacobiSVD&lt;LinearMatrixType&gt; svd(linear(), ComputeFullU | ComputeFullV);

  Scalar x = (svd.matrixU() * svd.matrixV().adjoint()).determinant(); // so x has absolute value 1
  VectorType sv(svd.singularValues());
  sv.coeffRef(0) *= x;
  if(scaling) scaling-&gt;lazyAssign(svd.matrixV() * sv.asDiagonal() * svd.matrixV().adjoint());
  if(rotation)
  {
    LinearMatrixType m(svd.matrixU());
    m.col(0) /= x;
    rotation-&gt;lazyAssign(m * svd.matrixV().adjoint());
  }
}
</code></pre>
<p><code>Isometry</code> in v3.3 is practically an <code>Affine</code> with some parts switched off (like <code>scale</code> stops you with an assert, and there are some more).</p>
<blockquote>
<p>I guess I am missing some basic concept here. What am doing wrong?</p>
</blockquote>
<p>Seemingly you're using an older version of the library. Consider trying the code with v3.4.</p>
",7916438,1,0,137591630,"Sorry, I made mistake while renaming my context specific variables. `translation()` and `rotation()` are methods of variable `Eigen::Isometry3f estimate`. Now corrected that.",,
