,questionId,questionTitle,Tags,questionBody,questionCreationDate,AcceptedAnswerId,questionViewCount,AnswerCount,CommentCount,questionScore,questionFavoriteCount,questionUserId,questionUserLocation,questionUserCreationDate,questionUserViews,answerId,answerBody,answerUserId,answerScore,answerCommentCount,commentId,commentText
127,166742,Robot simulation environments,|simulation|environment|robotics|,"<p>I would like to make a list of remarkable robot simulation environments including advantages and disadvantages of them. Some examples I know of are <a href=""http://www.cyberbotics.com/"" rel=""noreferrer"">Webots</a> and <a href=""http://playerstage.sourceforge.net/"" rel=""noreferrer"">Player/Stage</a>.</p>
",2008-10-03 13:07:00,166799.0,5688,12,2,15,0.0,21047.0,"Budapest, Hungary",9/23/2008 11:27,704.0,166799.0,"<p>This made me remember the <a href=""http://www.spiderland.org/node/2598"" rel=""nofollow noreferrer"">breve</a> project.</p>

<blockquote>
  <p><em>breve is a free, open-source software package which makes it easy to build 3D simulations of multi-agent systems and artificial life.</em></p>
</blockquote>

<p>There is also a wikipage listing <a href=""http://en.wikipedia.org/wiki/Robotics_simulator"" rel=""nofollow noreferrer"">Robotics simulators</a></p>
",842.0,4.0,0.0,16833762.0,We have several good answers: I prefer those that include a whole list like epatel's and Ezu's answers or is elaborated as Prometheus.one's answer.
100,106684,How to create a new type of entity in Microsoft Robotics Studio 2.0?,|artificial-intelligence|robotics|robotics-studio|,"<p>What I'm trying to do with MRS is to teach myself some basic AI; what I want to do is to make a rocket entity, with things such as vectored exhaust, and staging. Anyone have an idea on how to make an entity that can fly? Or do I just need to constantly apply a force upwards?</p>
",2008-09-20 01:16:00,110338.0,477,2,0,6,0.0,18658.0,"Brisbane, Australia",9/19/2008 8:49,1055.0,110338.0,"<p>Hey TraumaPony, your question looked lonely :)</p>

<p>I took a look at an MSDN article about MRS 2.0 <a href=""http://msdn.microsoft.com/en-us/magazine/cc546547.aspx"" rel=""nofollow noreferrer"">here</a> I believe you'll actually need to create a Rocket entity of some kind and then a Thruster entity that it can use. In the article they were able to reuse a DifferentialDrive entity to propel their bot forward. I hope that helps. I'm more or less shooting in the dark since no else has tried to help ya out yet. Cheers! :)</p>
",9401.0,3.0,0.0,,
297,1590073,Line Tracking Robot,|arduino|robotics|,"<p>Me and my friends are building a line tracking robot based on my previous question about how to track white line on a black surface. We settled on using photo resistors and a arduino board. Now all the reflectance sensors I've found are should be placed very close to the line 1 - 2 cm above the line. Now one of my team mates had a heated argument with the professor that there are reflectance sensors that can track 10cm or more but we could not find any.</p>

<p>Are there any type of sensor that would allow us to track the line farther away?</p>
",2009-10-19 17:41:00,,1357,2,3,2,0.0,89904.0,,4/12/2009 4:27,3143.0,1590138.0,"<p>Using an arduino, you are most likely going to use the pololu library for reflectance sensors. Even using an <a href=""http://www.pololu.com/catalog/product/961"" rel=""nofollow noreferrer"">array of sensors</a> of this type, you are looking at a maximum sensing distance of just UNDER a cm (9.5 mm.) I think your teammate was out by a factor of ten, you can score this one to the professor!</p>

<p>The lego light sensor is a good example of this type of sensor. If you can get your hands on an NXT kit, it is an alternative to the arduino. And who doesn't enjoy playing with lego!!</p>

<p>Kindness,</p>

<p>Dan</p>
",63627.0,0.0,0.0,1453373.0,"I'm interested in line tracking, but this questions seems to deal with finding some commodity hardware. I'm not sure that fits with SO."
280,1322019,Associating s/w programming with h/w programming,|hardware|robotics|,"<p>I've been in s/w programming for years. Thru out the years i also had interest in h/w programming (circuits + robotics + etc).</p>

<p>Please advice from where i can start h/w programming. my aim is to combine both s/w &amp; h/w to work together. </p>
",2009-08-24 12:17:00,1327292.0,205,4,1,5,0.0,147025.0,,7/29/2009 11:24,518.0,1322040.0,"<p>I'd recommend <a href=""http://www.arduino.cc"" rel=""nofollow noreferrer"">Arduinos</a> if you want to try some embedded programming. They're cheap, IDE works on multiple platforms and especially they are easy to start with.</p>
",89024.0,3.0,2.0,93084415.0,ask a stupid question: does h/w mean hardware?
260,1011602,How can you add a camera to a robot in the Breve Simulator?,|python|simulation|robotics|,"<p>I've created a two wheeled robot based on the braitenberg vehicle. Our robots have two wheels and a PolygonDisk body(Much like kepera and e-puck robots). I would like to add a camera to the front of the robot. The problem then becomes how to control the camera and how to keep pointing it in the right direction(same direction as the robot). How can you make the camera point in the same direction as the robot ?</p>
",2009-06-18 09:01:00,,362,1,0,1,,109730.0,,5/20/2009 5:01,13.0,1027059.0,"<p>After much trying and failing I finally made it work.
So here is how I did it:</p>

<p>The general idea is to have an link or object linked to the vehicle and then measuring 
its rotation and location in order to find out in which direction the camera should be aimed.</p>

<p>1) Add an object that is linked to the robot:</p>

<pre><code>def addVisualCam(self):
    joint = None
    cam = breve.createInstances(breve.Link,1)
    cam.setShape(breve.createInstances(breve.PolygonCone, 1).initWith(10,0.08,0.08))
    joint = breve.createInstances(breve.FixedJoint,1)
    # So ad-hoc it hurts. oh well...
    joint.setRelativeRotation(breve.vector(0,1,0), -3.14/2)
    joint.link(breve.vector(0,1.05,0), breve.vector(0,0,0), cam, self.vehicle.bodyLink, 0)
    joint.setDoubleSpring(300, 1.01000, -1.01000)
    self.vehicle.addDependency(joint)
    self.vehicle.addDependency(cam)
    cam.setColor(breve.vector(0,0,0))
    self.cam = cam
</code></pre>

<p>2) Add this postIterate:</p>

<pre><code>def postIterate(self):
    look_at = self.cam.getLocation() + (self.cam.getRotation() * breve.vector(0,0,1))
    look_from = -(self.cam.getRotation()*breve.vector(0,0,1))
    self.vision.look(look_at, look_from)
</code></pre>
",109730.0,1.0,0.0,,
274,1252428,Hardware Programming - Hands-On Learning,|arduino|robotics|hardware|,"<p>Besides <a href=""http://www.arduino.cc/"" rel=""nofollow noreferrer"">Arduino</a>, what other ways are there to learn hardware programming in a hands-on way?  Are there any nifty kits available, either a pre-assembled robot, that you can program to move a certain way, or do certain things, or anything similar to that?</p>
",2009-08-09 22:04:00,1252456.0,1164,6,0,4,0.0,83819.0,United States,9/16/2008 7:07,905.0,1252441.0,"<p>There's the <a href=""http://www.microsoft.com/netmf/default.mspx"" rel=""nofollow noreferrer"">.NET Micro Framework.</a></p>

<p>It's incredibly simple to use/setup and there's lots of hardware being made to target this framework. </p>
",74183.0,0.0,0.0,,
446,1967978,"Lego Mindstorm NXT, Cocoa, and HiTechnic Sensors",|objective-c|cocoa|robotics|lego-mindstorms|,"<p>I've taken the existing code from <a href=""http://code.google.com/p/legonxtremote/"" rel=""nofollow noreferrer"">this project</a>, and very happy with it so far.</p>

<p>I'm now however in a position where I need to make use of some third-party sensors which I've purchased from <a href=""http://www.hitechnic.com"" rel=""nofollow noreferrer"">hitechnic</a>, such as an accelerometer, a gyroscope, and a 3D compass - to mention a few.</p>

<p>I'm not sure where to start now, but what I need to do is add to my existing code base (which is based on the <a href=""http://code.google.com/p/legonxtremote/"" rel=""nofollow noreferrer"">this</a>), and effectively glue my framework to the new hardware.</p>

<p>Can anyone point me in the right direction?  I can't find any APIs from the device manufacturer, (but I have emailed them and asked - no reply yet).</p>

<p>I've also started to document my findings on <a href=""http://ai.autonomy.net.au/wiki/Project/Framework/Nimachine"" rel=""nofollow noreferrer"">this</a> page.</p>
",2009-12-28 04:35:00,1973289.0,2192,2,0,4,0.0,109362.0,"Seattle, Washington",5/19/2009 13:14,202.0,1972660.0,"<p>I heard back from HiTechnic today, and with their permission, I'm posting their response for everyone here.</p>

<pre><code>Hi Nima,

There are two types of sensors, digital and analog.  The Analog sensors you
can basically read like you would the LEGO light sensor.  If you have that
working then you can read the HiTechnic analog sensors.  These include the
EOPD, Gyro as well as the Touch Multiplexer.

For the TMUX there is [sample NXC code][1] on the product info page.
You should be able to use that as a basis if you want to support this device.

The other sensors are digital I2C sensors.  Most of these sensors have I2C
register information on their respective product information page and/or it
was included on a sheet that came with the sensor.  First of all, to make
these sensors work with your framework you need to have I2C communications
working.  After that it will be a matter of creating your own API that uses
the I2C interface with the sensors.  I recommend that you download and look
at Xander Soldaat's RobotC driver suite for the HiTechnic sensors.  You will
find this near the bottom of the HiTechnic downloads page.

Regards,
Gus
HiTechnic Support
</code></pre>

<p>References:</p>

<ul>
<li><a href=""http://www.hitechnic.com/cgi-bin/commerce.cgi?preadd=action&amp;key=NTX1060"" rel=""nofollow noreferrer"">Sample NXC code</a></li>
<li><a href=""http://mightor.wordpress.com/2009/11/08/robotc-i2c-howto-part-i/"" rel=""nofollow noreferrer"">Xander's Blog</a></li>
</ul>
",109362.0,2.0,1.0,,
226,764202,Is Lego MindStorms a good choice for basic robotics development?,|robotics|lego-mindstorms|nxt|,"<p>I would like to learn how to write software for controlling robots.</p>

<p>Is Lego MindStorms a good choice for this? Are there better alternatives?</p>

<p>I'd prefer MindStorms, but after reading a couple of articles I get the impression that Lego has stopped research and development of MindStorms.</p>

<p>What are your suggestions?</p>
",2009-04-18 21:05:00,764351.0,15585,9,0,28,0.0,169.0,"London, United Kingdom",8/2/2008 22:51,717.0,764222.0,"<p>I can't give you a good side-by-side comparison vs other robotics kits (I know MS has one), but I've spent a lot of time with mindstorms (to the point where I gave a user group presentation) and I think that it makes the programming enjoyable and teaches you the basics of sensors, input and output that you'd need to know with any kit.</p>

<p>It gives you the foundation and makes it fun which is a great way to start. There are probably more sophisticated alternatives though...</p>
",3329.0,2.0,0.0,,
907,4350199,"Does a wireless robot, or even robot sensors exist that I can control or read via command line or curl?",|wireless|robotics|,"<p>I see robots like Rovio out there, that claim to be ""wireless"" or ""wifi"", but it SEEMS like they expect you to use their included software to control the robots.  Is there any way to control an existing commercial robot (or even just read an available wireless sensor, such as light or motion) from a command line, or curl?  (For example, hitting the robots ip, and port it is listening on, and sending a web services, or soap, or even http message, or ANYTHING). Even just being able to listen to a sensor from a command line would be a help... </p>

<p>Basically, most ""programmable"" robots out there have lightweight languages on them, and you have to physically store the code on them, so you're pretty limited.  What I want to do (and SURELY this exists) is have a robot that is completely client-light and server heavy (i.e. all the intelligence and logic is stored on some machine that wirlessly commands the robot).  That way I could code in any language (and have arbitrarily long code base), so long as I could send dumb wireless commands to the robot (such as move forward, give me your sensor data, etc)</p>

<p>Does such a thing exist in any form?</p>
",2010-12-03 21:54:00,4350523.0,176,1,1,0,0.0,99502.0,,5/1/2009 13:31,620.0,4350523.0,"<p><a href=""http://www.ros.org/"" rel=""nofollow"">ROS</a>, which is quite popular these days, provides facilities to off-load code to a larger machine, featuring distributed nodes. As long as you remember that you need the communication stack, motor / manipulation control etc. right on the robot, you can pretty much implement the whole heavy-duty control anywhere you like.</p>
",407438.0,0.0,1.0,5489221.0,"Actually, looks like Lego NXT brick lets you send programmed bluetooth commands to it, which suits my purposes. Huzzah?"
784,3687113,How to control multiple robots through PC using Serial communication?,|c|robotics|serial-communication|,"<p>I want to <strong>control multiple robots using my laptop</strong>. Robots do not have intelligence, they are sending sensor values to PC which computes the sensors values and sends back result to robots.(Centralized control of robots using PC ).</p>

<p>Robots are communicating with PC through serial communication using Zigbee mudule. </p>

<p>Problem: <strong>How to make &amp; send a structure</strong> (from robot) <strong>like {sen1, sen2,sen3..,robot id}</strong> where sen1, sen2..are sensors values and robot id is to recognize particular robot. 
After editing.....
The code I was using for sending sensors was like.</p>

<pre><code> void TxData(unsigned char tx_data)

{   SBUF = tx_data; //Transmit data that is passed to this function
     while(TI == 0); //wait while data is being transmitted
}
</code></pre>

<p>and then sending sensor values one by one</p>

<pre><code> TxData(left_whiteline_sensor);
 TI=0; // resetting transmit interrupt after each character
 TxData(middle_whiteline_sensor);
 TI=0;
 TxData(right_whiteline_sensor);
 TI=0;
 TxData(front_sharp_sensor);
 TI=0;
</code></pre>

<p>At PC end reading these values in buffer</p>

<pre><code>read(fd, buf1, sizeof(buf1));
.....
options.c_cc[VMIN]=4; // wait till not getting 4 values 
</code></pre>

<p>This was <strong>working fine</strong> when there was <strong>only one robot</strong>, now as we have <strong>multiple robots</strong> and each robot is sending data using above function, I am <strong>getting mixed sensor values</strong> of all robots of <strong>at PC end</strong>. One solution is to <strong>make a structure</strong> which I mentioned above and send it to PC. This is what I want to ask ""<strong>How to make and send such a structure</strong>""
Sorry for not framing question correctly before.</p>

<p>Thanks...</p>
",2010-09-10 18:07:00,,1345,5,3,1,0.0,362250.0,"Bengaluru, Karnataka, India",6/9/2010 9:22,134.0,3687353.0,"<p>I don't know what API you are using from the PC to communicate with the endpoints (robots), but I'm guessing that when you send data you specify either the short address, the long address (MAC), or some socket/file ID which you opened using one of the long or short addresses.  Also, I'm guessing that the robot id is the same as the short address -- if not you will need to create some code to map back and forth between the robot ID and the short address.  I'm also guessing that you are either using something like the <code>select</code> system call to wait for data from any of your robots or trying to read from each one.</p>

<p>If this is the case then you should be able to create a state machine for each robot and whenever you get data from that robot you feed that robot's state machine which processes it and generate a reply to that robot (or even sends data to the other robots' state machines).  The state machine will be almost like your single robot program, except that it will rely on an event loop getting data from the robots for it.  You may also want the event loop to be able to supply timer alarms for the state machines.  This is similar to the way you would write a http server that could handle multiple clients at one time.</p>

<p>If I was totally wrong about your API and you have those zigbee radios that just act as a serial port without wires then you are in a mess, because I think that you will have to reconfigure them in order to use more than one endpoint at a time -- and will have to change the API you use to communicate with the robots.</p>
",299301.0,0.0,0.0,3884562.0,"Yeah, what exactly are you asking?"
636,3185471,Distance travelled by a robot using Optical Flow,|localization|opencv|robotics|opticalflow|,"<p>is there a way to find out the distance travelled by a robot using Optical Flow? For example, using OpenCV, I'm able to find out the velocity of each pixel between 2 images taken by a camera. However, I don't know where to go with this to find out the corresponding distance travelled by the robot. Can you suggest a way to do this?</p>

<p>My main aim is to do the localization of the robot and for that I need the distance travelled by it between 2 instances.</p>
",2010-07-06 10:39:00,,570,2,2,0,,280454.0,India,2/24/2010 15:38,573.0,5905756.0,"<p>No, not directly.  You can determine distance to objects, and then back calculate distance travelled from there, but it will likely be computationally expensive.</p>
",369609.0,0.0,0.0,3280502.0,"You have already asked this question: [Finding distance travelled by robot using Optical Flow](http://stackoverflow.com/questions/3069144/finding-distance-travelled-by-robot-using-optical-flow ""title"")"
778,3621244,Accounting for misalignment of wheels in diff drive odometry,|math|coordinates|robotics|,"<p>I have a differential drive robot, using odometery to infer its position.</p>

<p>I am using the standard equations:</p>

<pre><code>WheelBase = 35.5cm;
WheelRadius = 5cm;
WheelCircumference = (WheelRadius * 2 * Math.PI);
WheelCircumferencePerEncoderClick = WheelCircumference / 360;

DistanceLeft = WheelCircumferencePerEncoderClick * EncoderCountLeft
DistanceRight = WheelCircumferencePerEncoderClick * EncoderCountRight

DistanceTravelled = (DistanceRight + DistanceLeft) / 2
AngleChange (Theta) = (DistanceRight - DistanceLeft) / WheelBase
</code></pre>

<p>My (DIY) chassis has as a slight feature, where over the course of the wheel base (35.5cm), the wheels are misaligned, in that the left wheel is 6.39mm (I'm a software person not a hardware person!) more 'forwards' than the right wheel. (The wheels are the middle of the robot.)</p>

<p>I'm unsure how to calculate what I must add to my formulas to give me the correct values.. It doesn't affect the robot much, unless it does on the spot turns, and my values are way off, I think this is causing it.</p>

<p>My first thought was to plot the wheel locations on a grid, and calculate the slope of the line of their positions, and use that to multiply... something.</p>

<p>Am I on the right track? Can someone lend a hand? I've looked around for this error, and most people seem to ignore it (since they are using professional chassis). </p>
",2010-09-01 18:59:00,3623793.0,489,2,0,3,,282090.0,,1/10/2010 12:34,138.0,3622288.0,"<p>If I'm reading this right, the problem is that because the left wheel is ahead of the right, when they turn at different rates they cannot roll without slipping. The greater the difference in turning rates, the worse the problem gets, which is probably why it shows up while ""turning on a dime"", when the rotations are opposed.</p>

<p>I think the way to solve it is to consider a related problem: two wheels located correctly, but both skewed a little to the left (which is exactly the situation you're in, thinking of the diagonal between the wheels as the ""wheelbase""). The motion can then be broken into two components, the major forward-back component, which acts normally, and the minor sideways component which causes no angle change and depends only on the sum of the wheel rotations.</p>

<p>I'll see if I can come up with some math that makes sense...</p>
",128940.0,0.0,0.0,,
799,3713385,"Writing Hardware Drivers, APIs, and Services",|wcf|hardware|soa|robotics|hardware-interface|,"<p>What books/blogs/podcasts etc... discuss patterns and best practices for designing software systems that interact with custom hardware and robotics? I am particularly interested in any books that would discuss strategies for writing a program that has to orchestrate and coordinate actions between several different custom hardware and robotics systems.     </p>
",2010-09-14 22:29:00,3852334.0,136,1,0,0,,170347.0,,9/8/2009 18:15,47.0,3852334.0,"<p>I'm not sure of any particular blog or podcast based resources for this type of material.</p>

<p>However, it sounds like what you are doing would be a perfect fit for something like ROS (Robotics Operating System), an open-source robotics framework with a heavy emphasis on software engineering ideas.  While it's not a book or blog, the framework along with the associated community is a great way to learn about the hardware/software interaction necessary for robotics systems.</p>

<p>You can find the website at <a href=""http://www.ros.org/"" rel=""nofollow"">ros.org</a></p>
",460065.0,1.0,0.0,,
910,4557970,Best practices in Python for handling external exceptions (do I use raise?),|python|robotics|,"<p>I'm writing a procedure in Python which at it's fundamental level communicates with a motor controller. It is possible for the controller to throw flags indicating that an error has occurred. I'm trying to figure how to best handle these errors.</p>

<p>In the example below, there are three possible errors, a temperature fault, a current limit fault and a voltage fault. I've handled them differently. Is there a correct way or is it subjective?</p>

<pre><code>class motor_fault(Exception):
    def __init__(self,error):
        motor.move_at = 0  #Stop motor
        self.error = error
    def __str__(self):
        return repr(self.value)

motor.velocity_limit = motor.slow
motor.velocity_limit_enable = True
try:
    motor.move_to_absolute = motor.instrument_pos
    while motor.in_position == 0:
        if motor.current_limit == 1:
            motor.move_at = 0 #Stop motor
            print('Motor current error')
            break
        if motor.temp_fault == 1: raise motor_fault('Temperature Fault')
        if motor.voltage_fault == 1: raise voltage_fault:
        time.sleep(0.5)
    else:
        print('reached desired instrument position with no faults')
except motor_temp_fault as e:
    #Not sure what I'd do here...
    print('My exception occurred, value:', e.error)
    pass
except:
    motor.move_at = 0 #Stop motor just in case
    print(' some other fault, probably voltage')
else:
    print (' this is only printed if there were no errors')
finally:
    print ('this is printed regardless of how the try exits')
</code></pre>

<p>It seems a lot simpler to drop the whole <code>try:</code>. Just set a flag in the while loop and break. After the loop, look at the flag and see if the while loop exited successfully.</p>

<pre><code>fault = False
while motor.in_position == 0:
    if motor.current_limit == 1:
        fault = 'Motor current error'
        break
    if motor.temp_fault == 1:
        fault = 'Motor temperature error'
        break
    if motor.voltage_fault == 1:
        fault = 'Motor voltage error'
        break
    time.sleep(0.5)
else:
    print('reached waterline with no faults')
if fault:
    motor.move_at = 0 #Stop motor
    print(fault)
    # Now look at the fault string to determine the next course of action.
</code></pre>

<p>But that somehow seems wrong or non-pythonic to use a term I don't really understand. Is there really anything wrong with this?
Thanks and please keep in mind I'm not CS major and I haven't taken a programming class since 1982.</p>
",2010-12-29 21:22:00,4558138.0,1387,4,4,2,0.0,141489.0,"Morehead City, NC",7/20/2009 15:56,110.0,4558046.0,"<p>I'd go for exceptions with many except clauses for all the different exceptions you would like to handle at this point as these cases seem to be exceptional/failure scenarios.</p>

<p>I would not use flags to represent these scenarios as it would add more fields to motor which dont seem to be useful/relevant outside of this use case.</p>

<p>As far as knowing if it is the 'correct' way to handle this well, if both solution works they are both correct!</p>

<p>Hope I was clear enough... ;-)</p>
",73729.0,0.0,0.0,5005961.0,"...The motor controller driver (written in Java) sends status updates via JSON-RPC back to my python program whenever any values change. As you might suspect, there's a separate thread which listens for these messages and updates the motor's attributes.

The code I'm writing is a port from a simple version of BASIC which ran on a micro-controller. The code needs to be simple enough that someone with a somewhat limited programming knowledge can modify the behavior if not the functionality."
466,2329934,Robot Simulation in Java,|java|simulation|robotics|,"<p>I am doing a project concerning robot simulation and i need help. I have to simulate the activities of a robot in a warehouse. I am using mindstorm robots and lego's for the warehouse. The point here is i have to simulate all the activities of the robot on a Java GUI. That is whenever the robot is moving, users have to see it on the GUI a moving object which represents the robot.</p>

<p>When the roads/rails/crossings of the warehouse changes it must also be changed on the screen. The whole project is i have to simulate whatever the robot is doing in the warehouse in real-time. Everything must happen in real-time  </p>

<p>I am asking which libraries in Java i can use to do this simulations in real-time and if someone can also point me to any site for good information. Am asking for libraries in Java that i can use to visualize the simulation in real-time.</p>
",2010-02-24 22:00:00,,3773,3,1,6,0.0,,,,,2329960.0,"<p>Perhaps the easiest (if not best) place to start is the ""Java2D"" API: <a href=""http://java.sun.com/products/java-media/2D/index.jsp"" rel=""nofollow noreferrer"">http://java.sun.com/products/java-media/2D/index.jsp</a></p>
",65977.0,3.0,0.0,2304974.0,"Note that ""real-time"" has a special meaning in software. http://en.wikipedia.org/wiki/Real-time_computing"
535,2734897,C# Robotics / Hardware,|c#|robotics|,"<p>I'm aware of Phidgets, however, I'm looking for some other types of hardware that can be interfaced with C#.</p>

<p>Anyone got some good ones?</p>
",2010-04-29 05:08:00,2741125.0,3410,5,1,9,0.0,88770.0,"Portland, OR, USA",4/8/2009 19:22,317.0,2734931.0,"<p>Actually Lego Mindstorms kits are cheap and have a lot of different libraries to code in. 
<a href=""http://msdn.microsoft.com/robotics/"" rel=""nofollow noreferrer"">Microsoft Robotics</a> for example. More info can be pulled from <a href=""http://blogs.msdn.com/coding4fun/archive/2007/07/16/3902344.aspx"" rel=""nofollow noreferrer"">this article</a>. My experience with Lego Mindstorms was before the NXT versions and using C however it was a great and challenging time. I may even look into grabbing a kit now that this popped up..</p>
",36.0,2.0,0.0,2762956.0,There's excellent information about all types of physical computing (including robotics) at http://www.chiphacker.com.
805,4032394,What languages/internet protocols for controlling robots/electronics remotely?,|java|c++|c|programming-languages|robotics|,"<p>I wonder what languages are used in robots and electronics. Is it low level languages like Java, C, C++ etc?</p>

<p>And if these robots and electronics could be controlled from another place, what protocol is used?</p>

<p>It couldn't be HTTP Rest, could it? :)</p>
",2010-10-27 10:49:00,4032688.0,1005,8,2,2,,206446.0,,11/8/2009 22:19,2201.0,4032432.0,"<p>I recent made a simple remote controlled robot programmed in Java with the help of this book</p>

<p><a href=""http://www.google.co.uk/products/catalog?q=build+java+robots&amp;hl=en&amp;cid=346434932749925759&amp;ei=WATITISGE5_g2ASm_tilCQ&amp;sa=title&amp;ved=0CAcQ8wIwADgA#p"" rel=""nofollow"">http://www.google.co.uk/products/catalog?q=build+java+robots&amp;hl=en&amp;cid=346434932749925759&amp;ei=WATITISGE5_g2ASm_tilCQ&amp;sa=title&amp;ved=0CAcQ8wIwADgA#p</a></p>

<p>This book showed me how to talk to the robot using bluetooth.</p>

<p>I've also read that BASIC is a good language to get started with, when build your first robot.</p>
",393908.0,0.0,2.0,4326237.0,Java is not a low level language by any definition.
592,3068658,What techniques exist for the software-driven locomotion of a bipedal robot?,|language-agnostic|machine-learning|robotics|robocup|,"<p>I'm programming a software agent to control a robot player in a simulated game of soccer.  Ultimately I hope to enter it in the RoboCup competition.</p>

<p>Amongst the various challenges involved in creating such an agent, the motion of it's body is one of the first I'm facing.   The simulation I'm targeting uses a Nao robot body with 22 hinge to control.  Six in each leg, four in each arm and two in the neck:</p>

<p><a href=""http://simspark.sourceforge.net/wiki/index.php/Soccer_Simulation"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vXdAU.png"" width=""200"" /></a><br>
<sub>(source: <a href=""http://simspark.sourceforge.net/wiki/images/b/b4/Models_NaoVirtual.png"" rel=""nofollow noreferrer"">sourceforge.net</a>)</sub>  </p>

<p>I have an interest in machine learning and believe there must be some techniques available to control this guy.</p>

<p>At any point in time, it is known:</p>

<ul>
<li>The angle of all 22 hinges</li>
<li>The X,Y,Z output of an accelerometer located in the robot's chest</li>
<li>The X,Y,Z output of a gyroscope located in the robot's chest</li>
<li>The location of certain landmarks (corners, goals) via a camera in the robot's head</li>
<li>A vector for the force applied to the bottom of each foot, along with a vector giving the position of the force on the foot's sole</li>
</ul>

<p>The types of tasks I'd like to achieve are:</p>

<ul>
<li>Running in a straight line as fast as possible</li>
<li>Moving at a defined speed (that is, one function that handles fast and slow walking depending upon an additional input)</li>
<li>Walking backwards</li>
<li>Turning on the spot</li>
<li>Running along a simple curve</li>
<li>Stepping sideways</li>
<li>Jumping as high as possible and landing without falling over</li>
<li>Kicking a ball that's in front of your feet</li>
<li>Making 'subconscious' stabilising movements when subjected to unexpected forces (hit by ball or another player), ideally in tandem with one of the above</li>
</ul>

<p>For each of these tasks I believe I could come up with a suitable fitness function, but not a set of training inputs with expected outputs.  That is, any machine learning approach would need to offer <a href=""https://en.wikipedia.org/wiki/Unsupervised_learning"" rel=""nofollow noreferrer"">unsupervised learning</a>.</p>

<p>I've seen some examples in open-source projects of circular functions (sine waves) wired into each hinge's angle with differing amplitudes and phases.  These seem to walk in straight lines ok, but they all look a bit clunky.  It's not an approach that would work for all of the tasks I mention above though.</p>

<p>Some teams apparently use inverse kinematics, though I don't know much about that.</p>

<p>So, what approaches are there for robot biped locomotion/ambulation?</p>

<hr>

<p>As an aside, I wrote and published <a href=""https://code.google.com/archive/p/tin-man"" rel=""nofollow noreferrer"">a .NET library called TinMan</a> that provides basic interaction with the soccer simulation server.  It has a simple programming model for the sensors and actuators of the robot's 22 hinges.</p>

<p>You can read more about RoboCup's 3D Simulated Soccer League:</p>

<ul>
<li><a href=""http://en.wikipedia.org/wiki/RoboCup_3D_Soccer_Simulation_League"" rel=""nofollow noreferrer"">http://en.wikipedia.org/wiki/RoboCup_3D_Soccer_Simulation_League</a></li>
<li><a href=""http://simspark.sourceforge.net/wiki/index.php/Main_Page"" rel=""nofollow noreferrer"">http://simspark.sourceforge.net/wiki/index.php/Main_Page</a></li>
<li><a href=""http://code.google.com/p/tin-man/"" rel=""nofollow noreferrer"">http://code.google.com/p/tin-man/</a></li>
</ul>
",2010-06-18 09:54:00,,722,5,3,8,0.0,24874.0,"Melbourne, Australia",10/3/2008 15:02,15318.0,3069718.0,"<p>I was working on a project not that dissimilar from this (making a robotic tuna) and one of the methods we were exploring was using a <a href=""http://en.wikipedia.org/wiki/Genetic_algorithm"" rel=""nofollow noreferrer"">genetic algorithm</a> to tune the performance of an artificial <a href=""http://en.wikipedia.org/wiki/Central_pattern_generator"" rel=""nofollow noreferrer"">central pattern generator</a> (in our case the pattern was a number of sine waves operating on each joint of the tail).  It might be worth giving a shot, Genetic Algorithms are another one of those tools that can be incredibly powerful, if you are careful about selecting a fitness function. </p>
",369609.0,2.0,1.0,3210880.0,"@John - I just tried again.  Only two.  Maybe because I'm in Australia or because I don't generally use Bing and haven't configured it somehow...  Anyway, care to share some of the better links?"
462,2199696,How to avoid that the robot gets trapped in local minimum?,|robotics|a-star|motion-planning|,"<p>I have some time occupying myself with motion planning for robots, and have for some time wanted to explore the possibility of improving the opportunities as ""potential field"" method offers. My challenge is to avoid that the robot gets trapped in ""local minimum"" when using the ""potential field"" method. Instead of using a ""random walk"" approach to avoid that the robot gets trapped I have thought about whether it is possible to implement a variation of A* which could act as a sort of guide for precisely to avoid that the robot gets trapped in ""local minimum"".</p>

<p>Is there some of the experiences of this kind, or can refer to literature, which avoids local minimum in a more effective way than the one used in the ""random walk"" approach.</p>
",2010-02-04 12:43:00,,1605,2,0,5,0.0,163046.0,"Copenhagen, Denmark",8/25/2009 20:11,46.0,2202108.0,"<p>A* and potential fields are all search strategies. The problem you are experiencing is that some search strategies are more ""greedy"" than others, and more often than not, algorithms that are too greedy get trapped in local minimum.</p>

<p>There are some alternatives where the tension between greediness (the main cause of getting trapped on local minimum) and diversity (trying new alternatives that don't seem to be a good choice in the short term) are parameterized.</p>

<p>A few years ago I've researched a bit about ant algorithms (search for Marco Dorigo, ACS, ACO) and they have a family of search algorithms that can be applied to pretty much anything, and they can control the greediness vs. exploration of your search space. In one of their papers, they even compared the search performance solving the TSP (the canonical traveling salesman problem) using genetic algorithms, simulated annealing and others. Ant won.</p>

<p>I've solved the TSP in the past using genetic algorithms and I still have the source code in delphi if you like.</p>
",229081.0,5.0,1.0,,
954,4973854,C or C++ for a Robot?,|c++|c|robotics|,"<p>Greetings,</p>

<p>I am trying to decide between C and C++ for my robot. I am a 5+ year veteran of Visual Basic.NET, however I'm going with Linux (Ubuntu) on this robot, and there is a compatibility problem between Linux and the .NET Framework. I want to stick with whichever language I choose for all of my projects, so I want to make sure that I choose the most appropriate one for the task.
For reference, I will describe my current robot in progress and what I am going to do with it. I am in the process of building a full-sized R4 Astromech (yep, I'm one of those guys). I have incorporated a PC motherboard with an Intel Core 2 2.1 GHz processor, 1 GB RAM. I will be using a scratch-built parallel interface card to control the drive motors, head motor, as well as a secondary parallel interface card (going to a second parallel port) which all of the sensors (IR, Ultrasonic Ranging, Visual Recognition via webcam, etc.) will be going to. Commands will be given using speech recognition (currently have a VB.NET scratch-built recognition program that I will be adapting to the new language).
Given the specifications and desired goals listed above, would I be better off with C or C++? I greatly appreciate any suggestions that you might have.
Thanks!
Thaskalas</p>
",2011-02-11 20:45:00,,4910,7,6,17,0.0,613626.0,,2/11/2011 20:45,12.0,4973940.0,"<p>What do you mean by a compatibility problem? Have you looked at <a href=""http://en.wikipedia.org/wiki/Mono_%28software%29"" rel=""nofollow"">Mono</a>? It's an open-source implementation of the .NET libraries. It's geared toward C# not VB.NET but if you're more comfortable in a .NET environment use that. Speed isn't really an issue here as a Core2Duo is plenty fast for what you need to do.</p>

<p>If Mono won't work for you, I'd recommend C++. There are a lot more libraries out there for C++ (or at least, I am familiar with more, e.g. <a href=""http://www.boost.org/"" rel=""nofollow"">Boost</a>), which can use most C libraries too. There's no real speed penalty for using C++. While using C wouldn't be bad per-se, C++ has some benefits and no drawbacks, so it's probably the better choice.</p>
",185936.0,8.0,3.0,5552965.0,@Hellfrost What's your problem with VB.net? While I prefer C# that's just a stylistic choice.
1259,7861132,iterative lengthening pseudo code for traversal of all the nodes,|algorithm|artificial-intelligence|robotics|,"<p>What would the algorithm, pseudo code or actual code to traverse all the nodes in a graph using an iterative lengthening depth-first approach?</p>
",2011-10-22 17:00:00,,3006,1,0,1,0.0,452944.0,Stockholm,9/16/2010 8:23,371.0,7887942.0,"<p>I give you first the depth-first pseudo-code for graph</p>

<pre><code>DLS(node, goal, depth, visited) 
{
  if ( depth &gt;= 0 ) 
    {
    if ( node == goal )
      return node

    visited.insert(node)

    for each child in expand(node)
      if (child is not in visited)
          DLS(child, goal, depth-1, visited)
  }
}
</code></pre>

<p>and the iterative DLS is</p>

<pre><code>IDDFS(start, goal)
{
  depth = 0
  while(no solution)
  {
    visited = [] // &lt;-- Empty List
    solution = DLS(start, goal, depth,visited)
    depth = depth + 1
  }
  return solution
}
</code></pre>

<p>You can always transform a graph in a tree by removing graph loop with a <em>visited list</em>. :)</p>
",173787.0,1.0,3.0,,
1263,8091124,LeJOS NXT movement in centimeters,|java|robotics|lejos-nxj|,"<p>I just started learning LeJOS programming and have a small probelm. I understand that I can measure movement distance in seconds and degrees. Is it possible to measure distance in centimeters, for instance.
If yes, then how?</p>
",2011-11-11 07:55:00,,565,2,0,0,,,,,,8091204.0,"<p>I am assuming that your robot uses wheels to enable it to move. If you can obtain the amount of degrees that your wheel turns, you can use the <a href=""http://math.about.com/od/formulas/ss/surfaceareavol_9.htm"" rel=""nofollow"">Arc Length Formula</a> to obtain the linear distance that your wheel moved. </p>
",243943.0,2.0,0.0,,
1140,6007822,Finding path obstacles in a 2D image,|opencv|robotics|image-recognition|,"<p>what approach would you recommend for finding obstacles in a 2D image?</p>

<p>Here are some key points I came up with till now:</p>

<p>I doubt I can use object recognition based on ""database of obstacles"" search, since I don't know what might the obstruction look like.
I assume color recognition might be problematic if the path does not differ a lot from the object itself.</p>

<p>Possibly, adding one more camera and computing a 3D image (like a Kinect does) would work, but that would not run as smooth as I require.</p>

<p>To illustrate the problem; robot can ride either left or right side of the pavement. In the following picture, left side is the correct choice:
<img src=""https://i.stack.imgur.com/A95h0.jpg"" alt=""enter image description here""></p>
",2011-05-15 10:27:00,6027601.0,3043,1,4,3,0.0,326257.0,"Prague, Czech Republic",4/26/2010 18:39,1040.0,6027601.0,"<p>If you know what the path looks like, this is largely a <em>classification problem</em>. Acquire a bunch of images of path at different distances, illumination, etc. and manually label the ground in each image. Use this labeled data to train a classifier that classifies each pixel as either ""road"" or ""not road."" Depending upon the texture of the road, this could be as simple as classifying each pixels' RGB (or HSV) values or using OpenCv's built-in histogram back-projection (i.e. <code>cv::CalcBackProjectPatch()</code>).</p>

<p>I suggest beginning with manual thresholds, moving to histogram-based matching, and only using a full-fledged machine learning classifier (such as a Naive Bayes Classifier or a SVM) if the simpler techniques fail. Once the entire image is classified, all pixels that are identified as ""not road"" are obstacles. By classifying the <em>road instead of the obstacles</em>, we completely avoided building a ""database of objects"".</p>

<hr>

<p>Somewhat out of the scope of the question, the easiest solution is to add additional sensors (""throw more hardware at the problem!"") and directly measure the three-dimensional position of obstacles. In order of preference:</p>

<ol>
<li><strong>Microsoft Kinect</strong>: Cheap, easy, and effective. Due to ambient IR light, it only works indoors.</li>
<li><strong>Scanning Laser Rangefinder:</strong> Extremely accurate, easy to setup, and works outside. Also very expensive (~$1200-10,000 depending upon maximum range and sample rate).</li>
<li><strong>Stereo Camera:</strong> Not as good as a Kinect, but it works outside. If you cannot afford a pre-made stereo camera (~$1800), you can make a decent custom stereo camera using USB webcams.</li>
</ol>

<p>Note that professional stereo vision cameras can be <em>very fast</em> by using custom hardware (Stereo On-Chip, STOC). Software-based stereo is also reasonably fast (10-20 Hz) on a modern computer.</p>
",111426.0,2.0,1.0,6949549.0,"@Eric, thanks for your note. I think it's not necessary to get a Kinect-like setup but only to detect obstacles. Many cheap and simple robots do it using IR. Am I right?"
1224,6793028,Can we control a robot using an Android phone?,|android|robot|,"<p>After stepping into the world of Android, I wondered if an Android phone can be used as a remote to control a basic pick and place robot . If just an SMS could be send to control the action of a robot like say ""pick object 1 at distance x"" would result in the bot performing the specified action.</p>

<p>Yes, It will involve Artificial Intelligence coupled with the basics of developing a robot but then I wanted to know whether it's possible to develop a machine like this ? If yes , how should one kickstart things ? Would Android ADK be helpful ?</p>

<p>Thanks  </p>
",2011-07-22 16:24:00,6793208.0,3497,8,0,3,,841915.0,"Ahmadabad, India",7/13/2011 3:22,1513.0,6793087.0,"<p>To be honest, I'd recommend knowing how to do it with a computer first. Once you know that, learn how to program Android (that's what we're here for) and get started. Hell, you could make an application to control it - that might be more impressive. </p>

<p>Edit: if you're controlling it via SMS, why are you limited to Android? All of the coding for that would be done on the side of the robot, and you'd have to assign a number for that. I'd recommend an application and communicate via WIFI or Bluetooth. </p>
",801858.0,1.0,3.0,,
1267,8348698,Mindstorm NXT Programming Loop Exit Conditions,|robotics|nxt|lego-mindstorms|,"<p>I am developing a robot for an engineering class.  For the purposes of the class I am required to use the NXT programming language.  To move, the robot needs to follow a solid black line.  </p>

<p>If the robot looses the line, I have it scan to the left for 1 second, or until it reaches a black line.  If no line is found it scans to the right for 2 seconds so the initial position is reached then 1 more second or rotation is achieved.  </p>

<p>I have the loop set up so that if the line has not been found, the robot continues to move.  That runs for a full 1 second time period.  If the line is found, motion stops, but the full second still has to complete.  Ultimately that means that my program works perfectly, but is really really slow.  </p>

<p><strong>tl;dr Is there a way to make loops with two exit condition in the LEGO Mindstorm programming environment?  Either after 1 second has elapsed, or a sensor gets the desired input?</strong></p>
",2011-12-01 21:43:00,8563890.0,9956,3,1,3,,831913.0,United States,7/6/2011 15:32,495.0,8356106.0,"<p>What you could do is make the timeout shorter (100 ms for instance) and stop if the line is found OR the loop ran 10 times.</p>

<p>I am no mindstorms expert, but I expect it to have an OR function.</p>
",47860.0,0.0,1.0,10321868.0,Have I retagged it correctly?
1244,7541489,Obstacle avoidance using 2 fixed cameras on a robot,|graphics|opencv|computer-vision|robotics|,"<p>I will be start working on a robotics project which involves a mobile robot that has mounted 2 cameras (1.3 MP) fixed at a distance of 0.5m in between.I also have a few ultrasonic sensors, but they have only a 10 metter range and my enviroment is rather large (as an example, take a large warehouse with many pillars, boxes, walls .etc) .My main task is to identify obstacles and also find a roughly ""best"" route that the robot must take in order to navigate in a ""rough"" enviroment (the ground floor is not smooth at all). All the image processing is not made on the robot, but on a computer with NVIDIA GT425 2Gb Ram.   </p>

<p>My questions are :</p>

<ol>
<li><p>Should I mount the cameras on a rotative suport, so that they take pictures on a wider angle?</p></li>
<li><p>It is posible creating a reasonable 3D reconstruction based on only 2 views at such a small distance in between? If so, to what degree I can use this for obstacle avoidance and a best route construction?</p></li>
<li><p>If a roughly accurate 3D representation of the enviroment can be made, how can it be used as creating a map of the enviroment? (Consider the following example: the robot must sweep an fairly large area and it would be energy efficient if it would not go through the same place (or course) twice;however when a 3D reconstruction is made from one direction, how can it tell if it has already been there if it comes from the opposite direction  )</p></li>
</ol>

<p>I have found this <a href=""http://mkoval.org/downloads/projects/igvc/igvc_design.pdf"" rel=""nofollow"">response</a> on a similar question , but  I am still concerned with the accuracy of 3D reconstruction (for example a couple of boxes situated at 100m considering the small resolution and distance between the cameras).</p>

<p>I am just starting gathering information for this project, so if you haved worked on something similar please give me some guidelines (and some links:D) on how should I approach this specific task. </p>

<p>Thanks in advance,
Tamash</p>
",2011-09-24 19:50:00,,1824,2,2,5,0.0,920202.0,,8/30/2011 17:15,625.0,7572051.0,"<p>If you want to do obstacle avoidance, it is probably easiest to use the ultrasonic sensors. If the robot is moving at speeds suitable for a human environment then their range of 10m gives you ample time to stop the robot. Keep in mind that no system will guarantee that you don't accidentally hit something.</p>

<blockquote>
  <p>(2) It is posible creating a reasonable 3D reconstruction based on only 2 views at such a small distance in between? If so, to what degree I can use this for obstacle avoidance and a best route construction?</p>
</blockquote>

<p>Yes, this is possible. Have a look at ROS and their vSLAM. <a href=""http://www.ros.org/wiki/vslam"" rel=""nofollow"">http://www.ros.org/wiki/vslam</a> and <a href=""http://www.ros.org/wiki/slam_gmapping"" rel=""nofollow"">http://www.ros.org/wiki/slam_gmapping</a> would be two of many possible resources. </p>

<blockquote>
  <p>however when a 3D reconstruction is made from one direction, how can it tell if it has already been there if it comes from the opposite direction </p>
</blockquote>

<p>Well, you are trying to find your position given a measurement and a map. That should be possible, and it wouldn't matter from which direction the map was created. However, there is the loop closure problem. Because you are creating a 3D map at the same time as you are trying to find your way around, you don't know whether you are at a new place or at a place you have seen before. </p>

<p>CONCLUSION
This is a difficult task!</p>

<p>Actually, it's more than one. First you have simple obstacle avoidance (i.e. <code>Don't drive into things.</code>). Then you want to do simultaneous localisation and mapping (SLAM, read Wikipedia on that) and finally you want to do path planning (i.e. sweeping the floor without covering area twice).</p>

<p>I hope that helps?</p>
",461597.0,1.0,4.0,9180930.0,There's a reason DARPA has been doing grand challenges in this area.  It's not easy.
1146,6041316,iphone/ipad: create a remote control freature,|iphone|controller|robotics|,"<p>I am quite new to iOS development and just thought to take guidance from experts</p>

<p>Actually I have to do a project  in which I can use iPAd/iPhone to control some external device like camera movement or anything similar like that, some PIC programming or anything related to robotics, mechanics which can be controlled from iOS based device.</p>

<p>I am kind of lost goggling please guide me on this.</p>

<p>If you can help me  with these I can get some concrete redirections</p>

<p>1) Links to whitepapers / articles / blogs having relevant material</p>

<p>2) Links of third party libraries which can help me in this</p>

<p>3) Links of demo application which are already there</p>

<p>4) What stream should I focus on to get material regarding the same.</p>

<p>eg: something like survilance system</p>

<p>Thanks in advance</p>
",2011-05-18 07:33:00,6260588.0,654,1,3,1,,195504.0,India,10/23/2009 17:54,1161.0,6260588.0,"<p>So the practical ways to interface an iOS device to a robot are over WiFi, establishing either a UDP or TCP socket. Here are a few links:</p>

<p><a href=""http://www.iphonedevsdk.com/forum/iphone-sdk-development/2023-tcp-ip-udp-networking.html"" rel=""nofollow"">http://www.iphonedevsdk.com/forum/iphone-sdk-development/2023-tcp-ip-udp-networking.html</a>
http://www.youtube.com/watch?v=4XQeZE4nh6M
<a href=""http://www.youtube.com/watch?v=0ipAKzCwn4Y"" rel=""nofollow"">http://www.youtube.com/watch?v=0ipAKzCwn4Y</a></p>

<p>I would not recommend the Bluetooth path, as Apple considers bluetooth as an ""External Accessory"" and requires MFi certification (Made for iPhone)</p>
",229081.0,1.0,0.0,7303247.0,sure:http://www.youtube.com/watch?v=4XQeZE4nh6M
1018,5361791,Robot exploration algorithm,|algorithm|artificial-intelligence|robotics|,"<p>I'm trying to devise an algorithm for a robot trying to find the flag(positioned at unknown location), which is located in a world containing obstacles. Robot's mission is to capture the flag and bring it to his home base(which represents his starting position). Robot, at each step, sees only a limited neighbourhood (<strong>he does not know how the world looks in advance</strong>), but he has an unlimited memory to store already visited cells.  </p>

<p>I'm looking for any suggestions about how to do this in an efficient manner. Especially the first part; namely getting to the flag.</p>

<p><img src=""https://i.stack.imgur.com/cq17j.png"" alt=""enter image description here""></p>
",2011-03-19 11:31:00,,9593,8,5,21,0.0,42803.0,Europe,12/3/2008 13:09,199.0,5361847.0,"<p>With a simple <a href=""http://en.wikipedia.org/wiki/Depth-first_search"" rel=""nofollow"">DFS</a> search at least you will find the flag:)</p>
",222674.0,1.0,2.0,6057719.0,"@Lucasmus: I think it's hard to find some really smart algo, because the robot doesn't know anything about the world, beyond his limited neighbourhood.
@Mitch Wheat:
Well, so far I have only come up with a list of desired properties such algorithm would encompass:
- minimal repetition of visiting already visited cells
- global goal: finding the flag
- local goal: avoiding obstacles and infinite loops"
1282,8513998,Panda3d Robotics,|c++|python|ruby|robotics|panda3d|,"<p>The title makes it obvious, is this a good idea? I've been looking for a robotics simulator in languages i know (i know ruby best, then c++, then python -- want to strengthen here--, forget about javascript, but i know it). </p>

<p></p>

<p>i found something called pyro, but it probably doesn't fit my needs (listed below). </p>

<p></p>

<p>In my last university term i learned c++, then they took me to RobotC (which was only about 2 months of the term). Pyro seems similar but now i want something different.</p>

<p></p>

<p>I need something that allows to import graphics, allows 3d environments, allows to easily modify actions robot can perform. Also provides other things necessary for robot programming, like a sensor.</p>
",2011-12-15 01:24:00,,953,2,1,2,0.0,989635.0,,10/11/2011 13:44,61.0,10025027.0,"<p>Panda 3D is a good language to write your own robot system in.  It's written by CMU people, so it's very clean and makes a lot of sense.  It allows you to import very complex models from Maya or Blender.  It supports 3D environments.  Although it has its own scripting language for running actions (animations) imported from your modeling package, I prefer to write my own robot driver.  It supports three different physics engines, including its own basic version, Open Dynamics Engine (ODE), and most recently Bullet.  Although it supports collision detection, which allows triggering, it is an animation and graphic rendering system, not a robotics system per se, and so you'll have to craft your own sensor simulations beside or on top of it.  All in all, though, it is quite satisfactory.  Good luck.</p>
",841457.0,0.0,3.0,10651737.0,"Have a look here - [http://stackoverflow.com/questions/2533321/robotics-simulator][1]


  [1]: http://stackoverflow.com/questions/2533321/robotics-simulator"
949,4749693,Can I get Erlang OTP behaviors in C Nodes?,|event-handling|erlang|robotics|erlang-otp|erl-interface|,"<p>For example, right now I have a C Node (call it <strong>CN</strong>) which connects to an erlang node (call it <strong>EN</strong>) and uses a RPC to use OTP behaviors. Hence, to send an event from <strong>CN</strong> to an event manager on <strong>EN</strong> I connect <strong>CN</strong> to <strong>EN</strong> and do</p>

<pre><code>args = erl_format(""[data_man, {~f, ~f}]"", ch.at(0), ch.at(1));
erl_rpc_to(fd, ""gen_event"", ""notify"", args);
</code></pre>

<p>But, then, my C Node really isn't behaving as a node (i.e. why create a node that only uses remote procedure calls?).</p>

<p>Is there a way to directly use OTP behaviors within a C Node?</p>

<p>If there isn't, should I look under the hood at the message formats being used by OTP and send messages using that format (i.e. can I spoof OTP behaviors?)? <em>I don't like this idea, I'll have to watch for changes in the implementation of OTP etc.</em></p>

<p>I have hard latency limits in my requirements, how does this effect my choice of communication between a C process and Erlang (are RPCs going to bog me down? etc.)?</p>
",2011-01-20 16:24:00,,320,1,0,3,0.0,79168.0,,3/17/2009 19:19,823.0,4771331.0,"<p>There is no way to directly use OTP behaviours from C. I also don't think you should mimic the OTP behaviours to use them directly.</p>

<p>You should just first use RPC and then test your code against your performance requirements. If needed, you could always send a simple message to your gen_event process to make it notify itself through its handle_info/2 method.</p>
",493099.0,4.0,0.0,,
1274,8463489,How to make a robot follow a line using its video camera,|computer-vision|navigation|robotics|,"<p>I'm trying to get a robot to identify a line on the ground and follow it.<br>
I've searched the internet and found many examples of line following robots, but all of them are using specialized sensors to detect the line.<br>
I'd like to use the camera on the robot for that purpose.</p>

<p>I'm new to the field of computer vision, so I'd like some advice on how to approach the problem.  Specifically, how can I detect the line and its angle/direction in relation to the robot? How can I detect turns?</p>

<p>Update following nikies comment:<br>
How the line looks is up to me, i was thinking to put some bright colored tape on the ground, but i can use whatever is easiest...<br>
The camera can take both color and b&amp;w image.<br>
Lighting and location may vary but i'll worry about it later, i just want to know what to look for to get started. Is there a ""common"" way to do it ?</p>
",2011-12-11 11:12:00,8495755.0,7268,4,2,0,0.0,249878.0,,1/13/2010 14:28,620.0,8495755.0,"<p>Here's one approach, suitable for refinement.</p>

<p>Through a combination of zooming, a pixellation filter, and thresholding, reduce the camera input to a 3 by 3 grid of white or black squares.  With suitable adjustment, the reduction should be able to blow up the line such that it takes up exactly three of the reduced pixels.  The robot's logic then consists of moving in one of eight directions to keep the center pixel black.</p>

<p>The image after reduction:</p>

<pre><code>  
    move forward one unit
  
</code></pre>

<p>What a left turn looks like:</p>

<pre><code>  
    turn 90 degrees left
  
</code></pre>

<p>This is a very simple scheme, and converting the video input to a clean 3 by 3 grid isn't a trivial task, but it should be enough to get you moving in the right direction.</p>
",85950.0,6.0,1.0,10515605.0,i updated the post to answer your questions
1252,7804388,What is the right RTOS for a humanoid robot?,|real-time|robotics|rtos|freertos|,"<p>We're students developing a mid-size (~ 4.5 feet tall) humanoid robot as a sponsored research project in college. The major tasks that the robot should be able to perform include: moving around (forward, backward, sideways), running, picking up objects. We're considering using a <em>hard</em> real-time operating system to control the robot. However, since we're new to this field with practically no background in embedded systems or operating systems, and there are a wide range of options available, we're not sure which one would be a suitable choice. We've come across the following (in brackets is our current impression of them):</p>

<ol>
<li>RTLinux (now dead, kernel 2.4.x, gcc 2.95 (so difficult to build), little to no documentation)</li>
<li>FreeRTOS (good community and documentation, popular, ported to many architectures)</li>
<li>uc-OS II (small, clean core, light-weight)</li>
<li>RTAI (Linux-based)</li>
</ol>

<p>I have a number of questions:</p>

<ol>
<li>Which of the options would be better suited for this project? I know this sounds a little subjective, but <em>any</em> advice would be greatly appreciated. If you feel some important information is missing, please do point that out.</li>
<li>I've come across something called the <a href=""https://www.osadl.org/Realtime-Linux.projects-realtime-linux.0.html"" rel=""noreferrer"">CONFIG_PREEMPT_RT patch</a> for the Linux kernel, which grants hard real-time capabilities to the kernel. There are also pre-compiled kernels with this patch available for Debian-based distributions. Would that alone be sufficient for our requirements?</li>
<li>We have very little knowledge of operating systems in general. Is it necessary for us to learn about them first? If yes, what is a good, short primer to the subject?</li>
</ol>

<p><strong>UPDATE:</strong> Thank you for your incredibly detailed answers. It's clear that we're going about this the wrong way; diving in with no knowledge and vauge requirements certainly would be bad. We'll have to sit down and chalk out exactly what we need. As and when we're sufficiently ahead with that, we'll try to figure out a suitable OS. Let's see how that works out. I'm also going to read Chapter 2 of <a href=""https://rads.stackoverflow.com/amzn/click/com/1578201039"" rel=""noreferrer"" rel=""nofollow noreferrer"">MicroC OS II: The Real Time Kernel</a>.</p>
",2011-10-18 08:27:00,7813685.0,8113,3,1,8,0.0,504611.0,"Hyderabad, Telangana, India",11/11/2010 14:21,2017.0,7813685.0,"<p>Your choice of OS is unlikly to be determined by the ""humanoid robot"" constraint, there is no specific ""humanoid robot OS"", and certainly no OS would be determined by how tall such a robot is! ;-) !  The critical factors are </p>

<ul>
<li>real-time constraints (for motion control <a href=""http://en.wikipedia.org/wiki/PID_controller"" rel=""nofollow"">PID</a> update, sensor/actuator reaction time etc.)</li>
<li>processor architecture</li>
<li>system architecture (e.g. <a href=""http://distribured%20processing"" rel=""nofollow"">distributed processing</a>, <a href=""http://en.wikipedia.org/wiki/Symmetric_multiprocessing"" rel=""nofollow"">symetric-multiprocessing</a>)</li>
</ul>

<p>Other factors may be important such as:</p>

<ul>
<li>Communications and I/O requirements (e.g Ethernet, TCP/IP, USB, WiFi).</li>
<li>File system support</li>
</ul>

<p>although these need not necessarily be an intrinsic part of the OS in all cases, since third-party platform independent libraries are available in many cases, but where you need them, integration with the OS can be helpful since it avoids you having to deal with thread-safety and resource locking yourself.</p>

<p>Neither of the options you have suggested would likely make into my list.</p>

<p>Anything Linux based will require an MMU (unless using uCLinux or its derivitives, but MMU support is one of the few good reasons for using Linux in an embedded system).  Linux is not intended to be a real-time OS and any real-time support it has is pretty much an after-thought, and will seldom be as deterministic as a true RTOS.  Any Linux will also require significant memory resources just to boot-up, expect a minimum of 4Mb of RAM for anything usable, while RTOS kernels such as FreeRTOS and uC/OS-II require only about 4Kb - you are comparing chalk with cheese here.  That said they do not have the utility of a Linux based OS such as file-systems, or networking support (although those can be added as stand-alone libraries).</p>

<p>If you are going to be performing the motion-control and sensor/actuator functions on the same processor as your cognitive functions, then you certainly need a deterministic RTOS.  If however the platform will be a distributed system with separate processors dealing with motion-control and other real-time sensor/actuator I/O, then you may get away with a simple RTOS kernel or no OS at all in the I/O processors (which can also then be smaller, less powerful processors) and a GPOS in the cognitive (decision making and planning) processor.</p>

<p>I have evaluated FreeRTOS recently, it is minimalistic, simple and small, providing only the basic threading, timing and IPC mechanisms and little else.  It works, but so do many other more attractive options, both commercial and non-commercial.  I compared it with <a href=""http://www.keil.com/rl-arm/kernel.asp"" rel=""nofollow"">Keil's RTX kernel</a> (included with their MDK-ARM tool suite), and the commercial <a href=""http://www.segger.com/embos.html"" rel=""nofollow"">Segger embOS</a>.  It has significantly slower context switching time that the other two candidates (though still in the microseconds on a 72MHz Cortex-M3, and faster than anything you are likely to achieve with Linux).</p>

<p>uC/OS-II is well designed and documented (in Jean Labrosse's book), and is great if you aim were to see how an RTOS works.  Its biggest failing is its very restrictive priority scheduling scheme, which is efficient for very small targets, but possibly not as flexible as you might like.  Each thread must be assigned a distinct priority level so it has no support for round-robin scheduling, useful for non-real-time background tasks. uC/OS-III fixes that shortcoming, but again so do many other options.</p>

<p>If your target processor has an MMU I strongly suggest the use of an OS that supports it in such a way that each thread or process is protected from any other, the system will be far more robust and easy to debug, especially when developed as a team.  In such an OS an errant task that would otherwise stomp on some other thread's resources with non-deterministic and generally hard to debug results, will instead cause an exception and halt right where the error occurred, rather than perhaps sometime later when the corrupted data gets used.</p>

<p>You probably need not restrict yourself to a free or open-source RTOS, many vendors allow free use for education and evaluation.  I would strongly suggest that you consider <a href=""http://www.qnx.com/products/neutrino-rtos/index.html"" rel=""nofollow"">QNX Neutrino</a>, it is <a href=""http://www.qnx.com/products/evaluation/academic_faculty.html"" rel=""nofollow"">free for non-commercial and academic use</a>, and has the most robust intrinsic MMU support available in any RTOS, and all the development tools you need including the Eclipse based Momentics IDE are included.  It is more than just a mere scheduling kernel, including support for all the services you would expect of a complete OS. It runs on ARM, x86, SH-4 PowerPC and MIPS architectures.  Running on x86 is particularly useful since it means you can test and evaluate it, and even develop much of your code in a <a href=""http://www.qnx.com/products/evaluation/#target"" rel=""nofollow"">VM running on your desktop</a>.</p>

<p>Another alternative that is true hard-real-time, while supporting OS services beyond mere scheduling and IPC, is <a href=""http://ecos.sourceware.org/"" rel=""nofollow"">eCos</a>.  It has a native, POSIX and uITRON API, standard drivers for CAN, ADC, SPI, I2C, FLASH, PCI, Serial, Filesystems, USB and PCI and more, and includes TCP/IP networking support.  It is a complete OS in that sense, but unlike Linux is not monolithic; it is scalable and statically linked to your application code, so that features you do not use are simply not included in the runtime binary.  It runs on ARM, CalmRISC, Cortex-M, FR-V, FR30, H8, IA32, 68K/ColdFire, Matsushita AM3x, MIPS, NEC V8xx, PowerPC, SPARC, and SuperH.  Again in theory you could run the IA32 (x86) port it on a VM on a PC for test and development of high level code, though you'd have to get that working yourself unlike QNX's out of the box evaluation.</p>

<p><strong>Added regarding:</strong></p>

<blockquote>
  <p>We have very little knowledge of operating systems in general. Is it necessary for us to learn about them first? If yes, what is a good, short primer to the subject?</p>
</blockquote>

<p>This then is perhaps not the time to start learning Linux (although it has the advantages of wide familiarity and community support, it has a lot of stuff you will not need, and a lot of available support resources will not be familiar with real-time control systems applications.  </p>

<p>Chapter 2 of Labrosse's uC/OS-II book gives a general overview of RTOS concepts such as scheduling, synchronisation and IPC that are applicable to most RTOS not just uC/OS-II.  Similar material is presented in Jack Ganssle's recent <a href=""http://www.eetimes.com/electrical-engineers/education-training/courses/4213896/Fundamentals-of--Real-Time-Operating-Systems"" rel=""nofollow"">RTOS Fundamentals</a> course on EETimes (it is similar perhaps because it is sponsored by Mircium and uses uC/OS-II as a case study, but it is similarly general for the most part).</p>

<p>My solution to getting a quick start in any subject is to Google the subject with ""101"" after it (a common introductory course number in academia).  <a href=""http://www.google.com/search?q=rtos%20101"" rel=""nofollow"">""RTOS 101""</a> will get you some starting points of admittedly varying quality - check the reputability of the source, if it is a company, they may be peddling a specific product, if it is a hobbyist, they may have some insights, but perhaps a narrow view (often relating to specific favourite hardware), and may not have the rigour of an academic paper.</p>

<p><strong>Added regarding CONFIG_PREMPT_RT:</strong></p>

<p>It does not render Linux a hard real-time OS.  It may be suitable for some applications.  If you are doing PID motion control (rather than using a dedicated controller or separate processor), or any kind of closed loop feedback control for that matter, this patch will not enable it in my opinion, at least not reliably.  I found this: 
<a href=""http://archive.linuxgizmos.com/a-comparison-of-real-time-linux-approaches/"" rel=""nofollow"">A comparison of real-time Linux approaches</a>.  It discusses a number of approaches to using Linux in real-time applications, including CONFIG_PREMPT_RT.  It discusses these in detail in part C. </p>
",168986.0,21.0,6.0,9531638.0,The OS decision should not be the first decision you make. Design your *system solution* first and then look at it (or ask) to find an OS that meets the system requirements and constraints.  None of the OS requirements can be directly determined from your list of high level behaviours.
1588,14022469,Guiding a Quadrotor Towards a Waypoint,|math|language-agnostic|vector|robotics|,"<p>I am working on a quadrotor, I know its position (a) I know where I would like to go (b) so what I do is calculate a vector c,</p>

<pre><code>c = b - a
c = normalize(c)
</code></pre>

<p>that gives me a unit vector that will take me to my target. But a quadrotor can move in any direction without rotation so what I have tried to do is rotate that vector c by the robots yaw angle then split it into its x y components and pass them to the robot as roll and pitch angles the problem is when the yaw is 0 degrees (-+ 5) this works but when the yaw is say +90 or -90 it fails and steers to wrong directions. My question is am I missing something obvious here?</p>
",2012-12-24 14:15:00,,136,1,1,0,,1926790.0,,12/24/2012 13:49,7.0,14191503.0,"<p>Not doing all the research to be 100% sure of this answer, but it sounds like you are getting a (kind of?) <a href=""http://en.wikipedia.org/wiki/Gimbal_lock"" rel=""nofollow"">Gimbal Lock</a>. A solution to this is to use <a href=""http://en.wikipedia.org/wiki/Quaternion"" rel=""nofollow"">Quaternions</a> as they are not succeptible to Gimbal Lock.</p>
",571138.0,0.0,0.0,19375126.0,"Do a search on ""Euler angle"" or ""quaternion"" or ""rotation matrix"" - three solutions to the same problem.  Each has its advantages."
1350,10050554,Robot Motion in Python,|python|robotics|,"<p>I am trying to program a robot to move. The robot moves based on where currently is. There are four places where it can be:</p>

<pre><code>LOCATION1 Motion Plan is like so,
5 6
3 4
1 2
Initial positon is (x1,y1)
This gets coded as (x1,y1)-&gt;(x1+dx,y1)-&gt;(x1,y1+dy)-&gt;(x1+dx,y1+dy) ... and so on

LOCATION2 Motion Plan is like so,
5 3 1
6 4 2
The initial position is (x1,y1)
This gets coded as (x1,y1)-&gt;(x1,y1-dy)-&gt;(x1-dx,y1)-&gt;(x1-dx,y1-dy) ... and so on

LOCATION3 Motion Plan is like so,
6 5
4 3
2 1
Initial positon is (x1,y1)
This gets coded as (x1,y1)-&gt;(x1-dx,y1)-&gt;(x1,y1+dy)-&gt;(x1-dx,y1+dy) ... and so on

LOCATION4 Motion Plan is like so,
6 4 2
5 3 1
The initial position is (x1,y1)
This gets coded as (x1,y1)-&gt;(x1,y1+dy)-&gt;(x1-dx,y1)-&gt;(x1-dx,y1+dy) ... and so on
</code></pre>

<p>I am struggling to come up with a good pythonic way to code this. I am thinking on the lines of defining 4 different next move rules and then having a bunch of if statements that choose the correct rules</p>

<p>Has someone done something similar... Is there a better way</p>
",2012-04-07 00:03:00,10050806.0,1279,3,1,5,0.0,1257953.0,San Francisco Bay Area,3/8/2012 20:35,311.0,10050582.0,"<p>You could do that, what you would do is.</p>

<pre><code>def motion_loc1(x1,y1,dx,dy):
     # your operation


def motion_loc2(x1,y1,dx,dy):
     # your operation
</code></pre>

<p>And then in the main program, depending upon x1, y1 call the various motion methods.</p>
",18852.0,0.0,0.0,12863104.0,Use state machine design pattern
1406,11424438,detecting lump region in a 2D array,|matlab|image-processing|computer-vision|computational-geometry|robotics|,"<p>in the 2D array plotted below, we are interested in finding the ""lump"" region. As you can see it is not a continuous graph. Also, we know the approximate dimension of the ""lump"" region. A set of data are given below. First column contains the y values and the second contains the x values. Any suggestion as to how to detect lump regions like this?</p>

<p><img src=""https://i.stack.imgur.com/eQDZt.png"" alt=""enter image description here""></p>

<pre><code>   21048        -980
   21044        -956
   21040        -928
   21036        -904
   21028        -880
   21016        -856
   21016        -832
   21016        -808
   21004        -784
   21004        -760
   20996        -736
   20996        -712
   20992        -684
   20984        -660
   20980        -636
   20968        -612
   20968        -588
   20964        -564
   20956        -540
   20956        -516
   20952        -492
   20948        -468
   20940        -440
   20936        -416
   20932        -392
   20928        -368
   20924        -344
   20920        -320
   20912        -296
   20912        -272
   20908        -248
   20904        -224
   20900        -200
   20900        -176
   20896        -152
   20888        -128
   20888        -104
   20884         -80
   20872         -52
   20864         -28
   20856          -4
   20836          16
   20812          40
   20780          64
   20748          88
   20744         112
   20736         136
   20736         160
   20732         184
   20724         208
   20724         232
   20724         256
   20720         280
   20720         304
   20720         328
   20724         352
   20724         376
   20732         400
   20732         424
   20736         448
   20736         472
   20740         496
   20740         520
   20748         544
   20740         568
   20736         592
   20736         616
   20736         640
   20740         664
   20740         688
   20736         712
   20736         736
   20744         760
   20748         788
   20760         812
   20796         836
   20836         860
   20852         888
   20852         912
   20844         936
   20836         960
   20828         984
   20820        1008
   20816        1032
   20820        1056
   20852        1080
   20900        1108
   20936        1132
   20956        1156
   20968        1184
   20980        1208
   20996        1232
   21004        1256
   21012        1280
   21016        1308
   21024        1332
   21024        1356
   21028        1380
   21024        1404
   21020        1428
   21016        1452
   21008        1476
   21004        1500
   20992        1524
   20980        1548
   20956        1572
   20944        1596
   20920        1616
   20896        1640
   20872        1664
   20848        1684
   20812        1708
   20752        1728
   20664        1744
   20640        1768
   20628        1792
   20628        1816
   20620        1836
   20616        1860
   20612        1884
   20604        1908
   20596        1932
   20588        1956
   20584        1980
   20580        2004
   20572        2024
   20564        2048
   20552        2072
   20548        2096
   20536        2120
   20536        2144
   20524        2164
   20516        2188
   20512        2212
   20508        2236
   20500        2260
   20488        2280
   20476        2304
   20472        2328
   20476        2352
   20460        2376
   20456        2396
   20452        2420
   20452        2444
   20436        2468
   20432        2492
   20432        2516
   20424        2536
   20420        2560
   20408        2584
   20396        2608
   20388        2628
   20380        2652
   20364        2676
   20364        2700
   20360        2724
   20352        2744
   20344        2768
   20336        2792
   20332        2812
   20328        2836
   20332        2860
   20340        2888
   20356        2912
   20380        2940
   20428        2968
   20452        2996
   20496        3024
   20532        3052
   20568        3080
   20628        3112
   20652        3140
   20728        3172
   20772        3200
   20868        3260
   20864        3284
   20864        3308
   20868        3332
   20860        3356
   20884        3384
   20884        3408
   20912        3436
   20944        3464
   20948        3488
   20948        3512
   20932        3536
   20940        3564
</code></pre>
",2012-07-11 01:47:00,11424758.0,404,1,1,3,0.0,1284853.0,,3/22/2012 2:18,316.0,11424758.0,"<p>It may be just a coincidence, but the lump you show looks fairly parabolic.  It's not completely clear what you mean by ""know the approximate dimension of the lump region"" but if you mean that you know approximately how wide it is (i.e. how much of the x-axis it takes up), you could simply slide a window of that width along the x-axis and do a parabolic fit (a.k.a. polyfit with degree 2) to all data that fits into the window at each point.  Then, compute r^2 goodness-of-fit values at each point and the point with the r^2 closest to 1.0 would be the best fit.  You'd probably need a threshold value and to throw out those where the x^2 coefficient was positive (to find lumps rather than dips) for sanity, but this might be a workable approach.</p>

<p>Even if the parabolic look is a coincidence, I think this would ba a reasonable approach--a downward pointing parabola is a pretty good description of a general ""lump"" by any definition I can think of.</p>

<p><strong>Edit:  Attempted Implementation Below</strong></p>

<p>I got curious and went ahead and implemented my proposed solution (with slight modifications).  First, here's the code (ugly but functional):</p>

<pre><code>function [x, p] = find_lump(data, width)

  n = size(data, 1);

  f = plot(data(:,1),data(:,2), 'bx-');
  hold on;

  bestX = -inf;
  bestP = [];
  bestMSE = inf;
  bestXdat = [];
  bestYfit = [];

  spanStart = 0;
  spanStop = 1;
  spanWidth = 0;

  while (spanStop &lt; n)
    if (spanStart &gt; 0)
      % Drop first segment from window (since we'll advance x):
      spanWidth = spanWidth - (data(spanStart + 1, 1) - x);
    end

    spanStart = spanStart + 1;
    x = data(spanStart, 1);

    % Advance spanStop index to maintain window width:
    while ((spanStop &lt; n) &amp;&amp; (spanWidth &lt;= width))
      spanStop = spanStop + 1;
      spanWidth = data(spanStop, 1) - x;
    end

    % Correct for overshoot:
    if (spanWidth &gt; width) 
      spanStop = spanStop - 1;
      spanWidth = data(spanStop, 1) - x;
    end

    % Fit parabola to data in the current window:
    xdat = data(spanStart:spanStop, 1);
    ydat = data(spanStart:spanStop, 2);
    p = polyfit(xdat, ydat, 2);

    % Compute fit quality (mean squared error):
    yfit = polyval(p,xdat);
    r = yfit - ydat;
    mse = (r' * r) / size(xdat,1);

    if ((p(1) &lt; -0.002) &amp;&amp; (mse &lt; bestMSE))
      bestMSE = mse;
      bestX = x;
      bestP = p;
      bestXdat = xdat;
      bestYfit = yfit;
    end
  end

  x = bestX;
  p = bestP;

  plot(bestXdat,bestYfit,'r-');
</code></pre>

<p>...and here's a result using the given data (I swapped the columns so column 1 is x values and column 2 is y values) with a window width parameter of 750:</p>

<p><img src=""https://i.stack.imgur.com/J57sR.png"" alt=""Data plot with lump fit drawn in""></p>

<p><strong>Comments:</strong></p>

<p>I opted to use mean squared error between the fit parabola and the original data within each window as the quality metric, rather than correlation coefficient (r^2 value) due to laziness more than anything else.  I don't think the results would be much different the other way.</p>

<p>The output is heavily dependent on the threshold value chosen for the quadratic coefficient (see the bestMSE condition at the end of the loop).  Truth be told, I cheated here by outputing the fit coefficients at each point, then selected the threshold based on the known lump shape.  This is equivalent to using a lump template as suggested by @chaohuang and may not be very robust depending on the expected variance in the data.</p>

<p>Note that some sort of shape control parameter seems to be necessary if this approach is used.  The reason is that any random (smooth) run of data can be fit nicely to some parabola, but not necessarily around the maximum value.  Here's a result where I set the threshold to zero and thus only restricted the fit to parabolas pointing downwards:</p>

<p><img src=""https://i.stack.imgur.com/N4rbz.png"" alt=""Best fit is not a &quot;lump&quot;""> </p>

<p>An improvement would be to add a check that the fit parabola at least has a maximum within the window interval (that is, check that the first derivative goes to zero within the window so we at least find local maxima along the curve).  This alone is not sufficient as you still might have a tiny little lump that fits a parabola better than an ""obvious"" big lump as seen in the given data set.</p>
",23934.0,7.0,0.0,15070721.0,"Since the approximate dimension of the lump region is known, maybe you can use some 'average' or 'standard' lump to perform the cross-correlation to detect it."
1321,9321402,Associate file format with my program (Java),|java|file|associations|robotics|,"<p>I am making Scouting Software (in Java) for my <a href=""http://www.usfirst.org/roboticsprograms/frc"" rel=""nofollow"" title=""First Robotics Competition Official Website"">FRC</a> robotics team. Scouting is like collecting data on other teams' robots during competition. It is absolutely critical that my program makes that process as simple and easy as possible. My program can save its data in two ways, one of which is by writing a .scout file to the user's hard drive. All this is working well, but as a finishing touch i would like to implement a way to associate .scout files with my program so that .scout files are opened with my program. It's like .docx for Microsoft Word. It associates .doc/.docx/...etc to itself such that when the user clicks on a file with those extensions, Word opens itself up and then opens the file the user clicked on. I want something like this for my application. Keep in mind, it is written in Java and meant to work on different operating systems (Windows, OSX, Ubuntu Linux, etc). </p>
",2012-02-17 00:46:00,9322949.0,1868,1,2,2,,901880.0,"Bellevue, WA",8/19/2011 6:08,509.0,9322949.0,"<p>Does the program have a GUI?  If so, launch it with <a href=""https://stackoverflow.com/tags/java-web-start/info"">Java Web Start</a>.  </p>

<p>JWS can associate a file-type with an application on Windows, OS X &amp; *nix.  Here is a <a href=""http://pscode.org/jws/api.html#fs"" rel=""nofollow noreferrer"">demo. of the JNLP API file service</a> that associates the <code>.zzz</code> file type with the demo. app.</p>
",418556.0,2.0,1.0,11760975.0,"It is not provided through standard Java library in Java; it is native OS-specific feature. Under Windows, this can be done through Windows registry http://stackoverflow.com/questions/1387769/create-registry-entry-to-associate-file-extension-with-application-in-c. You can use installer builder like NSIS, InnoSetup, etc. to set a registry entry. Java Web Start (JWS) probably has a this feature to support cross-platform."
1562,13856211,Real time programming in robotics with c++,|c++|real-time|robotics|,"<p>I am working on a robotics project with C++ and <a href=""http://en.wikipedia.org/wiki/OpenCV"" rel=""nofollow"">OpenCV</a>. In this step, I have faced a problem which consists of:</p>

<p>I have two methods <code>moveRight()</code> and <code>moveLeft()</code> that I called successively in my code, but the problem is that the second one does not run, because the first one needs time (the time of robot movement), but when I put <code>Sleep(5000)</code> between them (I guessed that five seconds are enough for the movement), all is OK.</p>

<p>What is a programming solution that avoid the use of <code>Sleep</code> (because it makes some other problems)?</p>
",2012-12-13 08:56:00,13856254.0,1664,3,3,5,,1807373.0,"Madinah, Saudi Arabia",11/7/2012 20:49,793.0,13856254.0,"<p>You can try to add a <a href=""http://en.wikipedia.org/wiki/Indirection"" rel=""nofollow"">layer of indirection</a>. Add a queue of actions to perform, enqueue actions to moveLeft and moveRight, and somewhere else (different thread) execute actions from the queue correctly by waiting for previous action to complete before you do next action. Ideally you need a way to check if action is finished, so you can code it in a event based fashion.</p>
",1520364.0,6.0,7.0,19077952.0,"I created a robot control program for handling computer wafers. It could seamlessly change course in the middle of a move, based perhaps on data collected when the wafer passed through a laser light skirt. It is not a simple problem. I used a realtime operating system, with custom hardware that provided feedback to sync the controller with the robot. There is not nearly enough info in the question to begin to answer it. If this is a commercial project, I might be able to consult."
1320,9310212,How to write multi threaded testcases in robotium,|android|instrumentation|robotics|robotium|,"<p>How can I implement the <strong>Multithreading</strong> in test cases so that no test case will wait for a particular time in a test suit and can complete the test execution fast?</p>
",2012-02-16 11:08:00,,241,0,0,2,0.0,1213638.0,,2/16/2012 11:01,8.0,,,,,,,
1367,10550874,How to calc a cyclic arc through 3 points and parameterize it 0..1 in 3d,|c#|math|robotics|,"<p>How can i calculate an arc through 3 points A, B, C in 3d. from A to C passing B (order is taken care of).</p>

<p>Most robot arms have this kind of move command. I need to simulate it and apply different speed dynamics to it and need therefore a parameter 0..1 which moves a position from A to C.</p>

<p>EDIT:</p>

<p>what i have is radius and center of the arc, but how can i parameterize the circle in 3d if i know the start and end angle?</p>

<p>EDIT2:</p>

<p>getting closer. if i have two unit length perpendicular vectors v1 and v2 on the plane in which the circle lies, i can do a parameterization like: <strong>x</strong>(t) = <strong>c</strong> + r * cos(t) * <strong>v1</strong> + r * sin(t) * <strong>v2</strong></p>

<p>so i take v1 = a-c and i only need to find v2 now. any ideas?</p>
",2012-05-11 11:57:00,16641390.0,3612,3,10,5,0.0,355485.0,"Berlin, Germany",6/1/2010 14:00,343.0,10557037.0,"<p>Martin Doms recently wrote <a href=""http://blog.martindoms.com/2012/04/25/splines-and-curves-part-i-bezier-curves/"" rel=""nofollow"">a blog entry about splines and bezier curves</a> that you might find useful.  </p>

<p>Part of his post describes how to get a 2D curve defined by the three control points P<sub>0</sub>, P<sub>1</sub>, and P<sub>2</sub>.  The curve is parameterized by a value <code>t</code> that ranges from 0 to 1:</p>

<p>F(t) = (1-t)<sup>2</sup> P<sub>0</sub> + 2t (1-t) P<sub>1</sub> + t<sup>2</sup> P<sub>2</sub></p>

<p>It seems likely that you could adapt that to 3D with a little thought.  (Of course, bezier curves don't necessarily go through the control points.  This may not work if that's a deal-breaker for you.)</p>

<p>As an aside, Jason Davies put together <a href=""http://www.jasondavies.com/animated-bezier/"" rel=""nofollow"">a nice little animation of curve interpolation</a>.</p>
",98654.0,3.0,3.0,23933819.0,"I found a solution after all, check my answer."
1454,11810812,QMutexLocker() causes UI to freeze,|python|multithreading|pyqt|mutex|robotics|,"<p>I have a controller class which controls a robot (attached over serial interface). This controller is attached to a view. In addition to that I have a thread derived from <code>QThread</code> which periodically reads out the status of the robot.</p>

<p>Reading out the status must not colide with robot commands which get tiggered from the userinterface. Therefore I locked every robot acces with a mutex using <code>QMutexLocker</code> but this causes my userinterface to freeze if such a mutex block gets executed.</p>

<pre><code>class RobotControl(QObject):
    def __init__(self, view):
        super(RobotControl, self).__init__()
        self.view = view
        self.updatethread = UpdatePositionAndStatus(self.robot)
        self.mutex = QMutex()
        self.connect(self.updatethread, SIGNAL(""updateStatus( QString ) ""), self.print_error)
        self.updatethread.start()

@pyqtSlot()  
def init_robot(self):
    """"""
    Initializes the serial interface to the robot interface and checks if
    there is really a robot interface available.
    """"""
    with QMutexLocker(self.mutex):
        # Open interface
        try:
            index = self.view.robotcontrolui.get_selected_interface_index()
            interface = self.interfaces.item(index).text()
            self.robot = RobotController(interface)
        except DeviceError:
            self.view.error_dlg(self.tr(""Couldn't open interface {0}!"".format(interface)))
            self.robot = None
            return

        # Check if there is really a robot interface on the selected serial
        # interface with trying to read status byte
        try:
            self.robot.status()
        except DeviceError:
            # In case of failure release interface
            self.close_robot()
            self.view.error_dlg(self.tr(""Couldn't initialize robot interface!""))
            return

        self.view.robotcontrolui.bt_open_interface.setEnabled(False)
        self.view.robotcontrolui.bt_close_interface.setEnabled(True)

class UpdatePositionAndStatus(QThread):
    def __init__(self, robot, parent=None):
        QThread.__init__(self, parent) 
        self.robot = robot
        self.mutex = QMutex()
    def run(self):
        """""" 
        This function continously reads out the position and the status to for 
        updating it on the userinterface.
        """"""
        try:
            while True:
                if self.robot is not None:
                    # Do robot communication under a lock
                    self.mutex.lock()
                    (_, rel_pos) = self.robot.read_position()
                    status = self.robot.status()
                    self.mutex.unlock()

                    # Display position and status on userinterface
                    self.view.robotcontrolui.update_position_and_status(rel_pos, status)

                # Wait 1 seccond for next update
                QThread.sleep(1.0)
        except DeviceError:
            # Release lock on robot
            self.mutex.unlock()
            self.emit(SIGNAL(""updateStatus( QString )""), self.tr(""Error while updating current position and status!""))
</code></pre>

<p>After triggering the init method the userinterface freezes and the program crashes: Why is this so? How can I avoid that?</p>
",2012-08-04 17:57:00,,1555,1,0,1,,967164.0,,9/27/2011 13:39,554.0,11812398.0,"<p>It's hard to tell because your code sample is incomplete, but I see two fundamental problems with this code:</p>

<ol>
<li><p>You are locking two different QMutex objects. In order for mutual exclusion to work properly, both threads must be locking the <em>same</em> mutex object.</p></li>
<li><p>You appear to be directly interacting with the GUI from the update thread at this line:</p>

<pre><code>self.view.robotcontrolui.update_position_and_status(rel_pos, status)
</code></pre>

<p>Performing GUI operations may <em>only</em> be done from the GUI thread in Qt. It's a fair bet that this is causing your crash. See: <a href=""http://qt-project.org/doc/qt-4.8/threads.html"" rel=""nofollow"">http://qt-project.org/doc/qt-4.8/threads.html</a></p></li>
</ol>
",643629.0,3.0,7.0,,
1528,13482797,What is the purpose of calibrating in OpenCV?,|opencv|robotics|camera-calibration|calibration|,"<p>I am starting a new robotics project (with cameras), so i need to calibrate its two cameras.</p>

<p>My question is: why do i need to calibrate cameras?
Does it have any relation with defining object dimensions and distance to camera?</p>

<p>Any information could be helpful.</p>

<p>Thanks in advance</p>
",2012-11-20 21:49:00,48476175.0,375,1,3,2,,1807373.0,"Madinah, Saudi Arabia",11/7/2012 20:49,793.0,48476175.0,"<p>Camera calibration (and more generally) instrument calibration is an essential prior step for pretty much any application you want the device for.</p>

<p>For example suppose you have a single camera. In this case you would have to calibrate it (i.e. compute the values for) for its intrinsics parameters (focal length, principal point, skew coefficient). You would do that using a calibration board (e.g., <a href=""https://en.wikipedia.org/wiki/Chessboard_detection"" rel=""nofollow noreferrer"">Checkerboard</a>, <a href=""https://april.eecs.umich.edu/wiki/AprilTags"" rel=""nofollow noreferrer"">AprilTag</a>).</p>

<p>Now, you may say that these parameters are known and reported by the manufacturers of that camera and we can look them up either at the manual or at its corresponding documentation. However that's not always the case:</p>

<ul>
<li><p>Manufacturers might provide you with that calibration information. Even if they do though, those numbers would probably be rough so that they address the whole family of products. That is, they'll probably not going to calibrate every single camera. You could use these calibration parameters but if you want higher accuracy you are better off using them as a rough estimate and do the calibration yourself.</p></li>
<li><p>The device may be affected by the environment and the overall conditions it is operating in (temperature, humidity etc.). For example, say you have a stereo camera. In that case you would calibrate for the length between the two independent RGB cameras. But that length may change depending on the temperature (heating will make the rigid link between the cameras expand).</p></li>
<li><p>The specifications of the device may change depending on the its age and usage so it makes sense to recompute its calibration parameters every N number of years.</p></li>
</ul>

<p>All in all, by all means calibrate your cameras yourself. You'll be surprised at how significant this is.</p>

<p>Take a look at the following links for more information:</p>

<ul>
<li><p><a href=""https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Oth_Rolling_Shutter_Camera_2013_CVPR_paper.pdf"" rel=""nofollow noreferrer"">https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Oth_Rolling_Shutter_Camera_2013_CVPR_paper.pdf</a></p></li>
<li><p><a href=""https://uk.mathworks.com/help/vision/ug/camera-calibration.html"" rel=""nofollow noreferrer"">https://uk.mathworks.com/help/vision/ug/camera-calibration.html</a></p></li>
<li><p><a href=""https://www.measurement.govt.nz/services/temperature-and-humidity/humidity-calibration-service/calibration-certificate/purpose-of-a-"" rel=""nofollow noreferrer"">https://www.measurement.govt.nz/services/temperature-and-humidity/humidity-calibration-service/calibration-certificate/purpose-of-a-</a></p></li>
<li><p><a href=""http://www.npl.co.uk/reference/faqs/should-i-have-my-instrument-calibrated-and,-if-so,-how-often-(faq-force)"" rel=""nofollow noreferrer"">http://www.npl.co.uk/reference/faqs/should-i-have-my-instrument-calibrated-and,-if-so,-how-often-(faq-force)</a></p></li>
</ul>
",2843583.0,2.0,0.0,39968836.0,http://aishack.in/tutorials/two-major-physical-defects-in-cameras/ - Link in the first comment doesn't work.
1447,11782221,iPhone - Any examples of communicating with an Arduino board using Bluetooth?,|iphone|objective-c|bluetooth|arduino|robotics|,"<p>I'm tinkering with an iPhone-controlled <a href=""http://en.wikipedia.org/wiki/Radio-controlled_car"" rel=""nofollow"">RC car</a> chassis that is the base of my <strong>robotics project</strong>. The chassis is controlled with a <a href=""http://wirc.dension.com/"" rel=""nofollow"">WiRC</a> Wi-Fi module. It has eight outputs to control electronic speed controllers and servos. </p>

<p>I'd like to improve my robot's ability to avoid obstacles using sensors. For this purpose, I have an Arduino board which I can interface with various inexpensive rangefinders and proximity sensors. I'm looking for examples or demo projects that would <strong>connect an iPhone to an Arduino board using Bluetooth to send commands to the board and receive data from the board.</strong> Is what I'm thinking of possible? </p>

<p>Thank you for any links to projects or hardware boards that may interact with an iPhone using Bluetooth. It's great if some of these boards have an SDK to simplify development.</p>
",2012-08-02 16:54:00,11782737.0,5441,2,0,1,0.0,967484.0,"Chiang Mai, Thailand",9/27/2011 16:09,2011.0,11782737.0,"<p>Unfortunately, standard Bluetooth communications with devices on iOS is restricted to devices within the MFi program, so you're not going to be able to use that with your Arduino board. However, the new Bluetooth 4.0 LE protocol that is supported in newer iOS devices (iPhone 4S, Retina iPad) is open and can be used to connect any LE device.</p>

<p>iOS 5.0 introduced a new framework for this in Core Bluetooth, and I highly recommend watching the two sessions from WWDC 2012 about this. They also have <a href=""https://developer.apple.com/library/ios/#samplecode/TemperatureSensor/Introduction/Intro.html"">some sample code</a> on the topic. I've been using this myself to connect to some sensors, and it works well for a low-bandwidth application like temperature, proximity, or heart rate sensing.</p>

<p>There are several BT LE modules out there, and it looks like Dr. Michael Kroll is about to start producing an <a href=""http://www.mkroll.mobi/?page_id=386"">Arduino shield for LE communication</a>, which would make it trivial to add this kind of capability onto an Arduino board.</p>
",19679.0,6.0,1.0,,
1366,10544630,"python and ctypes cdll, not getting expected return from function",|python|ctypes|robotics|,"<p>I'm in the process of working on interfacing a haptic robot and eye tracker. So, both come with their own programming requirements, namely that the eye tracker software is based in python and is the main language in which I'm programming. Our haptic robot has an API in C, so I've had to write a wrapper in C, compile it as a DLL, and use ctypes in python to load the functions.</p>

<p>I've tested my DLL with MATLAB, and everything works just fine. However, something about my implementation of ctypes in my python class is not giving me the expected return value when i query the robot's positional coordinates.</p>

<p>I'll post the code here, and a clearer explanation of the problem at the bottom.</p>

<p>C source code for DLL wrapper:</p>

<pre><code>#include &lt;QHHeadersGLUT.h&gt;
using namespace std;
class myHapClass{
public:
void InitOmni()
{
    DeviceSpace* Omni = new DeviceSpace; //Find a the default phantom
}
double GetCoord(int index)
{
    HDdouble hapPos[3];
    hdGetDoublev(HD_CURRENT_POSITION,hapPos);
    return hapPos[index];
}
};

extern ""C"" {
int index = 1;
__declspec(dllexport) myHapClass* myHap_new(){ return new myHapClass();}
__declspec(dllexport) void myHapInit(myHapClass* myHapObj){ myHapObj-&gt;InitOmni();}
__declspec(dllexport) double myHapCoord(myHapClass* myHapObj){ double nowCoord = myHapObj-&gt;GetCoord(index); return nowCoord;}
}
</code></pre>

<p>The theory for this code is simply to have 3 available C (not C++) calls that will be compatible with python/ctypes:</p>

<ol>
<li>myHap_new() returns a pointer to an instance of the class</li>
<li>myHapInit initializes the haptics device</li>
<li>myHapCoord returns a double for the current position, of the axis referenced by int Index.</li>
</ol>

<p>The python class follows here:</p>

<pre><code>import sreb
from ctypes import cdll
lib = cdll.LoadLibrary('C:\Documents and Settings\EyeLink\My Documents\Visual Studio 2010\Projects\myHapDLLSolution\Debug\myHapDLL.dll')

class CustomClassTemplate(sreb.EBObject):
def __init__(self):
    sreb.EBObject.__init__(self)
    self.pyMyHapObj = pyMyHap()
    self.coordval=2.0

def setCoordval(self,c):
    self.coordval = c
    pass

def getCoordval(self):
    return self.coordval

def initOmni(self):
    self.pyMyHapObj.pyHapInit()
    pass

def getCoord(self):
    self.coordval = self.pyMyHapObj.pyHapCoord()
    return self.coordval

class pyMyHap(object):
def __init__(self):
    self.obj = lib.myHap_new()
    self.coord = 1.0

def pyHapInit(self):
        lib.myHapInit(self.obj)

    def pyHapCoord(self):
    self.coord = lib.myHapCoord(self.obj)
    return self.coord
</code></pre>

<p>The theory of the python custom class is to instantiate an object (self.pyMyHapObj = pyMyHap()) of the loaded DLL class. Making a call to the function 'initOmni' successfully initializes the robot, however a call to 'getCoord' does not return the expected value. In fact, the result I get from 'getCoord' is 1 (and it is listed as 1, not 1.0, so I think it is returning an integer, not a double as it should).</p>

<p>In MATLAB, I have use the DLL library, and both the myHapInit and myHapCoord functions work, and I can initialize the robot and query the position coordinates successfully.</p>

<p>So what is it about my python class that is causing ctypes to not have the proper value returned from myHapCoord from my DLL?</p>

<p>Any help would be appreciated.
Thanks</p>

<p>edit: Python version 2.3, if it matters... I'm stuck to that version.</p>
",2012-05-11 02:25:00,10544852.0,2838,1,0,3,0.0,1246135.0,,3/2/2012 22:52,45.0,10544852.0,"<p>Return values default to <code>int</code>.  Use something like:</p>

<pre><code>lib.myHapCoord.restype = ctypes.c_double
</code></pre>

<p>before calling the function to interpret the return value properly.</p>
",235698.0,5.0,3.0,,
1325,9483615,parallel programming for robot control,|controls|parallel-processing|robotics|,"<p>I need to write a program which does 2 tasks at the same time for better efficiency &amp; high response. First task is, for example, get vision data from a camera &amp; process it. </p>

<p>Second task is, receive processed data from first task &amp; do sth else with this data (robot control strategy). However, while robot control task is being performed, the camera data receiving should still be working. </p>

<p>Is there a solution for such type of programming in C++/C#?? I'm learning TBB, is it the right choice? However, I'm reading things like ""loop parallelization"", am I going in the right direction??</p>

<p>This links to a very common style in control programming where the computer is used as a central unit to connect to electronic devices (sensors) &amp; actuators and all these devices are processed concurrently</p>
",2012-02-28 14:20:00,9483712.0,535,2,0,0,0.0,1033713.0,Taipei,11/7/2011 12:16,355.0,9483712.0,"<p>No, your example of loop paralleling is using parallel programming to speed up the result of a calculation for one set of data.</p>

<p>What you need is multitasking. You didn't mention any target architecture. Assuming this will be an embedded system, like a microprocessor, you have several options. There are embedded micro-OSes like VXworks and uC-OS that allow you to do just what you are asking. These allow you to set up multiple ""tasks"" that run virtually concurrently. Of course true concurrency is impossible with one CPU, but the scheduler in these OSes is designed to be very deterministic, for quasi-real-time systems like you describe.</p>
",119527.0,2.0,1.0,,
1452,11804800,Speech Precognition and Do some action with hardware,|c#|speech-recognition|robotics|,"<p>I am good programmer but I am new for Robotics. I want to make small project which will recognize my speech and my light on and off.</p>

<p>Please provide me some reference library for all things.</p>

<p>What kind of hardware I need for it.</p>
",2012-08-04 00:09:00,,216,2,0,0,,1575399.0,,8/4/2012 0:05,51.0,11804858.0,"<p>If you are new to Robotics I would diffiently recommend you to check out the Arduino device. The Arduino device is an easily programmable circut board. You can read more at <a href=""http://www.arduino.cc"" rel=""nofollow"">http://www.arduino.cc</a></p>

<p>Regarding Speech recogniztion you could use Microsoft Speech recogniztion software, which allows you to communicate with your app via. An library. Find out how to install it here: <a href=""http://support.microsoft.com/kb/306537"" rel=""nofollow"">http://support.microsoft.com/kb/306537</a></p>
",1331739.0,0.0,2.0,,
1408,11656945,Computer Vision/Image Processing frameworks,|image-processing|computer-vision|augmented-reality|robotics|,"<p>I'm curious to know if there are any image processing/computer vision frameworks out there that allow you to create a filter pipeline by dynamically creating chains of filters/filter blocks (similar to simulink blocks in MATLAB). </p>

<p>The idea is mostly inspired by <a href=""http://www.roborealm.com/"" rel=""nofollow"">RoboRealm</a>, but I'd like to implement this mostly in C/C++ with the ability to graphically build image processing pipelines. I'm familiar with one such framework, <a href=""https://code.google.com/p/camunits/"" rel=""nofollow"">Camunits</a>, which I shall use as a foundation to build this graphical filter framework, but please do let me know if you are aware of any. CamUnits integrates well with LCM (Lightweight Communications and Marshalling) which handles most of the marshalling and networking needs that I'd like to avoid for now. Furthermore, CamUnits also integrates well with the logging framework within LCM, and has a bunch of tools for image acquisition (firewire cameras, automatic gain/exposure correction, fast de-bayering etc). </p>

<p>In short, I'd like to have the functionality to build a graphical interface that lets you dynamically create image processing pipelines (threaded if-needed), which would in turn help in rapid prototyping of image processing/computer vision algorithms. I'm also curious to know if there'd be any interest in this type of framework (modular, and quickly/highly reconfigurable). </p>
",2012-07-25 19:15:00,,3727,3,2,-1,0.0,768319.0,"Cambridge, MA",5/24/2011 18:41,13.0,11670155.0,"<p>This is (almost) the oldest idea in the zoo of image processing applications: the ""kitchen sink"" GUI app where filters are boxes, images are input to the left, data flow through boxes, images come out to the right.</p>

<p>The oldest I remember using firsthand was <a href=""http://www.agocg.ac.uk/reports/visual/vissyst/dogbo_45.htm"" rel=""nofollow"">Khoros</a> (and that may tell you how old I am), but am almost positive that the people at Xerox had something similar way earlier than that.
More recently, a host of image compositing apps have used a similar UI approach, most notably <a href=""http://en.wikipedia.org/wiki/Apple_Shake"" rel=""nofollow"">Shake</a>.</p>

<p>In my experience, they are quite useful for algorithm exploration, but I have never seen one where the GUI didn't get in the way of getting things done when the problems started getting complicated. ""Visual computing"" is appealing for getting the rough outline of a solution, but there is a reason why harder problems are best reasoned upon and communicated using equations - it's a more concise notation that dispenses with hundres of useless bubbles and lines drawn upon a screen.</p>

<p>In production practice, the usefulness of these apps ends up being tied to their output scripting capabilities: mouse-dragging gets quickly tiresome when you do find a solution to your problem, and you want to apply it to a truckload of images. Then the app better have a way to output code implementing the image transformation in a way that's easy to interface with the rest of your codebase.   </p>
",1435240.0,3.0,0.0,15478942.0,"As others have said here, there are many frameworks that do this (in fact, most of the image processing ones I've seen can be rigged up in this fashion). Apple's Core Image framework on Mac and iOS is built around this structure, and its Quartz Composer tool even lets you do the graphical drag and drop connection of filters, inputs, and outputs. I wrote my own open source iOS framework along these lines, with modular filters or processing operations that you chain together and can swap out as needed. I even know someone who has build a GUI for rapid prototyping of filter chains from this."
1535,13677658,What algorithm should I implement to program a room cleaning robot?,|algorithm|theory|robot|,"<p>For this question assume that the following things are unknown:</p>

<ul>
<li>The size and shape of the room</li>
<li>The location of the robot</li>
<li>The presence of any obsticles</li>
</ul>

<p>Also assume that the following things are constant:</p>

<ul>
<li>The size and shape of the room</li>
<li>The number, shape and location of all (if any) obsticles</li>
</ul>

<p>And assume that the robot has the following properties:</p>

<ul>
<li>It can only move forward in increments of absolute units and turn in degrees. Also the operation that moves will return true if it succeeded or false if it failed to move due to an obstruction</li>
<li>A reasonably unlimited source of power (let's say it is a solar powered robot placed on a space station that faces the sun at all times with no ceiling)</li>
<li>Every movement and rotation is carried out with absolute precision every time (don't worry about unreliable data)</li>
</ul>

<p>I was asked a much simpler version of this question (room is a rectangle and there are no obstacles, how would you move over it guaranteeing you could over every part at least once) and after I started wondering how you would approach this if you couldn't guarantee the shape or the presence of obstacles. I've started looking at this with <a href=""http://en.wikipedia.org/wiki/Dijkstra%27s_algorithm"" rel=""nofollow"">Dijkstra's algorithm</a>, but I'm fascinated to hear how others approach this (or if there is a well accepted answer to this? (How does Roomba do it?)</p>
",2012-12-03 05:29:00,13677758.0,1584,2,0,0,,16959.0,"Livermore, CA",9/17/2008 23:08,7219.0,13677758.0,"<p>Take a look at SLAM <a href=""http://openslam.org/"" rel=""nofollow"">http://openslam.org/</a> and for more  <a href=""http://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping"" rel=""nofollow"">Wiki</a> </p>
",780095.0,2.0,1.0,,
1774,17126898,Problems with bttv camera mobile robot - initialisation noisy image,|linux|video-capture|robot|v4l2|,"<p>I am working with Ubuntu 12.04 LTS. We are having problems receiving a non-noisy image from an eye-in-hand camera on a mobile robot.</p>

<p>The camera image is collected with a BTTV PCI device, specifically the bt878  driver. The BTTV device is a PCI-104 card based on a Bt848 chip, and is supported under Linux by the bttv and associated kernel driver modules. The devices is a capture-only card - which means video is taken directly from a video source without the use of a tuner. The frame grabber is connected to the on-board computer Sensorary 311 (so bttv card=73)</p>

<p>The bttv driver installed is version 0.9.19. The webcam is present in dev/video0. </p>

<pre><code>$ dmesg |grep bttv
[    1.937779] bttv: driver version 0.9.19 loaded
[    1.937783] bttv: using 8 buffers with 2080k (520 pages) each for capture
[    1.937850] bttv: Bt8xx card found (0)
[    1.937873] bttv: 0: Bt878 (rev 17) at 0000:04:01.0, irq: 16, latency: 32, mmio: 0xdfdfe000
[    1.937888] bttv: 0: detected: Sensoray 311 [card=73], PCI subsystem ID is 6000:0311
[    1.937890] bttv: 0: using: Sensoray 311/611 [card=73,autodetected]
[    1.940185] bttv: 0: tuner absent
[    1.940313] bttv: 0: registered device video0
[    1.940591] bttv: 0: registered device vbi0
</code></pre>

<p>The modules loaded are as follows:</p>

<pre><code>$ lsmod | grep video

videodev              105518  2 bttv,v4l2_common
media                  20516  1 videodev
videobuf_dma_sg        18711  1 bttv
videobuf_core          25098  2 bttv,videobuf_dma_sg
video                  19117  1 i915

$ lsmod

Module                  Size  Used by
bt878                  13588  0 
rfcomm                 38104  0 
bnep                   17791  2 
bluetooth             189585  10 rfcomm,bnep
parport_pc             32115  0 
ppdev                  12850  0 
coretemp               13362  0 
kvm                   365588  0 
snd_hda_codec_idt      60238  1 
gpio_ich               13160  0 
snd_hda_intel          32983  3 
snd_hda_codec         116477  2 snd_hda_codec_idt,snd_hda_intel
snd_hwdep              13277  1 snd_hda_codec
snd_pcm                81124  2 snd_hda_intel,snd_hda_codec
microcode              18396  0 
snd_seq_midi           13133  0 
snd_rawmidi            25426  1 snd_seq_midi
psmouse                91381  0 
snd_seq_midi_event     14476  1 snd_seq_midi
snd_seq                51594  2 snd_seq_midi,snd_seq_midi_event
bttv                  116393  1 bt878
v4l2_common            20517  1 bttv
videodev              105518  2 bttv,v4l2_common
media                  20516  1 videodev
videobuf_dma_sg        18711  1 bttv
serio_raw              13032  0 
snd_timer              28932  2 snd_pcm,snd_seq
snd_seq_device         14138  3 snd_seq_midi,snd_rawmidi,snd_seq
videobuf_core          25098  2 bttv,videobuf_dma_sg
btcx_risc              13401  1 bttv
rc_core                21172  1 bttv
tveeprom               17010  1 bttv
mac_hid                13078  0 
snd                    62675  15 snd_hda_codec_idt,snd_hda_intel,snd_hda_codec,snd_hwdep,snd_pcm,snd_rawmidi,snd_seq,snd_timer,snd_seq_device
i915                  479158  2 
drm_kms_helper         47459  1 i915
lpc_ich                16993  0 
drm                   240232  3 i915,drm_kms_helper
i2c_algo_bit           13317  2 bttv,i915
soundcore              14636  1 snd
snd_page_alloc         14109  2 snd_hda_intel,snd_pcm
video                  19117  1 i915
lp                     17456  0 
parport                40931  3 parport_pc,ppdev,lp
e1000e                177679  0 
</code></pre>

<p>We believe that the bt878 driver is loaded correctly, and that the frame grabber is working properly. When the camera is powered off the frame-grabber (viewed in either camorama, vlc or gstreamer-properties programs) is blue, and when the camera is powered on, the image received from the camera is very noisy. No features can be detected but changes in light can be seen.</p>

<p>We have had the camera working before properly (implying that the drivers are ok), however it appears that it is almost random, and dependent on what viewing programs we use and in what order. Once the camera is working, it remains working until we power off the robot. We think this means that the camera is not initialised properly, and so is sending noisy data. Does anyone know of a good way to initialise a camera linked to a BTTV PCI device, besides just ensuring the camera is powered on?</p>

<p>Specifically the camera is part of a manipulator which is installed on a SeekurJr robot from Mobile Robotics. It is required that the manipulator is initialised (turning the camera on) before viewing the stream, which we do. </p>

<p>The camera is a RVision SEE camera.</p>

<p>Questions: How can we initialise the camera? Any other ideas on how to fix the noise?</p>
",2013-06-15 18:52:00,17163118.0,269,1,0,1,,2177538.0,,3/16/2013 17:13,145.0,17163118.0,"<p>i would guess you are having hardware problems, either with your camera or with your framegrabber, most likely with your camera.</p>

<ul>
<li><p>make sure that your camera has it's optics mounted</p></li>
<li><p>try connecting the camera to another display device (e.g. an old analog TV with composite in), and see whether you still have that noisy image</p></li>
<li><p>try connecting another camera to your framegrabber card, and see what the image looks like.</p></li>
</ul>

<p>from the bttv-side, the only configuration that could help, is to select the correct video norm, e.g. whether camera and grabber both agree that they are using <em>PAL</em> or <em>NTSC</em> or whatever (and of course the various subformats).</p>

<ul>
<li>try changing the norm with any viewer program, that allows that during playback, e.g. <code>xawtv</code></li>
</ul>

<p>PS: the fact that you get a nice blue image when the camera is powered off only means, that the framegrabber correctly detects whether there is <em>any</em> signal.</p>
",1169096.0,0.0,0.0,,
1866,20433016,Teachable Robotic Arm coding error,|c++|arduino|robotics|,"<p>I am working on a robotic arm that has six servos which is being controlled by an arduino uno. The servos are analog servos from adafruit and they are special in the sense that you can get feedback from the servos, i.e their actual position after you told it where to go. What I am working on is trying to adapt this example code to include six servos instead of one. <a href=""https://github.com/adafruit/Feedback-Servo-Record-and-Play/blob/master/servo_recordplay.ino"" rel=""nofollow"">https://github.com/adafruit/Feedback-Servo-Record-and-Play/blob/master/servo_recordplay.ino</a></p>

<p>Here is my code so far and its errors </p>

<pre><code>// Example code for recording and playing back servo motion with a 
// analog feedback servo
// http://www.adafruit.com/products/1404


#include &lt;Servo.h&gt;
#include &lt;EEPROM.h&gt;

#define CALIB_MAX 512
#define CALIB_MIN 100
#define SAMPLE_DELAY 25 // in ms, 50ms seems good

uint8_t recordButtonPin = 12;
uint8_t playButtonPin = 7;
uint8_t servo1Pin = 9;
uint8_t servo2Pin = 10;
uint8_t servo1FeedbackPin = A0;
uint8_t servo2FeedbackPin = A1;
uint8_t servo3FeedbackPin = A2;
uint8_t servo4FeedbackPin = A3;
uint8_t servo5FeedbackPin = A4;
uint8_t servo6FeedbackPin = A5;
uint8_t ledPin = 13;

Servo servo1;
Servo servo2;
Servo servo3;
Servo servo4;
Servo servo5;
Servo servo6;

void setup() {
  Serial.begin(9600);
  pinMode(recordButtonPin, INPUT);
  digitalWrite(recordButtonPin, HIGH);
  pinMode(playButtonPin, INPUT);
  digitalWrite(playButtonPin, HIGH);
  pinMode(ledPin, OUTPUT);

  Serial.println(""Servo RecordPlay"");
}

void loop() {
 if (! digitalRead(recordButtonPin)) {
   delay(10);
   // wait for released
   while (! digitalRead(recordButtonPin));
   delay(20);
   // OK released!
   recordAllServos(servo1Pin, servo1FeedbackPin, recordButtonPin);
 }

  if (! digitalRead(playButtonPin)) {
   delay(10);
   // wait for released
   while (! digitalRead(playButtonPin));
   delay(20);
   // OK released!
   playAllServo(servo1Pin, playButtonPin);
 }
}

void playAllServo(uint8_t servoPin, uint8_t buttonPin) {
  uint16_t addr = 0;
  Serial.println(""Playing"");


  servo1.attach(servo1Pin);
  while (digitalRead(buttonPin)) {    
    uint8_t x = EEPROM.read(addr);
    Serial.print(""Read EE: ""); Serial.print(x);
    if (x == 255) break;
    // map to 0-180 degrees
    x = map(x, 0, 254, 0, 180);
    Serial.print("" -&gt; ""); Serial.println(x);
    servo1.write(x);
    delay(SAMPLE_DELAY);
    addr++;
    if (addr == 512) break;
  }
  Serial.println(""Done"");
  servo1.detach();
  delay(250);  
}

void recordAllServos(uint8_t servoPin, uint8_t buttonPin) {
  uint16_t addr = 0;

  Serial.println(""Recording"");
  digitalWrite(ledPin, HIGH);

  pinMode(servo1FeedbackPin, INPUT); 
  pinMode(servo2FeedbackPin, INPUT); 
  pinMode(servo3FeedbackPin, INPUT); 
  pinMode(servo4FeedbackPin, INPUT); 
  pinMode(servo5FeedbackPin, INPUT); 
  pinMode(servo6FeedbackPin, INPUT); 

  while (digitalRead(buttonPin)) 
  {
     readServo(servo1FeedbackPin);
     readServo(servo2FeedbackPin);
     readServo(servo3FeedbackPin);
     readServo(servo4FeedbackPin);
     readServo(servo5FeedbackPin);
     readServo(servo6FeedbackPin);

     if (addr &gt; 506) break;
     delay(SAMPLE_DELAY);
  }
  if (addr != 1024) EEPROM.write(addr, 255);

  digitalWrite(ledPin, LOW);

  Serial.println(""Done"");
  delay(250);
}

void readAllServo(uint8_t analogPin)
{
     uint16_t a = analogRead(analogPin);

     Serial.print(""Read analog pin ""); Serial.print(analogPin); Serial.print("": "");
     Serial.print(a);                  
     if (a &lt; CALIB_MIN) a = CALIB_MIN;
     if (a &gt; CALIB_MAX) a = CALIB_MAX;
     a = map(a, CALIB_MIN, CALIB_MAX, 0, 254);
     Serial.print("" -&gt; ""); Serial.println(a);
     EEPROM.write(addr, a);
     addr++;
}
</code></pre>

<p>The errors that I am getting from the arduino ide are</p>

<pre><code>Teachable_Arm_Mark.cpp: In function 'void loop()':
Teachable_Arm_Mark:10: error: too many arguments to function 'void recordAllServos(uint8_t, uint8_t)'
Teachable_Arm_Mark:49: error: at this point in file
Teachable_Arm_Mark.cpp: In function 'void recordAllServos(uint8_t, uint8_t)':
Teachable_Arm_Mark:100: error: 'readServo' was not declared in this scope
Teachable_Arm_Mark.cpp: In function 'void readAllServo(uint8_t)':
Teachable_Arm_Mark:127: error: 'addr' was not declared in this scope
</code></pre>

<p>Any help much appreciated 
Thanks</p>
",2013-12-06 20:29:00,,377,1,1,1,,2970920.0,,11/9/2013 1:03,3.0,20433317.0,"<p>In void loop you have</p>

<pre><code>  recordAllServos(servo1Pin, servo1FeedbackPin, recordButtonPin);
</code></pre>

<p>but recordAllServos is declared as</p>

<pre><code>void recordAllServos(uint8_t servoPin, uint8_t buttonPin)
</code></pre>

<p>Hence too many arguments 3 vs 2...</p>
",968969.0,0.0,1.0,30522763.0,Good question. Try to add what you have done to fixed these problems nad how what you did affected it.
1793,17942862,Threading in C# and library that does not work,|c#|service|thread-safety|robotics|,"<p>I am implementing a DSS service for a robot we are developing. I have already made an interface for the robot, which has some functions like connect, rotate, move etc. This interface is basically a wrapper around some functions that i got in a library, that does the controlling of the robot.</p>

<p>The whole thing works, as long as i dont have any asynchronous actions within my application. I can move the robot and whatnot and there are no issues. However when i try to implement this in my DSS service, it does not work. I can read whatever i want from the robot and it's outputs/inputs, but i am unable to move the robot itself, which makes me believe this is an issue with asynchronous operations, which makes some functions in the library unable to work.</p>

<p>I have looked around but i am unable to find any way or form in which i can get this done, because i would love to get my service running.</p>

<p>It hangs at the Robot.Move(move) statement, which is the one i am using from the library, making me believe that that is the function that is the culprit.</p>
",2013-07-30 09:12:00,,43,0,3,0,,2028781.0,Netherlands,1/31/2013 11:03,52.0,,,,,,26242321.0,"Could you confirm that by `DSS service` you mean [Distributed Software Services](http://msdn.microsoft.com/en-us/library/bb483067.aspx) rather than [Data Services Server](http://stackoverflow.com/tags/dss/info) as the tag implies? Also, what robot and library are you using, some robotics libraries are inherently single threaded, so you have to make all calls from a single manager thread and let your application only talk through that manager."
1623,14453201,Posterior probability after robot movement,|probability|robot|,"<p>Suppose robot moves in cells and there are 5 cells. Their dist. is as follows:
|   1/9  |   1/3  |   1/3  |   1/9  |  1/9  |</p>

<p>The robot moves one cells towards right. And the world is cyclic. When it moves to the most right cell, it return back to the most left one. </p>

<p>And the posterior probability after one cell movement is as follows:
|  1/9   |   1/9  |   1/3  |   1/3  |   1/9 |</p>

<p>The following diagram is a good illustration. 
<img src=""https://i.stack.imgur.com/IdJqV.png"" alt=""enter image description here""></p>

<p>Can any guy tell me why the posterior probability shifts to the right one cell?
Thanks in advance!</p>
",2013-01-22 07:00:00,14477186.0,449,2,0,1,,1418947.0,,5/26/2012 11:26,58.0,14477186.0,"<p>Think about the probability of the robot being in any cell A at Time t in terms of its probability of being or not being in cell A-1 at Time t-1:</p>

<p>Break event up into mutually exclusive joint events:</p>

<p>-->  <strong>P(Robot loc @ T = A ) = P(Robot loc @ T=A, Robot loc @ T-1 = A-1) + P(Robot loc @ T=A, Robot loc @ T-1 &lt;> A-1)</strong></p>

<p>Use conditional probability to break those joint events up into independent events:</p>

<p>-->  <strong>P(Robot loc @ T= A ) = P(Robot loc @ T=A | Robot loc @ T-1 = A-1) . P(Robot loc @ T-1 = A-1) + P(Robot loc @ T=A | Robot loc @ T-1 &lt;> A-1) . P(Robot loc @ T-1 &lt;> A-1)</strong></p>

<p>And that allows us to use the fact that the robot is moving to the right (the event that the robot has moved to the right has prob 1, any other possibility has prob 0).</p>

<p>-->  <strong>P(Robot loc @ T= A ) = 1 . P(Robot loc @ T-1 = A-1) + 0 . P(Robot loc @ T-1 &lt;> A-1)</strong></p>

<p>Simplify, and get the answer you wanted.</p>

<p>-->  <strong>P(Robot loc @ T= A ) = P(Robot loc @ T-1 = A-1)</strong></p>
",1616231.0,1.0,2.0,,
1649,14862810,How to put and use two different values in buffer in C?,|c|robotics|,"<p>The following is part of a C code to make a robot move in its simulator.</p>

<pre><code>while (1)
{
    sprintf(buf, ""M LR 100 100\n"");    //motor left and right moves with speed 100 each.
    write(sock, buf, strlen(buf));     //sends the buffer to the socket (simulator)
        int lme, rme;                  //lme and rme are right and left motor encoder values, the other value I need to send to buffer.
        sprintf(buf, ""S MELR\n"");      //sensor command to find ME values
        sscanf(buf, ""S MELR %i %i\n"", &amp;lme, &amp;rme);       //sending the actual ME values, that need to be sent to a(nother?) buffer.
        printf(buf, ""lme , rme"");      //the values of MEncoders.
    memset(buf, 0, 80);                //clear the buffer, set buffer value to 0
    read(sock, buf, 80);               //read from socket to get results.        
}
</code></pre>

<p>This does not work, as although the robot moves with speed 100, the terminal just shows S MELR and no motor encoder values, but it shows the value when the M LR command is removed so I think it has something to do with the MELR values not being sent to the buffer. How can this be improved or how can I set a new buffer for the MELR values?</p>
",2013-02-13 20:47:00,,320,3,4,1,,1950436.0,,1/5/2013 5:27,81.0,14863096.0,"<p>You dont need to read buf once more? What do you want to do with lme and rme?      </p>

<pre><code>while (1)
{
    sprintf(buf, ""M LR 100 100\n"");    //motor left and right moves with speed 100 each.
    write(sock, buf, strlen(buf));     //sends the buffer to the socket (simulator)
        int lme, rme;                  //lme and rme are right and left motor encoder values, the other value I need to send to buffer.
        sprintf(buf, ""S MELR\n"");      //sensor command to find ME values
    write(sock, buf, strlen(buf));     //sends the buffer to the socket 
    read(sock, buf, 80);               //read from socket to get results.    
        sscanf(buf, ""S MELR %i %i\n"", &amp;lme, &amp;rme);       //sending the actual ME values, that need to be sent to a(nother?) buffer.
        // ???? printf(buf, ""lme , rme"");      //the values of MEncoders.
    memset(buf, 0, 80);                //clear the buffer, set buffer value to 0
    read(sock, buf, 80);  //???        //read from socket to get results.        
}
</code></pre>

<p>How you can <em>compensate for the difference between the voltage you commanded and the speed you actually achieved</em>??. I dont have any idea about robots but you can try : ? </p>

<pre><code>int lme=100, rme=100;     //lme and rme are right and left motor encoder values, the other value I need to send to buffer.             
while (1)
{
    sprintf(buf, ""M LR %i %i\n"", lme, rme);    //motor left and right moves with speed 100 each.
    write(sock, buf, strlen(buf));     //sends the buffer to the socket (simulator)
        sprintf(buf, ""S MELR\n"");      //sensor command to find ME values
    write(sock, buf, strlen(buf));     //sends the buffer to the socket 
    read(sock, buf, 80);               //read from socket to get results.    
        sscanf(buf, ""S MELR %i %i\n"", &amp;lme, &amp;rme);       
    lme=100+(100-lme) ; rem=100+(100-rme);  // compensate  ????
}
</code></pre>
",1458030.0,0.0,1.0,20835249.0,"As I can understand the task should be: 1. send a motion command to simulator via socket, 2. send a ""get encoder values"" command to simulator via socket, 3. read response from socket, 4. parse response extracting encoders values, 5. print encoder values. Is that correct? If answer is ""yes"" then, how many times do you send commands in your code? I see only 1. And I can not see that you read response with encoders values for parsing."
1703,15764159,Trying to filter (tons of) noise from accelerometers and gyroscopes,|filtering|robotics|noise|kalman-filter|sensor-fusion|,"<p><strong>My project:</strong></p>

<p>I'm developing a slot car with 3-axis accelerometer and gyroscope, trying to estimate the car pose (x, y, z, yaw, pitch) but I have a big problem with my vibration noise (while the car is running, the gears induce vibration and the track also gets it worse) because the noise takes values between 4[g] (where g = 9.81 [m/s^2]) for the accelerometers, for example.</p>

<p>I know (because I observe it), the noise is correlated for all of my sensors</p>

<p>In my first attempt, I tried to work it out with a Kalman filter, but it didn't work because values of my state vectors had a really big noise.</p>

<p>EDIT2: In my second attempt I tried a low pass filter before the Kalman filter, but it only slowed down my system and didn't filter the low components of the noise. At this point I realized this noise might be composed of low and high frecuency components.</p>

<p>I was learning about adaptive filters (LMS and RLS) but I realized I don't have a noise signal and if I use one accelerometer signal to filter other axis' accelerometer, I don't get absolute values, so It doesn't work.</p>

<p>EDIT: I'm having problems trying to find some example code for adaptive filters. If anyone knows about something similar, I will be very thankful.</p>

<p><strong>Here is my question:</strong></p>

<p>Does anyone know about a filter or have any idea about how I could fix it and filter my signals correctly?</p>

<p>Thank you so much in advance,</p>

<p>XNor</p>

<p>PD: I apologize for any mistake I could have, english is not my mother tongue</p>
",2013-04-02 12:30:00,15870832.0,4061,3,0,5,0.0,2235928.0,,4/2/2013 11:48,82.0,15766349.0,"<p>Have you tried a simple low-pass filter on the data?  I'd guess that the vibration frequency is much higher than the frequencies in normal car acceleration data.  At least in normal driving.  Crashes might be another story...</p>
",805659.0,0.0,1.0,,
1614,14405659,"Describing nonlinear transformation between two images, using homography",|image-processing|computer-vision|robotics|camera-calibration|projective-geometry|,"<p>A one to one point matching has already been established 
between the blue dots on the two images. 
The image2  is the distorted version of the image1. The distortion model seems to be
eyefish lens distortion. The question is:
Is there any way to compute a transformation matrix which describes this transition.
In fact a matrix which transforms the blue 
dots on the first image to their corresponding blue dots on the second image?
The problem here is that we dont know the focal length(means images are uncalibrated), however we do have
perfect matching between around 200 points on the two images.
<img src=""https://i.stack.imgur.com/EFL1b.png"" alt=""image1(original)"">
the distorted image:
<img src=""https://i.stack.imgur.com/wEKYT.png"" alt=""eimage2""></p>
",2013-01-18 18:53:00,14460154.0,3241,1,7,8,0.0,699559.0,,4/9/2011 1:47,978.0,14460154.0,"<p>I think what you're trying to do can be treated as a distortion correction problem, without the need of the rest of a classic camera calibration.</p>

<p>A matrix transformation is a linear one and linear transformations map always straight lines into straight lines (<a href=""http://en.wikipedia.org/wiki/Linear_map"" rel=""noreferrer"">http://en.wikipedia.org/wiki/Linear_map</a>). It is apparent from the picture that the transformation is nonlinear so you cannot describe it with a matrix operation.</p>

<p>That said, you can use a lens distortion model like the one used by OpenCV (<a href=""http://docs.opencv.org/doc/tutorials/calib3d/camera_calibration/camera_calibration.html"" rel=""noreferrer"">http://docs.opencv.org/doc/tutorials/calib3d/camera_calibration/camera_calibration.html</a>) and obtaining the coefficients shouldn't be very difficult. Here is what you can do in Matlab:</p>

<p>Call (x, y) the coordinates of an original point (top picture) and (xp, yp) the coordinates of a distorted point (bottom picture), both shifted to the center of the image and divided by a scaling factor (same for x and y) so they lie more or less in the [-1, 1] interval. The distortion model is:</p>

<pre><code>x = ( xp*(1 + k1*r^2 + k2*r^4 + k3*r^6) + 2*p1*xp*yp + p2*(r^2 + 2*xp^2));
y = ( yp*(1 + k1*r^2 + k2*r^4 + k3*r^6) + 2*p2*xp*yp + p1*(r^2 + 2*yp^2));
</code></pre>

<p>Where</p>

<pre><code>r = sqrt(x^2 + y^2);
</code></pre>

<p>You have 5 parameters: k1, k2, k3, p1, p2 for radial and tangential distortion and 200 pairs of points, so you can solve the nonlinear system.</p>

<p>Be sure the x, y, xp and yp arrays exist in the workspace and declare them global:</p>

<pre><code>global x y xp yp
</code></pre>

<p>Write a function to evaluate the mean square error given a set of arbitrary distortion coefficients, say it's called 'dist':</p>

<pre><code>function val = dist(var)

global x y xp yp

val = zeros(size(xp));

k1 = var(1);
k2 = var(2);
k3 = var(3);
p1 = var(4);
p2 = var(5);

r = sqrt(xp.*xp + yp.*yp);
temp1 = x - ( xp.*(1 + k1*r.^2 + k2*r.^4 + k3*r.^6) + 2*p1*xp.*yp + p2*(r.^2 + 2*xp.^2));
temp2 = y - ( yp.*(1 + k1*r.^2 + k2*r.^4 + k3*r.^6) + 2*p2*xp.*yp + p1*(r.^2 + 2*yp.^2));
val = sqrt(temp1.*temp1 + temp2.*temp2);
</code></pre>

<p>Solve the system with 'fsolve"":</p>

<pre><code>[coef, fval] = fsolve(@dist, zeros(5,1));
</code></pre>

<p>The values in 'coef' are the distortion coefficients you're looking for. To correct the distortion of new points (xp, yp) not present in the original set, use the equations:</p>

<pre><code>r = sqrt(xp.*xp + yp.*yp);
x_corr = xp.*(1 + k1*r.^2 + k2*r.^4 + k3*r.^6) + 2*p1*xp.*yp + p2*(r.^2 + 2*xp.^2);
y_corr = yp.*(1 + k1*r.^2 + k2*r.^4 + k3*r.^6) + 2*p2*xp.*yp + p1*(r.^2 + 2*yp.^2);
</code></pre>

<p>Results will be shifted to the center of the image and scaled by the factor you used above.</p>

<p>Notes:</p>

<ul>
<li>Coordinates must be shifted to the center of the image as the distortion is symmetric with respect to it. </li>
<li>It should't be necessary to normalize to the interval [-1, 1] but it is comon to do it so the distortion coefficients obtained are more or less of the same order of magnitude (working with powers 2, 4 and 6 of pixel coordinates would need very small coefficients).</li>
<li>This method doesn't require the points in the image to be in an uniform grid.</li>
</ul>
",1755482.0,6.0,0.0,20045612.0,Can you tell what is the difference between this and image registration ?
1758,16954054,Should I use TCP or UDP packets to control a UAV?,|tcp|udp|robotics|,"<p>Should I use TCP or UDP connection to send control commands to an Unmanned Aerial Vehicle of a PC/base station?</p>

<p>The vehicle is small (approx. the size of a human nail) and needs continous control from a base station to stabilize it. </p>

<p>Here is what I am thinking: TCP is supposed to be reliable transmission but slow whereas UDP is does not provide a guarantee of packet transmission like TCP but is faster than TCP.<br>
Since I really care about getting the packets over to the UAV from the base station as quickly as possible I assume using UDP is the way to go. </p>

<p>Am I way off? Have I oversimplified this problem? </p>
",2013-06-06 05:08:00,17005622.0,1902,2,0,1,,1068636.0,,11/28/2011 3:22,317.0,17005622.0,"<p>In my oppinion i would say neither.</p>

<p>I would higly recommend that you have an internal control loop stabilising the UAV and only using the data connection for sending more behaviour oriented commands such as Fly west, Fly east etc. </p>

<p>I assume you are useing some kind of wireless transmitter for the connection. </p>

<p>If you use UDP you cannot be sure that the control packets reaches the UAV, which could cause it to become unstable and crash.</p>

<p>If you use TCP you cannot guarantee that the control packets reaches the UAV with regular time intervals, which might cause it become unstable and cause a crash.</p>

<p>If you really want to control everything from a base station i would recommend TCP as you can ensure that your control packets reaches the UAV. If you are using a standard wireless transmission you should have plenty of bandwith to retransmit packets that are lost.</p>

<p>If you wish to send large amounts of data eg. Video or sound without any direct impact on UAV stability i would definately go for UDP as you wouldn't care if you lost a frame or two.</p>

<p>I hope it makes sense.</p>

<p>Sigurd</p>
",1944249.0,2.0,1.0,,
1663,14935473,Good Pre-Built Stereo Camera and Robot Vehicle Suggestion,|opencv|camera|computer-vision|robotics|stereo-3d|,"<p>I am working on a hobby project related to autonomous navigation. I want to use stereo camera for obstacle detection and then control the movement of robot vehicle to avoid obstacle.</p>

<p>(1) I am trying to find a readymade robot vehicle that has Parallel/Serial/Ethernet port to interface with a computer. I should be able to send commands through PC to Vehicle to turn left/right, speed up/down, start/stop.</p>

<p>Are there any good,accurate,reliable,cheap robot vehicle available in market?</p>

<p>(2) I am thinking of building a stereo camera. But if a good,accurate,reliable,cheap stereo camera is available on market. I am ready to purchase it too.</p>

<p>Note: It is a small size pet project and hence looking for ""CHEAP yet RELIABLE"" stereo camera and robot vehicle that can be interfaced with PC.</p>

<p>Thanks in advance.</p>

<p>PS: Would be good if items are available in India.</p>
",2013-02-18 11:41:00,14942685.0,1184,1,3,0,,1343922.0,,4/19/2012 12:07,22.0,14942685.0,"<p>I have had some success with a Lego Mindstorm using the <a href=""http://lejos.sourceforge.net/"" rel=""nofollow"">Lejos</a> OS. This supports USB and Bluetooth control, so you can also use a cell phone. Although I have not used a camera with this, there are plenty of tutorials online which suggest it is quite simple to do so.</p>

<p>It comes with a selection of sensors and wheels (the kit I bought had an ultrasound rangefinder and a light sensor) so it's really easy to get something up and running if you're more interested in programming than electronics.</p>

<p>The stereo camera I have on my desk right now is a <a href=""http://nma.web.nitech.ac.jp/fukushima/minoru/minoru3D-e.html"" rel=""nofollow"">Novo Minoru</a>. It is a really cheap option but seems to work okay for simple vision tasks. It is also UVC compliant so seems to be pretty easy to interface with.</p>
",980866.0,1.0,5.0,20981748.0,"Do you have to use a stereo camera pair, or would you consider using a Microsoft Kinect or similar device?"
1728,16507542,Movement of a surgical robot's arm OpenGL,|opengl|graphics|robotics|,"<p>I have a question concerning surgical robot arm's movements in OpenGL. </p>

<p>Our arm consists of 7 pieces that suppose to be the arm's joints and they are responsible for bending and twisting the arm. We draw the arm this way: first we create the element which is responsible for moving the shoulder like ""up and down"" and then we ""move"" using Translatef to the point in which we draw the next element, responsible for twisting the shoulder (we control the movement using Rotatef) and so on with the next joints (elbow, wrist). </p>

<p>The point is to create an arm that can make human-like movements. Our mechanism works, but now our tutor wants us to draw a line strip with the end of the arm. We put the code responsible for drawing and moving an arm between push and pop matrix, so it works like in real, I mean when we move the soulder, any other elements in arm also moves. </p>

<p>There is a lot of elements moving, rotating, we have a couple of rotate matrices that are attached to different elements which we can control and now we have no idea how to precisely find a new location of the end of an arm in space to be able to add a new point to a line strip. Anyone can help?</p>

<pre><code>    glGetFloatv(GL_MODELVIEW_MATRIX,mvm2);

     x=mvm2[12];
     y=mvm2[13];
     z=mvm2[14];

     glPointSize(5.0f);
     glColor3f(1.0f, 0.0f, 0.0f);

     glBegin(GL_POINTS);
     glVertex3f(x,y,z);
     glEnd();
</code></pre>

<p>When I checked using watch what are the x,y,z values, I got (0,-1.16-12e,17.222222), what can't be true, as my arm has length about 9.0 (on z-axis). I think only the last column of modelview matrix is important and I don't have to muliply it by local coordinates of the vertex, as the they are (0,0,0) since I finish my drawning here.</p>
",2013-05-12 12:49:00,16528168.0,1210,2,2,0,,,,,,16510129.0,"<blockquote>
  <p>we have no idea how to precisely find a new location of the end of an arm in space to be able to add a new point to a line strip.</p>
</blockquote>

<p>You do this by performing the matrix math and transformations yourself.</p>

<p>(from comment)</p>

<blockquote>
  <p>To do this we are suppose to multiply the matrices and get some information out of glGetFloatv</p>
</blockquote>

<p>Please don't do this. Especially not if you're supposed to build a pretransformed line strip geometry yourself. OpenGL is not a matrix math library and there's absolutely no benefit to use OpenGL's fixed function pipeline matrix functions. But it has a lot of drawbacks. Better use a real matrix math library.</p>

<p>Your robot arm technically consists of a number of connected segments where each segment is transformed by the composition of transformations of the segments upward in the transformation hierachy.</p>

<pre><code>M_i = M_{i-1}  (R_i  T_i)
</code></pre>

<p>where R_i and T_i are the respective rotation and translation of each segment. So for each segment you need the individual transform matrix to retrieve the point of the line segment. </p>

<p>Since you'll place each segment's origin at the tip of the previous segment you'd transform the homogenous point (0,0,0,1) with the segment's transformation matrix, which has the nice property of being just the 4th column of the transformation matrix.</p>

<p>This leaves you with the task of creating the transformation matrix chain. Doing this with OpenGL is tedious. Use a real math library for this. If your tutor insists on you using the OpenGL fixed function pipeline please ask him to show you the reference for the functions in the specicifications of a current OpenGL version (OpenGL-3 and later); he won't find them because all the matrix math functions have been removed entirely from modern OpenGL.</p>

<p>For math libraries I can recommend <a href=""http://glm.g-truc.net/"" rel=""nofollow"">GLM</a>, <a href=""http://eigen.tuxfamily.org/"" rel=""nofollow"">Eigen</a> (with the OpenGL extra module) and <a href=""https://github.com/datenwolf/linmath.h"" rel=""nofollow"">linmath.h</a> (self advertisement). With each of these libraries building transformation hierachies is simple, because you can create copies of each intermediary matrix without much effort.</p>
",524368.0,3.0,2.0,23698922.0,"We are using only basic OpenGL in our project, no shaders. To do this we are suppose to multiply the matrices and get some information out of glGetFloatv."
1780,17714284,tracking the robot from the overhead cam,|opencv|robotics|,"<p>I am thinking of creating a robot that can navigate using a map. It is controlled from a PC. An 8-bit controller performs low level tasks and the PC is doing the image processing. I plan to implement it in a single room where the robot is placed and the robot and environment are tracked by a camera from a height or from the ceiling of the room. First, the robot needs to be mapped, like this <a href=""http://www.societyofrobots.com/programming_wavefront.shtml"" rel=""nofollow"">http://www.societyofrobots.com/programming_wavefront.shtml</a></p>

<p>To do:</p>

<p>Track the robot from some height using camera Following the wavefont algorithim to locate robot and obstacles.</p>

<p>Procedure:(just my idea)</p>

<ul>
<li><p>The camera will give image of the robot surrounded by obstacles in
the random places. using some opencv technique draw some grind over
the image.</p></li>
<li><p>Locating the grid which contain robot(by having some colored symbol
over the robot) and locating the grids containing the obstacle.</p></li>
<li><p>Now the grids with obstacle is thought as wall and the remaining is
the free space for the robot to navigate.</p></li>
<li><p>robot is going to get the goal place which should be reached is given
from the pc(may be like point the place to reach in the image by
mouse click).</p></li>
</ul>

<p>firstly finding the obstacle from the video stream is to be done for that iam going to catch a image of the robot which is in the room now the image is  edit by manually in M.S paint by filling the grid with stationary obstacle by some color say red.</p>

<p>now the edited image is going to be the reference and it is compared with video stream to tell the free space available to robot.</p>

<p>each grid is given some value using that place value of the robot should be determined </p>

<p>now i should be able to give some grid value in the free space to make it as goal for the robot </p>

<p>now the Pc should calculate and tell the robot which grid should be traveled to reach the goal. the only thing the robot need to figure out itself is dynamic obstacle avoidance like cat walking across the robot </p>

<p>comparing the area of the room with the image of some dimension to find how far the robot moved and how far to go</p>

<p>is it possible to do? can anybody help me to do this? </p>

<p>\thanks in advance</p>
",2013-07-18 03:59:00,19041798.0,1140,1,0,2,,2534242.0,,6/29/2013 8:38,25.0,19041798.0,"<p>Not sure if this will help you, but I did some work related to your post.  Though I'd share.</p>

<p><a href=""http://letsmakerobots.com/node/38208"" rel=""nofollow"">http://letsmakerobots.com/node/38208</a></p>
",2108441.0,0.0,0.0,,
1726,16419356,Swig Python/C Pointer in struct to another struct,|python-2.7|swig|robotics|,"<p>I'm using Player/Stage SWIG generated code from C and now I'm trying to access a structure where a value points to another structure. I wonder how I can extract the array of structures in python.</p>

<p>The C code looks like this:</p>

<pre><code>/** @brief Localization device data. */
typedef struct
{
  /** Device info; must be at the start of all device structures. */
  playerc_device_t info;

  /** Map dimensions (cells). */
  int map_size_x, map_size_y;

  /** Map scale (m/cell). */
  double map_scale;

  /** Next map tile to read. */
  int map_tile_x, map_tile_y;

  /** Map data (empty = -1, unknown = 0, occupied = +1). */
  int8_t *map_cells;

  /** The number of pending (unprocessed) sensor readings. */
  int pending_count;

  /** The timestamp on the last reading processed. */
  double pending_time;

  /** List of possible poses. */
  int hypoth_count;
  player_localize_hypoth_t *hypoths;

  double mean[3];
  double variance;
  int num_particles;
  playerc_localize_particle_t *particles;

} playerc_localize_t;

/** @brief Hypothesis format.

Since the robot pose may be ambiguous (i.e., the robot may at any
of a number of widely spaced locations), the @p localize interface is
capable of returning more that one hypothesis. */
typedef struct player_localize_hypoth
{
  /** The mean value of the pose estimate (m, m, rad). */
  player_pose2d_t mean;
  /** The covariance matrix pose estimate (lower half, symmetric matrix) 
      (cov(xx) in m$^2$, cov(yy) in $^2$, cov(aa) in rad$^2$, 
       cov(xy), cov(ya), cov(xa) ). */
  double cov[6];
  /** The weight coefficient for linear combination (alpha) */
  double alpha;
} player_localize_hypoth_t;
</code></pre>

<p>The code for python is automatically generated. I'm trying to access the elements in hypoths variable, how can I do this in python? I can access most of the variables. Below is the generated swig code...</p>

<pre><code>class playerc_localize(_object):
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, playerc_localize, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, playerc_localize, name)
    __repr__ = _swig_repr
    __swig_setmethods__[""info""] = _playerc.playerc_localize_info_set
    __swig_getmethods__[""info""] = _playerc.playerc_localize_info_get
    if _newclass:info = _swig_property(_playerc.playerc_localize_info_get, _playerc.playerc_localize_info_set)
    __swig_setmethods__[""map_size_x""] = _playerc.playerc_localize_map_size_x_set
    __swig_getmethods__[""map_size_x""] = _playerc.playerc_localize_map_size_x_get
    if _newclass:map_size_x = _swig_property(_playerc.playerc_localize_map_size_x_get, _playerc.playerc_localize_map_size_x_set)
    __swig_setmethods__[""map_size_y""] = _playerc.playerc_localize_map_size_y_set
    __swig_getmethods__[""map_size_y""] = _playerc.playerc_localize_map_size_y_get
    if _newclass:map_size_y = _swig_property(_playerc.playerc_localize_map_size_y_get, _playerc.playerc_localize_map_size_y_set)
    __swig_setmethods__[""map_scale""] = _playerc.playerc_localize_map_scale_set
    __swig_getmethods__[""map_scale""] = _playerc.playerc_localize_map_scale_get
    if _newclass:map_scale = _swig_property(_playerc.playerc_localize_map_scale_get, _playerc.playerc_localize_map_scale_set)
    __swig_setmethods__[""map_tile_x""] = _playerc.playerc_localize_map_tile_x_set
    __swig_getmethods__[""map_tile_x""] = _playerc.playerc_localize_map_tile_x_get
    if _newclass:map_tile_x = _swig_property(_playerc.playerc_localize_map_tile_x_get, _playerc.playerc_localize_map_tile_x_set)
    __swig_setmethods__[""map_tile_y""] = _playerc.playerc_localize_map_tile_y_set
    __swig_getmethods__[""map_tile_y""] = _playerc.playerc_localize_map_tile_y_get
    if _newclass:map_tile_y = _swig_property(_playerc.playerc_localize_map_tile_y_get, _playerc.playerc_localize_map_tile_y_set)
    __swig_setmethods__[""map_cells""] = _playerc.playerc_localize_map_cells_set
    __swig_getmethods__[""map_cells""] = _playerc.playerc_localize_map_cells_get
    if _newclass:map_cells = _swig_property(_playerc.playerc_localize_map_cells_get, _playerc.playerc_localize_map_cells_set)
    __swig_setmethods__[""pending_count""] = _playerc.playerc_localize_pending_count_set
    __swig_getmethods__[""pending_count""] = _playerc.playerc_localize_pending_count_get
    if _newclass:pending_count = _swig_property(_playerc.playerc_localize_pending_count_get, _playerc.playerc_localize_pending_count_set)
    __swig_setmethods__[""pending_time""] = _playerc.playerc_localize_pending_time_set
    __swig_getmethods__[""pending_time""] = _playerc.playerc_localize_pending_time_get
    if _newclass:pending_time = _swig_property(_playerc.playerc_localize_pending_time_get, _playerc.playerc_localize_pending_time_set)
    __swig_setmethods__[""hypoth_count""] = _playerc.playerc_localize_hypoth_count_set
    __swig_getmethods__[""hypoth_count""] = _playerc.playerc_localize_hypoth_count_get
    if _newclass:hypoth_count = _swig_property(_playerc.playerc_localize_hypoth_count_get, _playerc.playerc_localize_hypoth_count_set)
    __swig_setmethods__[""hypoths""] = _playerc.playerc_localize_hypoths_set
    __swig_getmethods__[""hypoths""] = _playerc.playerc_localize_hypoths_get
    if _newclass:hypoths = _swig_property(_playerc.playerc_localize_hypoths_get, _playerc.playerc_localize_hypoths_set)
    __swig_setmethods__[""mean""] = _playerc.playerc_localize_mean_set
    __swig_getmethods__[""mean""] = _playerc.playerc_localize_mean_get
    if _newclass:mean = _swig_property(_playerc.playerc_localize_mean_get, _playerc.playerc_localize_mean_set)
    __swig_setmethods__[""variance""] = _playerc.playerc_localize_variance_set
    __swig_getmethods__[""variance""] = _playerc.playerc_localize_variance_get
    if _newclass:variance = _swig_property(_playerc.playerc_localize_variance_get, _playerc.playerc_localize_variance_set)
    __swig_setmethods__[""num_particles""] = _playerc.playerc_localize_num_particles_set
    __swig_getmethods__[""num_particles""] = _playerc.playerc_localize_num_particles_get
    if _newclass:num_particles = _swig_property(_playerc.playerc_localize_num_particles_get, _playerc.playerc_localize_num_particles_set)
    __swig_setmethods__[""particles""] = _playerc.playerc_localize_particles_set
    __swig_getmethods__[""particles""] = _playerc.playerc_localize_particles_get
    if _newclass:particles = _swig_property(_playerc.playerc_localize_particles_get, _playerc.playerc_localize_particles_set)
    def __init__(self, *args): 
        this = _playerc.new_playerc_localize(*args)
        try: self.this.append(this)
        except: self.this = this
    def destroy(self): return _playerc.playerc_localize_destroy(self)
    def subscribe(self, *args): return _playerc.playerc_localize_subscribe(self, *args)
    def unsubscribe(self): return _playerc.playerc_localize_unsubscribe(self)
    def set_pose(self, *args): return _playerc.playerc_localize_set_pose(self, *args)
    def get_particles(self): return _playerc.playerc_localize_get_particles(self)
    __swig_destroy__ = _playerc.delete_playerc_localize
    __del__ = lambda self : None;
playerc_localize_swigregister = _playerc.playerc_localize_swigregister
playerc_localize_swigregister(playerc_localize)
</code></pre>

<p>Everything compiles in SWIG, but now my python code gives errors. I know that the alpha line may be wrong, but I get the same error for both lines:</p>

<pre><code>LOC = playerc_localize(CON, 0)
if LOC.subscribe(PLAYERC_OPEN_MODE) != 0:
   raise playerc_error_str()

CON.read()
# This should work right? If I omit the index,
# then I get the first value from the array.
print LOC.get_hypoth(0).alpha
print LOC.get_hypoth(1)
</code></pre>

<p>I get the following errors in python:</p>

<pre><code>print LOC.get_hypoth(1).alpha
File ""/usr/local/lib/python2.7/dist-packages/playerc.py"", line 7413, in get_hypoth
return _playerc._playerc_localize_get_hypoth(self, index)
TypeError: in method '_playerc_localize_get_hypoth', argument 1 of type 'playerc_localize_t *'
</code></pre>

<p><strong>PART II</strong>
I've come around another problem, again I'm trying to access some values but it doesn't work and I can't figure out whats wrong. I'm trying to access waypoints in the planner device:</p>

<pre><code>PLN = playerc_planner(CON, 0)
  if PLN.subscribe(PLAYERC_OPEN_MODE) != 0:
     raise playerc_error_str()
# saves the waypoints in PLN.waypoints
PLN.get_waypoints()
# gives: &lt;Swig Object of type 'double (*)[3]' at 0x38105d0&gt;
# if i try accessing members of it using (0) or [0] or something I get errors
print PLN.waypoints
</code></pre>

<p>Now the related files and parts: in playerc.py</p>

<pre><code>class playerc_planner(_object):
...
__swig_setmethods__[""waypoints""] = _playerc.playerc_planner_waypoints_set
    __swig_getmethods__[""waypoints""] = _playerc.playerc_planner_waypoints_get
    if _newclass:waypoints = _swig_property(_playerc.playerc_planner_waypoints_get, _playerc.playerc_planner_waypoints_set)
</code></pre>

<p>The part in playerc_wrap.i:</p>

<pre><code>%header
%{
    #define new_playerc_planner playerc_planner_create
    #define del_playerc_planner playerc_planner_destroy
    typedef playerc_planner_t playerc_planner;
%}

typedef struct
{
  playerc_device info;
  int path_valid;
  int path_done;
  double px, py, pa;
  double gx, gy, ga;
  double wx, wy, wa;
  int curr_waypoint;
  int waypoint_count;
  double (*waypoints)[3];
    %extend
    {
        playerc_planner (playerc_client *client, int index);
        void destroy(void);
        int subscribe (int access);
        int unsubscribe (void);
        int set_cmd_pose (double gx, double gy, double ga);
        int get_waypoints (void);
        int enable (int state);
    }
} playerc_planner;
</code></pre>

<p>And the related part in playerc_wrap.h:</p>

<pre><code>typedef struct
{
  playerc_device_t info;
  int path_valid;
  int path_done;
  double px, py, pa;
  double gx, gy, ga;
  double wx, wy, wa;
  int curr_waypoint;
  int waypoint_count;
  double (*waypoints)[3];
} playerc_planner_t;

PLAYERC_EXPORT playerc_planner_t *playerc_planner_create(playerc_client_t *client, int index);
PLAYERC_EXPORT void playerc_planner_destroy(playerc_planner_t *device);
PLAYERC_EXPORT int playerc_planner_subscribe(playerc_planner_t *device, int access);
PLAYERC_EXPORT int playerc_planner_unsubscribe(playerc_planner_t *device);
PLAYERC_EXPORT int playerc_planner_set_cmd_pose(playerc_planner_t *device,
                                  double gx, double gy, double ga);
PLAYERC_EXPORT int playerc_planner_get_waypoints(playerc_planner_t *device);

PLAYERC_EXPORT int playerc_planner_enable(playerc_planner_t *device, int state);
</code></pre>

<p>So I'm again lost in how to access the waypoints in this double(*)[3] variable, I tried some stuff but it all fails to compile in SWIG. Again thanks!</p>
",2013-05-07 12:42:00,16422401.0,789,1,0,0,,1226156.0,Belgium,2/22/2012 14:55,158.0,16422401.0,"<p>It looks like those dynamically sized arrays are not properly wrapped in libcplayer's SWIG interface file. The <code>LOC.hypoths</code> accessor will probably just give you the first element in the array, not any of the others.</p>

<p>I think the easiest thing to do would be to add something like this to playerc.i and regenerate the SWIG bindings:</p>

<pre><code>%inline %{
  player_localize_hypoth_t *_playerc_localize_get_hypoth(playerc_localize *localize, int index)
  {
    return &amp;(localize-&gt;hypoths[index]);
  }
%}

%extend playerc_localize {
  %pythoncode %{
    def get_hypoth(self, index):
        return _playerc._playerc_localize_get_hypoth(self, index)
  %}
}
</code></pre>

<p>Then you should be able to access element number <code>n</code> in Python with <code>LOC.get_hypoth(n)</code>.</p>

<p>References:</p>

<ul>
<li><a href=""http://www.swig.org/Doc2.0/Python.html#Python_nn40"" rel=""nofollow"">SWIG 2.0 manual section 34.6</a></li>
<li><a href=""http://sourceforge.net/p/playerstage/svn/HEAD/tree/code/player/trunk/client_libs/libplayerc/bindings/python/playerc.i"" rel=""nofollow"">libplayerc/bindings/python/playerc.i</a></li>
<li><a href=""http://sourceforge.net/p/playerstage/svn/HEAD/tree/code/player/trunk/client_libs/libplayerc/playerc.h"" rel=""nofollow"">libplayerc/playerc.h</a></li>
<li><a href=""http://sourceforge.net/p/playerstage/svn/HEAD/tree/code/player/trunk/client_libs/libplayerc/dev_localize.c"" rel=""nofollow"">libplayerc/dev_localize.c</a></li>
<li><a href=""http://sourceforge.net/p/playerstage/svn/HEAD/tree/code/player/trunk/client_libs/libplayerc/bindings/python/playerc_swig_parse.py"" rel=""nofollow"">libplayerc/bindings/python/playerc_swig_parse.py</a></li>
</ul>

<p>Notes:</p>

<p>The name of the struct is <code>playerc_localize</code> instead of <code>playerc_localize_t</code> because of the regular expressions applied to the interface file by the <code>playerc_swig_parse.py</code> script.</p>

<h2>Part II</h2>

<p>A similar problem. Again, there might be a cleverer way to do this, but this simple way is nice and clean and there's no need to try and make it any more complicated than it already is:</p>

<pre><code>%inline %{
  double _playerc_planner_get_waypoint_coord(playerc_planner *planner, int index, int coord)
  {
    return planner-&gt;waypoints[index][coord];
  }
%}

%extend playerc_planner {
  %pythoncode %{
    def get_waypoint(self, index):
        x = _playerc._playerc_planner_get_waypoint_coord(self, index, 0)
        y = _playerc._playerc_planner_get_waypoint_coord(self, index, 1)
        z = _playerc._playerc_planner_get_waypoint_coord(self, index, 2)
        return (x, y, z)
  %}
}
</code></pre>

<p>Then you should be able to access waypoint number <code>n</code> in Python like this:</p>

<pre><code>(x, y, z) = PLN.get_waypoint(n)
</code></pre>
",1639256.0,1.0,13.0,,
1871,20863327,Unable to receive data from serial port,|c++|serial-port|robotics|,"<p>Currently I try to write a serial port communication in VC++ to transfer data from PC and robot via XBee transmitter. But after I wrote some commands to poll data from robot, I didn't receive anything from the robot (the output of filesize is 0 in the code.). Because my MATLAB interface works, so the problem should happen in the code not the hardware or communication. Would you please give me help? </p>

<p>01/03/2014 Updated: I have updated my codes. It still can not receive any data from my robot (the output of read is 0). When I use ""cout&lt;&lt;&amp;read"" in the while loop, I obtain ""0041F01C1"". I also don't know how to define the size of buffer, because I don't know the size of data I will receive. In the codes, I just give it a random size like 103. Please help me.</p>

<pre><code>// This is the main DLL file.
#include ""StdAfx.h""
#include &lt;iostream&gt;

#define WIN32_LEAN_AND_MEAN //for GetCommState command
#include ""Windows.h""
#include &lt;WinBase.h&gt;

using namespace std;

int main(){


  char init[]="""";

  HANDLE serialHandle;

  // Open serial port
  serialHandle = CreateFile(""\\\\.\\COM8"", GENERIC_READ | GENERIC_WRITE, 0, 0, OPEN_EXISTING, FILE_ATTRIBUTE_NORMAL, 0);

// Do some basic settings
  DCB serialParams;
  DWORD read, written;
  serialParams.DCBlength = sizeof(serialParams);

  if((GetCommState(serialHandle, &amp;serialParams)==0))
  {
    printf(""Get configuration port has a problem."");
    return FALSE;
   }

   GetCommState(serialHandle, &amp;serialParams);
   serialParams.BaudRate = CBR_57600;
   serialParams.ByteSize = 8;
   serialParams.StopBits = ONESTOPBIT;
   serialParams.Parity = NOPARITY;

   //set flow control=""hardware""
   serialParams.fOutX=false;
   serialParams.fInX=false;
   serialParams.fOutxCtsFlow=true;
   serialParams.fOutxDsrFlow=true;
   serialParams.fDsrSensitivity=true;
   serialParams.fRtsControl=RTS_CONTROL_HANDSHAKE;
   serialParams.fDtrControl=DTR_CONTROL_HANDSHAKE;

   if (!SetCommState(serialHandle, &amp;serialParams))
   {
       printf(""Set configuration port has a problem."");
       return FALSE;

   }


   GetCommState(serialHandle, &amp;serialParams);

   // Set timeouts
   COMMTIMEOUTS timeout = { 0 };
   timeout.ReadIntervalTimeout = 30;
   timeout.ReadTotalTimeoutConstant = 30;
   timeout.ReadTotalTimeoutMultiplier = 30;
   timeout.WriteTotalTimeoutConstant = 30;
   timeout.WriteTotalTimeoutMultiplier = 30;

   SetCommTimeouts(serialHandle, &amp;timeout);

   if (!SetCommTimeouts(serialHandle, &amp;timeout))
   {
       printf(""Set configuration port has a problem."");
       return FALSE;

   }



   //write packet to poll data from robot
   WriteFile(serialHandle,""&gt;*&gt;p4"",strlen(""&gt;*&gt;p4""),&amp;written,NULL);



   //check whether the data can be received
   char buffer[103];



   do {
  ReadFile (serialHandle,buffer,sizeof(buffer),&amp;read,NULL);
      cout &lt;&lt; read;
    } while (read!=0);

     //buffer[read]=""\0"";



   CloseHandle(serialHandle);
   return 0;
}
</code></pre>
",2013-12-31 21:22:00,,1865,2,4,0,,3150235.0,,12/31/2013 21:10,4.0,20863550.0,"<p>GetFileSize is documented not to be valid when used with a serial port  handle. Use the ReadFile function to receive serial port data.</p>
",2360695.0,1.0,6.0,31302313.0,@Lokno Thanks for reply. Currently I don't have suitable cable to directly connect robot and PC serial... It may take me a little time to answer your question.
1678,15320030,Real time programming in C++,|c++|real-time|robotics|,"<p>I have two C++ classes, <code>objecManip</code> and <code>updater</code>.  The <code>updater</code> class has a timer to check the status of the robot arm of my application.  </p>

<p>If it moving then do nothing, else getNextAction() from the actions queue.</p>

<p>The actions queue is populated with class <code>objectManip</code>.  I have a global variable: <code>current_status</code> of the robot arm that I need in <code>objectManip</code>.</p>

<p>The problem is that when filling in actions queue <code>current_status</code> is taken constantly not dynamically.</p>
",2013-03-10 07:38:00,15320108.0,3101,1,4,-5,,1807373.0,"Madinah, Saudi Arabia",11/7/2012 20:49,793.0,15320108.0,"<p>Question is very unclear, so this is really a stab in the dark, but you need to use atomic data types. With C++11, you have <code>std::atomic</code> (see <a href=""http://en.cppreference.com/w/cpp/atomic/atomic"" rel=""nofollow"">here</a> or <a href=""http://www.cplusplus.com/reference/atomic/atomic/atomic/"" rel=""nofollow"">here</a>. For an earlier version of C++, I think you need to use some library or compiler specific data type, which offers atomic data types.</p>

<p>If you make some assumptions about how multithreading works for your CPU and operating system, you <em>may</em> get away with just declaring shared variables <code>volatile</code> and reading value to temp variable when you use it. <code>volatile</code> is really meant for cases like reading hardware-mapped values, where value must be read from the memory every time, so many optimizations are not possible. It does not guarantee atomic updates in itself, because a thread modifying a value may be interrupted in the middle of update, and then another read may read invalid, partially updated value. For booleans this should be fairly safe. For integers which do not cross memory word boundary and which word size or less, this may be safe on many CPUs, which will not interrupt a thread in the middle of writing single memory word. Otherwise, it is data corruption waiting to happen. Some (today uncommon) CPUs also do not synchronize caches between multiple CPU cores, and in that case <code>volatile</code> will not help, different threads may see different cached value. So conclusion: use <code>volatile</code> as last resort hack!</p>
",1717300.0,1.0,0.0,21630201.0,Why did post the code seem a bad idea?
1797,18150100,SocketServer used to control PiBot remotely (python),|python|raspberry-pi|robot|socketserver|,"<p>this is my first question! (despite using the site to find most of the answers to programming questions i've ever had)</p>

<p>I have created a PiBotController which i plan to run on my laptop which i want to pass the controls (inputs from arrow keys) to the raspberry pi controlling my robot. Now the hardware side of this isn't the issue i have created a program that responds to arrow key inputs and i can control the motors on the pi through a ssh connection.</p>

<p>Searching online i found the following basic server and client code using socketserver which i can get to work sending a simple string.</p>

<p>Server:</p>

<pre><code>import socketserver

class MyTCPHandler(socketserver.BaseRequestHandler):
""""""
The RequestHandler class for our server.

It is instantiated once per connection to the server, and must
override the handle() method to implement communication to the
client.
""""""

    def handle(self):
        # self.request is the TCP socket connected to the client
        self.data = self.request.recv(1024).strip()
        print(""{} wrote:"".format(self.client_address[0]))
        print(self.data)
        # just send back the same data, but upper-cased
        self.request.sendall(self.data.upper())

if __name__ == ""__main__"":
    HOST, PORT = ""localhost"", 9999

# Create the server, binding to localhost on port 9999
server = socketserver.TCPServer((HOST, PORT), MyTCPHandler)

# Activate the server; this will keep running until you
# interrupt the program with Ctrl-C
server.serve_forever()
</code></pre>

<p>Client:</p>

<pre><code>import socket
import sys

HOST, PORT = ""192.168.2.12"", 9999
data = ""this here data wont send!! ""


# Create a socket (SOCK_STREAM means a TCP socket)
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

try:
    # Connect to server and send data
    sock.connect((HOST, PORT))
    sock.sendall(bytes(data + ""\n"", ""utf-8""))

    # Receive data from the server and shut down
    received = str(sock.recv(1024), ""utf-8"")
finally:
    sock.close()

print(""Sent:     {}"".format(data))
print(""Received: {}"".format(received))
</code></pre>

<p>Now this works fine and prints the results both on my Raspberry Pi (server) and on my laptop (client) however i have tried multiple times to combine it into a function that activates along with my key press' and releases' like i have in my controller.</p>

<p>PiBotController</p>

<pre><code>#import the tkinter module for the GUI and input control
try:
    # for Python2
    import Tkinter as tk
    from Tkinter import *
except ImportError:
    # for Python3
    import tkinter as tk
    from tkinter import *

import socket
import sys

#variables
Drive = 'idle'
Steering = 'idle'




#setting up the functions to deal with key presses
def KeyUp(event):
    Drive = 'forward'
    drivelabel.set(Drive)
    labeldown.grid_remove()
    labelup.grid(row=2, column=2)
def KeyDown(event):
    Drive = 'reverse'
    drivelabel.set(Drive)
    labelup.grid_remove()
    labeldown.grid(row=4, column=2)
def KeyLeft(event):
    Steering = 'left'
    steeringlabel.set(Steering)
    labelright.grid_remove()
    labelleft.grid(row=3, column=1)
def KeyRight(event):
    Steering = 'right'
    steeringlabel.set(Steering)
    labelleft.grid_remove()
    labelright.grid(row=3, column=3)
def key(event):
    if event.keysym == 'Escape':
        root.destroy()

#setting up the functions to deal with key releases
def KeyReleaseUp(event):
    Drive = 'idle'
    drivelabel.set(Drive)
    labelup.grid_remove()
def KeyReleaseDown(event):
    Drive = 'idle'
    drivelabel.set(Drive)
    labeldown.grid_remove()
def KeyReleaseLeft(event):
    Steering = 'idle'
    steeringlabel.set(Steering)
    labelleft.grid_remove()
def KeyReleaseRight(event):
    Steering = 'idle'
    steeringlabel.set(Steering)
    labelright.grid_remove()

#connection functions
def AttemptConnection():
    connectionmessagetempvar = connectionmessagevar.get()
    connectionmessagevar.set(connectionmessagetempvar + ""\n"" + ""Attempting to        connect..."") 

def transmit(event):
    HOST, PORT = ""192.168.2.12"", 9999
    data = ""this here data wont send!! ""


    # Create a socket (SOCK_STREAM means a TCP socket)
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

    try:
         # Connect to server and send data
         sock.connect((HOST, PORT))
         sock.sendall(bytes(data + ""\n"", ""utf-8""))

         # Receive data from the server and shut down
         received = str(sock.recv(1024), ""utf-8"")
    finally:
         sock.close()

    print(""Sent:     {}"".format(data))
    print(""Received: {}"".format(received))





#setting up GUI window        
root = tk.Tk()
root.minsize(300,140)
root.maxsize(300,140)
root.title('PiBot Control Centre')
root.grid_columnconfigure(0, minsize=50)
root.grid_columnconfigure(1, minsize=35)
root.grid_columnconfigure(2, minsize=35)
root.grid_columnconfigure(3, minsize=35)
root.grid_rowconfigure(2, minsize=35)
root.grid_rowconfigure(3, minsize=35)
root.grid_rowconfigure(4, minsize=35)
root.configure(background='white')
root.option_add(""*background"", ""white"")




#set up the labels to display the current drive states
drivelabel = StringVar()
Label(root, textvariable=drivelabel).grid(row=0, column=1, columnspan=2)
steeringlabel = StringVar()
Label(root, textvariable=steeringlabel).grid(row=1, column=1, columnspan=2)
Label(root, text=""Drive: "").grid(row=0, column=0, columnspan=1)
Label(root, text=""Steering: "").grid(row=1, column=0, columnspan=1)

#set up the buttons and message for connecting etc..
messages=tk.Frame(root, width=150, height=100)
messages.grid(row=1,column=4, columnspan=2, rowspan=4)


connectionbutton = Button(root, text=""Connect"", command=AttemptConnection)
connectionbutton.grid(row=0, column=4)
connectionmessagevar = StringVar()
connectionmessage = Message(messages, textvariable=connectionmessagevar, width=100, )
connectionmessage.grid(row=1, column=1, rowspan=1, columnspan=1)
disconnectionbutton = Button(root, text=""Disconnect"")
disconnectionbutton.grid(row=0, column=5)







#pictures
photodown = PhotoImage(file=""down.gif"")
labeldown = Label(root, image=photodown)
labeldown.photodown = photodown
#labeldown.grid(row=4, column=1)

photoup = PhotoImage(file=""up.gif"")
labelup = Label(root, image=photoup)
labelup.photoup = photoup
#labelup.grid(row=2, column=1)

photoleft = PhotoImage(file=""left.gif"")
labelleft = Label(root, image=photoleft)
labelleft.photoleft = photoleft
#labelleft.grid(row=3, column=0)

photoright = PhotoImage(file=""right.gif"")
labelright = Label(root, image=photoright)
labelright.photoright = photoright
#labelright.grid(row=3, column=2)

photoupleft = PhotoImage(file=""upleft.gif"")
labelupleft = Label(root, image=photoupleft)
labelupleft.photoupleft = photoupleft
#labelupleft.grid(row=2, column=0)

photodownleft = PhotoImage(file=""downleft.gif"")
labeldownleft = Label(root, image=photodownleft)
labeldownleft.photodownleft = photodownleft
#labeldownleft.grid(row=4, column=0)

photoupright = PhotoImage(file=""upright.gif"")
labelupright = Label(root, image=photoupright)
labelupright.photoupright = photoupright
#labelupright.grid(row=2, column=2)

photodownright = PhotoImage(file=""downright.gif"")
labeldownright = Label(root, image=photodownright)
labeldownright.photodownright = photodownright
#labeldownright.grid(row=4, column=2)




#bind all key presses and releases to the root window
root.bind_all('&lt;Key-Up&gt;', KeyUp)
root.bind_all('&lt;Key-Down&gt;', KeyDown)
root.bind_all('&lt;Key-Left&gt;', KeyLeft)
root.bind_all('&lt;Key-Right&gt;', KeyRight)

root.bind_all('&lt;KeyRelease-Up&gt;', KeyReleaseUp)
root.bind_all('&lt;KeyRelease-Down&gt;', KeyReleaseDown)
root.bind_all('&lt;KeyRelease-Left&gt;', KeyReleaseLeft)
root.bind_all('&lt;KeyRelease-Right&gt;', KeyReleaseRight)

root.bind_all('&lt;Key&gt;', key)
root.bind_all('&lt;Key&gt;', transmit)




#set the labels to an initial state
steeringlabel.set('idle')
drivelabel.set('idle')
connectionmessagevar.set ('PiBotController Initiated')

#initiate the root window main loop
root.mainloop()
</code></pre>

<p>this program compiles fine but then doesn't send any data to the server? (i'm aware its still just sending a string but i thought id start with something easy... and well evidently i got stuck so it is probably for the best)</p>

<p>Any suggestions to make it work just sending the string or sending the varibales drive and steering every time they change would be greatly appreciated.</p>

<p>Dave xx</p>

<p><em><strong>EDIT</em></strong></p>

<p>here is the transmit function i got to it work in the sense it sends data whenever i do a key press/release (like i wanted before) however it only sends the initial setting for the variables of 'idle'. Looking at the code now as well i think i should probably take the host and port info and creating a socket connection out of the function that runs every time? but im not sure so here is what i have right now anyway.</p>

<pre><code>def transmit():
    HOST, PORT = ""192.168.2.12"", 9999
    DriveSend = drivelabel.get
    SteeringSend = steeringlabel.get


    # Create a socket (SOCK_STREAM means a TCP socket)
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

    try:
        # Connect to server and send data
        sock.connect((HOST, PORT))
        sock.sendall(bytes(Drive + ""\n"", ""utf-8""))
        sock.sendall(bytes(Steering + ""\n"", ""utf-8""))

        # Receive data from the server and shut down
        received = str(sock.recv(1024), ""utf-8"")
    finally:
        sock.close()

    print(""Sent:     {}"".format(Steering))
    print(""Sent:     {}"".format(Drive))
    print(""Received: {}"".format(received))
</code></pre>
",2013-08-09 15:16:00,18150274.0,4084,1,0,1,0.0,2668299.0,"Bristol, United Kingdom",8/9/2013 14:40,47.0,18150274.0,"<p>The problem is that when Tkinter catches a key event, it triggers the more specific binding first (for example 'Key-Up'), and the event is never passed to the more general binding ('Key').  Therefore, when you press the 'up' key, KeyUp is called, but transmit is never called.</p>

<p>One way to solve this would be to just call transmit() within all the callback functions (KeyUp, KeyDown, etc).</p>

<p>For example, KeyUp would become</p>

<pre><code>def KeyUp(event):
    Drive = 'forward'
    drivelabel.set(Drive)
    labeldown.grid_remove()
    labelup.grid(row=2, column=2)
    transmit()
</code></pre>

<p>Then you can get rid of the event binding to 'Key'.</p>

<p>Another option would be to make ""Drive"" and ""Steering"" into Tkinter.StringVar objects, then bind to write events using ""trace"", like this:</p>

<pre><code>Drive = tk.StringVar()
Drive.set('idle')
Drive.trace('w', transmit)
</code></pre>

<p>Note that <code>trace</code> sends a bunch of arguments to the callback, so you'd have to edit <code>transmit</code> to accept them.</p>

<p><strong>EDIT</strong></p>

<p>Ok, I see the problems - there are three.  </p>

<p>1.  When you write </p>

<pre><code>Drive = 'forward'
</code></pre>

<p>in your callback functions, you're <strong>not</strong> setting the variable <code>Drive</code> in your module namespace, you're setting <code>Drive</code> in the local function namespace, so the module-namespace <code>Drive</code> never changes, so when <code>transmit</code> accesses it, it's always the same.</p>

<p>2.  In <code>transmit</code>, you write </p>

<pre><code>DriveSend = drivelabel.get
SteeringSend = steeringlabel.get
</code></pre>

<p>This is a good idea, but you're just referencing the functions, not calling them.  You need</p>

<pre><code>DriveSend = drivelabel.get()
SteeringSend = steeringlabel.get()
</code></pre>

<p>3.  In <code>transmit</code>, the values you send through the socket are the module-level variables <code>Drive</code> and <code>Steering</code> (which never change as per problem #1), rather than <code>DriveSend</code> and <code>SteeringSend</code>.</p>

<p><strong>Solution:</strong></p>

<p>I would recommend doing away with all the <code>Drive</code> and <code>Steering</code> variables entirely, and just using the <code>StringVars</code> 'drivelabel<code>and</code>steeringlabel`.  Thus your callbacks can become:</p>

<pre><code>def KeyUp(event):
#    Drive = 'forward'   (this doesn't actually do any harm, but to avoid confusion I'd just get rid of the Drive variables altogether)
    drivelabel.set('forward')
    labeldown.grid_remove()
    labelup.grid(row=2, column=2)
</code></pre>

<p>(and so on for the rest of the callbacks) and your transmit function will become</p>

<pre><code>def transmit():
    HOST, PORT = ""192.168.2.12"", 9999
    DriveSend = drivelabel.get()        # Note the ()
    SteeringSend = steeringlabel.get()

    # Create a socket (SOCK_STREAM means a TCP socket)
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

    try:
        # Connect to server and send data
        sock.connect((HOST, PORT))
        sock.sendall(bytes(DriveSend + ""\n"", ""utf-8""))    # Note Drive ==&gt; DriveSend
        sock.sendall(bytes(SteeringSend + ""\n"", ""utf-8"")) # Note Steering ==&gt; SteeringSend

        # Receive data from the server and shut down
        received = str(sock.recv(1024), ""utf-8"")
    finally:
        sock.close()

    print(""Sent:     {}"".format(SteeringSend))   # Note Steering ==&gt; SteeringSend
    print(""Sent:     {}"".format(DriveSend))      # Note Drive ==&gt; DriveSend
    print(""Received: {}"".format(received))
</code></pre>

<p><strong>modified solution (from OP):</strong></p>

<p>Since playing around with this method for a little while i found that constantly changing the variables every 100ms due to key being held down is troublesome and causes isues with the smoothness of the motor control when i am for example just driving forward. to fix this i used the following edit into each function</p>

<pre><code>def KeyUp(event):
    if drivelabel.get() == ""forward"":
        pass
    else:
        drivelabel.set(""forward"")
        labeldown.grid_remove()
        labelup.grid(row=2, column=2)
        transmit()
        print (drivelabel.get())
</code></pre>

<p>The code now checks if the varibale is already set to the relevent direction if it is it does nothing, otherwise it modifies it. the print line is just there for me to check it was working properly and could be removed or commented out.</p>
",1460057.0,2.0,5.0,,
1691,15559079,Python threading for wheel encoders on a Robot,|python|python-multithreading|robot|,"<p>I'm writing the code for a robot which my college is entering into a competition. I'm currently trying to build some wheel encoders using reflectance sensors. I realised a while back that I would probably need to use threading to achieve this, seeing as the robot needs to monitor both the left and right encoders at the same time. The code below is what I have so far: </p>

<pre><code>from __future__ import division
import threading
import time
from sr import *
R = Robot()

class Encoder(threading.Thread):
    def __init__(self, motor, pin, div=16):
        self.motor = motor
        self.pin = pin
        self.div = div
        self.count = 0
        threading.Thread.__init__(self)

    def run(self):
        while True: 
            wait_for(R.io[0].input[self.pin].query.d)
            self.count += 1

    def rotations(self, angle, start_speed=50):
        seg = 360/self.div
        startcount = self.count
        current_dist = angle #Distance away from target
        R.motors[self.motor].target = start_speed
        while current_dist &gt; 360:
            newcount = self.count - startcount
            current_dist = angle - newcount*seg
            R.motors[self.motor].target = 50
        while abs(current_dist) &gt; seg/2:  
            newcount = self.count - startcount
            current_dist = angle - newcount*seg
            current_speed = start_speed * current_dist / 360
            if current_speed &lt; 5:
                R.motors[self.motor].target = 5
            else:
                R.motors[self.motor].target = current_speed
        R.motors[self.motor].target = 0

WheelLeft = Encoder(0,0)
WheelLeft.start()
WheelRight = Encoder(1,3)
WheelRight.start()

WheelRight.rotations(720)
WheelLeft.rotations(720)
</code></pre>

<p>The sr module is provided by Southampton University, who are running the competition. It allows us to interact with the robot's hardware. </p>

<p>Now, the threads which get created seem to allow the two reflectance sensors to be monitored separately. This bit of code: <code>R.io[0].input[self.pin].query.d</code> works out whether the value coming from the reflectance sensor has changed. The 'rotations' method turns the wheel through a certain angle by constantly checking how many degrees the wheel has already turned through, and slowing it down as it reaches the end. I would like both wheels to start turning when I run the program, and then slow down and stop when they have gone through 2 rotations. Currently though, when I run the program, one wheel starts turning and slows down and stops, followed by the other wheel. It seems to me like the 'rotations' method is not running in a thread, like the 'run' method is. Is it only the code under the 'run' method that runs in a thread, or is it the whole class?</p>

<p>If it helps, I've been following this tutorial: <a href=""http://www.devshed.com/c/a/Python/Basic-Threading-in-Python/1/"" rel=""nofollow"">http://www.devshed.com/c/a/Python/Basic-Threading-in-Python/1/</a></p>

<p>Also, I would like to know why it is possible to start a thread only with <code>Encoder(0,0).start()</code>. Why do you not have to create an object using the class (e.g. <code>Thread = Encoder(0,0).start()</code> for a new thread to be created?</p>

<p>Sorry if the terminoligy I've used isn't up to scratch, as you can probably tell I'm quite new to threading, and programming in general. </p>
",2013-03-21 22:04:00,15559137.0,1886,3,0,1,,2197015.0,,3/21/2013 21:20,6.0,15559137.0,"<p><code>Encoder(0,0).start()</code> is a call to the method to start the thread. In turn, this method calls your <code>run</code> implementation, which doesn't use the <code>rotations</code> method. If you want to do so, then you have to call it in the while loop of <code>run</code>.</p>

<p>With <code>Thread = Encoder(0,0).start()</code> you store the value retrieved from that call (which is None), but to get it you need to start the new thread first anyway.</p>
",1019227.0,1.0,0.0,,
1720,16394748,Controlling robot makes a shaking movement,|c|coordinates|robotics|,"<p>Im trying to control a robot by sending positions with 100hz. It's making a shaking movement when sending so much positions. When I send 1 position that is like 50 mm from his start position it moves smoothly. When I use my sensor to steer,(so it send every position from 0 to 50mm) it is shaking. I'm probably sending like X0-X1-X2-X1-X2-X3-X4-X5-X4-X5 and this is the reason why it might shake. How can I solve this making the robot move smoothly when I use my mouse to steer it?</p>

<ul>
<li>Robot is asking 125hz</li>
<li>IR sensor is sending 100hz</li>
<li>Otherwise does the 25hz makes the diffrent?</li>
</ul>

<p>Here is my code.</p>

<pre><code>        while(true)
        // If sensor 1 is recording IR light.
        if (listen1.newdata = true)
        {

            coX1 = (int) listen1.get1X();           // 
            coY1 = (int) listen1.get1Y();       
            newdata = true;
        } else {
            coX1 = 450;
            coY1 = 300;
        }

        if (listen2.newdata = true)
        {       
            coX2 = (int) listen2.get1X();
            coY2 = (int) listen2.get1Y();
            newdata = true; 
        } else {
            coY2 = 150;
        }
        // If the sensor gets further then the workspace, it will automaticly correct it to these 
        // coordinates.

        if (newdata = true)
        {
            if (coX1&lt; 200 || coX1&gt; 680)
            {
                coX1 = 450;
            }
            if (coY1&lt;200 || coY1&gt; 680)
            {
                coY1 = 300;
            }
            if (coY2&lt;80 || coY2&gt; 300)
            {
                coY2 = 150;
            }
        }
        // This is the actually command send to a robot.
        Gcode = String.format( ""movej(p[0.%d,-0.%d, 0.%d, -0.5121, -3.08, 0.0005])""+ ""\n"", coX1, coY1, coY2);

        //sends message to server
        send(Gcode, out);     
            System.out.println(Gcode);
            newdata = false;

        }


}


private static void send(String movel, PrintWriter out) {
     try {


         out.println(movel); /*Writes to server*/
        // System.out.println(""Writing: ""+ movel);
        // Thread.sleep(250);
         }

         catch(Exception e) {
         System.out.print(""Error Connecting to Server\n"");
         } 
        }
}
</code></pre>

<p>@ Edit</p>

<p>I discovered on wich way I can do this. It is via min and max. So basicly what I think I have to do is: 
 * put every individual coordinate in a array( 12 coordinates)
 * Get the min and max out of this array 
 * Output the average of the min and max</p>
",2013-05-06 08:22:00,,164,1,2,0,,2234928.0,,4/2/2013 6:57,33.0,16395070.0,"<p>Without knowing more about your robot characteristics and how you could control it, here are some general considerations:</p>

<p>To have a smooth motion of your robot, you should control it in speed with a well designed PID controller algorithm.</p>

<p>If you can only control it in position, the best you can do is monitoring the position &amp; waiting for it to be ""near enough"" from the targetted position before sending the next position.</p>

<p>If you want a more detailed answer, please give more information on the command you send to the robot (<code>movej</code>), I suspect that you can do much more than just sending [x,y] coordinates. </p>
",2622924.0,1.0,1.0,23500928.0,This question should be moved to http://robotics.stackexchange.com
1923,21999921,"distance measuring from ultrasonic sensor, variable overflow",|c|avr|robotics|,"<p>Hy, I have ultrasonic sensor measuring distance and no mather which type is my variable ""range""  (uint8_t, uint16_t, 32, 64)I always get overflow, and than sensor starts from 0 again..Is there a way that I can limit ""range"" variable or I must limit that on harder way with pulsewidth... Thanks</p>

<pre><code>SENSOR_DDR |= (1&lt;&lt;TRIGGER_PIN);  
SENSOR_DDR &amp;= ~(1&lt;&lt;ECHO_PIN) &amp; ~(1&lt;&lt;PB3) &amp; ~(1&lt;&lt;PB2) &amp; ~(1&lt;&lt;PB1) &amp; ~(1&lt;&lt;PB0); 
DDRD = DDRD | _BV(4); 
PORTD = PORTD | _BV(4);
ENGINE_DDR = 0xff; 
ENGINE_PORT = 0;

lcd_init(LCD_DISP_ON);
lcd_clrscr();
lcd_puts(""Something wrong..."");


while(1)
{

PORTB |= (1&lt;&lt;PB4); //Send Trigger
_delay_us(10);
PORTB &amp;= ~(1&lt;&lt;PB4); //Send trigger


timer0counter=0;
TCNT0=0; //Clear timer
while(bit_is_clear(PINB,5)); //Wait for rising edge
TCCR0 |= (1&lt;&lt;CS02); //Select prescalar 256
TIMSK |= (1&lt;&lt;TOIE0) | (1&lt;&lt;TOIE2); //Enable timer0 overflow interrupt

lcd_clrscr();

while(bit_is_set(PINB,5) &amp;&amp; timer0counter&lt;9) //wait for falling edge of echo
{
_delay_us(5);
}
TCCR0 &amp;= ~(1&lt;&lt;CS02); //Stop timer
TIMSK &amp;= ~(1&lt;&lt;TOIE0);
if(bit_is_set(PINB,5))
{
lcd_puts(""No OBSTACLE"");
}
else
{
range=(256*timer0counter+TCNT0)*32*0.017; //range conversion

lcd_clrscr();
lcd_puts(""Distance:"");
lcd_puts(itoa(range,buffer,10));
lcd_puts_P(""cm"");
}
if(range&lt;15){
...

ISR(TIMER0_OVF_vect) 
{
TIMSK &amp;= ~(1&lt;&lt;TOIE0);
TCNT0=0;
timer0counter++;

TIMSK |= (1&lt;&lt;TOIE0);

if(timer0counter&gt;8)
{
TCCR0 &amp;= ~(1&lt;&lt;CS02);
TIMSK &amp;= ~(1&lt;&lt;TOIE0);

}
</code></pre>
",2014-02-24 21:56:00,22000520.0,437,1,2,0,,3214354.0,,1/20/2014 8:38,6.0,22000520.0,"<p>The calculation </p>

<pre><code>256*timer0counter+TCNT0
</code></pre>

<p>saves temporary value in 'default' size int, which on AVR is 16b. so, every time timer0counter is higher than 256 it will overflow regardless of the final type of the variable.</p>

<p>instead of doing</p>

<pre><code>range=(256*timer0counter+TCNT0)*32*0.017;
</code></pre>

<p>try going with:</p>

<pre><code>double range_real = 256.0 * (double)timer0counter + (double)TCNT0 * 32.0 * 0.017;
range = (int) range_real;
</code></pre>

<p>Being explicit about types can really save your skin.</p>
",3010316.0,0.0,8.0,33344367.0,now I added some code I hope it helps you so you could help me :)
2020,24998038,Path planning and collision avoidance for multiple autonomous robots in static environment.,|algorithm|path|robotics|motion-planning|,"<p>I am going to start my work on a robotic project. Before jumping into the question, let me first give a brief description of the set up of this project.</p>

<p>The set up consists of a facility where there is a rail system and there are multiple robots mounted on them. The environment is static with only mobile robots. As for now it can be 3 robots wagon on these rails. These robots are for pick-and-place tasks.There is, as such no communication between these robots but they are connected to the server, which gives the robots tasks.</p>

<p>Please have a look at the rough sketch (pardon me for this bad diagram) to have an idea of the set-up. <img src=""https://i.stack.imgur.com/ddM3X.jpg"" alt=""enter image description here""> </p>

<p>From the above diagram, R1 and R2 are robots on the rails. The server may assign a job to robot R1 for picking an object at ""A"" and dropping it at ""B"" and the robot has to move completely autonomous. 
Now, my queries are as follows:</p>

<ul>
<li>How the robot R1 moves to ""A"" and then to ""B"", taking the optimal path, concerning Path Planning of the robot?</li>
<li>How the robot avoid collision in a static map, with other mobile robots on the rails, concerning collision avoidance ? (I am thinking of using a camera to detect the other robot)</li>
</ul>

<p>I have looked into some literature and have a basic idea. I have also gone through some of the asked <a href=""https://stackoverflow.com/questions/19406147/path-planning-and-obstacle-avoidance-algorithms"">question</a> in here. But I dont have any concrete idea to start working. I am looking for some advice/ideas/algorithms/literature to go about the problem.
Please help me out. Thanks in advance !!</p>

<p>Note: I will be simulating the whole set-up in a 3D environment.</p>
",2014-07-28 14:57:00,,671,1,0,1,,1800026.0,,11/5/2012 11:41,65.0,24998363.0,"<p>For the first question, consider the entire rail network as a graph and use a shortest path algorithm to get the optimal path.</p>

<p>I do not know if you are allowed to move other robots when moving <code>R1</code> to <code>A</code> and <code>B</code>. If some of the robots cannot be moved, then remove those portions of the railway from the graph, and calculate the path.</p>

<p>To avoid collision, one method would be to allow motion in only one direction along the rails (a figure of 8 in this case).  Overall it should not be a problem since you are controlling the bots from a central server.</p>
",2963623.0,1.0,6.0,,
1942,23009549,"Roll, pitch, yaw calculation",|3d|robotics|,"<p>How can I calculate the roll, pitch and yaw angles associated with a homogeneous transformation matrix?</p>

<p>I am using the following formulas at the moment, but I am not sure whether they are correct or not.</p>

<pre><code>pitch = atan2( -r20, sqrt(r21*r21+r22*r22) );
yaw   = atan2(  r10, r00 );
roll  = atan2(  r21, r22 );
</code></pre>

<p>r10 means second row and first column.</p>
",2014-04-11 10:06:00,23010193.0,71398,4,2,8,0.0,2983111.0,,11/12/2013 12:06,53.0,23009933.0,"<p>[This might be better suited as a comment but it is to long for that ;) ]</p>

<p>When I compare your formula with the one on the german Wikipedia page about roll, pitch an yaw (<a href=""http://de.wikipedia.org/wiki/Roll-Nick-Gier-Winkel#Berechnung_aus_Rotationsmatrix"" rel=""nofollow"">see here</a>) there is a difference in the calculation of the pitch. According to Wikipedia your formula should look like this:</p>

<pre><code>pitch = atan2(-r20,(sqrt(pow(r21,2)+pow(r00,2))); // replaced r22 by r00
</code></pre>

<p>Note that on the wikipedia page they use a different indexing for the matrix elements (thex start with 1 and not with 0 for the first row/column). Furthermore they call pitch beta, yaw alpha and roll gamma. Also, they divide the coefficents for <code>atan2</code> in the yaw and roll calculation by the <code>cos(pitch)</code>, but that should cancel out.</p>

<p>Otherwise your formula looks fine to me.</p>
",1191041.0,2.0,2.0,35200721.0,"@Sh3ljohn no I am not satisfied because I know these things already. I found another way to find out roll pitch yaw, kindly see this link http://code.google.com/p/matlab-toolboxes-robotics-vision/source/browse/matlab/robot/tags/R9.4/tr2rpy.m?r=822 ..Do you know what should be input for this matlab function?"
1903,21458336,bayes rule -> practical application with Sharp IR sensor,|python|robot|,"<p>I have a robot using IR and Sonar sensors for measuring distances. I build an occupancy map/grid.
So far I use a simple integer based system to ""calculate"" the probability of a cell being occupied. Something like +1 if sensor hit and -1 for all calls between 0 and sensor reading-1. If the number in the array for one cell if above the threshold the cell is counted as occupied and via versa for unoccupied. All between in uncertain. (a bit more complex but based on this idea)</p>

<p>I now wonder if it is worth to use a Bayes theorem based solution for this (first code snip below). As most people do it this way the answer is most likely yes :-).</p>

<p>What do p1 and p2 mean in this specific example - let's say for an IR distance sensor? I understand the examples when the theorem is explained. But somehow I can't translate them to the IR sensor situation. (my mind got a bit stuck here)
I have no clue what and how to estimate the values I should put in there and how to apply them to my array/map.</p>

<p>Would be nice if someone could enlighten me :-)
If somehow possible with some pseudo code.</p>

<p>Below also the class for my current map handling.</p>

<p>Thanks
Robert</p>

<p><br>
Bayes functions -- but how to apply?</p>

<pre><code>def pos(p0, p1, p2):
    return (p0 * p1)/(p0 * p1 + (1-p0) * (1-p2))

def neg(p0, p1, p2):
    return (p0 * (1-p1))/(p0 * (1-p1) + (1-p0) * p2)
</code></pre>

<p><br></p>

<p>My current Map class:</p>

<pre><code>templateData = {
    'MapWidth' : 800,
    'MapHeight': 600,
    'StartPosX' : 500,
    'StartPosY' : 300,
    'StartTheta' : 0,
    'Resolution' : 5,
    'mapThresholdFree' : 126,
    'mapThresholdOcc' : 130,
    'EmptyValue' : 128,
    'mapMaxOcc' : 255,
    'mapMaxFree' : 0,
    'ServoPos' : 0,
    'CurrentPosX' : 0,
    'CurrentPosY' : 0,
    'CurrentTheta' : 0
}

templateData[""MapHeight""] = templateData[""MapHeight""] / templateData[""Resolution""]
templateData[""MapWidth""] = templateData[""MapWidth""] / templateData[""Resolution""]
templateData[""StartPosX""] = templateData[""StartPosX""] / templateData[""Resolution""]
templateData[""StartPosY""] = templateData[""StartPosY""] / templateData[""Resolution""]

#map
map=robotmap.NewRobotMap(templateData[""MapWidth""],templateData[""MapHeight""], templateData[""Resolution""], templateData[""StartPosX""],templateData[""StartPosY""], templateData[""StartTheta""], templateData[""ServoPos""],templateData[""mapMaxOcc""],templateData[""mapMaxFree""],templateData[""EmptyValue""])
map.clear()

class NewRobotMap(object): 
    def __init__(self, sizeX, sizeY, Resolution, RobotPosX, RobotPosY, RobotTheta, ServoPos, mapMaxOcc, mapMaxFree, EmptyValue):
        self.sizeX = sizeX 
        self.sizeY = sizeY 
        self.RobotPosX = int(RobotPosX)
        self.RobotPosY = int(RobotPosY)
        self.mapResolution = int(Resolution)
        self.StartPosX = int(RobotPosX)
        self.StartPosY = int(RobotPosY)
        self.RobotTheta = float(RobotTheta)
        self.EmptyValue = EmptyValue
        self.ServoPos = ServoPos
        self.mapMaxOcc = mapMaxOcc
        self.mapMaxFree = mapMaxFree
    def clear(self):
        self.RobotMap = [[self.EmptyValue for i in xrange(self.sizeY)] for j in xrange(self.sizeX)]
    def updateMap(self ,x ,y , Val):
        oldval = self.RobotMap[x][y]
        self.RobotMap[x][y]=self.RobotMap[x][y] + Val
        if self.RobotMap[x][y] &gt; self.mapMaxOcc:
            self.RobotMap[x][y] = self.mapMaxOcc
        elif self.RobotMap[x][y] &lt; self.mapMaxFree:
            self.RobotMap[x][y] = self.mapMaxFree            
        return oldval, self.RobotMap[x][y]
    def updateRobot(self,theta,x,y):
        self.RobotTheta = float(theta)
        self.RobotPosX = int(round(self.StartPosX + float(int(x)/self.mapResolution), 0))
        self.RobotPosY = int(round(self.StartPosY - float(int(y)/self.mapResolution),0))
    def getRobotPos(self):
        return self.RobotPosX, self.RobotPosY
    def display(self):
        s = [[str(e) for e in row] for row in self.RobotMap]
        lens = [len(max(col, key=len)) for col in zip(*s)]
        fmt = '\t'.join('{{:{}}}'.format(x) for x in lens)
        table = [fmt.format(*row) for row in s]
        print '\n'.join(table)
    def updateServoPos(self, newServoPos):
        self.ServoPos = newServoPos
</code></pre>
",2014-01-30 13:49:00,,248,1,6,0,,3173818.0,,1/8/2014 15:04,21.0,21461033.0,"<p>In <code>pos</code>:</p>

<ul>
<li><code>p0</code> is the prior probability that an object is there. This would probably require knowing in advance the density of objects in the map.</li>
<li><code>p1</code> is the probability of a sensor hit given that an object is there (IOW a true positive)</li>
<li><code>p2</code> is the probability of no sensor hit given that an object is not there (a true negative)</li>
</ul>

<p>Now <code>1 - p2</code> is therefore the probability of a sensor hit given that an object is not there (a false positive). Additionally, <code>1 - p0</code> is the probability of no object being there. If we plug these values into Bayes' Rule, we get:</p>

<pre><code>Pr(Object|Hit) = Pr(Hit|Object)Pr(Object) / ( Pr(Hit|Object)Pr(Object) + Pr(No Hit|Object)Pr(No Object)

= p1 * p0 / ( p1 * p0 + (1 - p2) * (1 - p0) )
</code></pre>

<p>Which is the <code>pos()</code> function you gave.</p>

<p><em>Note:</em> In the event that you don't know the density of objects in the map beforehand, you can use <code>p0 = 0.5</code>, in which case the equation simplifies to:</p>

<pre><code>Pr(Hit|Object) / ( Pr(Hit|Object) + Pr(No Hit|Object) )
= p1 / (p1 + (1-p2))
</code></pre>
",1142167.0,0.0,2.0,32385249.0,"Yes, that's correct. But determining the current probability usually involves calculating frequency within a sample. If you have information about the underlying distribution through some other means, than that would work just fine."
2088,26327267,Myro programming - Making robot stop when it sees and obstacle,|python|robotics|myro|,"<p>I'm using a scribbler robot and writing code in Python.  I'm trying to get it to stop when it sees an obstacle</p>

<p>So I created variables for the left obstacle sensor, the center obstacle sensor and the right obstacle sensor</p>

<pre><code>    left = getObstacle(0)
    center = getObstacle(1)
    right = getObstacle(2)
</code></pre>

<p>Then an if statement</p>

<pre><code>if (left &lt; 6400 &amp; center &lt; 6400 &amp; right &lt; 6400):
        forward(1,1)
    else:
        stop()
</code></pre>

<p>Basically the idea is if the sensors read less than 6400, it should move forward, otherwise, it should stop.  When testing the scribbler with the <code>senses</code> function, I noticed when I put the robot close to an object, it would read around 6400.</p>

<p>Here's what I have for the <code>main()</code> code</p>

<pre><code>def main():
      while True: 
        left = getObstacle(0)
        center = getObstacle(1)
        right = getObstacle(2)
        lir = getIR(0)
        rir = getIR(1)
    if (left &lt; 6400 &amp; center &lt; 6400 &amp; right &lt; 6400):
        forward(1,1)
    else:
        stop()
</code></pre>

<p>Why isn't my robot responding?  The Python code doesn't show any errors when I put it into the shell, but nothing is happening with my robot.</p>

<p>EDIT:</p>

<p>Some code changes.  So far the robot will move, but it won't stop.  Are my if and else statements incorrect?</p>

<pre><code>center = getObstacle(1)
def main():


    if (center &lt; 5400):
        forward(0.5)
    else:
        stop()
</code></pre>
",2014-10-12 16:22:00,26330019.0,2977,2,7,0,,3577397.0,,4/27/2014 2:15,141.0,26327495.0,"<p><code>&amp;</code> is the <a href=""https://en.wikipedia.org/wiki/Bitwise_operation#AND"" rel=""nofollow"">Bitwise operator</a></p>

<p><code>and</code> is the <a href=""https://en.wikipedia.org/wiki/Logical_conjunction"" rel=""nofollow"">logical AND operator</a></p>

<p>So your condition should be :</p>

<pre><code>if (left &lt; 6400 and center &lt; 6400 and right &lt; 6400):
    forward(1,1)
else:
    stop()
</code></pre>
",4133467.0,0.0,1.0,41319578.0,I corrected the indentation in my editor.  But it still doesn't work.
1934,22680580,Robotics: Want MicroC Pro PIC Sample Code?,|pic|robotics|microc|,"<p>I'm new to robotics.
I have to develop a line following robot. <br>
I hope to use PIC18F452 microchip.
I am looking for alredy developed source code to start with. <br>
Thank you. </p>
",2014-03-27 07:09:00,22680736.0,187,1,0,-1,,,,,,22680736.0,"<p><a href=""https://github.com/diunuge/SLIIT--Robofest-2012"">This</a> codebase is developed for a robot with line following and quite advanced other functionalities. This also includes some circuits, you may find useful.</p>
",3441894.0,1.0,0.0,,
2103,26814872,Haskell for Robotics,|haskell|functional-programming|embedded|robotics|,"<p>I am looking up on web about Haskell and its applications. Basically i trying to learn functional programming language and i see Haskell is very famous among them. What i want to know is, is it possible to use Haskel as substitute for c in robotics? Can i use Haskell for embedded system programming and getting data from sensors, moving the motors, implementing mathematical model that is used to design the robot and its behaviour and if possible apply machine learning algorithms? </p>

<p>I am just starting off in this field so if the question is naive enough, please answer like you would answer any newbie.</p>

<p>Update: If the Question is too broad, i would like to know the specifics. Do people compile down the haskell to the embedded hardware or use haskell as a remote control in most of the cases? Which one is more approachable using haskell? What is the general way of using haskell in hardware embedded programming? If it is only used as a remote control, how to implement genetic algorithms and machine learning algorithms using haskell? I know its too broad but i would just like to know the general usage if my requirement is such.</p>
",2014-11-08 07:26:00,,1086,0,8,5,0.0,902817.0,,8/19/2011 16:52,337.0,,,,,,42313456.0,"I just want to know, how people use haskell on robotics like on quadrocopter, do they compile down the haskell and run it on embedded or use computer as a remote control for robots or quadrocopter. Is it even possible to do such thing using haskell??"
1887,21002816,real time object detection based on color in opencv and python,|python|opencv|robotics|,"<p>I am new to opencv. I saw many tutorials on color detection. My camera is rotating and I want it to stop rotation when it detects, say, a blue color. How can I check whether any blue object has come into my frame? I want it to be in the centre of the frame. Basically I want a feedback, maybe a flag bit, when a blue color is detected. I am using opencv2 and python. Please help</p>

<pre><code>    import cv2
    import numpy as np

    cap = cv2.VideoCapture(0)

    while(1):

    # Take each frame
    _, frame = cap.read()

    # Convert BGR to HSV
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)

    # define range of blue color in HSV
    lower_blue = np.array([110,50,50])
    upper_blue = np.array([130,255,255])

    # Threshold the HSV image to get only blue colors
    mask = cv2.inRange(hsv, lower_green, upper_green)

    # Bitwise-AND mask and original image
    res = cv2.bitwise_and(frame,frame, mask= mask)
</code></pre>

<p>I did this much. Now what should I check?? if <code>res</code> is a zero matrix??  That is to get an acknowledgement if a blue object is detected.</p>

<p>I want something like this: <a href=""http://www.youtube.com/watch?v=0PXnzAAKro8"" rel=""nofollow"">http://www.youtube.com/watch?v=0PXnzAAKro8</a>. So, how should I check whether a blue object has been detected.</p>
",2014-01-08 17:44:00,,7072,0,4,2,0.0,3174434.0,,1/8/2014 17:35,19.0,,,,,,31565931.0,please check https://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_imgproc/py_colorspaces/py_colorspaces.html#converting-colorspaces
2117,26847636,"Newbie python script, odometry",|python|robotics|ros|,"<p>I was doing one online course where on page was special frame to run python script. 
My task in this exercise was to compute the odometry, velocities are given. </p>

<p>This script on page looks: <a href=""http://snag.gy/NTJGz.jpg"" rel=""nofollow"">http://snag.gy/NTJGz.jpg</a></p>

<p>Now I would like to do the same using ROS: 
there is nearly the same exercise but in ROS:</p>

<p>clear code looks: <a href=""https://github.com/tum-vision/autonavx_ardrone/blob/master/ardrone_python/src/example1_odometry.py"" rel=""nofollow"">https://github.com/tum-vision/autonavx_ardrone/blob/master/ardrone_python/src/example1_odometry.py</a></p>

<p>There is information that I should add code from this online_course version to function callback, I try, but it doesn't work.</p>

<p>My code: </p>

<pre><code>#!/usr/bin/env python

#ROS
import rospy
import roslib; roslib.load_manifest('ardrone_python')
from ardrone_autonomy.msg import Navdata
import numpy as np

def __init__(self):
    self.position = np.array([[0], [0]])


def rotation_to_world(self, yaw):
        from math import cos, sin
        return np.array([[cos(yaw), -sin(yaw)], [sin(yaw), cos(yaw)]])


def callback(self, t, dt, navdata):
        self.position = self.position + dt * np.dot(self.rotation_to_world(navdata.rotZ), np.array([[navdata.vx], [navdata.vy]]))      
        print(""received odometry message: vx=%f vy=%f z=%f yaw=%f""%(navdata.vx,navdata.vy,navdata.altd,navdata.rotZ))
        print(self.position)


if __name__ == '__main__':
    rospy.init_node('example_node', anonymous=True)

    # subscribe to navdata (receive from quadrotor)
    rospy.Subscriber(""/ardrone/navdata"", Navdata, callback(self, t, dt, navdata))

    rospy.spin()
</code></pre>

<p>Please correct me, I am totally newbie to python. </p>

<p>Now i got message: </p>

<blockquote>
  <p>Traceback (most recent call last):   File ""./example1_odometry.py"",
  line 28, in 
      rospy.Subscriber(""/ardrone/navdata"", Navdata, callback(self, t, dt, navdata)) NameError: name 'self' is not defined</p>
</blockquote>
",2014-11-10 15:52:00,,1136,2,4,-1,,2898281.0,,10/19/2013 16:44,52.0,26847702.0,"<p>The direct error you posted:</p>

<pre><code>Traceback (most recent call last): File ""./example1_odometry.py"", line 28, in rospy.Subscriber(""/ardrone/navdata"", Navdata, callback(self, t, dt, navdata)) NameError: name 'self' is not defined
</code></pre>

<p>is because in the line:</p>

<pre><code> rospy.Subscriber(""/ardrone/navdata"", Navdata, callback(self, t, dt, navdata))
</code></pre>

<p><code>self</code> is undefined. You might mean <code>rospy</code> ?</p>
",8002.0,0.0,2.0,42258129.0,"@hagubear you mean `class MyClass:`, right?"
1926,22228890,Neural Network for a Robot,|neural-network|robotics|,"<p>I need to implement a Robot Brain, I used feedforward neural network as a Controller. The robot has 24 sonar sonsor, and only one ouput which is R=Right, L=Left, F=Forward, B=Back. I also have a large dataset which contain sonar data and the desired output. The FNN is trained using backpropagation algorithm.</p>
<p>I used neuroph Studio to construct the FNN and to do the trainnig. Here the network params:</p>
<p>Input layer: 24
Hidden Layer: 10
Output Layer: 1
LearnningRate: 0.5
Momentum: 0.7
GlobalError: 0.1</p>
<p>My problem is that during iteration the error drop slightly and seems to be static. I tried to change the parameter but I'm not getting any useful result!!</p>
",2014-03-06 15:31:00,22233582.0,169,1,0,0,,3345867.0,,2/24/2014 8:50,19.0,22233582.0,"<p>Use 1 of n encoding for the output. Use 4 output neurons, and set up your target (output) data like this:</p>

<pre><code>1 0 0 0 = right
0 1 0 0 = left
0 0 1 0 = forward
0 0 0 1 = back
</code></pre>

<p>Reduce the number of input sensors (and corresponding input neurons) to begin with, down to 3 or 5. This will simplify things so you can understand what's going on. Later you can build back up to 24 inputs.</p>

<p>Neural networks often get stuck in local minima during training, that could be why your error is static. Increasing the momentum can help avoid this.  </p>

<p>Your learning rate looks quite high. Try 0.1, but play around with these values. Every problem is different and there are no values guaranteed to work.</p>
",3387972.0,0.0,3.0,,
1900,21211025,Trouble programming MindstormsNXT with RobotC,|c|robotics|lego-mindstorms|nxt|,"<p>I am having trouble making my robot controlling my Mindstorms NXT robot with RobotC. I want my robot to be able to move forward on a table and when reaching the end, the ultrasonic sensor, which is facing down, will determine if it is on the edge by seeing how far the ground is. When the ultrasonic sensor finds that it is on the edge, the robot will move back from the edge, turn around, and go the other way.</p>

<p>Here is my code:</p>

<pre><code>#pragma config(Sensor, S1,     ,               sensorSONAR)
//*!!Code automatically generated by 'ROBOTC' configuration wizard               !!*//

task main()

{

int Ultrasonic; 
Ultrasonic = SensorValue[S1];

while (true)
{
    if (Ultrasonic &gt;10)
{
motor[motorB] = -100;
motor[motorC] = -100;
wait1Msec(2000);

motor[motorB] = 100;
wait1Msec(2000);
}
if (Ultrasonic &lt;= 10)
{
    motor[motorB] = 50;
    motor[motorC] = 50;
    wait1Msec(5000);
}
}
}
</code></pre>
",2014-01-18 23:05:00,,219,1,0,0,,3211005.0,,1/18/2014 23:03,10.0,22058479.0,"<p>The problem with this program is that you really only read from the ultrasonic sensor once. Here's what happens when you run your program:</p>

<ol>
<li>The variable <code>Ultrasonic</code> is created, and the sensor value is assigned to it.</li>
<li>The program checks the value of the ultrasonic sensor.</li>
<li>The program does something based on what the ultrasonic sensor is read.</li>
<li>The program does something based on what the ultrasonic sensor is read.</li>
</ol>

<p>...</p>

<p>To fix this problem, all you need to do is to move the reading of the ultrasonic sensor into the <code>while</code> loop, so that the NXT continually checks the value of the sensor. Here's what a modified version would look like:</p>

<pre><code>task main()
{
    int Ultrasonic; 

    while (true)
    {
        Ultrasonic = SensorValue[S1];
        if (Ultrasonic &gt; 10)
        {
            // ... Do stuff here...
        }
        if (Ultrasonic &lt;= 10)
        {
            // ... Do more stuff here...
        }
    }
}
</code></pre>

<p>In fact, you could make this code even ""cleaner"" by combining the check for the value of the ultrasonic sensor by using an ""if... else..."" statement. This checks one value, and then makes a decision based on whether that value is true--or else. Just replace the line <code>if (Ultrasonic &lt;= 10)</code> with <code>else</code>.</p>
",1292093.0,1.0,0.0,,
2142,27648625,Is it possible to measure depth of an image(JPEG/PNG),|image|image-processing|robotics|camera-calibration|point-cloud-library|,"<p>I am wondering there must be a way to get a depth of image. Certainly some portions can be extruded so that we get 3d version of 2d image. Any sources that will help in this out.</p>

<p>FYI: I would like to get point cloud from 2d image.
Thank you in advance..</p>
",2014-12-25 15:42:00,,877,2,3,0,0.0,4024298.0,,9/9/2014 19:44,2.0,27651998.0,"<p>Reconstruction from single 2D image is not possible. As mentioned by others, there is loads of literature to refer to. Multi-View Geometry by Hartely and Zisserman can be a good start. An example tutorial to start with: <a href=""http://vgl-ait.org/cvwiki/doku.php?id=matlab:tutorial:3d_reconstruction_with_calibrated_image_sequences"" rel=""nofollow"">Reconstruction</a>. you can also refer to computer vision toolbox from matlab/opencv</p>
",2577219.0,0.0,0.0,43715022.0,This is an extremely broad topic and out-of-scope for StackOverflow. This site is about answering specific questions to programming issues.
2075,26007202,Calculate new point offset based on angle of rotation?,|c#|.net|math|trigonometry|robotics|,"<p>I am working on an application for the past few weeks which involves some trigonometry and am currently stuck.  As shown in the diagram below, I have a circular item (green circle at position #1) which I know the center point (let's call that X1,Y1).  The circle has another point (orange circle) that is off-centered a bit - midway between two other marks (blue circles).  These marks can move around.  The coordinates of the orange point are calculated (let's call it X2, Y2) and the angle of the blue line is calculated (call it Angle) in relation to the horizontal of the circle.</p>

<p><img src=""https://i.stack.imgur.com/JEs4k.png"" alt=""Diagram""></p>

<p>I can calculate the difference between the center of the circle and the point by:</p>

<p>deltaX = X2-X1</p>

<p>deltaY = Y2-Y1</p>

<p>I need to move and rotate the green circle (either CW or CCW - whichever is shorter) from it's start location (position 1) over to position 2.  This means the angle could be negative or positive.  The blue line must end up vertical and the orange dot at the center of position 2 (red square).  I know the coordinates for the center of position 2 (let's call this point X3,Y3). Position #1 and position #2 are exactly 90 degrees from each other.</p>

<p>I thought I could use some trig identity formulas that calculate the rotation of a point, as such:</p>

<p>offsetX = deltaX * cos(90-Angle) - deltaY * sin(90-Angle)</p>

<p>offsetY = deltaX * sin(90-Angle) + deltaY * cos(90-Angle)</p>

<p>I was hoping these offsets would be what I need to adjust the circle to it's new center when it moves/rotates over to position 2.</p>

<p>X3 = X3 + offsetX</p>

<p>Y3 = Y3 + offsetY</p>

<p>However, when I try use this math, it's not placing the orange mark of the circle in the center of the square.  Not sure if my equations and calculations are correct based on the angle of rotation (positive or negative, CW or CCW) or if I'm using the angle correctly (where I subtract the known angle from 90 degrees).  How do I correctly calculate the final point/position?  Any help and examples would be greatly appreciated!</p>

<p>Thank you very much for your time!</p>
",2014-09-24 01:38:00,,11171,2,0,7,0.0,1218207.0,,2/18/2012 15:14,71.0,26007927.0,"<p>So you need to rotate your circle by <code>90 - Angle</code> and then move orange point to (X3, Y3)?<br />
First you need to find orange point coordinate after rotation:</p>

<pre><code>newX = X2 * cos(90 - Angle) - Y2 * sin(90 - Angle);
newY = X2 * sin(90 - Angle) + Y2 * cos(90 - Angle);
</code></pre>

<p><code>newX</code> and <code>newY</code> are orange point coordinates after rotation. To find move transformation simply substract:</p>

<pre><code>moveX = X3 - newX;
moveY = Y3 - newY;
</code></pre>

<p>Now if you rotate circle by <code>90 - Angle</code> and move it by (moveX, moveY) orange point will move to (X3, Y3). That is if you rotate circle around (0, 0) point. If you rotating around some (X, Y) point, you first need to substract X from X2, Y from Y2 and then add X to newX, Y to newY. That substraction 'moves' your rotation base point to (0, 0), so after rotation you need to move it back:</p>

<pre><code>newX = (X2 - X) * cos(90 - Angle) - (Y2 - Y) * sin(90 - Angle) + X;
newY = (X2 - X) * sin(90 - Angle) + (Y2 - Y) * cos(90 - Angle) + Y;
</code></pre>
",800613.0,1.0,3.0,,
2023,25181253,Using objects (that require a parameter) and their methods within another class,|c++|beagleboneblack|robotics|,"<p>I'm trying to program an omni directional robot with the BeagleBone Black. I've found the BlackLib library works realy well for controlling pins, but I'm having a little trouble integrating it into the larger scheme of things. If I simply create objects associated with pins and set them high or low (or somewhere in between) I am able to control the motor as desired. But when I try to create an object that contains BlackLib ojects, I run into difficulty. Seems to me, a good way to go about this would be to create a motor object that contains a GPIO and a PWM BlackLib object. Then I can create a function to easily set the power and direction with</p>

<pre><code>rightMotor(50); //Should set the power to 50% in one direction
</code></pre>

<p>I've got some of it working, but am having trouble accessing the functions within the objects from BlackLib that are within my own Motors class.</p>

<p>Here's the code I'm working with now.</p>

<pre><code>#include ""BlackLib/BlackLib.h""

struct Motors
{
  BlackPWM* pwm;
  BlackGPIO* gpio;

  void drive(int power)
  {
    if(power &gt;= 0)
    {
      gpio.setValue(low);
      //gpio.setValue(low); generates the error described below. I'm not familiar enough with the intricacies of pointers to know how to handle this
      //motors.cpp: In member function void Motors::drive(int):
      //motors.cpp:15:10: error: request for member setValue in ((Motors*)this)-&gt;Motors::gpio, which is of non-class type BlackGPIO*
//      pwm.setDutyPercent(power);
    }
    else
    {
//      gpio.setValue(high);
//      pwm.setDutyPercent(100+power);
    }
  }
};

int main()
{
  struct Motors rightMotor;
  rightMotor.pwm = new BlackPWM(P9_16);
  rightMotor.gpio = new BlackGPIO(GPIO_49,output);

  //Give the BeagleBone a little time to create the objects
  usleep(10000);

  rightMotor.pwm.setPeriodTime(500000);
  //I will eventually need to set the period time but I'm not sure how. I'm guessing this is the incorrect syntax
  //Ideally, I would create a function so that I could simply write rightMotor.setPeriodTime(500000);

  rightMotor.drive(50);
  sleep(1);
  rightMotor.drive(0);
}
</code></pre>

<p>If I'm totally off base and there's a much better way to do this, please let me know. My end goal is to be able to easily control multiple motors. Eventually I would like to create functions and classes such as</p>

<pre><code>robot.turn(30);
</code></pre>

<p>or</p>

<pre><code>robot.forward(100);
</code></pre>
",2014-08-07 11:23:00,,453,1,1,0,,3865173.0,,7/22/2014 14:51,6.0,25213129.0,"<p>Got the following solution from a professor of computer science at the University of Alaska Fairbanks. Works as desired now.</p>

<pre><code>struct Motors
{
        BlackPWM pwm;
        BlackGPIO gpio;

        Motors(pwm_pin_name pwmPin,gpio_name gpioPin) //motor constructor parameters
                : pwm(pwmPin),gpio(gpioPin,output) //member initializer list
        {
                pwm.setPeriodTime(500000);
        }

        void drive(int power)
        {
                if(power &gt;= 0)
                {
                        gpio.setValue(low);
                        pwm.setDutyPercent(power);
                }
                else
                {
                        gpio.setValue(high);
                        pwm.setDutyPercent(100+power);
                }
        }
};

int main()
{
        //User interface pins
        BlackGPIO button(GPIO_44,input);

        Motors rightMotor(P9_16,GPIO_49);

        while(!button.isHigh())
        {
                rightMotor.drive(0);
        }

        //The BeagleBone needs a little time to create the PWM object
        usleep(10000);

        rightMotor.drive(-50);

        sleep(1);

        rightMotor.drive(0);
}
</code></pre>
",3865173.0,0.0,0.0,39208871.0,"What is your experience with C++? You're doing some basic mistakes here (like accessing a member function of a pointer via `.` instead of `->`, or bad encapsulation). I'd strongly recommend to get some introduction for C++/OOP before tackling such a project."
1970,23731077,How to write a .sdf file in Gazebo 3D simulator for a robot arm with 3 revolute joints?,|simulation|robotics|,"<p>Something similar to the CRS arm robot with 3 revolute joints. I am having trouble writing the sdf(Simulator Description Format) for the arm in Gazebo.</p>
",2014-05-19 06:33:00,24164926.0,823,1,0,0,,3125386.0,,12/21/2013 13:57,27.0,24164926.0,"<p>In case you havent already read it, this is the sdf manual with description of each tag: <a href=""http://gazebosim.org/sdf/1.5.html"" rel=""nofollow"">http://gazebosim.org/sdf/1.5.html</a>.</p>

<p>If you don't succeed in writing the sdf from scratch, maybe you can find a description file written in other format (eg urdf) and convert it with some tools.</p>
",1248675.0,0.0,0.0,,
2021,25002980,"Python, Webiopi and Raspberry Pi",|python|raspberry-pi|robotics|webiopi|,"<p>I am trying to control my Raspberry Pi car via wireless and webiopi.  The basic function works fine - have the interface where I click fwd and the car will go forward and when I release the button it will stop.</p>

<p>I now want to integrate the Ultrasonic distance sensor so that when I drive forward, the car should stop when something is in front of it.  I have it going where when I click to fwd button, the car will drive and stop when something is in range but it will only stop when something is in range and not when I release the button. My while loop is somehow looping (stuck) and not reading the release button function from webiopi.</p>

<p>Can somebody please help - been at it for days now and not sure where I am going wrong :-( </p>

<p>Here is the loop from my python script:</p>

<pre><code>def go_forward(arg):
  global motor_stop, motor_forward, motor_backward, get_range
  print ""Testing""
  print mousefwd()
  while (arg) == ""fwd"":
      print (arg)
      direction = (arg)
      dist = get_range()
      print ""Distance %.1f "" % get_range()
      if direction == ""fwd"" and get_range() &lt; 30:
          motor_stop()
          return
      else:
          motor_forward()
</code></pre>

<p>And here is the code from my webiopi function call:</p>

<pre><code> function go_forward() {
              var args = ""fwd""
              webiopi().callMacro(""go_forward"", args);
      }
 function stop1() {
              var args = ""stop""
              webiopi().callMacro(""go_forward"", args);
              webiopi().callMacro(""stop"");
    }
</code></pre>

<p>This is how I have it now but still not working (I am a total noob :-) ) :</p>

<pre><code> def go_forward(arg):
  global motor_stop, motor_forward, motor_backward, get_range
  print ""Testing""
  direction = arg
  while direction == ""fwd"":
      print arg
      dist = get_range()
      print ""Distance %.1f "" % get_range()
      if get_range() &lt; 30:
         motor_stop()
      elif direction == ""fwd"":
         motor_forward()
      else:
         motor_stop()
</code></pre>

<p>Maybe a slight step forward.  See that webipi uses its own 'loop' and I added the loop code to check what the status of the motor GPIO's are and the distance and if the motor is running and if the distance is too short then stop. Car moves now when I press the forward button and stops when I release it and when moving forward and distance is less then 30cm it will stop.  Only problem is that when the distance is too short and I press the forward button too quickly multiple times, I now get a ""GPIO.output(Echo,1)
_webiopi.GPIO.InvalidDirectionException: The GPIO channel is not an OUTPUT""  error :-(  .</p>

<p>The Code looks now like this:</p>

<pre><code> def go_forward(direction):
   motor_forward()

 def loop():
   if get_range() &lt; 30 and GPIO.digitalRead(M1) == GPIO.HIGH:
     stop()
     print ""stop""
   sleep(1)
</code></pre>
",2014-07-28 19:37:00,25007454.0,2310,2,0,0,,3817890.0,"Windhoek, Namibia",7/8/2014 19:29,20.0,25003178.0,"<p>Try this - you always stop on range but may want to stop on direction</p>

<pre><code>  if get_range() &lt; 30:
      motor_stop()
  elif direction == ""fwd"":
      motor_forward()
  else:
      motor_stop()
</code></pre>

<p>btw you dont need the (arg) only arg when you reference it</p>

<p>print arg </p>

<p>the parens in the call signature are there to show its a parm to a function</p>
",3704291.0,1.0,2.0,,
1932,22547738,RobotC VEX / Lego Programming: How to make a robot run multiple reactions in parallel?,|java|robotics|lego-mindstorms|nxt|,"<p>I need to get my robot to be able to use the bump switches so that either one can be pressed and the motor corresponding to that bumpswitch will run for as long as the bump switch is pressed. The problem I'm having is getting the LEDs to light up correctly. While the bump switch code block is running, I need the LEDs to light up and go off seven times for one second every time the Light sensor value gets higher than 400. How do I do it? Please Help!
My Code is posted below:</p>

<pre><code>#pragma config(Sensor, in2,    lightSensor,    sensorReflection)
#pragma config(Sensor, dgtl3,  bumpSwitch,     sensorTouch)
#pragma config(Sensor, dgtl4,  bumpSwitch2,    sensorTouch)
#pragma config(Sensor, dgtl10, ledGreen,       sensorLEDtoVCC)
#pragma config(Sensor, dgtl11, ledRed,         sensorLEDtoVCC)
#pragma config(Motor,  port1,           leftMotor,     tmotorVex269, openLoop)
#pragma config(Motor,  port10,          rightMotor,    tmotorVex269, openLoop)

task main() {

while(true) {

    if (SensorValue(lightSensor) &gt; 400) {
        int count = 0;
        while (count &lt; 7) {
            turnLEDOn(ledGreen);
            turnLEDOn(ledRed);
            wait(1);
            turnLEDOff(ledGreen);
            turnLEDOff(ledRed);
            count++;
        }
    }

    while (SensorValue(bumpSwitch) == 0 &amp;&amp; SensorValue(bumpSwitch2) == 0) {
        stopMotor(rightMotor);
        stopMotor(leftMotor);
    }

    while (SensorValue(bumpSwitch2) == 1) {
        startMotor(rightMotor, 55);
        startMotor(leftMotor, 55);
    }

    while (SensorValue(bumpSwitch) == 1){
        startMotor(rightMotor, -55);
        startMotor(leftMotor, -55);
    }
}
}
</code></pre>
",2014-03-21 00:11:00,,2707,2,0,0,,3376956.0,,3/3/2014 23:34,11.0,22549198.0,"<p>Experience with robot programming (although I haven't used Robot C) shows that to make the robot run two or more sensor reactions in parallel, you have to structure the program very differently than for just one reaction. The Robot C tutorial does not cover this or even introduce the language and library features needed to do it.</p>

<p>In this case, you want the light sensor to trigger an LED on/off sequence that lasts for 1 second, while the bump sensors trigger motor actions that last until those sensors are no longer bumping.</p>

<p>The problem here is your motor control's inner <code>while</code> loops are blocking up the thread, mostly keeping it from getting around to running the LED control loop. The LED control loop likewise blocks up the thread for 7 seconds, delaying it from running the motor control loop.</p>

<p>In general, each iteration of a robot control loop should read sensors, send output commands, remember any state that it needs to process later, then loop again so all parts of the control loop get to run each cycle (as often as possible or at a controlled rate). Don't block the control flow by staying in an inner <code>while</code> loop until a sensor changes.</p>

<p>The first step is to unblock the motor control loop:</p>

<pre><code>while (true) {

  // Light sensor LED control code ...

  // Bumper/motor control
  if (SensorValue(bumpSwitch) == 1) {
    startMotor(rightMotor, -55);
    startMotor(leftMotor, -55);
  } else if (SensorValue(bumpSwitch2) == 1){
    startMotor(rightMotor, 55);
    startMotor(leftMotor, 55);
  } else {
    stopMotor(rightMotor);
    stopMotor(leftMotor);
  }
}
</code></pre>

<p>Now your current LED control code still calls <code>wait(1)</code> 7 times. According to <a href=""http://www.robotc.net/support/nxt/ROBOTC-for-Beginners/"" rel=""nofollow"">http://www.robotc.net/support/nxt/ROBOTC-for-Beginners/</a> <code>wait(1)</code> waits for 1.0 seconds, so this part of the code currently takes 7 seconds to run. That's a long time between checking the bump switches.</p>

<p>Furthermore, your code has no significant delay between turning the LEDs off and back on again, so you won't actually notice the LEDs turn off until the end of that sequence.</p>

<p>So the second step is to fix the LED control code from blocking the control loop. Basically there are two approaches (I don't know if Robot C supports the first choice, but it's simpler):</p>

<ol>
<li>When the light sensor is above-threshold <em>and</em> it was below-threshold in the previous iteration, i.e. it just transitioned, then start [or ""fork""] a thread (or task) to run a LED on/off loop. That thread should loop 7 times: {turn the LEDs on, <code>wait(1.0/14.0)</code> seconds, turn the LEDs off, then <code>wait(1.0/14.0)</code> seconds}. That way, 7 cycles around the loop will take 7 * (1/14 + 1/14) = 1.0 seconds. I wrote that as <code>1.0/14.0</code> rather than <code>1/14</code> because many programming languages compute <code>1/14</code> in integer math, yielding <code>0</code>, while <code>1.0/14.0</code> uses floating point math. I don't know about Robot C. Or, you could use <code>wait1Msec(71)</code> to wait 71 msec, which is about 1/14 second.</li>
<li>Use variables to track the sequencing through the flash-LED loop. Each cycle around the main loop, use <code>if</code> statements to check those variables, do the next step in the flash-LED cycle if it's time, and update those variables. The steps are: turn on the LEDs when it was in the initial state and the light sensor is above-threshold, turn off the LEDs when it's 1/14 of a second later, turn on the LEDs when it's 1/14 of a second later, ... turn off and set the tracking variables back to the initial state after 14 of those steps. The simplest tracking variable would be a counter from 0 (initial state) to 14 (doing the last LED ""OFF"" phase) and the value of the system clock from the last change, to let you detect when the clock is 71 msec later than that. Once it finishes step 14, go back to 0. There are alternatives for the tracking variables, such as 3 separate variables for whether it's currently flashing, which of the 7 flash cycles it's currently doing, whether it's in the ON or OFF phase of that cycle, and the clock reading.</li>
</ol>

<p>Approach #2 is trickier because doing two things at once is tricky. (People who think they can multitask by texting while driving are dangerously wrong.)</p>

<p>A much simpler approach works if the robot doesn't need to run the bumper/motor control loop while flashing the LEDs, that is, if it doesn't mind being unresponsive to the bumpers while flashing the LEDs for 1 second. If those bumper sensors are keeping the robot from running off the end of the table, being unresponsive for 1 second is bad news. But if it's OK for the robot to press against the bumper for 1 second while flashing the lights, then the light sensor LED part of the code could go something like this:</p>

<pre><code>while (true) {
  // Light sensor LED control code
  if (SensorValue(lightSensor) &gt; 400) {
    for (int i = 0; i &lt; 7; ++i) {
      turnLEDOn(ledGreen);
      turnLEDOn(ledRed);
      wait1Msec(71); // 1/14 second
      turnLEDOff(ledGreen);
      turnLEDOff(ledRed);
      wait1Msec(71);
    }
  }

  // Bumper/motor control code ...
}
</code></pre>

<p>This makes another simplifying assumption: That if the light sensor is still bright after finishing the 1 second LED flashing loop, it's OK to do another 1 second flashing loop next time around the main control loop, which occurs very soon. So as long as the light sensor is bright, the LEDs will keep flashing and the motors will only check the bumpers once per second. To fix that, use a variable to figure out when the light sensor goes from dark to bright.</p>
",1682419.0,0.0,2.0,,
2178,28865322,Raspberry Pi quadcopter thrashes at high speeds,|java|raspberry-pi|robotics|control-theory|pid-controller|,"<p>I am attempting to build a raspberry pi based quadcopter. So far I have succeeded in interfacing with all the hardware, and I have written a PID controller that is fairly stable at low throttle. The problem is that at higher throttle the quadcopter starts thrashing and jerking. I have not even been able to get it off the ground yet, all my testing has been done on a test bench. Is this a problem with my code, or perhaps a bad motor? any suggestions are greatly appreciated.</p>

<p>here is my code so far:</p>

<p>QuadServer.java:</p>

<pre><code>package com.zachary.quadserver;

import java.net.*;
import java.io.*;
import java.util.*;

import se.hirt.pi.adafruit.pwm.PWMDevice;
import se.hirt.pi.adafruit.pwm.PWMDevice.PWMChannel;

public class QuadServer {
    private static Sensor sensor = new Sensor();

    private final static int FREQUENCY = 490;

    private static double PX = 0;
    private static double PY = 0;

    private static double IX = 0;
    private static double IY = 0;

    private static double DX = 0;
    private static double DY = 0;

    private static double kP = 1.3;
    private static double kI = 2;
    private static double kD = 0;

    private static long time = System.currentTimeMillis();

    private static double last_errorX = 0;
    private static double last_errorY = 0;

    private static double outputX;
    private static double outputY;

    private static int val[] = new int[4];

    private static int throttle;

    static double setpointX = 0;
    static double setpointY = 0;

    static long receivedTime = System.currentTimeMillis();

    public static void main(String[] args) throws IOException, NullPointerException {

        PWMDevice device = new PWMDevice();
        device.setPWMFreqency(FREQUENCY);

        PWMChannel BR = device.getChannel(12);
        PWMChannel TR = device.getChannel(13);
        PWMChannel TL = device.getChannel(14);
        PWMChannel BL = device.getChannel(15);

        DatagramSocket serverSocket = new DatagramSocket(8080);


        Thread read = new Thread(){
                public void run(){
                    while(true) {
                    try {
                            byte receiveData[] = new byte[1024];
                            DatagramPacket receivePacket = new DatagramPacket(receiveData, receiveData.length);
                            serverSocket.receive(receivePacket);
                            String message = new String(receivePacket.getData());
                            throttle = (int)(Integer.parseInt((message.split(""\\s+"")[4]))*12.96)+733;
                            setpointX = Integer.parseInt((message.split(""\\s+"")[3]))-50;
                            setpointY = Integer.parseInt((message.split(""\\s+"")[3]))-50;

                        receivedTime = System.currentTimeMillis();

                        } catch (IOException e) {
                            e.printStackTrace();
                        }
                    }
                }
        };
        read.start();

        while(true)
        {
            Arrays.fill(val, calculatePulseWidth((double)throttle/1000, FREQUENCY));

            double errorX = -sensor.readGyro(0)-setpointX;
            double errorY = sensor.readGyro(1)-setpointY;

            double dt = (double)(System.currentTimeMillis()-time)/1000;

            double accelX = sensor.readAccel(0);
            double accelY = sensor.readAccel(1);
            double accelZ = sensor.readAccel(2);

            double hypotX = Math.sqrt(Math.pow(accelX, 2)+Math.pow(accelZ, 2));
            double hypotY = Math.sqrt(Math.pow(accelY, 2)+Math.pow(accelZ, 2));


            double accelAngleX = Math.toDegrees(Math.asin(accelY/hypotY));
            double accelAngleY = Math.toDegrees(Math.asin(accelX/hypotX));

            if(dt &gt; 0.01)
            {

                PX = errorX;
                PY = errorY;

                IX += errorX*dt;
                IY += errorY*dt;

                IX = 0.95*IX+0.05*accelAngleX;
                IY = 0.95*IY+0.05*accelAngleY;

                DX = (errorX - last_errorX)/dt;
                DY = (errorY - last_errorY)/dt;

                outputX = kP*PX+kI*IX+kD*DX;
                outputY = kP*PY+kI*IY+kD*DY;
                time = System.currentTimeMillis();
            }

            System.out.println(setpointX);

            add(-outputX+outputY, 0);
            add(-outputX-outputY, 1);
            add(outputX-outputY, 2);
            add(outputX+outputY, 3);

            //System.out.println(val[0]+"", ""+val[1]+"", ""+val[2]+"", ""+val[3]);
            if(System.currentTimeMillis()-receivedTime &lt; 1000)
            {
                BR.setPWM(0, val[0]);
                TR.setPWM(0, val[1]);
                TL.setPWM(0, val[2]);
                BL.setPWM(0, val[3]);
            } else 
            {
                BR.setPWM(0, 1471);
                TR.setPWM(0, 1471);
                TL.setPWM(0, 1471);
                BL.setPWM(0, 1471);
            }

        }
    }

    private static void add(double value, int i)
    {
        value = calculatePulseWidth(value/1000, FREQUENCY);
        if(val[i]+value &gt; 1471 &amp;&amp; val[i]+value &lt; 4071)
        {
            val[i] += value;
        }else if(val[i]+value &lt; 1471)
        {
            //System.out.println(""low"");
            val[i] = 1471;
        }else if(val[i]+value &gt; 4071)
        {
            //System.out.println(""low"");
            val[i] = 4071;
        }
    }

    private static int calculatePulseWidth(double millis, int frequency) {
        return (int) (Math.round(4096 * millis * frequency/1000));
    }
}
</code></pre>

<p>Sensor.java:</p>

<pre><code>package com.zachary.quadserver;

import com.pi4j.io.gpio.GpioController;
import com.pi4j.io.gpio.GpioFactory;
import com.pi4j.io.gpio.GpioPinDigitalOutput;
import com.pi4j.io.gpio.PinState;
import com.pi4j.io.gpio.RaspiPin;
import com.pi4j.io.i2c.*;
import com.pi4j.io.gpio.GpioController;
import com.pi4j.io.gpio.GpioFactory;
import com.pi4j.io.gpio.GpioPinDigitalOutput;
import com.pi4j.io.gpio.PinState;
import com.pi4j.io.gpio.RaspiPin;
import com.pi4j.io.i2c.*;

import java.net.*;
import java.io.*;

public class Sensor {
    static I2CDevice sensor;
    static I2CBus bus;
    static byte[] accelData, gyroData;
    static long accelCalib[] = new long[3];
    static long gyroCalib[] = new long[3];

    static double gyroX = 0;
    static double gyroY = 0;
    static double gyroZ = 0;

    static double accelX;
    static double accelY;
    static double accelZ;

    static double angleX;
    static double angleY;
    static double angleZ;

    public Sensor() {
        //System.out.println(""Hello, Raspberry Pi!"");
        try {
            bus = I2CFactory.getInstance(I2CBus.BUS_1);

            sensor = bus.getDevice(0x68);

            sensor.write(0x6B, (byte) 0x0);
            sensor.write(0x6C, (byte) 0x0);
            System.out.println(""Calibrating..."");

            calibrate();

            Thread sensors = new Thread(){
                    public void run(){
                        try {
                            readSensors();
                        } catch (IOException e) {
                        System.out.println(e.getMessage());
                    }
                    }
            };
            sensors.start();
        } catch (IOException e) {
            System.out.println(e.getMessage());
        }
    }

    private static void readSensors() throws IOException {
        long time = System.currentTimeMillis();
        long sendTime = System.currentTimeMillis();

        while (true) {
            accelData = new byte[6];
            gyroData = new byte[6];
            int r = sensor.read(0x3B, accelData, 0, 6);
            accelX = (((accelData[0] &lt;&lt; 8)+accelData[1]-accelCalib[0])/16384.0)*9.8;
            accelY = (((accelData[2] &lt;&lt; 8)+accelData[3]-accelCalib[1])/16384.0)*9.8;
            accelZ = ((((accelData[4] &lt;&lt; 8)+accelData[5]-accelCalib[2])/16384.0)*9.8)+9.8;
            accelZ = 9.8-Math.abs(accelZ-9.8);

            double hypotX = Math.sqrt(Math.pow(accelX, 2)+Math.pow(accelZ, 2));
            double hypotY = Math.sqrt(Math.pow(accelY, 2)+Math.pow(accelZ, 2));


            double accelAngleX = Math.toDegrees(Math.asin(accelY/hypotY));
            double accelAngleY = Math.toDegrees(Math.asin(accelX/hypotX));

            //System.out.println((int)gyroX+"", ""+(int)gyroY);

            //System.out.println(""accelX: "" + accelX+"" accelY: "" + accelY+"" accelZ: "" + accelZ);

            r = sensor.read(0x43, gyroData, 0, 6);
            if(System.currentTimeMillis()-time &gt;= 5)
            {
                gyroX = (((gyroData[0] &lt;&lt; 8)+gyroData[1]-gyroCalib[0])/131.0);
                gyroY = (((gyroData[2] &lt;&lt; 8)+gyroData[3]-gyroCalib[1])/131.0);
                gyroZ = (((gyroData[4] &lt;&lt; 8)+gyroData[5]-gyroCalib[2])/131.0);

                angleX += gyroX*(System.currentTimeMillis()-time)/1000;
                angleY += gyroY*(System.currentTimeMillis()-time)/1000;
                angleZ += gyroZ;

                angleX = 0.95*angleX + 0.05*accelAngleX;
                angleY = 0.95*angleY + 0.05*accelAngleY;

                time = System.currentTimeMillis();
            }
            //System.out.println((int)angleX+"", ""+(int)angleY);
            //System.out.println((int)accelAngleX+"", ""+(int)accelAngleY);
        }
    }

    public static void calibrate() throws IOException {
        int i;
        for(i = 0; i &lt; 3000; i++)
        {
            accelData = new byte[6];
            gyroData = new byte[6];
            int r = sensor.read(0x3B, accelData, 0, 6);
            accelCalib[0] += (accelData[0] &lt;&lt; 8)+accelData[1];
            accelCalib[1] += (accelData[2] &lt;&lt; 8)+accelData[3];
            accelCalib[2] += (accelData[4] &lt;&lt; 8)+accelData[5];

            r = sensor.read(0x43, gyroData, 0, 6);
            gyroCalib[0] += (gyroData[0] &lt;&lt; 8)+gyroData[1];
            gyroCalib[1] += (gyroData[2] &lt;&lt; 8)+gyroData[3];
            gyroCalib[2] += (gyroData[4] &lt;&lt; 8)+gyroData[5];
            try {
                Thread.sleep(1);
            } catch (Exception e){
                e.printStackTrace();
            }
        }
        gyroCalib[0] /= i;
        gyroCalib[1] /= i;
        gyroCalib[2] /= i;

        accelCalib[0] /= i;
        accelCalib[1] /= i;
        accelCalib[2] /= i;
        System.out.println(gyroCalib[0]+"", ""+gyroCalib[1]+"", ""+gyroCalib[2]);
    }

    public double readAngle(int axis)
    {
        switch (axis)
        {
            case 0:
                return angleX;
            case 1:
                return angleY;
            case 2:
                return angleZ;
        }

        return 0;
    }

    public double readGyro(int axis)
    {
        switch (axis)
        {
            case 0:
                return gyroX;
            case 1:
                return gyroY;
            case 2:
                return gyroZ;
        }

        return 0;
    }

    public double readAccel(int axis)
    {
        switch (axis)
        {
            case 0:
                return accelX;
            case 1:
                return accelY;
            case 2:
                return accelZ;
        }

        return 0;
    }
}
</code></pre>
",2015-03-04 21:27:00,,412,1,3,0,,4634134.0,,3/4/2015 21:17,24.0,40819766.0,"<p>You can try to setup different values for the gains of your controller based on different operating conditions. Then you should only be able to identify each condition of operation, change the gain of your PID accordingly, and validate the design. In your case, you could try to schedule the gain of the PID with throttle or other associated variable read from the available sensors.</p>

<p>Search for Gain Scheduling implementations:</p>

<p><a href=""https://www.mathworks.com/help/control/ug/gain-scheduled-control-systems.html"" rel=""nofollow noreferrer"">https://www.mathworks.com/help/control/ug/gain-scheduled-control-systems.html</a></p>

<p>This is a very useful technique which applies linear control design to non-linear systems with very satisfactory results.</p>
",7207220.0,1.0,0.0,45995723.0,Impossible to tell whether the problem is with the code or the motor unless you post the code and/or the motor.
2257,30881607,Camera Calibration with OpenCV: Using the distortion and rotation-translation matrix,|c++|opencv|computer-vision|robotics|,"<p>I am reading the following documentation: <a href=""http://docs.opencv.org/doc/tutorials/calib3d/camera_calibration/camera_calibration.html"" rel=""nofollow noreferrer"">http://docs.opencv.org/doc/tutorials/calib3d/camera_calibration/camera_calibration.html</a></p>

<p>I have managed to successfully calibrate the camera obtaining the camera matrix and the distortion matrix. </p>

<p>I had two sub-questions:</p>

<p>1) How do I use the distortion matrix as I don't know 'r'? </p>

<p><img src=""https://i.stack.imgur.com/3NPRy.png"" alt=""Formula to get corrected coordinates""> </p>

<p>2) For all the views I have the rotation and translation vectors which transform the object points (given in the model coordinate space) to the image points (given in the world coordinate space). So a total of 6 coordinates per image(3 rotational, 3 translational). How do I make use of this information to obtain the rotational-translational matrix?</p>

<p>Any help would be appreciated. Thanks!</p>
",2015-06-17 03:02:00,30939087.0,920,1,0,1,0.0,3572768.0,,4/25/2014 11:40,75.0,30939087.0,"<p>Answers in order:</p>

<p>1) ""r"" is the pixel's radius with respect to the distortion center. That is: </p>

<pre><code>r = sqrt((x - x_c)^2 + (y - y_c)^2)
</code></pre>

<p>where (x_c, y_c) is the center of the nonlinear distortion (i.e. the point in the image that has zero nonlinear distortion. This is usually (and approximately) identified with the principal point, i.e. the intersection of the camera focal axis with the image plane. The coordinates of the principal point are in the 3rd column of the matrix of the camera intrinsic paramers.</p>

<p>2) Use <a href=""https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula"" rel=""nofollow"">Rodrigues's formula</a> to convert between rotation vectors and rotation matrices.</p>
",1435240.0,1.0,0.0,,
2208,29226752,Differential drive robots: converting wheel speeds to lin/ang velocities,|box2d|robotics|kinematics|,"<p>I am modeling a simple differential drive robot (such as the e-Puck, Khepera, etc.) with pyBox2D. This class of robots is usually controlled by indicating speeds for the right and left wheel in rad/sec.<br>
However, Box2D can only control a (kinematic) body through two parameters: linear velocity (in meters/sec, as a 2D vector) and angular velocity (in rad/sec). I need to convert my wheel speeds to linear + angular velocities.</p>

<p>Linear velocity is actually straightforward. Given a wheel radius r in meters, and current robot orientation theta in radians, the forward speed is simply the average of the two wheel speeds in meters/sec and reduced to a vector according to current orientation:</p>

<p>(1) forwardSpeed = ((rightSpeed * r) + (leftSpeed * r)) / 2</p>

<p>(2) linearVelocity = (forwardSpeed * cos(theta), forwardSpeed * sin(theta))  </p>

<p>I cannot quite figure out the correct formula for angular velocity, though. Intuitively, it should be the difference between the two speeds modulo the distance between the wheels:</p>

<p>(3) angularVelocity = (rightSpeed - leftSpeed) / wheelSeparation  </p>

<p>in the limit cases: when right = left, the robot spins in place, and when either rightSpeed = 0 or leftSpeed = 0, the robot spins (pivots) around the stationary wheel, i.e. in a circle with radius = to the separation between the wheels.</p>

<p>I do not get the expected behavior with formula (3), though. As a test, I set the left wheel speed to 0 and progressively increased the value of the right wheel 's speed. The expected behavior is that the robot's should spin around the left wheel with increased velocity.
Instead, the robot spins in circles of increasing radius, i.e it spirals outward, which suggests that the angular velocity is insufficient. </p>

<p>Notice that I am using a Box2D kinematic body for the robot, so friction does not play a role in my results. </p>
",2015-03-24 07:05:00,,1772,1,0,0,,1212775.0,,2/16/2012 1:13,108.0,29266214.0,"<p>Keep in mind that angular velocities in Box2D work around the origin (0,0) in local body coordinates, which makes it very awkward to accurately reproduce this type of movement. Presumably the origin of the robot is in the center of the body, in which case you can never get it to rotate around one wheel (that would need both a rotate and translate). It might actually be easier to model the robot as a dynamic body and apply forces where the wheels are. If you don't want to deal with friction losses etc, you could use a kinematic body for each wheel and connect them to the main (dynamic) body with revolute joints.</p>
",624593.0,0.0,3.0,,
2364,32465462,"Python script in Abaqus raises ""TypeError: keyword error on mergeWire""",|python|robotics|abaqus|,"<p>I'm new at Python scripting on Abaqus. I want to do fiber reinforced actuators which is described <a href=""http://softroboticstoolkit.com/book/fr-abaqus-step-1b"" rel=""nofollow noreferrer"">here</a> but I couldn't run the script properly. It gives me:</p>

<blockquote>
  <p>TypeError: keyword error on mergeWire</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/i9DAA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i9DAA.png"" alt=""Screenshot of error message""></a></p>

<p>How can I fix this?  I'm using Abaqus Student Edition which is free.</p>
",2015-09-08 19:10:00,,3911,0,5,0,0.0,5314278.0,Turkey,9/8/2015 19:00,12.0,,,,,,88002637.0,"I believe there are two issues with that code: 1) abaqusConstants (which includes ON,OFF,..) is not imported; 2) WireSpline only has ""points"", ""mergeType"" and ""smoothClosedspline"" as arguments, so I'd investigate which type of merge you are after by looking at the documentation section 38.2.55 WireSpline()"
2305,32013704,How to send data to cloud from arduino uno?,|arduino|cloud|sensors|robotics|arduino-uno|,"<p>I have some sensors connected to my ardunio uno and get periodically data from that sensors now I wanted to send that data to cloud.
I dont have any idea about how should I connect my arduino to Internet using GSM. How should I solve this problem or any alternative is there.</p>
",2015-08-14 15:34:00,,3503,1,0,0,,4834394.0,,4/26/2015 12:37,6.0,32013799.0,"<p>Just get an appropriate shield (like <a href=""https://www.arduino.cc/en/Main/ArduinoGSMShield"" rel=""nofollow"">this one</a>) and follow the <a href=""https://www.arduino.cc/en/Guide/ArduinoGSMShield"" rel=""nofollow"">documents</a> included with it.  The linked shield includes some basic service plan as part of the purchase.  Once connected, it's just like using any other TCP/IP application.</p>
",1168588.0,0.0,1.0,,
2210,29323850,Send data to Arduino on keypress Raspberry Pi,|python|serial-port|arduino|raspberry-pi|robotics|,"<p>I'm trying to build a car (Wild Thumper) that i can drive from my Raspberry Pi. Currently i'm using my Raspberry Pi over SSH. It should send to data to my Arduino so it knows when it has to go forward or when to turn.</p>

<p>I've tried making scripts that get called by jQuery (apache on Pi) and send an integer over the serial port but it requires a delay and this is not ideal. (example forwardStart.py:)</p>

<pre><code>import serial
ser = serial.Serial('/dev/ttyACM0', 9600)
ser.open()
# here a delay is needed
ser.write('4') # go forward
ser.close()
</code></pre>

<p>To solve this i tried looking for a single python script that read my keyboard and send the correct integer. However, all keylisteners require display and can't be used over SSH.</p>

<p>Can anybody help me with the Python script or another idea that would works?</p>

<p>Thanks!</p>
",2015-03-28 23:30:00,,1081,1,4,0,,4725211.0,,3/28/2015 23:18,33.0,29376428.0,"<p>You should start reading from <a href=""https://stackoverflow.com/questions/983354/how-do-i-make-python-to-wait-for-a-pressed-key"">here</a>. The idea would be something like</p>

<pre><code>import serial
ser = serial.Serial('/dev/ttyACM0', 9600)
ser.open()
# here a delay is needed

try:
    while 1:
        try:
            key = sys.stdin.read(1) # wait user input
            actionKey = key2action(key) # translate key to action
            ser.write(actionKey) # go forward
        except IOError: pass
finally:
    ser.close()
</code></pre>

<p><strong>Note:</strong> this code will fail, it's more like pseudo-code to illustrate the idea.</p>
",3443596.0,0.0,0.0,46894575.0,"The delay shouldn't be a problem. You should open the serial port only once at startup to avoid this delay every time you want to send a command. Also, Arduino resets every time you establish a serial connection (and maybe that's why you need that delay)"
2214,29385348,2-D Multi-Robots Avoiding Obstacles to Goal,|java|a-star|robot|,"<p>I am thinking of making and a three robots connecting together and forming a triangle (not in a physical way) and try to avoiding static obstacles on their way to the goal location in Java applet. Besides, I am focusing on A* algorithm for the path finding and choosing the center of system as reference point for the heuristic value. But I have found out that even the A* has generated a path based on the center of the system, the multi-robots might still bump into the obstacles while traveling to the goal. Is there any good way to solve this?</p>
",2015-04-01 07:40:00,,339,1,5,2,,4737091.0,Los Angeles,4/1/2015 7:31,2.0,30063618.0,"<p>If you are using an occupancy map for navigation make each tile at least the size of the robot network. Then if a cell is ""not occupied"" you know with certainty your entire network can fit in it without bumping any obstacles. The robots themselves could use a subdivided map. So you can imagine that one cell in the occupancy map represents a 3x3 cell to the bots. So the bots could still have precision in their formation using their subdivided cell while the path planning algorithm can ensure no matter how the bots are arranged they will be safe</p>

<p>I have included a toy example. The dark lines represent the cell for the occupancy map, this is what your path planning algorithm sees. The bots (the small yellow squares) subdivide the occupancy map internally (the planning algorithm doesn't need to know this goes on) and can also change their own configuration within a single occupancy cell. </p>

<p>The dark pink cells show where a physical block may be, but we consider the entire region (large cell) occupied. Even if the network could potentially navigate past the physical obstruction. By simply saying the region near it is occupied we reduce the odds of getting in tricky situations.</p>

<p><img src=""https://i.stack.imgur.com/iMXxl.png"" alt=""enter image description here""></p>

<p>A* can only see the light pink and the light green cells. The robots can further subdivide the larger blocks internally</p>
",2705382.0,0.0,0.0,46969886.0,"@user902383 Sorry for making this question unclear. All the implementation is simulating in Java applet and robots will be moving around and it can spread inward and outward but connecting as a triangle shape all the time. If I consider it as a single point, how can I count the heuristic value? or is there other good algorithm to solve coordinated moving robots other than A*?"
2290,31605893,How do I create an Android app that allows me to control a robot fitted with raspberry pi?,|android|python|web-services|raspberry-pi|robotics|,"<p>I would like to create an Android App that allows me to remotely control a robot fitted with raspberry pi using Wi-Fi. I have searched various threads and one of the ways is to make my rspi a web server. I've come across this code that I should use on my Android app to make http request.</p>

<pre><code>    class RequestTask extends AsyncTask&lt;String, String, String&gt; {
@Override
protected String doInBackground(String... uri) {
    HttpClient httpclient = new DefaultHttpClient();
    HttpResponse response;
    String responseString = null;
    try {
        response = httpclient.execute(new HttpGet(uri[0]));
        StatusLine statusLine = response.getStatusLine();
        if(statusLine.getStatusCode() == HttpStatus.SC_OK){
            ByteArrayOutputStream out = new ByteArrayOutputStream();
            response.getEntity().writeTo(out);
            out.close();
            responseString = out.toString();
        } else{
            //Closes the connection.
            response.getEntity().getContent().close();
            throw new IOException(statusLine.getReasonPhrase());
        }
    } catch (ClientProtocolException e) {
        //TODO Handle problems..
    } catch (IOException e) {
        //TODO Handle problems..
    }
    return responseString;
}

@Override
protected void onPostExecute(String result) {
    super.onPostExecute(result);

    Toast.makeText(getApplicationContext(), result, 0).show();
}
}
</code></pre>

<p>To make the connection,I have to write this code:</p>

<pre><code>new RequestTask().execute(""http://192.168.1.145:80/3"");
</code></pre>

<p>However, my question is:
After connecting to my Raspi,how do I send python code from the android app to be executed on my Raspi?</p>

<p>Thanks.</p>
",2015-07-24 08:40:00,,592,0,4,1,0.0,5151362.0,,7/24/2015 8:09,132.0,,,,,,51162564.0,"On a side note HttpClient is deprecated, if you intend to use this over a WIFI [this](http://elinux.org/RPI-Wireless-Hotspot) may interest you. Check [this](https://www.raspberrypi.org/forums/viewtopic.php?f=63&t=31596) and [this](http://dishingtech.blogspot.jp/2012/01/realtek-wi-fi-direct-programming-guide.html)"
2168,28526200,How to program LEGO Mindstorms EV3 using C language?,|c|sensors|robotics|lego-mindstorms|lego-mindstorms-ev3|,"<p>First of all, I'm new for this and I need a little help!</p>

<p>I have a LEGO Mindstorms EV3 robot, I downloaded (LEGO Mindstorms EV3 Home Edition) to control the EV3. Unfortunately, I couldn't find the source code for the EV3 in the mentioned software. So, please if anybody could tell me the name of the software that enables you to program EV3! I would be most appreciated!</p>

<p>I also downloaded (Bricxcc) software but it was an old version. I couldn't find a newer version which contains EV3. </p>

<p>Can I use C language to program EV3 ? Or to add some features to the sensors?</p>

<p>Note: I ended with leJOS software to program the code with java it is much easier and there are a lot of resources for the EV3 brick in java. Wish you all the best!</p>
",2015-02-15 12:49:00,,28761,2,1,12,0.0,,,,,28529537.0,"<p>You can find the EV3 source code here: <a href=""https://github.com/mindboards/ev3sources"">https://github.com/mindboards/ev3sources</a></p>

<p>The generated documentation from this source code is available <a href=""http://ev3.fantastic.computer/doxygen/index.html"">here</a> and <a href=""http://ev3.fantastic.computer/doxygen-all/"">here</a>.</p>

<p>Bricxcc has some experimental support for EV3 but it is not being actively developed (since Oct. 2013). You can find the latest test version <a href=""http://bricxcc.sourceforge.net/test_releases/"">here</a>. Searching the web for ""bricxcc ev3"" will come up with some tutorials (for example, the one at <a href=""http://www.robotnav.com"">http://www.robotnav.com</a> looks good).</p>

<p><a href=""http://www.robotc.net/"">ROBOTC</a> is a good alternative, although it is not free.</p>

<p>There is also <a href=""http://www.ev3dev.org"">ev3dev</a>. There is a C library for ev3dev <a href=""https://github.com/in4lio/ev3dev-c"">here</a> or you can write your own.</p>
",1976323.0,13.0,2.0,45370020.0,"Just a tip: 'robots.txt' is not the tag you want. It's not about robots, it's about web."
2380,32978569,Foward/Inverse Kinematics Calculations 2-DOF python,|python|math|robotics|kinematics|inverse-kinematics|,"<p>The program calculates the points of an end-effector with forward kinematics using the equation, </p>

<p><code>x = d1cos(a1) + d2cos(a1+a2)</code> </p>

<p><code>y = d1sin(a1) + d2sin(a1+a2)</code></p>

<p>where <code>d1</code> is the length of the first joint, <code>d2</code> is the length of the second joint, <code>a1</code> is the angle of the first joint and <code>a2</code> is the angle of the second joint.</p>

<p>It calculates inverse kinematics by this equation</p>

<p><a href=""https://i.stack.imgur.com/e5cCm.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e5cCm.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/Oi6R4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Oi6R4.jpg"" alt=""enter image description here""></a></p>

<p>So, by entering the input required for forward kinematics I should get the points of the end effector. By entering in the same input and the points found in forward kinematics for inverse kinematics, I should get the angles I entered in as input for forward kinematics. But I do not get them back. 
Here is my code, </p>

<pre><code>'''
Created on Oct 5, 2015

@author: justin
'''
import math
def getOption():
    print('Select an option:\n')
    print('\t1) Forward Kinematics\n')
    print('\t2) Inverse Kinematics\n')
    option = input()
    try:
        option = int(option)
        if option == 1:
            fowardKinematics()
        elif option == 2:
            inverseKinematics()
        else:
            print('Not an option')
            return
    except ValueError:
        print('Not an integer/Point cannot be reached')
        return
def fowardKinematics():
    '''
    Ask user for input and computing points of end-effector 
    '''
    length1 = input('Enter length of joint 1 (a1):\n')  # Getting input from keyboard
    angle1 = input('Enter angle of joint 1 (theta1):\n')
    length2 = input('Enter length of joint 2 (a2):\n')
    angle2 = input(""Enter angle of join 2 (theta2)\n"")

    try:
        length1 = float(length1)  # Testing to see if user entered a number
        length2 = float(length2)  # Testing to see if user entered a number
        angle1 = float(angle1)  # Testing to see if user entered a number
        angle2 = float(angle2)  # Testing to see if user entered a number
    except ValueError:
        print('Invalid input, check your input again')
        return
    x = (length1 * math.cos(math.radians(angle1))) + (length2 * math.cos((math.radians(angle1 + angle2))))  # a1c1 + a2c12
    y = (length1 * math.sin(math.radians(angle1))) + (length2 * math.sin((math.radians(angle1 + angle2))))  # a1s1 + a2s12
    print('The position of the end-effector P(x,y) is:\n')
    print('X: ' + str(x))  # Convert x to string
    print('Y: ' + str(y))  # Convert y to string
def inverseKinematics():
    length1 = input('Enter length of joint 1 (a1):\n')
    length2 = input('Enter length of joint 2 (a2):\n')
    x = input('Enter position of X:\n')
    y = input('Enter position of Y:\n')
    try:
        length1 = float(length1)
        length2 = float(length2)
        x = float(x)
        y = float(y)
    except ValueError:
        print('Invalid input, check your input again')
        return
    # Computing angle 2 Elbow up/down
    numerator = ((length1 + length2)**2) - ((x**2) + (y**2))
    denominator = ((x**2) + (y**2)) - ((length1 - length2)**2)
    angle2UP = math.degrees(math.atan(math.sqrt(numerator/denominator)))
    angle2DOWN = angle2UP * -1

    # Angle 1 Elbow up
    numerator = (length2 * math.sin(math.radians(angle2UP)))
    denominator = ((length1 + length2) * math.cos(math.radians(angle2UP)))
    angle1UP = math.degrees(math.atan2(numerator, denominator))
    # Angle 1 Elbow down
    numerator = (length2 * math.sin(math.radians(angle2DOWN)))
    denominator = ((length1 + length2) * math.cos(math.radians(angle2DOWN)))
    angle1DOWN = math.degrees(math.atan2(numerator, denominator))
    print(""Angle 1 Elbow up: "" + str(angle1UP))
    print(""Angle 1 Elbow down: "" + str(angle1DOWN))
    print(""Angle 2 Elbow up: "" + str(angle2UP))
    print(""Angle 2 Elbow down: "" + str(angle2DOWN))
if __name__ == '__main__':
    getOption()
    pass
</code></pre>

<p>I think the problem is when the trig functions get introduced. The parameters for them are supposed to be in radians, they return the answer is degrees. Somewhere I am mixing up the two. I just don't know where. Thanks</p>
",2015-10-06 19:56:00,,1885,1,0,1,,,,,,32995487.0,"<p>There is, I'm afraid, quite a bit wrong with this, either in your code or in the equations you're using. Your equation for theta2 doesn't make any sense to me if x and y are distances and a1 and a2 are angles (check your equation or give a source). Even if these should be d1 and d2, this equation involves subtracting two quantities with different dimensions (length^4 and length^2).</p>

<p>Then check your implementation of it, which does not evaluate the equation as given.</p>

<p>My advice about radians / degrees is to use radians throughout: accept the angles in degrees if you want, but then immediately convert to radians for the calculations, and convert angular results back to degrees for output.</p>

<p>Some more advice:</p>

<ul>
<li><p>you don't need to cast your floats to strings for output using print, just use <code>print('x: ', x)</code> and so on.</p></li>
<li><p>Give your variables the same names as the symbols they represent in your formula. This would make it easier to debug (well, it would if the  equations were correct).</p></li>
</ul>

<p>Hope that helps.</p>
",293594.0,1.0,0.0,,
2312,32057612,How to connect usb webcam to ev3?,|image-processing|labview|robotics|lego-mindstorms|,"<p>I would like to connect an USB webcam to my EV3.
is it possible to do that?</p>

<p>and how can I do it and how can I reach the cam from my code?</p>

<p>I use Labview for programming</p>

<p>and thanks in advance :)</p>
",2015-08-17 18:51:00,32063010.0,3206,2,2,2,0.0,4750112.0,,4/4/2015 19:53,35.0,32063010.0,"<p>Both <a href=""http://lejos.org"" rel=""nofollow"">leJOS</a> and <a href=""http://www.evdev.org"" rel=""nofollow"">ev3dev</a> support USB web cams connected to the EV3 brick, however, neither work with LabView.</p>

<p>To get this working on the official LEGO firmware, you need to compile proper kernel modules, sideload them on the EV3 and as suggested in the comments, sideload another program that converts the image data into something that can communicate with labview remotely.</p>
",1976323.0,2.0,0.0,52019555.0,[ev3dev](http://www.ev3dev.org/) looks like something you should look into. It should allow install a driver and use a programming language of your choice to stream the webcam image over udp to labview
2252,30499463,Render arbitrary CSG solid given boolean function?,|c#|robotics|csg|,"<p>I'm looking to implement my own CSG classes for a Robotics project, and I'm thinking to implement each solid as a function that returns a boolean value, given a 3D point; this function will return true if the 3D point is contained within the solid. I figured by doing things this way, I can easily perform union, intersection and subtraction of solids.</p>

<p>This will be sufficient for performing collision detection .etc. by itself, but I'll want to actually render the solids, so my question is this; are there any methods of rendering a solid given its boolean function as described above? I'm more than happy to implement this myself as I want to ideally know exactly what's going on so I can streamline and add to the code as required. I'm also open to suggestions for representing solids in a different way if it will make things easier!</p>

<p>An interesting thing to note is that it would be useful if I could derive things such as the center-of-mass of a solid.</p>

<p>Thanks in advance!
Lee.</p>
",2015-05-28 07:06:00,30558601.0,1262,2,0,1,,3767259.0,,6/23/2014 11:41,47.0,30558601.0,"<p>Constructive Solid Geometry looks simple, but like most geometry problems, there enough subtleties that you really don't want to implement this for yourself unless it's the core of your work / research. </p>

<p>My suggestion is that you instead look for a high quality computational geometry library (ideally supported by good academic pedigree and published as open source). <a href=""http://www.cgal.org/"" rel=""nofollow"">CGAL</a> is a good option.</p>

<p>If speed isn't a huge priority (ie/ you can solve the problems offline in a separate tool), the problem is common enough problem that other people have done a lot of the hard work already. Check out <a href=""http://www.openscad.org/"" rel=""nofollow"">OpenSCAD</a> ""The Programmers Solid 3D CAD Modeller"", which uses CGAL to do the boolean operations.</p>

<p>If you need a compromise between the very low, and very high level interfaces, <a href=""https://github.com/SolidCode/SolidPython"" rel=""nofollow"">SolidCode</a> might not be a bad intermediate api that lets you call OpenSCAD like commands from Python code.</p>

<p>For rendering, I would advise that you consider treating everything as a (triangulated) surface mesh or volumetric mesh as appropriate, and using CSG or any other tool (eg/ blender) as just a mechanism for data entry. You may also find that collision libraries are readily available for triangulated meshes - and while you may gain some runtime performance improvement for using CSG, it's likely to take longer to develop the whole project.</p>
",2246.0,1.0,0.0,,
2198,29143763,Need some tips which localization method to use for my NAO robot (robotic),|localization|orientation|project-management|robotics|nao-robot|,"<p>for university I'm working on a project in which I have to teach a robot(Nao-robot) play nine men's morris. Unfortunately I'm fairly new to the area of robotics and I need some tips how to solve some problems. Currently I'm working on the localization/orientation of the robot and I'm wondering which approach of localization would fit best in my project.</p>

<p><strong>A short explanation of the project:</strong>
The robot has a fixed starting position and has to walk around on a boardwhich has a size of about 3x3 meter ( I will post a picture of the board when i reach 10 reputation). There are no obstacles on the field except the game tokens and the game lines are marked yellow on the board. For orientation I use the two camera devices the robot has.</p>

<p>I found some approaches like </p>

<p><strong>Monte Carlo Localization</strong></p>

<p><strong>SLAM</strong> (Simultaneous Localization and Mapping)</p>

<p>but these approaches seem to be quite complex for a beginner like me and I would really appreciate if some has some good ideas what would be a simpler way to solve this problem. Functionality has for me a far higher priority than performance.</p>
",2015-03-19 11:47:00,29150211.0,256,1,0,-2,0.0,4685999.0,,3/18/2015 15:19,5.0,29150211.0,"<p>I have vague knowledge about the nine men's morris game as such, but I will try to give you my simpler idea.</p>

<p>First thing first, you need to have a map of your board. This should be easy in your case, cause your environment is static. There are few <a href=""http://forums.trossenrobotics.com/tutorials/introduction-129/an-introduction-to-mapping-and-localization-3274/"" rel=""nofollow"">technique</a> to do this mapping from your board. For your case I would suggest to have a metric map, which is an occupancy grid. Assign coordinates to each cell in the grid. This will be helpful in robot navigation.</p>

<p>As you have mentioned, your robot starts from a fixed position. On start up, initialize your robot with this reference location and orientation (with respect to X-Y axes of the grid, may be you don't need the cameras, I am not sure!!). By initialization I mean, mark your position on the grid.</p>

<p>Use <a href=""http://en.wikipedia.org/wiki/Dead_reckoning"" rel=""nofollow"">Dead Reckoning</a> for localization and keep updating position and orientation of your robot as it move through the board. I would hope that your robot get some feedback from the servos, like number of rotations and so forth. Do that math and update the position coordinates of your robot as it move into different cell in the grid.</p>

<p>You can use <a href=""http://wiki.gamegardens.com/Path_Finding_Tutorial"" rel=""nofollow"">A-Star</a> algorithm to find a path for your robot. You need to do the path planning before you want to navigate. You also have to mark those game tokens on the grid, to avoid collisions in planning the path. </p>
",1800026.0,0.0,0.0,,
2388,33186486,Advanced Line Follower Robot,|arduino|robotics|robotics-studio|,"<p>I know about line follower mainly the grid solving robots i know the basics actually. Actually they have to trace the path of the grid in an arena and then reach back to starting point in a shortest distance. Here my doubt is regarding the line follower in this link I have attached.<a href=""https://www.youtube.com/watch?v=5At_u5rnh2U"" rel=""nofollow noreferrer"">Advanced line follower robot for grid solving and maze solving</a></p>
<p>My doubt is what are the procedures to do it? They have mapped the path and used Dijkstra algorithm to solve the path. But how do they transfer the code(i.e) where it has to turn which direction it has to turn. how are they generating the what function should be passed? Please explain i need the procedure alone. am going to try it with python.</p>
",2015-10-17 12:11:00,33242153.0,1147,1,10,0,0.0,,,,,33242153.0,"<p>From the comments we exchanged, I feel more confident to assume your actual question is this:</p>

<blockquote>
  <p>What data structure could be used to store the structure (geometry, topology) of the map into the robot's memory?</p>
</blockquote>

<p>Well there should be many possible ways to do that. Basically, this is a connected graph where nodes sit on a rectangular grid. So, for a start, one could describe the nodes as a set of coordinate pairs:</p>

<pre><code>// just an example, this is not the actual map
// it doesn't need to be variables, could be array of arrays, or dictionary
var A = (0,0);
var B = (1,0);
var C = (2,1);
var D = (4,2);
// etc.
</code></pre>

<p>Then, you could describe the edges as pairs of points:</p>

<pre><code>var edges = [(A,B), (A,D), (B,C), ...];
</code></pre>

<p>With these, you surely could calculate a good path from a list of points, and also the location and direction of each node.</p>

<p>I'm not sure at all if this is the most efficient data structure, but it is already a start. You only need to know the location of each node, and the edges are defined simply by linking two nodes together.</p>
",401828.0,0.0,2.0,54280119.0,"The downvote wasn't mine. Anyway, I agree your question is broader than Arduino's scope. Honestly, I think it may be downvoted actually by being too broad. In your second paragraph, it is not quite clear what you are asking, in my opinion."
2420,34522095,GUI Button hold down - tkinter,|python|tkinter|robotics|,"<p>I'm trying to do a GUI in python to control my robotic car. My question is how I do a function that determine a hold down button. I want to move the car when the button is pressed and held down and stop the car when the button is released.</p>

<pre><code>from Tkinter import * 

hold_down = False 
root = Tk()

def button_hold(event):
      hold_down=true
      while hold_down== True: 
               print('test statement')
               hold_down = root.bind('&lt;ButtonRelease-1&gt;',stop_motor)

def stop_motor(event):
       hold_down= False
       print('button released')

button = Button(root, text =""forward"")
button.pack(side=LEFT)
root.bind('&lt;Button-1&gt;',button_forward)
root.mainloop()
</code></pre>

<p>I'm trying to simulate what I found in this <a href=""https://stackoverflow.com/a/2440643"">answer</a></p>

<p>I try to do it in a <code>while</code> loop with a boolean. When the user presses the button the boolean changes to <code>True</code> and code enters the while loop. When user releases the button the boolean changes to <code>False</code> and code exits from loop but in this code the boolean stay always true no matter if I released the button or not. </p>

<p>Edit: I want a function to be called until a condition occurs.The function to be called is hold_down() and the condition to check  is the button is released.</p>

<p>Update: I found a way to make it work.</p>
",2015-12-30 02:20:00,,25805,5,1,3,0.0,5729115.0,,12/30/2015 2:55,21.0,34522141.0,"<p>Try this...</p>

<pre><code>from Tkinter import *  
root = Tk()
global hold_down

def button_hold(event):
    hold_down = True
    while hold_down: 
        print('test statement')

def stop_motor(event):
    hold_down = False
    print('button released')

button = Button(root, text =""forward"")
button.pack(side=LEFT)
root.bind('&lt;Button-1&gt;',button_hold)
root.bind('&lt;ButtonRelease-1&gt;',stop_motor)
root.mainloop()
</code></pre>
",4655537.0,0.0,2.0,82612777.0,can you share your solution here? I have the same problem.
2306,32027233,How to multithread/multiprocess just one instance of a particular function in Python?,|python|multithreading|multiprocessing|robotics|,"<p>I'm running a Python script that controls a robot, but I'm a bit confounded as to how to multithread the motor control function.</p>

<p>The issue is that the hardware is designed such that the motors won't move unless there are several sleeps in the motor-control function, because the hardware requires time to send the electrical signals to the motors. Because of these sleeps in the motor-control function, the entire program halts and stops reading in sensor data.</p>

<p>What I would like to do is know how to multithread/multiprocess the motor-control function once it is called, but once the program hits the call again in the following iteration of the loop, it checks if the motor-control is still running (i.e. it isn't done with the sleep). If it is still running, it simply skips over the motor-control call and continues on with looping, reading sensor data, and then checking again if the motor-control function is still running. Of course, if the motor-control function is no longer running, I would like for it to once again be called.</p>

<p>Basically, the whole program only requires two threads: one that runs the main program, and one that branches off and continuously re-runs one instance of the motor-control function every time the motor-control function has completed its execution.</p>

<p>I had tried using the concurrent.futures import but got messages saying it was not supported and I couldn't find any usages particular to the way I intend to use it.</p>
",2015-08-15 17:17:00,32034462.0,843,1,6,4,0.0,3293742.0,"Pensacola, Florida",2/10/2014 16:31,368.0,32034462.0,"<p>I think you don't need threading, but I may misunderstand your requirements so I will present 2 alternatives.</p>

<ol>
<li>Without threading and sleeps</li>
</ol>

<p>Assuming your current program flows like that :</p>

<pre><code>while True:
    data = read_sensors()
    command = process(data)
    motor_do(command)
    time.sleep(delay)
</code></pre>

<p>Then you could just remove the sleep, and only call motor_do if the last call was at least <code>delay</code> seconds ago.</p>

<pre><code>last_call_time = -delay # to be sure that motor_do can be called the first time
# ""On Windows, [time.clock] returns wall-clock seconds elapsed since the first call to this
# function, as a floating point number, based on the Win32 function QueryPerformanceCounter()
# The resolution is typically better than one microsecond."" (python 2.7 doc)
# i.e. close to 0 on first call of time.clock()

while True:
    data = read_sensors()
    command = process(data)
    motor_try(command)

def motor_try(command):
    global last_call_time

    current_time = time.clock()
    # on win that works, on unix... you may want to take a look at the NB

    elapsed = current_time - last_call_time
    if elapsed &gt;= delay:
        motor_do(command)
        last_call_time = current_time
</code></pre>

<ol start=""2"">
<li>With threading (this is an example, I have no experience in threading/async with python 2.7 so there may be better ways to do this)</li>
</ol>

<p>Assuming your current program flows like that :</p>

<pre><code>while True:
    data = read_sensors()
    command = process(data)
    motor_do(command) // this function sleeps and you CANNOT change it
</code></pre>

<p>Then you have to launch 1 thread that will only push the commands to the motor, asynchronously.</p>

<pre><code>import thread

command = None
command_lock = thread.allocate_lock()

def main():
    thread.start_new_thread(motor_thread)

    global command
    while True:
        data = read_sensors()
        with command_lock:
            command = process(data)

def motor_thread():
    while True:
        while not command: # just to be sure
            time.sleep(0.01)
            # small delay here (for instance, the time consumed by read_sensors + process)
        with command_lock:
            motor_do(command)
            command = None
        time.sleep(delay)
</code></pre>

<p>NB : on Unix, <code>time.clock()</code> returns processor time (= without the idling time), so it would be better to use <code>time.time()</code>... unless the system clock is changed : ""While this function normally returns non-decreasing values, it can return a lower value than a previous call if the system clock has been set back between the two calls."" (python 2.7 doc)</p>

<p>I don't know how <code>time.sleep()</code> react to system clock changes.</p>

<p>see <a href=""https://stackoverflow.com/questions/1205722/how-do-i-get-monotonic-time-durations-in-python"">How do I get monotonic time durations in python?</a> for precise time delays on unix/py2.7 (and <a href=""https://stackoverflow.com/questions/25785243/understanding-time-perf-counter-and-time-process-time"">Understanding time.perf_counter() and time.process_time()</a> can be useful)</p>

<p>Python3 : just use <code>time.monotonic()</code>... or <code>time.perf_counter()</code>.</p>
",3125565.0,1.0,0.0,51957192.0,"You need to consider that you create a thread which should call your function, which itself should loop round forever. Different threads each call different functions."
2255,30604094,Arduino Making Spiral,|arduino|robotics|,"<p>I have a robot sweeper that I am creating it detects walls and turns when getting to a certain distance so it does not hit. However the movements are pretty random and I would like the device to start out making a small circle and then growing to a larger one. It seems to be getting stuck in just one size circle with maybe a little growth. I created multiple size circle functions however it does not seem to be taking hold. Thanks ahead of time. Any help no matter how small would be greatly appreciated. </p>

<pre><code>#include&lt;NewPing.h&gt;
#define MOTOR_A 0 
#define MOTOR_B 1
#define TRIGGER_PIN 5
#define ECHO_PIN 4
#define MAX_DISTANCE 200

#define CW 0 
#define CCW 1 

const byte PWMA = 3; 
const byte PWMB = 11; 
const byte DIRA = 12; 
const byte DIRB = 13; 

NewPing sonar (TRIGGER_PIN, ECHO_PIN, MAX_DISTANCE); 


void setup() {
  // put your setup code here, to run once:
setupArdumoto(); 
}

void loop() {
  delay(50); 
  unsigned int uS= sonar.ping();

  if(uS/US_ROUNDTRIP_CM&gt;50||uS/US_ROUNDTRIP_CM==0)
  {
    forward(); 
    curve();
    //stopArdumoto(MOTOR_A);
    // stopArdumoto(MOTOR_B);
  }
  else if(uS/US_ROUNDTRIP_CM&gt;=90)
  {
    smallerCurve();
  }

 else if(uS/US_ROUNDTRIP_CM&gt;=110)
 {
   smallestCurve();
 }
  else if(uS/US_ROUNDTRIP_CM&lt;20)
  { 
    //turnRight(100); 
    delay(500); 
  }
  // put your main code here, to run repeatedly:

}
void driveArdumoto(byte motor, byte dir, byte spd) 
{
  if(motor == MOTOR_A)
  {
    digitalWrite(DIRA, dir);
    analogWrite(PWMA, spd); 
  }
  else if(motor==MOTOR_B)
  {
    digitalWrite(DIRB,dir); 
    analogWrite(PWMB, spd); 
  }
}
void curve() 
{
 driveArdumoto(MOTOR_A, CW, 200);
 driveArdumoto(MOTOR_B, CW, 150);
}
void smallerCurve()
{
 driveArdumoto(MOTOR_A, CW, 200);
 driveArdumoto(MOTOR_B, CW, 120);
}
void smallestCurve()
{
 driveArdumoto(MOTOR_A, CW, 200);
 driveArdumoto(MOTOR_B, CW, 100);
}
void forward()
{
  driveArdumoto(MOTOR_A,CW,170);
  driveArdumoto(MOTOR_B,CW,170); 
}

void turnRight(byte spd)
{
  stopArdumoto(MOTOR_B); 
  driveArdumoto(MOTOR_A,CW,250); 
}
void turnLeft(byte spd) 
{
   stopArdumoto(MOTOR_A); 
   driveArdumoto(MOTOR_B,CW,250); 
}
void stopArdumoto(byte motor)
{
    driveArdumoto(motor, 0,0); 
}

void setupArdumoto()
{
   pinMode(PWMA,OUTPUT);
   pinMode(PWMB,OUTPUT);
   pinMode(DIRA,OUTPUT);
   pinMode(DIRB,OUTPUT); 

   digitalWrite(PWMA, LOW);
   digitalWrite(PWMB, LOW);
   digitalWrite(DIRA, LOW);
   digitalWrite(DIRB, LOW);
}
</code></pre>
",2015-06-02 18:44:00,,590,1,0,0,,3006893.0,California,11/19/2013 0:42,11.0,30631067.0,"<p>It's late but I think that your code rarely will execute <code>smallerCurve();</code> or <code>smallestCurve();</code> because if <code>uS/US_ROUNDTRIP_CM</code> is greater than 50, code in first <code>if</code> will execute and jump the code for <code>uS/US_ROUNDTRIP_CM &gt;= 90</code> and <code>uS/US_ROUNDTRIP_CM &gt;= 110</code> only when it was between 0 and 20 will execute different things. Take care of your <code>else-if</code>, once you have executed one block of code in an <code>if</code> you will jump over the <code>else</code>, and your first <code>else</code> take all the rest of your code.
In fact, a good compiler doesn't compile this <code>else</code>'s because they are unreacheable code.</p>

<p>And, moreover, you can improve your code with some simple things. Be careful with the function <code>drive Ardumoto</code> and don't use it so frequently, <code>forward</code> won't have time to move your robot. Use a delay between movements.</p>
",3861548.0,0.0,0.0,,
2256,30724864,"Graph-SLAM when it uses only odometry information, will it still run? and what is the outcome?",|robotics|kalman-filter|slam-algorithm|,"<p>This is a kind of difficult question.</p>

<p>I know about <strong>EKF-SLAM</strong>, which uses a state from previous time to estimate next state as an online filter, 
I also know about <strong>Graph-SLAM</strong>, which uses all states in past as full SLAM, and represents them as merely whole bunch of nodes and edges, and optimize structure of nodes and eges by minimizing error to estimate better states.</p>

<p>Now,
I know that there is no meaning in running EKF-SLAM with odometry info only, since what EKF does is estimating future state by balancing weight between Odometry info AND Observation of landmark info. so both are needed.</p>

<p>My question is, is it possible to run <strong>Graph-SLAM with only Odometry info</strong> and no landmark observation info whatsoever?
It seems like Graph-SLAM can run by gathering all Odometry info state upto current ones and converting them to nodes and edges  just like it does when both Odo and obs are provided, and it can optimize the structure of nodes and edges.
Is is possible?
What does output mean? ""Optimized"" Odometry?
Any thought to it?
Thank you in advance. :)</p>
",2015-06-09 06:50:00,,211,1,0,0,,4989045.0,,6/9/2015 6:30,0.0,30850314.0,"<p><strong>preface:</strong> I am not 100% certain, these are just my assumptions/opinions</p>

<p>the point of SLAM is <strong>S</strong>imultaneous <strong>L</strong>ocalization <strong>A</strong>nd <strong>M</strong>apping In order to do any mapping you need Observation of landmarks, or some other feature. Otherwise you are only performing localization. </p>

<p>Think if I dropped you a building you've never been in before and I said, create a map for me, BUT you can ONLY count your footsteps. You must not use any other senses (eyes closed, ears plugged, etc). You would quickly realize this is a nearly impossible task. If you use only odometry, something like a Kalman Filter, or EKF should work nicely, but again this is only doing localization, not mapping.</p>

<p>hope that helps</p>
",2705382.0,0.0,0.0,,
2347,32228629,IMU Orientation constantly changing,|orientation|ros|hardware|robotics|imu|,"<p>We have XSENS MTi IMU-Device and use the ROS-Framework (Ubuntu / Fuerte). 
We subscribe to the IMU-Data and all data looks good except <strong>orientation</strong>.</p>

<p>In Euler-Outputmode like in Quaternion-Outputmode the values are constantly changing. Not randomly, they increase or decrease slowly at a more or less constant rate, and sometimes I observed the change to flatten out and then change it's direction. </p>

<p>When the Value at Second X may be:</p>

<pre><code>x: 7.79210457616
y: -6.58661204898
z: 41.2841955308
</code></pre>

<p>the Z value changes in a range of about 10 within a few seconds (10-20 seconds I think). </p>

<p>What can cause this behaviour? Do we misinterpret the data or is there something wrong with the driver? The strange thing is, this also happend with 2 other drivers, and one other IMU device (we have 2). Same results in each combination. </p>

<p>Feel free to ask for more precise data or whatever you'd like to know that may be able to help us out. We are participating at the Spacebot-Cup in November, so it would be quite a relief to get the IMU done. :)</p>
",2015-08-26 13:54:00,,2585,2,0,3,0.0,4315243.0,,12/2/2014 10:23,32.0,32231388.0,"<p>If you have the IMU version, I assume that no signal processing has been done on the device. (but I don't know the product). So the data you get for the orientation should be only the integral of the gyroscope data.</p>

<p>The drift you can see is normal and can come from the integration of the noise, a bad zero rate calibration, or the bias of the gyroscope.</p>

<p>To correct this drift, we usually use an <code>AHRS</code> or a <code>VRU</code> algorithm (depending the need of a corrected yaw). It's a fusion sensor algorithm which take the gravity from the accelerometer and the magnetometer data (for AHRS) to correct this drift.</p>

<p>The algorithms often used are the <code>Kalman filter</code> and the <code>complementary filter</code> (Madgwick/Mahony).</p>

<p>It's not an easy job and request a bit of reading and experimenting on matlab/python to configure these filters :)</p>
",5269403.0,2.0,0.0,,
2461,35771915,Find closest value in a 2d grid c#,|c#|multidimensional-array|robotics|a-star|,"<p>I have created an c# console application that is used to simulate a robot application.
I have created a 2D grid for the robot to move around:</p>
<pre><code>List&lt;List&lt;int&gt; Map;
</code></pre>
<p>The map is a 25x25 grid (to start with) and filled with the following values:</p>
<pre><code>0 = Unexplored space,
1 = Explored space,
2 = Wall, 
3 = Obstacle, 
9 = Robot
</code></pre>
<p>The robot begins in position (12,12).
I would like to be able to search this grid for the nearest Unexplored space and return that position so that I can then feed that position and the robots position to an A* search algorithm for planning.</p>
<p>What would be the most efficient method to search through the Map for said value?</p>
",2016-03-03 12:13:00,35772727.0,1525,1,2,-1,,1379704.0,"Hull, UK",5/7/2012 12:14,192.0,35772727.0,"<p>wrote this on a notepad so i havent tested it, but you should get an idea.
Basically get all unexplored places and sort them by the distance from current and get the first value in the list.
CalculateDistance method should implement the formula Nikola.Lukovic mentioned.</p>

<pre><code>public KeyValuePair&lt;int, int&gt; GetClosestUnexploredPosition(List&lt;List&lt;int&gt;&gt; map, int currentX, int currentY)
{
    Dictionary&lt;KeyValuePair&lt;int, int&gt;, double&gt; unexploredPlaces = new Dictionary&lt;KeyValuePair&lt;int, int&gt;, double&gt;();

    foreach (List&lt;int&gt; valueList in map)
    {
        foreach (int value in valueList)
        {
            if (value == 0)
            {
                int x = map.IndexOf(valueList);
                int y = valueList.IndexOf(value));
                if (x != currentX &amp;&amp; y != currentY)
                {
                    unexploredPlaces.Add(new KeyValuePair(x, y), CalculateDistance(currentX, currentY, x, y));
                }
            }
        }
    }

    return unexploredPlaces.OrderBy(x =&gt; x.Value).FirstOrDefault();
}
</code></pre>
",6013031.0,2.0,0.0,59215948.0,"The problem is I do not know the location of the other position. I need to search the grid for the nearest position first. 
I could use this distance formula on every '0' in the grid but I do not think that would be the most efficient way to do it."
2495,36829189,How to receive data from Microsoft Kinect device?,|kinect|kinect-sdk|robotics|,"<p>I'm wondering are their any codes or application that I can use to receive data from Kinect device. </p>

<p>The idea is to use the kinect to send its signals to a surface and get back range of signal data. this could be set of numbers which will change according to the light, distance and angel 
Thanks.</p>
",2016-04-24 21:38:00,36847102.0,1450,1,2,-1,,2948959.0,"Perth, Western Australia",11/3/2013 1:05,82.0,36847102.0,"<p>Start with downloading the sdk from the link below (windows 8 or above is required for kinect to pc) and use their demo projects to see how to set up a program to read data in from the Kinect. It will have colour and depth cameras and you will be able to get the pixel data back and be able to use the depth feedback to tell how far away something is. As far as angle, the Kinect doesn't have a built in gyroscope so you'll have to use known points and trig to find angles.</p>

<p><a href=""https://www.microsoft.com/en-us/download/details.aspx?id=44561"" rel=""nofollow"">https://www.microsoft.com/en-us/download/details.aspx?id=44561</a></p>
",5758722.0,3.0,1.0,61254970.0,Are you using Kinect v1 or Kinect v2? If you are using Kinect v2 there are many examples included in the SDK!
2557,38710748,Arduino radio frequency receiver does not work with motor shield,|c++|c|arduino|robotics|,"<p>Micro-controller : SainSmart Mega 2560<br>
Motor Shield: Osepp Motor Shield  V1.0<br>
I am trying to implement Radio Frequency communication on my wheeled robot however when the motors are running the radio frequency code will not receive messages.  </p>

<p>The motor shield uses pins 4,7,8,12<br>
I have setup the radio frequency to occur on pins 22,23 ,5 </p>

<p>I see this reference to 
<a href=""https://stackoverflow.com/questions/21069867/why-does-virtualwire-conflicts-with-pwm-signal-in-arduino-atmega328-pin-d10"">Why does VirtualWire conflicts with PWM signal in Arduino/ATmega328 pin D10?</a>
but am not sure if this applies to my situation.</p>

<p><strong>How do I get Radio Frequency receiver/transmitter to work while motor shield in use?</strong>  </p>

<p>code demonstrating the situation:</p>

<pre><code>  #include &lt;RH_ASK.h&gt;
  #include &lt;SPI.h&gt; // Not actually used but needed to compile
  RH_ASK driver(2000, 22, 23, 5,true); // ESP8266: do not use pin 11
  /// *************************
  //      MOTOR SETUP
  /// *************************
  // Arduino pins for the shift register
  #define MOTORLATCH 12
  #define MOTORCLK 4
  #define MOTORENABLE 7
  #define MOTORDATA 8

  // 8-bit bus after the 74HC595 shift register
  // (not Arduino pins)
  // These are used to set the direction of the bridge driver.
  #define MOTOR1_A 2
  #define MOTOR1_B 3
  #define MOTOR2_A 1
  #define MOTOR2_B 4
  #define MOTOR3_A 5
  #define MOTOR3_B 7
  #define MOTOR4_A 0
  #define MOTOR4_B 6

    // Arduino pins for the PWM signals.
    #define MOTOR1_PWM 11
    #define MOTOR2_PWM 3
    #define MOTOR3_PWM 6
    #define MOTOR4_PWM 5
    #define SERVO1_PWM 10
    #define SERVO2_PWM 9

    // Codes for the motor function.
    #define FORWARD 1
    #define BACKWARD 2
    #define BRAKE 3
    #define RELEASE 4

    void setup()
    {
        Serial.begin(9600); // Debugging only
        if (!driver.init())
             Serial.println(""init failed"");

      //comment out code below  to allow receiver to read radio frequency  communication
      //BEGIN
        motor(1, FORWARD, 255);
        motor(2, FORWARD, 255);
        motor(4, FORWARD, 255);
        motor(3, FORWARD, 255);
       //END

    }
    void loop()
    {
        uint8_t buf[RH_ASK_MAX_MESSAGE_LEN];
        uint8_t buflen = sizeof(buf);
        if (driver.recv(buf, &amp;buflen)) // Non-blocking
        {
            int i=0;
            // Message with a good checksum received, dump it.
            driver.printBuffer(""Got:"", buf, buflen);
            buf[5]= '\0';
            Serial.println((char*)buf);
        }
    }


    void motor(int nMotor, int command, int speed)
    {
      int motorA, motorB;

      if (nMotor &gt;= 1 &amp;&amp; nMotor &lt;= 4)
      {  
        switch (nMotor)
        {
        case 1:
          motorA   = MOTOR1_A;
          motorB   = MOTOR1_B;
          break;
        case 2:
          motorA   = MOTOR2_A;
          motorB   = MOTOR2_B;
          break;
        case 3:
          motorA   = MOTOR3_A;
          motorB   = MOTOR3_B;
          break;
        case 4:
          motorA   = MOTOR4_A;
          motorB   = MOTOR4_B;
          break;
        default:
          break;
        }

        switch (command)
        {
        case FORWARD:
          motor_output (motorA, HIGH, speed);
          motor_output (motorB, LOW, -1);     // -1: no PWM set
          break;
        case BACKWARD:
          motor_output (motorA, LOW, speed);
          motor_output (motorB, HIGH, -1);    // -1: no PWM set
          break;;
        case RELEASE:
          motor_output (motorA, LOW, 0);  // 0: output floating.
          motor_output (motorB, LOW, -1); // -1: no PWM set
          break;
        default:
          break;
        }
      }
    }

    void motor_output (int output, int high_low, int speed)
    {
      int motorPWM;

      switch (output)
      {
      case MOTOR1_A:
      case MOTOR1_B:
        motorPWM = MOTOR1_PWM;
        break;
      case MOTOR2_A:
      case MOTOR2_B:
        motorPWM = MOTOR2_PWM;
        break;
      case MOTOR3_A:
      case MOTOR3_B:
        motorPWM = MOTOR3_PWM;
        break;
      case MOTOR4_A:
      case MOTOR4_B:
        motorPWM = MOTOR4_PWM;
        break;
      default:
        speed = -3333;
        break;
      }

      if (speed != -3333)
      {
        shiftWrite(output, high_low);
        if (speed &gt;= 0 &amp;&amp; speed &lt;= 255)    
        {
          analogWrite(motorPWM, speed);
        }
      }
    }

    void shiftWrite(int output, int high_low)
    {
      static int latch_copy;
      static int shift_register_initialized = false;
      if (!shift_register_initialized)
      {
        // Set pins for shift register to output
        pinMode(MOTORLATCH, OUTPUT);
        pinMode(MOTORENABLE, OUTPUT);
        pinMode(MOTORDATA, OUTPUT);
        pinMode(MOTORCLK, OUTPUT);

        // Set pins for shift register to default value (low);
        digitalWrite(MOTORDATA, LOW);
        digitalWrite(MOTORLATCH, LOW);
        digitalWrite(MOTORCLK, LOW);
        // Enable the shift register, set Enable pin Low.
        digitalWrite(MOTORENABLE, LOW);
        // start with all outputs (of the shift register) low
        latch_copy = 0;
        shift_register_initialized = true;
      }
      bitWrite(latch_copy, output, high_low);
      shiftOut(MOTORDATA, MOTORCLK, MSBFIRST, latch_copy);
      delayMicroseconds(5);    // For safety, not really needed.
      digitalWrite(MOTORLATCH, HIGH);
      delayMicroseconds(5);    // For safety, not really needed.
      digitalWrite(MOTORLATCH, LOW);
    }
</code></pre>
",2016-08-02 02:08:00,,468,2,0,2,,3140515.0,,12/27/2013 20:10,2.0,38721025.0,"<p>It looks like the reference you give could be the problem. To try that fix just find the RH_ASK.cpp file and uncomment line 16 like this</p>

<pre><code>// RH_ASK on Arduino uses Timer 1 to generate interrupts 8 times per bit interval
// Define RH_ASK_ARDUINO_USE_TIMER2 if you want to use Timer 2 instead of Timer 1 on Arduino
// You may need this to work around other libraries that insist on using timer 1
#define RH_ASK_ARDUINO_USE_TIMER2
</code></pre>
",3604898.0,0.0,1.0,,
2440,35200665,Using OpenCV on raspberry pi for vision tracking FRC,|java|python|opencv|raspberry-pi|robotics|,"<p>I'm a senior in high school currently a programmer for the robotics team.  This year we plan on doing some vision processing/tracking to automatically find the goal and align ourselves with the goal.  We use java to program our robot and are apart of the FRC (First Robotics Competition).  We're having some trouble with the standard way of getting vision tracking to work, using RoboRealm, and I had a thought to use a Raspberry Pi as a co-processor solely for vision tracking purposes.  I've done a little bit of research as to what to use and it appears OpenCV is the best.  I have little experience in coding on the Raspberry Pi, but a basic understanding of python.  I was thinking of having the raspberry pi do all the tracking of the goal (has retro-reflective tape along the outer edge of the goal), and somehow sending that signal (through roborio -- on-board FRC standard processor) and to my java code, which will then tell our robot to either turn more left or more right depending on how far off from the target we are.  I'm just curious if this is in the realm of do-ability for a beginner programmer such as myself.  Any feedback would be great! </p>

<p>Thanks!</p>
",2016-02-04 12:10:00,35212584.0,3003,1,0,1,,3259325.0,New Jersey,2/1/2014 0:31,36.0,35212584.0,"<p>Everything you've said sounds very doable using <a href=""http://docs.opencv.org/master/dd/d49/tutorial_py_contour_features.html#gsc.tab=0"" rel=""nofollow"">contour features</a> You can use a bounding rectangle/circle,etc to extract center of mass (COM) coordinates of your goal. At this point you can do a simple thresholding like you said, if the COM is to the left, move left and vice versa.</p>

<p>The biggest issue would be reliably locating the goal, if you've never done CV before its easy to underestimate the difficulty of this task. My advice is to try to make the goal as apparent as possible. Since its reflective perhaps you can illuminate it to make it stand out more? Maybe shine IR(infra-red) from the robot and use an IR filter on the camera. You could also do this with any regular light in the visible spectrum. Once you've created adequate contrast between the goal and the background you could do simple <a href=""http://docs.opencv.org/master/d7/d4d/tutorial_py_thresholding.html#gsc.tab=0"" rel=""nofollow"">thresholding</a> or possibly do <a href=""http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_imgproc/py_template_matching/py_template_matching.html"" rel=""nofollow"">template matching</a> (though much slower, and it won't work if the goal is at an angle or skewed). </p>

<p>I hope I've given you some ideas, good luck.</p>

<p><strong>EDIT</strong><br>
In your comment you mentioned your target is green which can simplify your problem. I'm not sure how much you know about CV, but images come in RGB format. Each pixel has a red, green, and blue component. If you are looking for green it might be nice to <a href=""http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_core/py_basic_ops/py_basic_ops.html#splitting-and-merging-image-channels"" rel=""nofollow"">split the colors</a> and only the green channel of the image for <a href=""http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_imgproc/py_thresholding/py_thresholding.html#thresholding"" rel=""nofollow"">thresholding</a> THe open CV site has GREAT <a href=""http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_tutorials.html"" rel=""nofollow"">tutorials</a> for getting started. I would highly recommend you (and anyone else on your team) take a look at this. I'd recommend you read:</p>

<ol start=""2"">
<li>Gui Features in OpenCV<br>
a. Images<br>
b. videos</li>
<li>Core operations - Basic Operations on Images</li>
<li>Image Processing (this is the big one)<br>
c. Image Thresholding<br>
d. Smoothing (almost every image in every cv algorithm is smooth during pre processing)<br>
e. Morphological Transformations (may help clean the image after thresholding)<br>
i. Contours (this is where you get coordinates)  </li>
</ol>

<p>another tip is during algorithm development work with still images. Take a few different images of your goal from a perspective the robot would likely encounter. Do all your testing and development on those. Once you have a high confidence level then move to video. Even then I would start with <em>offline</em>
 video (a recording you capture, not real time). If you find issues its easy to reproduce them (just go back to that troublesome time-stamp in the vid and tweak your algorithm). Then finally do it with <em>online</em> (real-time) video.</p>

<p>Last piece of advice, even if your ultimate goal is to run on the RPi feel free to test your CV algorithm on any computer you have. If you use a laptop most of the time, throw opencv on that, the main difference in porting to Rpi will be the way you address the RPi camera module. But if you are still at early stages using stills and offline video this won't make a difference. But that's just my opinion, I know I myself have trouble dragging out my Pi to code when I'm on my windows laptop all day. I'm far more likely to work on code using my everyday PC.</p>
",2705382.0,1.0,2.0,,
2594,40748032,Jinput Poll Data Never Changes,|java|input|controller|robotics|jinput|,"<p>I am attempting to create a simple test program to get familiar with the JInput Library for another project. I have tested my controller with the all of the provided test classes and it works as expected. However, when I attempt to poll the controller, all values remain unchanged regardless of my input. Here is the code I am working with:</p>

<p><code>
public class ControllerTest {</p>

<pre><code>public static void main(String[] args){
    //System.out.println(""Hello World"");

    Controller[] ca = ControllerEnvironment.getDefaultEnvironment().getControllers();
    Controller gamepad = null;
    Component[] components = null;
    EventQueue eventQueue;
    // Run through the list of available input devices and get the gamepad
    for(int i = 0; i &lt; ca.length; i ++){
        if(ca[i].getType().equals(Controller.Type.GAMEPAD)){
            gamepad = ca[i];
        }
    }
    // Print the name of the controller and its type
    if(gamepad != null){
        System.out.println(gamepad.getName() + "": "" + gamepad.getType());
        components = gamepad.getComponents();
        System.out.println(""\tComponents:"");
        for(int i = 0; i &lt; components.length; i ++){
            System.out.println(""\t\tComponent #"" + i + "": "" + components[i].getName() + ""\n\t\t\tIs Relative: ""
                    + components[i].isRelative());
        }
    }
    else{
        System.out.println(""No gamepad connected"");
    }

    while (true){
        // If we have no gamepad connected, exit
        if(gamepad == null){
            System.out.println(""No Gamepad detected, exiting..."");
            System.exit(0);
        }
        // Poll controller
        gamepad.poll();
        Component[] comp = gamepad.getComponents();

        for(int i = 0; i &lt; comp.length; i ++){
            StringBuffer buffer = new StringBuffer();
            buffer.append(comp[i].getName());
            buffer.append("", Value: "" + comp[i].getPollData());
            System.out.println(buffer.toString());
        }
        try{
            Thread.sleep(20);  // Sleep before polling again
        }
        catch(InterruptedException e){
            e.printStackTrace();
        }
    }
}
</code></pre>

<p>}
</code></p>

<p>I have been trying to find an answer online, but this library is not very well documented and seems to usually be wrapped in other libraries specific for making games. (The aforementioned project is robotic in nature) </p>
",2016-11-22 17:25:00,,479,2,0,0,,3033582.0,"Somewhere, beyond the sea.",11/25/2013 18:37,18.0,42728931.0,"<p>You have to use an EventQueue </p>

<pre><code>player.poll();
        EventQueue queue = player.getEventQueue();
        Event event = new Event();
        while (queue.getNextEvent(event)) {
            Component comp = event.getComponent();
            if (comp.getIdentifier() == Component.Identifier.Button._6){
                if (comp.getPollData() == 1){
                    example
                }
</code></pre>
",7553140.0,-1.0,1.0,,
2589,40419901,What is difference of occupancy grid map and elevation map,|navigation|robotics|elevation|,"<p>I want to use occupancy grid  map or elevation map for navigation. what is difference of occupancy grid  map and elevation map in navigation?
thanks</p>
",2016-11-04 10:05:00,,390,1,0,0,,6812176.0,,9/9/2016 5:58,29.0,40445523.0,"<p>Create a map of an environment around you [Say a 2D array where the dimensions are your position in x and y] - Lets call this a grid for this concept. This grid becomes a  binary occupancy grid map, if you denote the ""value"" of each point (i.e an xy coordinate) to be 1 if it is occupied, and 0 if it is not.
These are useful to model obstacles to avoid collisions.
We can make denser and more detailed occupancy maps by adding different features and representations of what we wish to map.</p>

<p>It becomes an elevation map, when the ""value"" that we spoke about above, indicates the height of the terrain or environment around you.</p>
",5477866.0,0.0,0.0,,
2442,35330232,How to program ESC to increase/decrease PWM in increments,|arduino|robotics|pwm|,"<p>I've coded with Python before however, I am in the process of learning C and from what I have been told Arduino is quite similar to C in some aspects (at least with coding). I noticed that my when I run the code on my robot it jolts due to the quick changes in PWM. So I'd like some guidance as to how to do an if statement on Arduino because I am trying to increase/decrease the PWM in increments.</p>

<pre><code>//On Roboclaw set switch 1 and 6 on. // &lt;-- what does this refer to?
//mode 2 option 4 // &lt;-- my note based on user manual pg 26


#include &lt;Servo.h&gt; 


Servo myservo1;  // create servo object to control a Roboclaw channel
Servo myservo2;  // create servo object to control a Roboclaw channel

//int pos = 0;    // variable to store the servo position  //&lt;-- left-over from arduino ide servo sweep example?

void setup() 
{ 
  myservo1.attach(9);  // attaches the RC signal on pin 5 to the servo object (Left Motor)
  myservo2.attach(11);  // attaches the RC signal on pin 6 to the servo object (Right Motor)
} 


void loop() 
{ 
  //forward
  myservo1.writeMicroseconds(1000);
  myservo2.writeMicroseconds(1000);
  delay(2000);

  //backward
  myservo1.writeMicroseconds(2000);
  myservo2.writeMicroseconds(2000);
  delay(2000);

  //left
  myservo1.writeMicroseconds(1500);
  myservo2.writeMicroseconds(1000);
  delay(2000);

  //right
  myservo1.writeMicroseconds(1000);
  myservo2.writeMicroseconds(1500);
  delay(2000);

}
</code></pre>
",2016-02-11 03:24:00,,764,1,1,0,,5412716.0,,10/6/2015 6:23,27.0,35337814.0,"<p>Ok, whenever you write a different value to the servo it will move to that position as fast as possible. So you will need to move your servo step by step.</p>

<p>For this task, however, you will not be able to use delays, since they block the processor. You will need to mimic the ""blink with no delay"" example (i.e. using the millis() function to do something when time passes by.</p>

<p>The acceleration control simply moves 1 every X milliseconds (in this case 6ms, which makes a full movement - 180 - last for about one second). Every X milliseconds, so, the uC moves 1 in the direction of a target position. In the other code you should just set the target to your desired position and you are done.</p>

<p>Here is the code I wrote. Let me know if it works for you</p>

<pre><code>#include &lt;Servo.h&gt; 

Servo myservo1;
Servo myservo2;
unsigned long prevMillisAccel = 0, prevMillisAction = 0;
uint8_t servo1_target;
uint8_t servo2_target;
uint8_t currAction;

#define TIME_FOR_ONE_DEGREE_MS 6

void setup()
{
    myservo1.attach(9)
    myservo2.attach(11);
    prevMillisAccel = millis();
    prevMillisAction = millis();
    servo1_target = 0;
    servo2_target = 0;
    myservo1.write(0);
    myservo2.write(0);
    currAction = 0;
}

void moveServoTowardsTarget(Servo *servo, uint8_t target)
{
    uint8_t pos = servo-&gt;read();
    if (pos &gt; target)
        servo-&gt;write(pos-1);
    else if (pos &lt; target)
        servo-&gt;write(pos+1);
}

void loop()
{
    unsigned long currMillis = millis();
    if (currMillis - prevMillisAccel &gt;= TIME_FOR_ONE_DEGREE_MS)
    {
        prevMillisAccel += TIME_FOR_ONE_DEGREE_MS;

        moveServoTowardsTarget(&amp;myservo1, servo1_target);
        moveServoTowardsTarget(&amp;myservo2, servo2_target);
    }

    if (currMillis - prevMillisAction &gt;= 2000)
    {
        prevMillisAction += 2000;
        currAction = (currAction + 1) % 4;
        switch(currAction)
        {
            case 0: // forward
                servo1_target = 0;
                servo2_target = 0;
                break;
            case 1: // backward
                servo1_target = 180;
                servo2_target = 180;
                break;
            case 2: // left
                servo1_target = 90;
                servo2_target = 0;
                break;
            case 3: // right
                servo1_target = 0;
                servo2_target = 90;
                break;
        }
    }
}
</code></pre>

<p>PS. I used the <code>write</code> function instead of the writeMicroseconds because you can <code>read</code> the value you wrote. If you really need to use <code>writeMicroseconds</code> (which is pretty useless in my opinion, since servo precision is less than 1, so <code>write</code> is more than sufficient) just store the target as a <code>uint16_t</code> and store also the last set value (and use that instead of the <code>read</code> function)</p>
",3368201.0,0.0,0.0,58371171.0,The web is full of C/C++ tutorials on how to do If statements. So give it a shot and show the code if it doesn't work and let us see what we can do about it.
2511,37007417,Inverse-Kinematics: How to calculate angles for servos of a robotic arm to reach all possible points in a canvas?,|trigonometry|robotics|inverse-kinematics|servo|,"<p>I have a robotic arm composed of 2 servo motors. I am trying to calculate inverse kinematics such that the arm is positioned in the middle of a canvas and can move to all possible points in both directions (left and right). This is an image of the system <a href=""https://i.stack.imgur.com/e5RX9.png"" rel=""nofollow"">Image</a>. The first servo moves  0-180 (Anti-clockwise). The second servo moves 0-180 (clockwise). </p>

<p>Here is my code:</p>

<pre><code>    int L1 = 170;
    int L2 = 230;
    Vector shoulderV;
    Vector targetV;
    shoulderV = new Vector(0,0);
    targetV = new Vector(0,400);


    Vector difference = Vector.Subtract(targetV, shoulderV);
    double L3 = difference.Length;
    if (L3 &gt; 400) { L3 = 400; }
    if (L3 &lt; 170) { L3 = 170; }

    // a + b is the equivelant of the shoulder angle
    double a = Math.Acos((L1 * L1 + L3 * L3 - L2 * L2) / (2 * L1 * L3));  
    double b = Math.Atan(difference.Y / difference.X);

   // S1 is the shoulder angle
   double S1 = a + b;
  // S2 is the elbow angle
  double S2 = Math.Acos((L1 * L1 + L2 * L2 - L3 * L3) / (2 * L1 * L2));

  int shoulderAngle = Convert.ToInt16(Math.Round(S1 * 180 / Math.PI));
  if (shoulderAngle &lt; 0) { shoulderAngle = 180 - shoulderAngle; }
  if (shoulderAngle &gt; 180) { shoulderAngle = 180; }
  int elbowAngle = Convert.ToInt16(Math.Round(S2 * 180 / Math.PI));

  elbowAngle = 180 - elbowAngle; 
</code></pre>

<p>Initially, when the system is first started, the arm is straightened with shoulder=90, elbow =0.
When I give positive x values I get correct results in the left side of the canvas. However, I want the arm to move in the right side as well. I do not get correct values when I enter negatives. What am I doing wrong? Do I need an extra servo to reach points in the right side?</p>

<p>Sorry if the explanation is not good. English is not my first language.</p>
",2016-05-03 14:49:00,,976,1,2,0,0.0,5082242.0,,7/5/2015 11:20,6.0,38584421.0,"<p>I suspect that you are losing a sign when you are using Math.Atan(). I don't know what programming language or environment this is, but try and see if you have something like this: </p>

<p>Instead of this line:</p>

<p><code>double b = Math.Atan(difference.Y / difference.X);</code></p>

<p>Use something like this: </p>

<p><code>double b = Math.Atan2(difference.Y, difference.X);</code></p>

<p>When <code>difference.Y</code> and <code>difference.X</code> have the same sign, dividing them results in a positive value. That prevents you from differentiating between the cases when they are both positive and both negative. In that case, you cannot differentiate between 30 and 210 degrees, for example. </p>
",679553.0,0.0,0.0,62953451.0,Have you tried using Denavit hartenberg convention to do the inverse kinematics calculation?
2527,37303953,Robotics Maze Representation in C,|c|robotics|,"<p>So I'd like to represent a rectangular maze of say dimensions 5x4 (rows x columns) using a 2D array in C language. However I am having trouble specifying what actually needs to be put into the 2D array.</p>

<pre><code>int a[5][4] = {
    {},
    {},
    {},
    {},
    {}, 
};
</code></pre>

<p>Here is the skeleton of the 2D array, in each row there will be 4 values, I assume that each of these values is a single integer that tells us the properties of a cell in the maze. My problem is, is that really enough? How does a single value tell a robot weather there are 3 walls, 2 walls etc</p>

<p>Someone please enlighten me D:</p>

<p><a href=""https://i.stack.imgur.com/EojXM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EojXM.png"" alt=""Maze""></a></p>
",2016-05-18 15:25:00,,146,2,7,2,,6262982.0,,4/27/2016 17:54,10.0,37304048.0,"<p>use specific bits for specific properties of the room</p>

<pre><code>#define ROOM_WALL_ABOVE (1 &lt;&lt; 0)
#define ROOM_WALL_LEFT  (1 &lt;&lt; 1)
#define ROOM_WALL_BELOW (1 &lt;&lt; 2)
#define ROOM_WALL_RIGHT (1 &lt;&lt; 3)
#define ROOM_DOOR       (1 &lt;&lt; 4)

int a[5][4] = {0};
a[0][0] = ROOM_WALL_ABOVE | ROOM_WALL_LEFT;

if (a[x][y] &amp; ROOM_WALL_RIGHT) printf(""Cannot walk right.\n"");
</code></pre>
",25324.0,7.0,5.0,62130226.0,"@EugeneSh. For sure, as pmg already answered."
2474,36738868,"Programming inverse kinematic solution provided only x,y,z of tool",|c++|robotics|inverse-kinematics|,"<p>I at moment trying to implement a inverse kinematic solution capable of finding all possible Q-states a robot can have given the position of the tool is x,y,z.. </p>

<p>I am choosen to do it using the least square approach, but something tells me that it won't provide all possible solution but just the one with smallest error, which in this case i am interested in all possible Q-states that fulfill the position of the tool. </p>

<p>My implementation looks as such. </p>

<pre><code>Eigen::MatrixXd jq(device_.get()-&gt;baseJend(state).e().cols(),device_.get()-&gt;baseJend(state).e().rows());
      jq = device_.get()-&gt;baseJend(state).e(); //Extract J(q) directly from robot


      //Least square solver - [AtA]1AtB

      Eigen::MatrixXd A (6,6);
      A = jq.transpose()*(jq*jq.transpose()).inverse();



      Eigen::VectorXd du(6);
      du(0) = 0.1 - t_tool_base.P().e()[0];
      du(1) = 0 - t_tool_base.P().e()[1];
      du(2) = 0 - t_tool_base.P().e()[2];
      du(3) = 0;  // Should these be set to something if i don't want the tool position to rotate?
      du(4) = 0;
      du(5) = 0;

      ROS_ERROR(""What you want!"");
      Eigen::VectorXd q(6);
      q = A*du;


      cout &lt;&lt; q &lt;&lt; endl; // Least square solution - want a vector of solutions. 
</code></pre>

<p>first of all the inverse kin doesn't seem right as the Q-state doesn't move the robot to the desired position. I can't seem to see where my implementation is wrong?</p>
",2016-04-20 09:05:00,36739131.0,762,1,7,0,,5585502.0,,11/20/2015 11:10,287.0,36739131.0,"<p>Solving the inverse kinematics problem with numerical solution is not the best option. You should go for that option just in case you could not find the analytical solution. BTW, well designed robots should have a clear inverse kinematics model.</p>

<p>For your robot, here is the inverse kinematics model: <a href=""https://smartech.gatech.edu/bitstream/handle/1853/50782/ur_kin_tech_report_1.pdf"" rel=""nofollow"">https://smartech.gatech.edu/bitstream/handle/1853/50782/ur_kin_tech_report_1.pdf</a></p>
",4523099.0,0.0,1.0,61060329.0,"Analytical solution will provide you with the all solution without any epsilon error. If you are using a well-known robot, you should find a ready model on the internet or even in the manual of the robot."
2546,37739705,BAM(Bidirectional Associative Memory),|neural-network|artificial-intelligence|robotics|recurrent-neural-network|biological-neural-network|,"<p>Suppose [1 0 0 1 0 1] &lt;--> [0 0 0 1], this is a association then in implementing BAM, why we convert 0 to -1 and then calculate weight matrix.</p>
",2016-06-10 04:05:00,,781,2,0,0,,4520795.0,"Bengaluru, India",2/2/2015 16:33,61.0,39056391.0,"<p>The fundamental reason why 0 are unsuitable for BAM storage is that 0's in binary patterns are ignored when added, but -1's in bipolar patterns are not: 1+0=1 but 1+(-1)=0. If the numbers are matrix entries that represent synaptic strengths, then multiplying and ass in binary quantities can only produce excitatory connections. </p>

<p>Meanwhile multiplying and add in bipolar quantities produces excitatory and inhibitory connections. The connections strengths represent the frequency of excitatory and inhibitory connections int the individual correlation matrices.</p>

<p>Refer:</p>

<ul>
<li><a href=""http://sipi.usc.edu/~kosko/BAM.pdf"" rel=""nofollow"">http://sipi.usc.edu/~kosko/BAM.pdf</a> ""Bidirectional Associative Memory, Bart Kosko""</li>
</ul>
",1600172.0,1.0,0.0,,
2439,34964928,CMUcam5 Pixy on Raspberry 2,|camera|raspbian|raspberry-pi2|robotics|,"<p>I just got my Raspberry B and a new CMUcam 5 from Pixy.
I follow this tutorial:
<a href=""http://cmucam.org/projects/cmucam5/wiki/Hooking_up_Pixy_to_a_Raspberry_Pi"" rel=""nofollow"">Hooking up Pixy to a Raspberry Pi</a></p>

<p>However, once I plug-in the camera to the RB the servos keep on doing some noise like trying to move even when they are at their top.
The LED light keeps on flashing.
The worst part is that for some reason the mouse and keyboard are no longer connected until I click camera button.
Once I have the keyboard back and I try to run the hello_pixy script I get an error:</p>

<pre><code>Hello Pixy:
libpixyusb: Version: 0.4
pixy_init(): USB ERROR: Target not found.
</code></pre>

<p>Am I missing something? Is something missing?
All suggestions are welcome</p>

<p>Thanks!</p>
",2016-01-23 14:55:00,34998166.0,532,1,1,0,,2011375.0,,1/25/2013 14:55,79.0,34998166.0,"<p>The issue is the power the servos need to move.
This was a hardware issue rather than a software one. 
Adding a simple powered-usb hub did the trick.</p>
",2011375.0,0.0,0.0,57724185.0,"Well, doing some test I found that the issue was not about the software but the hardware. Seems like the servos need extra power that is beyond the simple USB cable. I added a powered-uss hub and now is working just fine."
2556,38659146,How can I use gyro or encoders for robot moving in straight line?,|c++|raspberry-pi2|gyroscope|robotics|encoder|,"<p>I've recently succeeded in building my Autonomous robot with DC motors, and it works well. However, it doesn't move in a straight line yet, when it should. I'm now studying which method should I implement to make the robot go straight. I pretty much understand how to use the encoders, but I'm not sure about the gyro. I had written a program for straight motion using encoder, but it's not moving straight exactly because of front brush speed, for further improvement I have decided to use gyro, If I use gyro possible to make straight motion ? or else any suggestion ?</p>
",2016-07-29 12:40:00,,1112,1,1,0,,,,,,38661450.0,"<p>First, make sure you conceptually understand what it means for a robot to drive in a straight line. You will not get a robot that moves perfectly in a straight line. You may get one to move straight-enough (perhaps more perfect than humans can determine) but there will always be error.</p>

<p>A gyroscope may not be able to determine you're going off course if it's gradual enough and depending on the quality of your gyro it may have a buildup of drift, causing it to think it's turning ever-so-slightly when it's sitting perfectly still. The more expensive of a gyro you get, the less this will be, but still.</p>

<p>Even if you assume perfect sensors, there is still a difference between ""driving straight"" and ""keeping on the straight and narrow"". Imagine you had coded a robot to drive such that it tried to keep its bearing as consistent as possible (drive straight). If, while it was moving you knocked it a bit, it would swerve to the side some and then correct itself to the original angle. However, it would be on a different path. Sure, the path would be parallel to the original one, but it would not be the same path.</p>

<p>Then there's the option of having it try to figure out just how far off the beaten path it was pushed and try to get back on it. That'll either take a constant reference (as a line follower robot would do) or more sensors (like a 3D gyro and 3D accelerometer).</p>

<p>That second option sounds a bit more than what you're doing, so here's the first option done in no particular robotics framework:</p>

<pre><code>//initialize
double angle = gyro.get_heading_degrees();
//...
//logic code that may be looped or fired by events
{
  //this is our error; how far off we are from our target heading
  double error = angle - gyro.get_heading_degrees() - 360;

  drive_system.drive_arcade(error / 180, 1);
}
</code></pre>

<p>This assumes driving in an arcade fashion; you can adapt it to tank drive or swerve drive or mecanum drive or H-drive or...</p>

<p>The '1' is just a speed</p>

<p>The '-360' and '180' are values to reduce the angle to a value between -1 and 1. If your drive method uses a different range to determine angle, that'll have to be adapted.</p>

<p>Finally, this example isn't foolproof, but it should get you thinking about how to correct for errors when you've detected them.</p>
",5344725.0,0.0,0.0,64700281.0,Could you show anything that helps readers understand what you are trying to achieve and how?
2627,41411217,publishing trajectory_msgs/jointtrajectory msgs,|ros|robotics|,"<p>When i set the position and velocities of the joints in the trajectory msgs i got an error: \</p>

<pre><code>[state_publisher-2] process has died [pid 13362, exit code -11, cmd /home/rob/catkin_ws/devel/lib/r2d2/state_publisher __name:=state_publisher __log:=/home/rob/.ros/log/9980f352-cf74-11e6-8644-d4c9efe8bd37/state_publisher-2.log].
log file: /home/rob/.ros/log/9980f352-cf74-11e6-8644-d4c9efe8bd37/state_publisher-2*.log
</code></pre>

<p>My ros node to send geometry_msgs is:</p>

<pre><code>#include &lt;string&gt;
    #include &lt;ros/ros.h&gt;
    #include &lt;sensor_msgs/JointState.h&gt;
    #include &lt;tf/transform_broadcaster.h&gt;
    #include &lt;trajectory_msgs/JointTrajectory.h&gt;
    #include &lt;vector&gt;
    int main(int argc, char** argv) {
        ros::init(argc, argv, ""state_publisher"");
        ros::NodeHandle n;
        ros::Publisher joint_pub = n.advertise&lt;trajectory_ms

gs::JointTrajectory&gt;(""set_joint_trajectory"", 1);
   ros::Rate loop_rate(30);

   const double degree = M_PI/180;

   // robot state
   double tilt = 0, tinc = degree, swivel=0, angle=0, height=0, hinc=0.005;

   // message declarations
   trajectory_msgs::JointTrajectory joint_state;
   std::vector&lt;trajectory_msgs::JointTrajectoryPoint&gt; points_n(3);
   points_n[0].positions[0] = 1; points_n[0].velocities[0]=10;
   while (ros::ok()) {
       //update joint_state
       joint_state.header.stamp = ros::Time::now();
       joint_state.joint_names.resize(3);
       joint_state.points.resize(3);

       joint_state.joint_names[0] =""swivel"";
       joint_state.points[0] = points_n[0];
       joint_state.joint_names[1] =""tilt"";
       joint_state.points[1] = points_n[1];
       joint_state.joint_names[2] =""periscope"";
       joint_state.points[2] = points_n[2];


       joint_pub.publish(joint_state);



       // This will adjust as needed per iteration
       loop_rate.sleep();
   }


   return 0;
   }
</code></pre>

<p>Here when i donot set the position and velocity value it runs without error and when i run <code>rostopic echo /set_joint_trajectory</code> i can clearly see the outputs as all the parameters of points is 0. I also tried below program but it published nothing:</p>

<pre><code> #include &lt;string&gt;
    #include &lt;ros/ros.h&gt;
    #include &lt;sensor_msgs/JointState.h&gt;
    #include &lt;tf/transform_broadcaster.h&gt;
    #include &lt;trajectory_msgs/JointTrajectory.h&gt;
    #include &lt;vector&gt;
    int main(int argc, char** argv) {
        ros::init(argc, argv, ""state_publisher"");
        ros::NodeHandle n;
        ros::Publisher joint_pub = n.advertise&lt;trajectory_msgs::JointTrajectory&gt;(""set_joint_trajectory"", 1);

        trajectory_msgs::JointTrajectory joint_state;

           joint_state.header.stamp = ros::Time::now();
           joint_state.header.frame_id = ""camera_link"";
           joint_state.joint_names.resize(3);
           joint_state.points.resize(3);

           joint_state.joint_names[0] =""swivel"";
           joint_state.joint_names[1] =""tilt"";
           joint_state.joint_names[2] =""periscope"";

           size_t size = 2;
           for(size_t i=0;i&lt;=size;i++) {
              trajectory_msgs::JointTrajectoryPoint points_n;
              int j = i%3;
              points_n.positions.push_back(j);
              points_n.positions.push_back(j+1);
              points_n.positions.push_back(j*2);
              joint_state.points.push_back(points_n);
              joint_state.points[i].time_from_start = ros::Duration(0.01);
           }
           joint_pub.publish(joint_state);
           ros::spinOnce();
       return 0;
   }
</code></pre>
",2016-12-31 19:35:00,41413111.0,3654,1,0,0,,,,,,41413111.0,"<p>You are accessing <code>points_n[0].positions[0]</code> and <code>points_n[0].velocities[0]</code> without allocating the memory for positions and velocities. Use</p>

<pre><code>...
// message declarations
trajectory_msgs::JointTrajectory joint_state;
std::vector&lt;trajectory_msgs::JointTrajectoryPoint&gt; points_n(3);
points_n[0].positions.resize(1);
points_n[0].velocities.resize(1);
...
</code></pre>

<p>then set the values or use <code>points_n[0].positions.push_back(...)</code> instead. The same applies to <code>points_n[1]</code> and <code>points_n[2]</code>.</p>

<p>In your second example it looks like your program terminates before anything is sent. Try to publish repeatedly in a while-loop with </p>

<pre><code>while(ros::ok()){ 
  ...
  ros::spinOnce();
}
</code></pre>
",1563315.0,0.0,0.0,,
2580,39838761,Robotics library in Forth?,|robotics|forth|,"<p>I have read the documentation for the Roboforth environment from <a href=""http://www.strobotics.com/"" rel=""nofollow"">STrobotics</a> and recognized that this a nice way for programming a robot. What I missed is a sophisticated software library with predefined motion primitives. For example, for picking up a object, for regrasping or for changing a tool.</p>

<p>In other programming languages like Python or C++, a library is a convenient way for programming repetitive tasks and for storing expert knowledge into machine-readable files. Also a library is good way for not-so-talented programmers to get access on higher-level-functions. In my opinion Forth is the perfect language for implementing such an API, but I didn't find information about it. Where should I search? Are there any examples out there?</p>
",2016-10-03 19:10:00,39915796.0,333,1,0,1,,6904362.0,,9/30/2016 11:11,301.0,39915796.0,"<p>I am author of RoboForth, and you make a good point. I have approached the problem of starting off new users with videos on YouTube; see <a href=""https://www.youtube.com/playlist?list=PLBcXpOAbOrG3pnlRlJDISQmWOVJP9xX3L"" rel=""nofollow"">How to...</a> (playlist with 6 items, e.g ""ST Robotics How-to number 1 - getting started"") which is a playlist covering basics and indeed tool changing.</p>

<p>I never wrote any starter programs, because the physical positions (coordinates) would be different from one user to the next, however I think it can be done, and I will do it. Thanks for the heads up.</p>
",6936882.0,4.0,4.0,,
2553,38101097,Disjoint Movement of Joints of Aldebaran Nao,|c++|robotics|nao-robot|servo|,"<p>I am working on a locomotion system for the Aldebaran Nao.  I noticed that my robot's motions are very disjoint as compared to those on other robots - a problem that I am pretty sure is code related.</p>

<p>I am updating my robots motions using code similar to Aldebaran's fast get set DCM.</p>

<p>(<a href=""http://doc.aldebaran.com/1-14/dev/cpp/examples/sensors/fastgetsetdcm/fastgetsetexample.html"" rel=""nofollow"">http://doc.aldebaran.com/1-14/dev/cpp/examples/sensors/fastgetsetdcm/fastgetsetexample.html</a>).</p>

<p>I am updating the joint angles every 10 ms (the fastest possible update rate).  However, it is clear that the motors move to the newly commanded angle very quickly and are motionless for the majority of the 10 ms.  Is there any way to control the velocity of the motors during this 10ms update period?</p>
",2016-06-29 13:30:00,,136,1,1,-1,,5428956.0,,10/9/2015 18:58,10.0,38392539.0,"<p>There's many way to send order to joints using DCM or ALMotion.</p>

<p>The easiest way is using ALMotion, with that you can use </p>

<ul>
<li>angleInterpolationWithSpeed, where you'll specify a ratio of speed.</li>
<li>angleInterpolation, where you'll specify a time. In this example 0.01 sec.</li>
</ul>

<p>Using the DCM, you just have to ask for the movement to finish 10ms later.</p>

<p>The ALMotion and DCM documentation are well wrotten, you should have a look...</p>
",1081418.0,1.0,0.0,94966882.0,"SO won't let me comment so I have to use ""answer"". You can't directly control the current inflow of Nao's motors. In my experience 10 ms is a very fine time step already - 100 frames of motion per second and should not appear ""disjoint"" to naked eyes given that your entire motion is smooth. Could you elaborate a little on what you are trying to achieve? Did you interpolate / smooth the actuator positions with respect to time?"
2577,39719830,Public Key is not available,|raspberry-pi|raspbian|ros|robotics|,"<p>I am trying to install ROS Kinetic on the Raspberry Pi 3 Model B running Raspbian GNU7Linux 8 (Jessie) following these <a href=""http://wiki.ros.org/ROSberryPi/Installing%20ROS%20Kinetic%20on%20the%20Raspberry%20Pi"" rel=""nofollow"">steps</a>.</p>

<p>Setting up the repositories I get this output:</p>

<pre><code>Executing: gpg --ignore-time-conflict --no-options --no-default-keyring --homedir /tmp/tmp.vAO4o1tMMY --no-auto-check-trustdb --trust-model always --keyring /etc/apt/trusted.gpg --primary-keyring /etc/apt/trusted.gpg --keyserver hkp://ha.pool.sks-keyservers.net:80 --recv-key 0xB01FA11
</code></pre>

<p>And when trying to run a sudo apt-get update I get this error:</p>

<pre><code>W: GPG error: http://packages.ros.org jessie InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 5523BAEEB01FA116
</code></pre>

<p>Anyone had this problem adding a public key?</p>
",2016-09-27 08:31:00,,6019,2,0,4,0.0,6886568.0,Berlin,9/27/2016 8:16,79.0,39721706.0,"<p>Solved it.</p>

<p>This manually adds the key:</p>

<pre><code>sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys &lt;key number&gt;    

sudo apt-get update
</code></pre>
",6886568.0,8.0,0.0,,
2507,36925245,Exporting Nao robot Simulator in Unity3D,|unity-game-engine|robot|,"<p>I'm using the Coregraphe 2.1.4 software as simulator, I'd like to export this simulator into Unity 3D in order to make a videogame where people can interact and play with Nao.</p>

<p>I've successfully exported the model of the Robot in Unity but i can not animate it.</p>

<p>Is it possible to export the Nao robot simulator in Unity3D? Or, are there other way to run the simulator in Unity3D?</p>

<p>Thank :)</p>
",2016-04-28 20:53:00,36925445.0,624,1,1,1,,6268708.0,,4/28/2016 20:39,14.0,36925445.0,"<blockquote>
  <p>I've successfully exported the model of the Robot in Unity but i can
  not animate it.</p>
</blockquote>

<p>Export it to a 3D software such as Maya or Blender, then animate it there and import the 3D model and the animation back to Unity. Blender is free. You just need to learn animation. </p>

<blockquote>
  <p>Is it possible to export the Nao robot simulator in Unity3D?</p>
</blockquote>

<p>No. You can't export a software to Unity. If the company that made it produced a DLL plugin for Unity then you can. They don't have plugin for Unity, so you can't.</p>

<blockquote>
  <p>are there other way to run the simulator in Unity3D?</p>
</blockquote>

<p>No. All the answer to your questions are No, No and No. Although, you can animate your 3D model in a 3D software then import into Unity.</p>
",3785314.0,0.0,13.0,61415836.0,"To be honest it sounds like you've stolen the ""Nao"" 3D model from their software! That's just a commercial product isn't it? It's not in the slightest available for open source, or anything like that - unless I'm mistaken."
2788,44123750,How can I calculate pitch value with gyroscope in MPU9250 of Navio2 (python2),|python|raspberry-pi|sensors|gyroscope|robotics|,"<p>I can obtain raw values of gyro sensor.</p>

<p>But, I don't know how to calculate the pitch value with these values.</p>

<p>I want to get pitch value between -180 and +180.</p>

<p>This is because I want to obtain a precise pitch value using a complementary filter with a pitch value obtained with an accelerometer and a pitch value obtained with a gyroscope.</p>

<p>Please help me.</p>

<hr>

<p><strong>My Code (obtain raw values of gyro sensor)</strong></p>

<pre><code>import spidev
import time
import argparse
import sys
import navio.mpu9250
import navio.util

navio.util.check_apm()

imu = navio.mpu9250.MPU9250()
imu.initialize()

m9a, m9g, m9m = acc.imu.getMotion9()

#These are raw values of gyroscope
gyro_x = m9g[0]
gyro_y = m9g[1]
gyro_z = m9g[2]
</code></pre>

<hr>

<p><strong>Detail(setting)</strong></p>

<p><em>gyro_scale = 2000DPS</em></p>

<p><em>gyro_divider = 16.4</em></p>

<p>'raw values of gyro sensor' = (PI/180)*data/gyro_divider</p>

<p><a href=""https://i.stack.imgur.com/IfzqR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IfzqR.png"" alt=""enter image description here""></a></p>

<hr>

<p><a href=""https://github.com/emlid/Navio2/blob/master/Python/navio/mpu9250.py#L350"" rel=""nofollow noreferrer""><b>Reference code.</b></a></p>

<pre><code>...

def getMotion9(self):
    self.read_all()
    m9a = self.accelerometer_data
    m9g = self.gyroscope_data
    m9m = self.magnetometer_data

    return m9a, m9g, m9m

...

def read_all(self):
    # Send I2C command at first
    # Set the I2C slave addres of AK8963 and set for read.
    self.WriteReg(self.__MPUREG_I2C_SLV0_ADDR, self.__AK8963_I2C_ADDR | self.__READ_FLAG)
    # I2C slave 0 register address from where ; //Read 7 bytes from the magnetometerto begin data transfer
    self.WriteReg(self.__MPUREG_I2C_SLV0_REG, self.__AK8963_HXL)
    # Read 7 bytes from the magnetometer
    self.WriteReg(self.__MPUREG_I2C_SLV0_CTRL, 0x87)
    # must start your read from AK8963A register 0x03 and read seven bytes so that upon read of ST2 register 0x09 the AK8963A will unlatch the data registers for the next measurement.

    # time.sleep(0.001)
    response = self.ReadRegs(self.__MPUREG_ACCEL_XOUT_H, 21);

    # Get Accelerometer values
    for i in range(0, 3):
        data = self.byte_to_float(response[i*2:i*2+2])
        self.accelerometer_data[i] = self.G_SI*data/self.acc_divider

    # Get temperature
    i = 3
    temp = self.byte_to_float(response[i*2:i*2+2])
    self.temperature = (temp/340.0)+36.53

    # Get gyroscope values
    for i in range(4, 7):
        data = self.byte_to_float(response[i*2:i*2+2])
        self.gyroscope_data[i-4] =(self.PI/180)*data/self.gyro_divider

    # Get magnetometer values
    for i in range(7, 10):
        data = self.byte_to_float_le(response[i*2:i*2+2])
        self.magnetometer_data[i-7] = data * self.magnetometer_ASA[i-7]
</code></pre>
",2017-05-22 23:57:00,,1307,0,3,0,0.0,6694663.0,,8/9/2016 8:40,11.0,,,,,,75271788.0,"Does that mean that the angle at which sampling starts should be zero degrees?
what if the actual sampling start angle is not 0 C ?"
2634,41604310,What is wpilibj 2017 replacement method for initDigitalPort?,|java|robotics|,"<p>I've come into a little problem with some legacy code from the 2016 control system. I'm trying to control the adis16448 board with <a href=""https://github.com/juchong/ADIS16448-RoboRIO-Driver/blob/master/Java/com/analog/adis16448/frc/ADIS16448_IMU.java"" rel=""nofollow noreferrer"">this library</a>
which compiled fine in the 2016 wpilibj, but doesn't compile in the 2017 version. Now, I'd like to get this up and running quickly without having to wait for the dev to update, and there are actually only two errors.
Relevant code here:</p>

<pre><code>private static class InterruptSource extends DigitalSource {
    public InterruptSource(int channel) {
      initDigitalPort(channel, true);
    }
}
</code></pre>

<p>First is that the <code>InterruptSource</code> class has some unimplemented methods from the parent class. I just added empty definitions for these and that error obviously went away. Next is that the method <code>initDigitalPort</code> is not defined from the parent class. This is the part that I get stuck on. </p>

<p>Upon examination of the API Javadoc, the Source Code on github, and the context of this code, I still can't seem to figure out what this does or how to fix it. I'm guessing this has been depreciated in the 2017 wpilibj library. </p>

<p>My question is, what is the replacement method for initDigitalPort? </p>

<p>Forgive me for anything simple I've overlooked, we are a new FRC team so we have 0 experience with using wpilibj. </p>

<p>Also, it might help if I understood what the DigitalSource class actually does, it seems to involve encoders but that can't be right since this board has none. Could someone explain this to me?</p>

<p>Thanks, help is greatly apreciated!</p>
",2017-01-12 02:47:00,41819137.0,43,2,0,0,,4484072.0,"Orange County, CA, United States",1/22/2015 19:42,725.0,41819137.0,"<p>The library in question has now been updated as of <a href=""https://github.com/juchong/ADIS16448-RoboRIO-Driver/commit/077bb65af2c325d30e105bc4ffdc3d79322e8786"" rel=""nofollow noreferrer"">this commit</a>. The new class is called <code>DigitalInput</code> and the <code>initDigitalPort</code> method is called in the constructor of this class which is given the parameter for the port. </p>

<p><strong>Ex:</strong></p>

<pre><code>public InterruptSource(int channel) {
       initDigitalPort(channel, true);
}
</code></pre>

<p>can be subsituted with </p>

<pre><code>DigitalInput m_interrupt = new DigitalInput(10)
</code></pre>

<p>and will provide nearly the same functionality including class structure and methods. </p>
",4484072.0,0.0,0.0,,
2763,43416538,How to increase the speed of transfer of data from android to arduino in bluetooth?,|android|bluetooth|arduino|robotics|arduino-ide|,"<p>I'm trying to use an android app to do the processing of a path finding algorithm for a robot using Bluetooth. But currently, it takes 1 or 2 seconds for the transfer to complete, so that there is an output in the Arduino. Is there a way to minimise this to make the transfer-output instant?</p>

<p>This kind of delay is causing problems such as stopping instantly when an obstacle is detected. Is there any better way of doing this?
Thanks in advance!</p>
",2017-04-14 17:44:00,44488390.0,1677,3,0,0,,6552687.0,,7/5/2016 16:40,29.0,43624927.0,"<p>Simple answer: You can't, bluetooth is laggy like that. If you instead had your path finding algorithm on the arduino board itself, you could avoid the issue. You can also try adding a delay to your arduino code, because it is possible that the arduino is sending messages repeatedly without taking into account the lag that bluetooth has.</p>
",7701649.0,0.0,4.0,,
2731,42674680,I need to run parallel functions that keep track of variables that are in a buffer.,|python|class|robotics|,"<p>Here's my dilemma. I've developed some code I want to modularize but I'm stuck on what should not be a difficult issue.</p>

<p>My code first does something like this:</p>

<pre><code>While True:
     value = getsensorvalue()
     values = deque(maxlen=10)

     # Get the mode value of the sensor readings
     Values = np.array(list(values))
     u, indices = np.unique(Values , return_inverse=True)
     mode = u[np.argmax(np.bincount(indices))]
     print mode      
</code></pre>

<p>Basically, I'm reading a sensor value continuously, I'm storing the values in a doubly linked list (import deque from collections). This is a simplified version of my program, but basically, it's returning the mode value of the queue we formed from our sensor readings.</p>

<p>What I want to do is create this into a class or function that can be used to read multiple sensors. So something like this:</p>

<pre><code>While True:
    GetSensorA()
    GetSensorB()
    GetSensorC()
    GetSensorD()
</code></pre>

<p>With all of the above having different queue lengths (buffer sizes) etc. </p>

<p>This has to be quite simple. I'm just a scientists who's self taught in programming. Not sure but I feel as if there is some basic programming principle I'm missing here.</p>
",2017-03-08 14:58:00,,32,0,4,0,,6432363.0,,6/6/2016 20:53,18.0,,,,,,72480466.0,"Sure, my apologies i should have elaborated."
2756,43223123,Kalman Filter Prediction Implementation,|algorithm|prediction|robotics|kalman-filter|,"<p>I am trying to implement a Kalman filter in order to localize a robot.
I am confused with the prediction step (excluding process noise) x = Fx + u</p>

<p>If x is a state estimation vector: [xLocation, xVelocity] and F is the state transition matrix [[1 1],[0 1]], then the new xLocation would be equal to xLocation + xVelocity + the corresponding component of the motion vector u.</p>

<p>Why is the equation not x = x + u?  Shouldn't the predicted location of the robot be the location + motion of the robot?</p>
",2017-04-05 06:10:00,43223710.0,1292,1,3,0,0.0,7818382.0,"Cedar Park, TX, United States",4/5/2017 5:43,5.0,43223710.0,"<p>Maybe there is some confusion with respect to what the matrices actually represent.</p>

<p>The ""control vector"", <em>u</em>, might be the acceleration externally applied to the system.</p>

<p>In this case, I would expect the equations to look like this:</p>

<p>x<sub>location</sub> = x<sub>location</sub> + x<sub>velocity</sub></p>

<p>x<sub>velocity</sub> = x<sub>velocity</sub> + u<sub>velocity</sub></p>

<p>These two equations assume that the update is applied every 1 second (otherwise some ""delta time"" factors would need to be applied and included the transition matrix and the control vector).</p>

<p>For the situation mentioned above, the matrices and vectors are:</p>

<ul>
<li><p>The state vector (column vector with 2 entries):</p>

<p>x<sub>location</sub></p>

<p>x<sub>velocity</sub></p></li>
</ul>

<hr>

<ul>
<li><p>The transition matrix (2 x 2 matrix):</p>

<p>1 1</p>

<p>0 1</p></li>
</ul>

<hr>

<ul>
<li><p>The control vector (column vector with 2 entries):</p>

<p>0</p>

<p>u<sub>velocity</sub></p></li>
</ul>

<hr>

<p><a href=""http://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/"" rel=""nofollow noreferrer"">This link</a> contains nice explanations and visualizations for the Kalman Filter.</p>
",6184684.0,1.0,0.0,73517391.0,"I think this question is better suited for http://math.stackexchange.com/ as you don't seem to have a problem implementing this algorithm, but you have a problem understanding the algorithm."
2968,47727674,How to connect keyboard arrows with robotino sim and view?,|c#|mqtt|robotics|,"<p>So I have a project to do, which is to connect a Robotino view with a Robotino Sim, and also connect to a C# client program to <strong>move the Robotino in the View with the arrows on the keyboard</strong>. </p>

<p>The C# client is supposed to send the data about pressing the arrow buttons 4 times per second through cloud MQTT.</p>

<p>So far I connected the View with the Sim, added some basic navigator ( on which you click and the robot moves), but I have no idea how to do the rest.</p>

<p>I also found this cloud MQTT .Net thingy, but it just doesn't speak to me.
<a href=""https://www.cloudmqtt.com/docs-dotnet.html"" rel=""nofollow noreferrer"">https://www.cloudmqtt.com/docs-dotnet.html</a></p>

<p>I would really appreciate if someone could help me with this issue, since I found nothing really useful, only this video (basically that is what I want to do ), which has no description but the project is working for him.
<a href=""https://www.youtube.com/watch?v=gmKyFNLWv_A&amp;t=11s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=gmKyFNLWv_A&amp;t=11s</a></p>
",2017-12-09 10:40:00,,116,0,2,0,,8934727.0,"Dunajvros, Hungary",11/13/2017 19:06,6.0,,,,,,82437476.0,"Stack Overflow doesn't work this way. You have to try yourself, then when you get stuck, you show us what you have done, explain how it doesn't work and somebody will help you fix it."
2787,44098251,How to get closest point on cubic spline for Robotics project?,|python|matplotlib|scipy|robotics|cubic-spline|,"<p>I've got a robot and I want to make this robot follow a predefined path in the form of an eight figure on a field outside. The robot is not easy to steer and there are some external factors such as wind which make it very likely that the robot will not follow the exact path so it will often be next to the path. </p>

<p>The path I want to make it follow was formed by defining five points and create a line over those points using a cubic spline (the code I used to define this path is below this message):</p>

<p><a href=""https://i.stack.imgur.com/G4is2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G4is2.png"" alt=""At any given moment I want to be able""></a> </p>

<p>I always want to be able to supply the robot a point to steer to which is on the cubic spline line. To be able to do this I thought the easiest way is to:</p>

<ol>
<li>Calculate the nearest point on the cubic spline from the current location of the robot</li>
<li>Advance <code>0.2</code> units along the cubic spline line to determine the new waypoint for the robot to aim at.</li>
</ol>

<p>For example, if the location of the robot in the grid above is <code>x=0.4, y=-0.5</code>, the nearest point is approximately <code>x=0.4, y=-0.28</code> and the new waypoint would be approximately <code>x=0.22, y=-0.18</code>:</p>

<p><a href=""https://i.stack.imgur.com/t1MQd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t1MQd.png"" alt=""enter image description here""></a></p>

<p>Now I've got three questions:</p>

<ol>
<li>How do I find the nearest point on the cubic spline?</li>
<li>How do I ""advance"" 0.2 units from the found point on the cubic spline?</li>
<li>How do I stay on the given path, even when the path crosses itself in the middle?</li>
</ol>

<p>All tips are welcome!</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
from scipy import interpolate

x = [-0.5, 0, 0.5, 0.5,  0, -0.5,  -0.5, 0, 0.5]
y = [0.25, 0, -0.25, 0.25, 0, -0.25, 0.25, 0, -0.25]

tck, u = interpolate.splprep([x, y], s=0)
unew = np.arange(0, 1.01, 0.01)
out = interpolate.splev(unew, tck)

plt.figure()
plt.plot(x, y, 'o', out[0], out[1])
plt.legend(['Predefined points', 'Cubic Spline'])
plt.axis([-0.75, 0.75, -0.75, 0.75])
plt.show()
</code></pre>
",2017-05-21 14:52:00,,1083,1,0,1,,1650012.0,,9/5/2012 19:08,2447.0,44104308.0,"<blockquote>
  <p>Now I've got three questions:</p>
  
  <p>How do I find the nearest point on the cubic spline? How do I
  ""advance"" 0.2 units from the found point on the cubic spline? How do I
  stay on the given path, even when the path crosses itself in the
  middle?</p>
</blockquote>

<p>I believe a help would be to take advantage of the spline array index as a navigaion dimension    </p>

<p>with no initial info, a crude but not that slow startup step would be to simply find the min distance to the whole spline x, y trace, <code>out</code> - many SE ans for finding closest</p>

<p>then use that point's index as a state variable, and along with direction (increasing or decreasing <code>out</code> index) find the next index you want to head for</p>

<p>if you have started up, are close, and know your last index position (or estimated target point's index), then you could just search for ""closest"" in a slice near (in whatever is the current ""forward"" direction) your internal state index, updating your internal out index as you step...</p>

<p>with state involved I would consider a OOP Robot Class with XY position, heading, move magnitude, out spline index estimate, index direction</p>

<p>and some fun to be had with actual programming</p>

<p><strong>[edits:]</strong><br>
1st pass at ""closest""    </p>

<pre><code># out is a list of arrays, covert to 2d array
aout=np.array(out)

tpoint = np.array([[0.5],[-0.7]])

diff = aout-tpoint

sd = diff[0]*diff[0] + diff[1]*diff[1]  # squared distance

np.min(sd)
cpi=np.where(sd&lt;=np.min(sd)+0.00001)[0]
plt.plot((aout[0, cpi],tpoint[0]),( aout[1, cpi], tpoint[1]), linewidth=2)
</code></pre>

<p>just checking if out index is srictly montonic/cyclic - and it is, but the linspace gives more than one exact cycle, I eyeballed ~ 75 points for a full cycle:
   # animate plotting of spline points by index</p>

<pre><code>#import numpy as np
#import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

fig1, ax = plt.subplots()
xdata, ydata = [], []
ln, = plt.plot([], [], 'ro', animated=True)

def update(frame):
    xdata.append(aout[0][frame])
    ydata.append(aout[1][frame])
    ln.set_data(xdata, ydata)
    return ln,


def init():
    ax.set_xlim(-1, 1)
    ax.set_ylim(-1, 1)
    return ln,

data = np.array(out) #np.random.rand(2, 25)
l, = plt.plot([], [], 'r-')
plt.xlim(0, 1)
plt.ylim(0, 1)
ani = FuncAnimation(fig, update,
                              frames=np.array(range(74)),
                               init_func=init, blit=True)
plt.show()
</code></pre>
",6876009.0,1.0,0.0,,
2919,46748212,What is the best and simplest file type to use to read and run a pick and place program c#?,|c#|csv|robotics|,"<p>Im working on making a C# program to control a 3-axis pick and place machine and trying to figure out which type of file type I want to use to read and run the pick and place program.
The program would look something like this:</p>

<p>Move X to 999.9
Wait for input 1
Move Y to 1.0
Set output 1
And simple things like that.</p>

<p>So I figure that in a CSV file it would look like this:</p>

<pre><code>move,x,999.9
wait,in,1
move,y,1.0
set,out,1
</code></pre>

<p>And then I would have to check whats in the first column in the first line, if its move then check which axis in the second column then where to move in the last column.</p>

<p>Would that be the simplest way or should I look for something else?
Later I would also like to implement IF statements in the program.</p>
",2017-10-14 18:54:00,,85,3,0,0,,8418682.0,,8/4/2017 17:22,11.0,46748322.0,"<p>There surely are other ways to do this, but I would do it this way (just a  <strong>basic</strong> concept, you have to implement exception handling, unknown commands, illegal parameters and stuff like that...)</p>

<p>Command definition:  </p>

<pre><code>interface ICommand
{
    object Context { get; set; }
    bool IsResponsible(string line);
    void Execute(string line);
}
public class Move: ICommand
{
    public bool IsResponsible(string line)
    {
        return line.StartsWith(""move"");
    }
    public void Execute(string line)
    {
        var tmp = line.Split(',');
        // Validate and verify parameters here
        // code here to move the context to x/y 999.9
        // Context.Move(tmp[1], Convert.ToDouble(tmp[2]);
    }
}
public class Wait: ICommand
{
    public bool IsResponsible(string line)
    {
        return line.StartsWith(""wait"");
    }
    public void Execute(string line)
    {
        var tmp = line.Split(',');
        // code here to wait...
    }
}
</code></pre>

<p>Execution:  </p>

<pre><code>public void RunFile()
{
    var ctx = new YourContext();
    var commands = new List&lt;IMyCommand&gt;();
    // You could look for all ICommand implementations here with reflection or just hard-code all known classes...
    command.Add(new Move() { Context = ctx; });
    command.Add(new Wait() { Context = ctx; });

    var lines = File.ReadAllLines(""your-file.txt"");
    foreach(var line in lines)
    {
        var cmd = commands.FirstOrDefault(x =&gt; x.IsResponsible(line));
        if (cmd == null)
            throw new IOException(""unknown command!"");
        cmd.Execute(line);
    }
}
</code></pre>

<p><strong>not tested</strong></p>
",8400446.0,0.0,1.0,,
2978,47970583,What is the future of filtering methods vs incremental-SFM in visual-SLAM,|robotics|kalman-filter|slam-algorithm|,"<p>In the Visual SLAM area, there's the well-known solution of <strong>EKF/UKF/Particle-SLAM</strong> , like the ""mono-slam"".</p>

<p>Recently , there was a direction to <strong>Local Bundle Adjustment</strong> methods , like <em>lsd-slam or orb-slam</em> ..</p>

<p>My question is : </p>

<blockquote>
  <p>Do the filtering ways still have a future or steady usage? in what applications? what are the pros/cons?</p>
</blockquote>

<p>I read these papers but, I failed to extract a final answer,(mostly out of misunderstanding):</p>

<ol>
<li><p><a href=""https://www.doc.ic.ac.uk/~ajd/Publications/strasdat_etal_ivc2012.pdf"" rel=""nofollow noreferrer"">Visual SLAM: why filter?</a></p></li>
<li><p><a href=""https://arxiv.org/pdf/1606.05830"" rel=""nofollow noreferrer"">Past, Present, and Future of Simultaneous Localization and Mapping </a></p></li>
</ol>

<p><strong>P. S.</strong>: I know the first is saying that Local BA is better somehow, and the second rarely mentioned filtering, so.., that's it.. , is it the end of the awesome Kalman filter in Visual-SLAM area?!!</p>
",2017-12-25 17:09:00,47970866.0,842,2,0,4,0.0,6087307.0,Germany,3/19/2016 17:54,166.0,47970692.0,"<p>No, the second paper does not describe the end of the Kalman filter in Visual-Slam.  The Kalman filter is a special case of the Maximum Likelihood Estimator for Gaussian noise.  I want to direct your attention to Page 4, paragraph 3 of the second paper.  There, the authors should clarify that the Kalman Filter and MAP are both extensions of Maximum Likelihood Estimation.  As written, that insight is only implicit.</p>
",,1.0,0.0,,
2766,43429195,Arduino IDE error: cannot declare variable <object> to be of abstract type <class>,|c++|arduino|accelerometer|sensors|robotics|,"<p>I can't figure out this error.  I initially copied from CurieIMU.h (which builds ok) to ashIMU.h...</p>

<p>Error is this:
ashIMU.h:17: error: cannot declare variable 'ashIMU' to be of abstract type 'ashIMUClass'</p>

<p>My sketch ARDUINO_LED_DEMO.ino .................</p>

<pre><code>#include ""ashIMU.h""
...
</code></pre>

<p>ashIMU.h..................</p>

<pre><code>        #ifndef ASH_IMU_API_H
        #define ASH_IMU_API_H

        #include ""ash_BMI160.h""

        class ashIMUClass : public ashBMI160Class {

            public:
                bool begin(void);

                void setAccelerometerRange(int range);
        };

    extern ashIMUClass ashIMU;

#endif // ASH_IMU_API_H
</code></pre>

<p>ash_BMI160.h .....................................
This is just BMI160.h but with class name changed from ""BMI160Class"" to ""ashBMI160Class"".</p>

<pre><code>...

class ashBMI160Class {
    public:

...

};
</code></pre>
",2017-04-15 17:36:00,,1177,1,4,0,,746100.0,"Silicon Valley, California, USA",8/21/2010 13:02,1922.0,43437712.0,"<p>If your <code>ashBMI160Class</code> is the same as the original <code>BMI160Class</code> from the CurieIMU library, then you are missing the implementation of pure virtual function:</p>

<p><code>virtual int serial_buffer_transfer(uint8_t *buf, unsigned tx_cnt, unsigned rx_cnt) = 0;</code></p>
",4760587.0,0.0,0.0,73932119.0,"If nothing fishy is going on, it simply means that your class has a pure virtual method. The class of which you'd like to create an instance needs to implement that method."
2781,43693218,Controlling Arduino Braccio arm using data from processing,|arduino|processing|robotics|,"<p>I am working on a project that uses processing to read data from a file and then sends these data to an arduino Uno board using serial, then supply these values to the arduino braccio Braccio.ServoMovment() function to move the robotic arm according to them. however whenever I run the code I keep getting wrong values into the function and the arm moves in a weird way, I have tested the same code and tried to light up an led and it worked fine for some reason.</p>



<pre class=""lang-java prettyprint-override""><code> import processing.serial.*;
 import java.io.*;
 int mySwitch=0;

 int counter=0;

 String [] subtext;

 Serial myPort;



 void setup(){

 //Create a switch that will control the frequency of text file reads.

 //When mySwitch=1, the program is setup to read the text file.

 //This is turned off when mySwitch = 0

 mySwitch=1;


 //Open the serial port for communication with the Arduino

 //Make sure the COM port is correct

 myPort = new Serial(this, ""COM5"", 9600);
  myPort.bufferUntil('\n');
 }

 void draw() {
  // print("" the switch "" + mySwitch);

  if (mySwitch&gt;0){
  /*The readData function can be found later in the code.
  This is the call to read a CSV file on the computer hard-drive. */
  readData(""C:/Users/Tareq/Desktop/data.txt"");

  /*The following switch prevents continuous reading of the text file,           until
  we are ready to read the file again. */
  mySwitch=0;
  }

  /*Only send new data. This IF statement will allow new data to be sent      to
  the arduino. */
  if(counter&lt;subtext.length){
  /* Write the next number to the Serial port and send it to the Arduino 
  There will be a delay of half a second before the command is
  sent to turn the LED off : myPort.write('0'); */
 // print(subtext[counter]);
  delay(5000);
  myPort.write(subtext[counter]);
  print(subtext[counter]);
  delay(2000);
  //Increment the counter so that the next number is sent to the arduino.
  counter++;
  } else{
  //If the text file has run out of numbers, then read the text file again      in 5 seconds.
  delay(5000);
  mySwitch=1;
  }
 } 


 /* The following function will read from a CSV or TXT file */
 void readData(String myFileName){

  File file=new File(myFileName);
  BufferedReader br=null;

  try{
  br=new BufferedReader(new FileReader(file));
  String text=null;

  /* keep reading each line until you get to the end of the file */
  while((text=br.readLine())!=null){

  /* Spilt each line up into bits and pieces using a comma as a separator */
  subtext = splitTokens(text,"","");

  }
  }catch(FileNotFoundException e){
  e.printStackTrace();
  }catch(IOException e){
  e.printStackTrace();
  }finally{
  try {
  if (br != null){
  br.close();
  }
  } catch (IOException e) {
  e.printStackTrace();
  }
  }
 }
</code></pre>

<p>Arduino Code</p>

<pre class=""lang-java prettyprint-override""><code>#include &lt;Braccio.h&gt;
#include &lt;Servo.h&gt;
#include&lt;SoftwareSerial.h&gt;

Servo base;
Servo shoulder;
Servo elbow;
Servo wrist_rot;
Servo wrist_ver;
Servo gripper;

String stringData[6] ;
int intData[6] ;

void setup() {
 Serial.begin(9600);
 Braccio.begin();
 pinMode(1, OUTPUT); // Set pin as OUTPUT
 pinMode(2, OUTPUT); // Set pin as OUTPUT
 pinMode(3, OUTPUT); // Set pin as OUTPUT
 pinMode(4, OUTPUT); // Set pin as OUTPUT
 pinMode(5, OUTPUT); // Set pin as OUTPUT
 pinMode(6, OUTPUT); // Set pin as OUTPUT
 pinMode(7, OUTPUT); // Set pin as OUTPUT
 pinMode(8, OUTPUT); // Set pin as OUTPUT
 pinMode(9, OUTPUT); // Set pin as OUTPUT
}

void loop() {




for(int y=0;y&lt;6;y++){

  while(Serial.available()== 0){}

stringData[y]= Serial.readString();

if(stringData[y]==""90"")
intData[y]=90;
else if(stringData[y]==""120"")
intData[y]=120;
else if(stringData[y]==""180"")
intData[y]=180;
else if(stringData[y]==""45"")
intData[y]=45;
else if(stringData[y]==""160"")
intData[y]=160;
else if(stringData[y]==""170"")
intData[y]=170;
else if(stringData[y]==""165"")
intData[y]=165;
else if(stringData[y]==""60"")
intData[y]=60;
else if(stringData[y]==""30"")
intData[y]=30;
else if(stringData[y]==""110"")
intData[y]=110;
else if(stringData[y]==""80"")
intData[y]=80;
else if(stringData[y]==""155"")
intData[y]=155;
}

Braccio.ServoMovement(20 ,         intData[0], intData[1], intData[2] , intData[3] ,intData[4] , intData[5]);

}
</code></pre>

<p>NOTE: The file im reading from is a notepad containing numbers separated with a "" , "" something like 150,30,60, etc for my code I used 6 for testing
someone might say its weird why send string then map it into an int instead of sending int directly, well I tried the int first and it didn't work then I tried this.</p>
",2017-04-29 07:54:00,,783,1,1,0,,6613671.0,,7/20/2016 12:18,4.0,43764473.0,"<p>I would try to isolate the issue among the following culprits:</p>

<p>Can you make the robot do repeatable things driving it from a static path in the arduino (ie hardcode the values in the arduino and check that you know how the robot is driven and it is working)</p>

<p>Are you cleanly passing your data between arduino and processing.  To test this create some objects/ arrays of numeric values and transfer them from processing to arduino and have arduino verify that it received what was expected.</p>

<p>Is your file read correct.  Read the file and write it to the processing console or something to verify you have what you think you have.</p>
",7298298.0,0.0,0.0,74570126.0,Have you verified that the values are being successfully passed and parsed on the arduino side?  (you could write some basic test files to push through a state machine on the arduino to demonstrate the accuracy of the values)
2900,46398319,Simultaneous Localization and Mapping(SLAM) simulation,|artificial-intelligence|robotics|,"<p>I am planning to do a project on Simultaneous Localization and Mapping(SLAM) using simulation since I am completely new to robotics I have no idea where to start and how to proceed.Please do help me to begin my work and I need some references for the tutorial. </p>
",2017-09-25 05:45:00,46425576.0,383,1,0,0,,6813937.0,,9/9/2016 14:16,15.0,46425576.0,"<p>I would suggest you to use <a href=""http://www.ros.org"" rel=""nofollow noreferrer"">Robot Operating System (ROS)</a>. It has good development tools and the algorithms for SLAM and planning already available. You can start with <a href=""http://wiki.ros.org/kinetic/Installation"" rel=""nofollow noreferrer"">installing ROS</a> followed by <a href=""http://wiki.ros.org/ROS/Tutorials"" rel=""nofollow noreferrer"">ROS tutorials</a>.</p>
",1595504.0,2.0,0.0,,
2671,42251735,I want to build a voice controlled robot please tell how should I proceed,|robotics|,"<p>I want to make a robot that follows my voice commands. So what sort of micro controller should I use? Should I go with Arduino or Should I use A Raspberry Pi. And also please tell me what language should I code in. Thank you!</p>
",2017-02-15 14:12:00,,80,1,0,-1,,7569298.0,,2/15/2017 14:03,8.0,42458308.0,"<p>You need two things here: a processor and a controller. You need a processor that can process data, do computations, run code etc. You need a controller for I/O. Raspberry pi has a strong(compared to Arduino) processor but the 40 GPIO pins are all digital i.e. you won't be able to use it alone for analog I/O (controlling the speed of the vehicle, reading sensor data etc.). Arduino on the other hand can handle both analog and digital I/O, but it has very low processing power. Generally people use both of them together. You will also need motor drivers, battery management circuit etc. Coming to the code, it should have two parts: one that processes your voice and understands the command, one that takes the command and tells the robot what to do.  Both could be done in C. For speech synthesis, you could use existing APIs and for controlling the robot, you can use Arduino libraries. I hope this answers your question.</p>
",6398022.0,0.0,0.0,,
2745,43056359,matlab smooth vertical and horizontal rectangle,|matlab|computer-vision|robotics|image-morphology|,"<p>Could someone help me to find what morphological operations should I use in order to smoothen the vertical and horizontal rectangle on this image </p>

<p><a href=""https://i.stack.imgur.com/kmZsn.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kmZsn.jpg"" alt=""enter image description here""></a></p>

<p>More precisely what I would want is that the white rectangles become continuous. The final application of that would be to detect vertical and horizontal lines in the image, indeed this image is a map where white element represent obstacle and where i would want to detect the walls.</p>

<p>So the result i would want should be something like that: </p>

<p><img src=""https://i.stack.imgur.com/8JkBm.jpg"" alt=""enter image description here""></p>
",2017-03-27 20:56:00,,67,0,4,0,,7776403.0,"Lige, Belgique",3/27/2017 20:48,5.0,,,,,,73208131.0,could you please upload some visualization of the desired output?
2880,45845630,Integration of signal in embedded matlab function with variable timestep size,|matlab|signals|simulink|robotics|,"<p>I need to get a position from a velocity signal. I need to do it in SIMULINK and MATLAB and I need to do it to control a real robot. 
I can't use the standard continuous time integrator of SIMULINK because my time step changes at each iteration. </p>

<p>Therefore I was thinking to use a simple forward-euler method in the following embedded matlab function. </p>

<p>I am not sure this is the best solution and therefore I would like to know your opinion about it|</p>

<pre><code>function pCurr = fcn(vCurr, dt,p0)
%#codegen

persistent pOld
persistent vOld
if isempty(pOld)
    pOld = p0;
end

if isempty(vOld)
    vOld = vCurr;
end


pOld = pOld + trapz([vOld vCurr])*dt;
vOld = vCurr;
pCurr = pOld;
end
</code></pre>

<p>Thanks in advance </p>
",2017-08-23 17:07:00,,190,0,2,0,,2761849.0,,9/9/2013 14:54,630.0,,,,,,78661596.0,"If you're controlling a real robot, presumably with a real-time processor, how are using a variable time step?  That's very unusual."
2894,46154545,Bounding box orientation,|c++|computer-vision|point-cloud-library|robotics|,"<p>I have used a bounding box to get the pair of 4-points around an object after RANSAC and conditional outlier removal during the segmentation and filtration process.  After computing the 8 corner points of the bounding box, I have used 2 of them (front below) to localize the robot. for e.g following is a visual of the extracted object with the corner-points I intend to use </p>

<p><a href=""https://i.stack.imgur.com/WDka0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WDka0.png"" alt=""enter image description here""></a></p>

<p>The problem is if the object is straight like above the robot moves correctly almost in the middle but if the object is tilted the robot moves more on the right or left depending on which side it is tilted for example </p>

<p><a href=""https://i.stack.imgur.com/XmsK3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XmsK3.png"" alt=""enter image description here""></a></p>

<p>After analysis it seems that bounding box points are not exactly but with a offset on the object. What can be helpful in this situation ?  </p>

<p>-adding a manual offset on the corner points </p>

<p>-adjusting segmentation thresholds</p>

<p>-Possible to use surface normal or angle information from these points</p>

<p>anyother possibility </p>
",2017-09-11 11:23:00,,1110,1,0,0,,7575638.0,Germany,2/16/2017 14:36,31.0,46154833.0,"<p>I assume that when you tilt the object, the point of the object corner that closer to you gives more precise information than the other point. And if you can take angle information, find the surface normal and then calculate the angle between the vector or line you draw between two points that you got initially and the normal vector. And try change the point according to the result you got, in the area of your offset that the normal and our vector will be 90 to each other. </p>

<p>For example you know the normal vector and you have two points (x1,y1), (x2,y2). draw a vector between these two points, calculate the angle between this vector and normal. If the angle comes like 85 degrees. Manipulate (x2,y2) to get 90 degrees. It is like you hold a vector from it's start point and playing with it's direction. Because we assume that our starting point is precise.</p>
",,1.0,4.0,,
2835,45647371,Issue with finding Euler angles,|matlab|robotics|euler-angles|rotational-matrices|kinematics|,"<p>When i try to calculate roll, pitch, yaw values from a rotation matrix given below:</p>

<p><a href=""https://i.stack.imgur.com/iGEgp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iGEgp.png"" alt=""enter image description here""></a></p>

<p>i get math error in my calculator. But, matlab (using peter corke's robotics toolbox) gives me some values as result. </p>

<pre><code>%Rotation Matrix 5
R = [-0.9122 0.4098 0; -0.4098 -0.9122 0; 0 0 1];
tr2rpy(R,'deg')


Output:  0         0        -155.8083
</code></pre>

<p>Does that mean the rotation matrix is invalid ? Can i believe the matlab output ?</p>

<p>Thanks and regards !</p>
",2017-08-12 06:46:00,,174,1,1,1,,3841374.0,,7/15/2014 14:44,34.0,45647720.0,"<p>This is correct answer, so you have only roll, you can see it from rotation matrix, last row-column is [0,0,1] meaning that no change in z axis, meaning no pitch or yaw applied.
(In case of roll only, the roll angle is arccos(R(1,1)) )</p>
",2840531.0,2.0,0.0,78261193.0,Have you tried calculating the rotation matrix for a rotation of -155.8083 degrees about the z-axis and see if it comes out the same as `R`?
2663,42080006,Avoiding cycles in an directed graph efficiently,|algorithm|graph|robotics|graph-traversal|,"<p>I am trying to implement an algorithm called the 'rapidly exploring random belief tree'. The aim of this algorithm is to come up with a path for a robot, which, instead of connecting start and goal with a least distance metric or something similar, drives the robot into areas where it can get high accuracy measurements, and only then moves to the goal. </p>

<p>In the implementation of the algorithm, I start off by building a graph through random sampling of a given space. Every time a new point is sampled, a new edge is added to the graph. Initially, the error in the position of the robot will be high, as it won't be getting good measurements, but once the tree I am exploring through reaches the 'good' area, the robot's error suddenly drops, and with this knowledge of where the 'good' area is, I traverse backwards through the graph, pruning previously connected edges and connecting those vertices towards the good area. Given enough samples, in an optimal scenario, the graph would be propagating outward from the good area, hence connecting any two vertices would guarantee that you would pass through these points.  </p>

<p>Here's where I have a problem. Once I start the traversal to prune and update my edges, the algorithm shouldn't prune the edges that connect the good area to the starting point, which would result in an infinite loop. An example is shown in this picture:</p>

<p><a href=""https://i.stack.imgur.com/JQgJ2.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JQgJ2.jpg"" alt=""enter image description here""></a></p>

<ol>
<li><p>The algorithm finds vertices 1, 2, 3 and 4. The connections are 1 -> 2, 2 -> 3, 2 -> 4. </p></li>
<li><p>Now we find #5, which is a 'good' area for the robot. Visiting this vertex will magically enhance its accuracy in the future. I find neighboring vertices from 5: I find #3. I update the parent of #3 to #5 instead of #2. Now if there were more vertices surrounding #3, I would traverse those too, and update their parents to #3 etc., thus creating a path from #5 to all of those. (For more information, the traversal essentially computes the covariance of the robot if it were to move from #5 to #3 and whether that's more desirable than moving from #2 to #3). </p></li>
<li>But in no circumstances should I update the parent of #2 to #3: because the only way the robot got to #5 in the first place was through #2.</li>
</ol>

<p>To avoid this, a simple (but highly inefficient?) thing I did was to compute the shortest path to the vertex from where I began my traversal, and if any vertex I come across during traversal is part of this path, I don't update its parent. This works reasonably well, at the expense of a lot of computational cost because I need to do this every time I have to traverse backwards with an update, so I was wondering if there was a better, efficient way of doing this. </p>
",2017-02-07 01:09:00,,897,0,3,1,,2588390.0,,7/16/2013 17:12,162.0,,,,,,71336541.0,"you can reduce the cost of an edge from +infinity to 1 when you add it to the graph. For +infinity you can use any number you know is higher than the maximum possible distance from one point in the graph to another. Depending on the details of your minimum distance calculation you may not need to use +infinity explicitly. Your diagrams make it look like you are based on a grid, in which case I would have thought that Dijkstra distance finding and reducing edge costs would work quite well."
2649,41827191,How to turn any camera into a Depth Camera?,|computer-vision|virtual-machine|robotics|perspectivecamera|,"<p>I want to build a depth camera that finds out any image from particular distance. I have already read the following link.</p>

<ol>
<li><a href=""http://www.i-programmer.info/news/194-kinect/7641-microsoft-research-shows-how-to-turn-any-camera-into-a-depth-camera.html"" rel=""nofollow noreferrer"">http://www.i-programmer.info/news/194-kinect/7641-microsoft-research-shows-how-to-turn-any-camera-into-a-depth-camera.html</a></li>
<li><a href=""https://jahya.net/blog/how-depth-sensor-works-in-5-minutes/"" rel=""nofollow noreferrer"">https://jahya.net/blog/how-depth-sensor-works-in-5-minutes/</a></li>
</ol>

<p>But couldn't understand clearly which hardware requirements need &amp; how to integrated into all together?</p>

<p>Thanks</p>
",2017-01-24 11:33:00,,3919,3,2,2,,7303836.0,"Rangpur, Rangpur Division, Bangladesh",12/15/2016 19:50,8.0,41868969.0,"<p>Certainly, a depth sensor needs an IR sensor, just like in Kinect or Asus Xtion and other cameras available that provides the depth or range image. However, Microsoft came up with machine learning techniques and using algorithmic modification and research which you can find <a href=""https://techxplore.com/news/2014-08-microsoft-2d-camera-depth-sensor.html"" rel=""nofollow noreferrer"">here</a>. Also here is a video <a href=""https://youtu.be/hpCSebkYtUI"" rel=""nofollow noreferrer"">link</a> which shows the mobile camera that has been modified to get depth rendering. But some hardware changes might be necessary if you make a standalone 2D camera into a new performing device. So I would suggest you to see the hardware design of the existing market devices as well.</p>
",4750060.0,0.0,0.0,93310195.0,"I just wrote an answer for you but I probably could have just said ""google 'rgbd slam 2d'"""
2659,42030132,Inverse Kinematics programming C++,|c++|linux|macos|robotics|inverse-kinematics|,"<p>I want to write my own kinematics library for my project in C++. I do understand that there are a handful of libraries like RL (Robotics Library) and ROS with inverse kinematics solvers. But for my dismay, these libraries DO NOT support MacOS platform. I have already written the Forward Kinematics part, which was quite straight forward. But for the Inverse Kinematics part, I am quite skeptical since the solution to a IK problem involves solving sets of non-linear simultaneous equation. I found out the Eigen/Unsupported 3.3 module has a APIs to non-linear equations. But before I begin on this uncertain path, I want to get some insight from you guys on the plausibility and practicality of writing my IK library. My manipulator design is rather simple with 4 DoF and the library will not be used for other designs of manipulators. So what I am trying to achieve is taylor-made IK library for my particular manipulator design than a rather a 'Universal' library. </p>

<p>So,</p>

<ul>
<li>Am I simply trying to reinvent the wheel here by not exploring the already available libraries? If yes, please suggest examples of IK libraries for MacOS platform.</li>
<li>Has anyone written their own IK library? Is it a practical solution? Or is it rather complex problem which is not worth solving for a particular manipulator design?</li>
<li>Or should I just migrate all my project code (OpenCV) to a Linux environment and develop the code for IK in Linux using existing libraries?</li>
</ul>

<p>Thank you,</p>

<p>Vino</p>
",2017-02-03 17:48:00,42030892.0,2707,1,4,-1,,4122660.0,"Sydney, New South Wales, Australia",10/8/2014 19:12,73.0,42030892.0,"<p>I have built some robots in the past and been required to solve kinematic equations.
As you stated ""manipulator design is rather simple with 4 DoF"" in my opinion you can write a fairly small function/module and you will not require the complexity of a general purpose library.
I used Maple to assist with creating the inverse equations, you may want to look for an alternative <a href=""http://alternativeto.net/software/maple/"" rel=""nofollow noreferrer"">Alternative</a>
On the other hand, the libraries you mention ROS and RL may not support your Mac at the highest level, but at the low level it is just c++ code, there is no reason you cannot use the libraries on your Mac and only use the low level functions.</p>
",7502032.0,1.0,1.0,71234720.0,Also: These kind of questions are explicitly _off-topic_ here. Read point #4 from this [help center article](http://stackoverflow.com/help/on-topic).
2771,43580279,c++ 2D servos robot arm inverse kinematics,|c++|robotics|mbed|inverse-kinematics|,"<p>I am currently trying to write code to instruct a robot arm (2 servos motors, 3 sticks) to write out words and am having trouble getting it right. At the moment I am just trying to get it to move to a vector. I think that the inverse kinematics part is correct but all I'm getting is a twitch from one of the motors (if I'm lucky). A Nucleo- F411RE board is being used and I'm using the mbed developer. </p>

<pre><code>#include ""mbed.h""
#include ""Servo.h""
#include ""math.h""

Servo servo1(PA_8), servo2(PA_9);
    float len1 = 9.0;
    float len2 = 7.5;
    double pi = 3.1415926535897;

int lawOfCosines (float a, float b, float c )
{
    return acos((a*a + b*b - c*c) / (2 * a * b));
}
int distance(float x, float y) {
    return sqrt(x*x + y*y);
}

int deg(float rad) {
    return rad * 180 / pi;
}
int main() {

    //fmt.Println(""Lets do some tests. First move to (5,5):"");
    float x = 5.0;
    float y = 5.0;
    float dist = distance(x, y);
    float D1 = atan2(y, x);
    float D2 = lawOfCosines(dist, len1, len2);
    float A1 = D1 + D2;
    float A2 = lawOfCosines(len1, len2, dist);

   float  m1 = A1 * (100/90); 
   float  m2 = A2 * (100/90);
    for(int i=0; i&lt;m1; i++) {
              servo2 = i/100.0;

              wait(0.01);
          }
    for(int i=0; i&lt;m2; i++) {
               servo1 = i/100.0;              
              wait(0.01);
          }


    }
   `}
</code></pre>

<p>Any help to where I'm going wrong is greatly appreciated</p>
",2017-04-24 05:17:00,,351,0,4,0,,7225744.0,,11/29/2016 13:43,11.0,,,,,,74217792.0,"Thank you @sniper ! I have fixed the return types and changed the for loops to a ""servos.write(0)"" expression. However still not getting the desired response."
2902,46530131,How to find change of direction in 3d space,|math|vector|coordinates|physics|robotics|,"<p>I am working in a project that takes data from moving sensor. we have converted sensor data in 3d coordinates. one of the sensor is planted on hand of the object.<br />
PROBLEM: When object moves his hand in backward direction and then play in forward direction. So i am looking for the frame number at which object hand ends moving back.</p>
<p>Just to clear the problem, this kind of points can come in 3D Here sample image data:</p>
<p><img src=""https://i.stack.imgur.com/kTjuJ.jpg"" alt=""img"" /></p>
<p>initially i implemented this, using change in y axis. With having some threshold value it's result is good. But only when  object hand moves back and comes forward with decreasing value of y axis. But in ideal case(image) hand can come forward in any way ie with increasing value of y or decreasing value of y. But i am sure that while hand moving in back direction it will always be y increasing (except some noise frame or outlier). using this i implemented this. `</p>
<pre class=""lang-cpp prettyprint-override""><code>std::pair&lt;std::pair&lt;int, int&gt;, bool&gt; calculateBacklift(std::vector&lt;glm::vec3&gt; points)
{
    std::pair&lt;std::pair&lt;int, int&gt;, bool&gt; backlift;
    backlift.second = false;
    std::vector&lt;glm::vec3&gt; distanceVec;
    for (int i = 0; i &lt; points.size() - 1; ++i)
    {

        // vector 
        glm::vec3 distance;
        distance.x = points[i + 1].x - points[i].x;
        distance.y = points[i + 1].y - points[i].y;
        distance.z = points[i + 1].z - points[i].z;
        distanceVec.push_back(distance);
    }
    writeTodisk(&quot;distanceVector.csv&quot;, distanceVec);
    for (int i = 0; i &lt; distanceVec.size(); ++i)
    {

        // y is major axis and if any of x or z hand changed then i am assuming there can be direction change. this comes from experiment.
        if (distanceVec[i].y &lt;= -0.09 &amp;&amp; (distanceVec[i].x &lt;= -0.1 || distanceVec[i].z &gt;= 0.9))
        {
            backlift.first = std::make_pair(1, i + 1);
            backlift.second = true;
            break;
        }
    }
    return backlift;
}`
</code></pre>
<p>But it doesn't work when hand comes forward in up direction. because change of y axis is positive.</p>
<p>Then i think of find change of direction using dot product. And cos value. But it also detect direction change on axis(X,Z). Or in simple way i can say i want to find corner point in moving sensor data in y axis(major) direction.  Can any one help me to resolve this issue.</p>
<p>Thanks.</p>
",2017-10-02 16:56:00,46534082.0,614,1,6,0,,7664597.0,"New Delhi, Delhi, India",3/6/2017 6:07,16.0,46534082.0,"<p>Assuming you have the entiry point trajectory, I'd just look for the point which has the largest distance to the line connecting the start to the end point. I don't know the vector library you are using, but using the <a href=""http://eigen.tuxfamily.org/"" rel=""nofollow noreferrer"">Eigen-library</a> this would be something like this:</p>

<pre class=""lang-cpp prettyprint-override""><code>std::vector&lt;Eigen::Vector3f&gt; points; // input set of points

typedef Eigen::ParametrizedLine&lt;float, 3&gt; Line3;
// Calculate line through first and last point of points:
Line3 start_to_end = Line3::Through(points[0], points.back());
float max_dist_squared=0.0f;
size_t max_idx=0;
for(size_t i=0; i&lt;points.size(); ++i) {
    // calculate squared (perpendicular) distance from points[i] to the line
    float dist2 = start_to_end.squaredDistance(points[i]);
    // check if distance is bigger than previous maximum:
    if(dist2 &gt; max_dist_squared) { max_dist_squared = dist2; max_idx = i; }
}
</code></pre>

<p>You should be able to implement something equivalent with the library of your choice.</p>

<p>If you need to handle outliers or noise, use some kind of moving median or moving average instead of single points.</p>
",6870253.0,0.0,5.0,80022510.0,"Do you want to find the ""corner point"" as soon as it occurs, or will you always have the entire point set?"
2834,45624243,PID Tuning Line Following Robot,|python|robotics|pid-controller|,"<p>I am working in a line following robot and I'm trying to implement a PID control loop, but i don't know how to tune it effectively.
I have two sensors, with the black line in the middle of them. I'm calculating the error with the intensity of sensor1 - the intensity of sensor2, and then calculating the speed.
This is my code simplified:</p>

<pre><code>error= sensor1_value-sensor2_value
error_sum+=error
dif_speed=error*kp+error_sum*ki+(error-last_error)*kd
run(speed-dif_speed+speed+dif_speed)
last_error=error
</code></pre>
",2017-08-10 22:17:00,,196,0,0,1,,8350403.0,,7/22/2017 15:42,3.0,,,,,,,
2802,44620509,Robot Motion - Dynamic Programming,|c++|dynamic-programming|robotics|,"<p>Given a 1D world of infinite length (x),
and available moves (y) of, for example [1, 2, 3, -1, -2, -3],
and a destination (d) (ie 15), write a function that returns
the smallest number of moves (result) needed to reach d.</p>

<p>For example if d = 15, result = 5
since the most optimal move is 3, and it can be done 5 times.</p>

<p>This problem is very similar to this: <a href=""https://www.youtube.com/watch?v=Y0ZqKpToTic"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=Y0ZqKpToTic</a> 
except that negative values are allowed.</p>

<p>I have the code below that only works for positive number. Any ideas to make it work for mixed positive and negative values?</p>

<pre><code>class Solution {
public:
    int Robotmotion(vector&lt;int&gt; &amp;moves, int &amp;d) {
        if (d == 0) return 0;
        if (d &lt; 0) {
            d = -d;
            for (auto &amp;move : moves) move *= -1;
        }

        sort(moves.begin(), moves.end());

        vector&lt;int&gt; dp(d + 1, d + 1);

        dp[0] = 0;

        for (int i = 1; i &lt;= d; i++) {
            for (int j = 0; j &lt; moves.size(); j++) {
                if (moves[j] &lt;= i) {
                    dp[i] = min(dp[i], dp[i - moves[j]] + 1);
                }
            }
        }

        return dp[d] == d + 1 ? -1 : dp[d];
    }
};



int main() {

    Solution s;
    vector&lt;int&gt; moves = {1,2,3};
    int d = 15;
    int min_steps = s.Robotmotion(moves, d);
    cout &lt;&lt; ""Mim steps:"" &lt;&lt; endl &lt;&lt; min_steps &lt;&lt; endl;
    return 0;
}
</code></pre>
",2017-06-18 23:35:00,,277,1,6,0,,1527457.0,,7/15/2012 20:59,59.0,44627727.0,"<p>I don't think dynamic programming can solve the problem. Instead, you should view the number as vertices in a graph and use BFS to solve the problem. You can even use a bidirectional BFS to speed up the process.</p>
",4017815.0,0.0,0.0,76228645.0,Is it legal to overshoot and backtrack? As in if you can move 10 and then -1 instead of having to got 3 three times?
2783,43857864,Robotics Cape Library not working on Beaglebone Blue,|c|beagleboneblack|robotics|,"<p>I have been trying to compile the example off of the <a href=""http://strawsondesign.com/files/BeagleBoneRobotics.pdf"" rel=""nofollow noreferrer"">BeagleBoneRobotics PDF</a>, but the compiler can't find the header.  </p>

<p>Here's the code I'm trying to compile:</p>

<pre><code>#include &lt;robotics_cape.h&gt;
#define PIN 67

int main (void){
 // export gpio pin for use
 if(gpio_export(PIN)){
  printf(""Unable to open export.\n"");
  return -1;
 }
 // set pin for output
 if(gpio_set_dir(PIN, OUTPUT_PIN)){
  printf(""Unable to open gpio67_direction.\n"");
  return -1;
 }
 // start blinking loop
 printf(""blinking LED\n"");
 int i = 0;
 while(i&lt;10){
  // turn pin on
  gpio_set_value(PIN, 1);
  printf(""ON\n"");
  sleep(1);
  // turn pin off
  gpio_set_value(PIN, 0);
  printf(""OFF\n"");
  i++; // increment counter
  sleep(1);
 }
 return 1;
}
</code></pre>

<p>Here's the error I'm getting:</p>

<p><a href=""https://i.stack.imgur.com/Aodqb.jpg"" rel=""nofollow noreferrer"">Error</a></p>

<pre class=""lang-none prettyprint-override""><code>root@beaglebone:/var/lib/cloud9# gcc Testing.c -lrobotics_cape -o Testing
Testing.c:1:27: fatal error: robotics_cape.h: No such file or directory
 #include &lt;robotics_cape.h&gt;
                           ^
compilation terminated.
</code></pre>

<p>I'm on the BeagleBone Blue with robotics cape version 0.3.4.</p>

<p>I checked the appropriate folders, and the header and library seem to be in place.  I've tried downloading the installer off of GitHub and making the library again, but still receive the same error.  I've tried to reinstall the the cape with the same result.  I've also dug through the source code to look for an error, but I can't find anything.  </p>

<p>Any help would be greatly appreciated</p>
",2017-05-08 21:55:00,43882000.0,1481,3,0,2,0.0,7981942.0,,5/8/2017 16:58,1.0,43858052.0,"<p>I would comment, but i cannot, until i get 50 reputation. </p>

<p>if the libriary is in the same directory as your program, then you use include ""</p>

<p><code>#include ""robotics_cape.h""</code> instead of <code>#include &lt;robotics_cape.h&gt;</code></p>

<p><code>#include &lt;name&gt;</code>  is used if the library is in the search path that your compiler used.</p>

<p>If the filename is quoted, searching for the file
typically begins where the source program was found.</p>

<p>If the library is not in the same directory as your file, then make sure it's in the gcc search path.  </p>

<p>this link will give you the gcc paths. </p>

<p><a href=""https://gcc.gnu.org/onlinedocs/gcc-3.3.3/cpp/Search-Path.html"" rel=""nofollow noreferrer"">https://gcc.gnu.org/onlinedocs/gcc-3.3.3/cpp/Search-Path.html</a></p>

<p>and</p>

<p><a href=""https://gcc.gnu.org/onlinedocs/cpp/Search-Path.html#Search-Path"" rel=""nofollow noreferrer"">https://gcc.gnu.org/onlinedocs/cpp/Search-Path.html#Search-Path</a></p>
",7892010.0,3.0,0.0,,
3116,50060927,"how to use ""robotlocomotion drake"" codes in my project, for example ""qp_inverse_dynamics""",|installation|open-source|robotics|drake|,"<p>i want to implement drake in my project, but i found it almost impossible. 
Although there are notebooks and course explain robotics theory and how drake works, 
<a href=""http://underactuated.csail.mit.edu/underactuated.html?chapter=drake"" rel=""nofollow noreferrer"">http://underactuated.csail.mit.edu/underactuated.html?chapter=drake</a>
<a href=""https://www.edx.org/course/underactuated-robotics-mitx-6-832x-0"" rel=""nofollow noreferrer"">https://www.edx.org/course/underactuated-robotics-mitx-6-832x-0</a></p>

<p>But how could i use the codes in drake in my project. For example, i want to simulate a 6-Dof arm using V-rep and ROS, and i want to inplement force control to the arm using ""qp_inverse_dynamics"" in drake, do i need to include all files that ""qp_inverse_dynamics"" used, and construct the build system? There are tons of files.</p>

<p>I have made a quadruped robot using position control and PID controller, and have a little bit experience of using open source convex quadratic programs solver(osqp)
And now, after build and tested drake using bazel, what coule I do to use codes in drake in my project? Or should I just write my own codes using the method in Underactuated Robotics notebook?</p>

<p>thanks a lot.</p>
",2018-04-27 10:56:00,50075307.0,398,1,0,1,,9708755.0,,4/27/2018 6:35,34.0,50075307.0,"<p>This repository is our working example of how to use drake in your own project:<br>
<a href=""https://github.com/RobotLocomotion/drake-shambhala"" rel=""nofollow noreferrer"">https://github.com/RobotLocomotion/drake-shambhala</a></p>

<p>We do support OSQP as one of many solver backends.  There is a chance that you will find that you want some feature in drake that is not yet exposed in the binary installation, in which case please make a request on github.  But I suspect it should work well for you.</p>

<p>N.B.  The lectures you've pointed to on edX are a few years old now.  The current version of the course is running right now, with streamed/recorded lectures available at <a href=""http://underactuated.csail.mit.edu/Spring2018/index.html#textbook/assignments/videos"" rel=""nofollow noreferrer"">http://underactuated.csail.mit.edu/Spring2018/index.html#textbook/assignments/videos</a></p>
",9510020.0,2.0,1.0,,
2999,48299129,How can I get coordinates of a path through the remote API of v-rep?,|python|simulator|robotics|motion-planning|,"<p>Question in the title. Using the remote python API of <a href=""https://github.com/Troxid/vrep-api-python"" rel=""nofollow noreferrer"">v-rep</a>, I am able to get images and control motor properties of robots, but I cannot find any way to get the coordinates of a path object that was made with the path planning functionality in v-rep. I would like to get them as an array in my external python script.</p>
<p>I have found that there is no remote API functions dedicated to path objects, but there might be a more generic function that could be used for this.</p>
",2018-01-17 10:41:00,,929,2,0,0,,5074515.0,Norway,7/2/2015 15:30,20.0,48419671.0,"<p>A workaround provided by the developers on the v-rep forum as there does not seem to be a straightforward way to do this:
<a href=""http://www.forum.coppeliarobotics.com/viewtopic.php?f=9&amp;t=7095&amp;p=28040#p28040"" rel=""nofollow noreferrer"">http://www.forum.coppeliarobotics.com/viewtopic.php?f=9&amp;t=7095&amp;p=28040#p28040</a></p>

<p>First sample the path coordinates with simGetPathPosition, inside of a script function. Then call that script function from the python remote Api client, via simxCallScriptFunction.</p>
",5074515.0,0.0,0.0,,
3155,51161843,Create interrupt (ISR) to create smooth robotic arm motion,|python|multithreading|robotics|isr|,"<p>My school has a robotic arm (UR-10) that's hooked up to some buttons and I wanted to program it so the arm could move left and right smoothly when those buttons are clicked. </p>

<p>Currently, I'm finding the arm just moves then stops then moves and stops in a jerking fashion.</p>

<p>I wanted to implement an interrupt that could sense while a button is pressed down then the arm would continuously move by iterating through the loop fast enough so the arm would move without stopping. That is until the button is released.</p>

<p>If there is an easier way to implement this I would love to hear it.</p>

<p>Thanks!</p>

<p>Below is my code.</p>

<pre><code>import urx #UR-10 library
from threading import Thread

class UR10(object):
def __init__(self):

    # Establishes connection to UR-10 via IP address
    print(""establishing connection"")
    Robot = urx.Robot(""192.168.100.82"")
    self.r = Robot
    print(""Robot object is available as robot or r"")

    CheckingDigital_Inputs = Thread(target=self.ThreadChecker)
    CheckingDigital_Inputs.setDaemon(True)
    CheckingDigital_Inputs.start()

    MoveThread = Thread(target = self.MoveThread)
    MoveThread.setDaemon(True)
    MoveThread.start()

def ThreadChecker(self):
    while True:
        self.TestingInputs()

def TestingInputs(self):
    #Values = self.r.get_digital_in()

    self.In_1 = self.r.get_digital_in(1)
    self.In_2 = self.r.get_digital_in(2)

    #print(""Digital Input 1 is {},  Digital Input 2 is {}"".format(Values1, Values2))

def MoveThread(self):
    while True:
        self.LinearMovement()

def LinearMovement(self):
    #print(""linear move"")
    #need to run at 125khz
    while self.In_1:
        print(""move right linear"")

        l = 0.05
        v = 0.05
        a = 0.05

        pose = self.r.getl()
        print(pose)
        pose[0] += .01
        self.r.movel(pose, acc=a, vel=v)
</code></pre>
",2018-07-03 19:40:00,51162299.0,176,1,0,1,,8524686.0,,8/27/2017 20:42,84.0,51162299.0,"<p>The library code you are using says (in its description of a method):</p>

<blockquote>
  <p>This method is usefull since any new command from python to robot make the robot stop</p>
</blockquote>

<p><a href=""https://github.com/SintefRaufossManufacturing/python-urx/blob/193577c0efed8a24d00bd12b3b6e0c7ffefb9dd9/urx/urrobot.py#L356-L357"" rel=""nofollow noreferrer"">https://github.com/SintefRaufossManufacturing/python-urx/blob/193577c0efed8a24d00bd12b3b6e0c7ffefb9dd9/urx/urrobot.py#L356-L357</a></p>

<p>So it may not be possible to have the robot move continuously by issuing a bunch of commands, since it stops every time it gets a new command.</p>

<p>You need to figure out where the furthest place you want to move the robot to is, issue a command to move it there when the button is first pressed (with <code>wait=False</code>, so that your code doesn't sit around waiting for the robot to finish moving), and then use <code>self.r.stop()</code> to stop the robot whenever the button is released.</p>
",402891.0,1.0,0.0,,
3001,48302877,My inverse compositional homograhy image align cannot converge?,|c++|computer-vision|robotics|,"<p>I implement the algorithm based on <a href=""https://github.com/albertoCrive/homographyTrackingDemo"" rel=""nofollow noreferrer"">homographyTrackingDemo</a> and <a href=""http://www.ncorr.com/download/publications/bakerunify.pdf"" rel=""nofollow noreferrer"">LK-20-years</a>. It's adaption of inverse compositional LK algorithm for estimating affine transformation. There are several steps I have tried: </p>

<ol>
<li><p>Derive the derivative of template image;</p></li>
<li><p>Derive the derivative of homography transformation on template control points at identity transformation</p></li>
<li>Compute the Steepest Gradient Matrix and Hessian matrix</li>
</ol>

<p>These steps are precomputed before stepping into iteration and we iterate:</p>

<ol>
<li>Warp target image by homography</li>
<li>Compute the difference between warped target image and template image</li>
<li>Solve the linear equations and then update homography parameters</li>
</ol>

<p>I hope some guy can give me a hint about whether my understanding about this algorithm is wrong. The c++ code is as follows:</p>

<pre><code>AlignmentResults ICA::GaussNewtonMinimization(const StructOfArray2di &amp; pixelsOnTemplate, const vector&lt;Mat&gt; &amp; images, const vector&lt;Mat&gt; &amp; templates, const OptimizationParameters optParam, vector&lt;float&gt; &amp; parameters)
{
    AlignmentResults alignmentResults;
    alignmentResults.nIter = 0;
    alignmentResults.exitFlag = 1e6;

    //parameters must contain the initial guess. It is updated during optimization
    uint nChannels(images.size());
    vector&lt;vector&lt;float&gt; &gt; templatePixelIntensities(nChannels,vector&lt;float&gt;(pixelsOnTemplate.size()));//, templatePixelDx(nChannels,vector&lt;float&gt;(pixelsOnTemplate.size())), templatePixelDy(nChannels,vector&lt;float&gt;(pixelsOnTemplate.size()));
    vector&lt;Mat&gt; imageDx(nChannels), imageDy(nChannels);
    uint nParam = parameters.size();

    //vector&lt;Eigen::Matrix&lt;float, 2, N_PARAM&gt;, Eigen::aligned_allocator&lt;Eigen::Matrix&lt;float, 2, N_PARAM&gt; &gt; &gt; warpJacobians(pixelsOnTemplate.size());

    Eigen::MatrixXf sdImages(pixelsOnTemplate.size(), nParam);
    vector&lt;float&gt;  errorImage(pixelsOnTemplate.size(), 0.0);

    StructOfArray2di warpedPixels;
    Eigen::Matrix&lt;float, N_PARAM, N_PARAM&gt; hessian;
    Eigen::Matrix&lt;float, N_PARAM, N_PARAM&gt; hessianInv;
    hessian.setZero();


    std::vector&lt;Eigen::Matrix&lt;float, N_PARAM, N_PARAM&gt;, Eigen::aligned_allocator&lt;Eigen::Matrix&lt;float, N_PARAM, N_PARAM&gt;&gt;&gt; hessians(nChannels);
    std::vector&lt;Eigen::MatrixXf&gt; JacosT(nChannels);
    Eigen::Matrix&lt;float, N_PARAM, 1&gt; rhs;
    Eigen::Matrix&lt;float, N_PARAM, 1&gt; deltaParam;

    std::vector&lt;float&gt; zeroPara(8, 0.);
#pragma omp parallel for
    for(int iChannel = 0; iChannel&lt;nChannels; ++iChannel)
    {
        ComputeImageDerivatives(templates[iChannel], imageDx[iChannel], imageDy[iChannel]);
        hessians[iChannel].setZero();
        JacosT[iChannel] = Eigen::MatrixXf(N_PARAM, pixelsOnTemplate.size());

        for(int iPoint = 0; iPoint &lt; pixelsOnTemplate.size(); ++iPoint)
        {
            int pos = templates[iChannel].cols * pixelsOnTemplate.y[iPoint] + pixelsOnTemplate.x[iPoint];
            templatePixelIntensities[iChannel][iPoint] = ((float*)templates[iChannel].data)[pos];

            float Dx = ((float*)imageDx[iChannel].data)[pos];
            float Dy = ((float*)imageDy[iChannel].data)[pos];
            //printf(""%f %f\n"", Dx, Dy);
            Eigen::Matrix&lt;float, 2, N_PARAM&gt; warpJ;
            Homography::ComputeWarpJacobian(pixelsOnTemplate.x[iPoint], pixelsOnTemplate.y[iPoint], zeroPara, warpJ);
            //std::cout &lt;&lt; warpJ &lt;&lt; std::endl;
            Eigen::Matrix&lt;float, 1, N_PARAM&gt; J2H;
            Eigen::Matrix&lt;float, 1, 2&gt; ID;
            ID[0] = Dx;
            ID[1] = Dy;
            J2H = ID*warpJ;

            JacosT[iChannel].col(iPoint) = J2H.transpose();
            hessians[iChannel] += JacosT[iChannel].col(iPoint) * J2H;
        }
        hessian += hessians[iChannel];
    }

    while (alignmentResults.exitFlag == 1e6)
    {
        rhs.setZero();

        ComputeWarpedPixels(pixelsOnTemplate, parameters, warpedPixels);

#pragma omp parallel for
        for(int iChannel = 0; iChannel&lt;images.size(); ++iChannel)
        {
            ComputeResiduals(images[iChannel], templatePixelIntensities[iChannel], warpedPixels, errorImage);
            for (int i = 0; i&lt;nParam; ++i)
            {
                for(uint iPoint(0); iPoint&lt;pixelsOnTemplate.size(); ++iPoint)
                {
                    float val = (errorImage[iPoint] == std::numeric_limits&lt;float&gt;::infinity() ? 0: errorImage[iPoint]); 
                    rhs(i,0) -= (JacosT[iChannel])(i, iPoint) * val;
                }
            }
        }


        deltaParam = hessian.fullPivLu().solve(rhs);

        std::vector&lt;float&gt; vDelta(8, 0.), invDelta(8, 0.);
        for(size_t i = 0; i&lt;N_PARAM; ++i)
        {
            vDelta[i] = deltaParamd(i,0);
        }

        Homography::InverseWarpParameters(vDelta, invDelta);
        parameters = Homography::ParametersUpdateCompositional(parameters, invDelta);


        alignmentResults.poseIntermediateGuess.push_back(parameters);
        alignmentResults.residualNorm.push_back(ComputeResidualNorm(errorImage));
        if(alignmentResults.nIter &gt; 0)
            alignmentResults.exitFlag = CheckConvergenceOptimization(deltaParam.norm(), alignmentResults.nIter, abs(alignmentResults.residualNorm[alignmentResults.nIter] - alignmentResults.residualNorm[alignmentResults.nIter-1]), optParam);
        alignmentResults.nIter++;

    return alignmentResults;
}
</code></pre>
",2018-01-17 13:57:00,,150,0,1,1,,6409227.0,,6/1/2016 11:01,3.0,,,,,,83599710.0,"You should post your images, and visually show the initial solution and the final solution. This is an iterative optimization that can easily get stuck in a local minimum if your initial solution is far from the true solution."
3013,48801772,Gazebo model does not stand,|ros|robotics|,"<p>Apologies for a very long post. I created the following xacro file, when I load in gazebo using the following launch file, the robot does not stand straight and falls down. I tried with different values of mass for different links, but no luck. It looks like I am missing something, can anyone help? </p>

<pre><code>&lt;?xml version=""1.0"" ?&gt;

&lt;robot xmlns:xacro=""http://www.ros.org/wiki/xacro"" 
    xmlns:sensor=""http://playerstage.sourceforge.net/gazebo/xmlschema/#sensor""
        xmlns:controller=""http://playerstage.sourceforge.net/gazebo/xmlschema/#controller""
        xmlns:interface=""http://playerstage.sourceforge.net/gazebo/xmlschema/#interface""
    name=""rosbot_v1""&gt;

&lt;!--Formula for calculation of mass moment of inertia of a cylinder
is given by the following formula:
Reference: http://www.amesweb.info/SectionalPropertiesTabs/Mass-Moment-of-Inertia-Cylinder.aspx
Mass moment of inertia about x axis     Ix  Ix= (m/12) * (3r^2+h^2)
Mass moment of inertia about y axis     Iy  Iy= (m/12) * (3r^2+h^2)
Mass moment of inertia about z axis     Iz  Iz= (mr2)/2--&gt;

&lt;xacro:macro name=""inertial_matrix_cylinder"" params=""mass arm_radius arm_length""&gt;
               &lt;inertial&gt;
                       &lt;mass value=""${mass}"" /&gt;
                       &lt;inertia ixx=""${mass*(3*arm_radius*arm_radius+arm_length*arm_length)/12}"" 
                                ixy = ""0"" ixz = ""0""
                                iyy=""${mass*(3*arm_radius*arm_radius+arm_length*arm_length)/12}"" iyz = ""0""
                                izz=""${mass*arm_radius*arm_radius/2}"" /&gt;
               &lt;/inertial&gt;
&lt;/xacro:macro&gt;

&lt;!--Physical attributes definition for base box--&gt;

&lt;xacro:property name=""base_box_length"" value=""1"" /&gt;
&lt;xacro:property name=""base_box_width"" value=""1"" /&gt;
&lt;xacro:property name=""base_box_height"" value=""0.6"" /&gt;
&lt;xacro:property name=""base_box_mass"" value=""4"" /&gt;

&lt;!--Physical attributes definition for the swivel arm--&gt;
&lt;xacro:property name=""swivel_arm_length"" value=""0.2"" /&gt;
&lt;xacro:property name=""swivel_arm_radius"" value=""0.2"" /&gt;
&lt;xacro:property name=""swivel_arm_mass"" value=""1"" /&gt;

&lt;!--Physical attributes definition for the arms--&gt;
&lt;xacro:property name=""arm_length"" value=""1"" /&gt;
&lt;xacro:property name=""arm_radius"" value=""0.1"" /&gt;
&lt;xacro:property name=""arm_mass"" value=""0.1"" /&gt;

&lt;!--Physical attributes definition for gripper box--&gt;
&lt;xacro:property name=""gripper_box_length"" value=""0.5"" /&gt;
&lt;xacro:property name=""gripper_box_width"" value=""0.4"" /&gt;
&lt;xacro:property name=""gripper_box_height"" value=""0.2"" /&gt;
&lt;xacro:property name=""gripper_box_mass"" value=""0.01"" /&gt;

&lt;!--Physical attributes definition for gripper fingers--&gt;
&lt;xacro:property name=""gripper_finger_length"" value=""0.12"" /&gt;
&lt;xacro:property name=""gripper_finger_width"" value=""0.4"" /&gt;
&lt;xacro:property name=""gripper_finger_height"" value=""0.12"" /&gt;
&lt;xacro:property name=""gripper_finger_mass"" value=""0.001"" /&gt;

&lt;!--Formula for calculation of mass moment of inertia of a cuboid
is given by the following formula: a=x(length); b=y(width)
Mass moment of inertia about x axis     Ix  Ix= (M/12) * a^2
Mass moment of inertia about y axis     Iy  Iy= (M/12) * b^2
Mass moment of inertia about z axis     Iz  Iz= (1/12)*M*(a^2+b^2)--&gt;

&lt;xacro:macro name=""inertial_matrix_cuboid"" params=""mass box_length box_width""&gt;
               &lt;inertial&gt;
                       &lt;mass value=""${mass}"" /&gt;
                       &lt;inertia ixx=""${mass/12*(box_length*box_length)}"" 
                                ixy = ""0"" ixz = ""0""
                                iyy=""${mass/12*(box_width*box_width)}"" iyz = ""0""
                                izz=""${mass/12*(box_length*box_length + box_width*box_width)}"" /&gt;
               &lt;/inertial&gt;
&lt;/xacro:macro&gt;

&lt;material name=""blue""&gt;
  &lt;color rgba=""0 0 0.8 1""/&gt;
&lt;/material&gt;

&lt;material name=""white""&gt;
  &lt;color rgba=""1 1 1 1""/&gt;
&lt;/material&gt;

&lt;material name=""green""&gt;
  &lt;color rgba=""0 1 0 1""/&gt;
&lt;/material&gt;

&lt;material name=""cyan""&gt;
  &lt;color rgba=""0 1 1 1""/&gt;
&lt;/material&gt;

&lt;!-- world link --&gt;
&lt;link name=""base_link""/&gt;

&lt;link name=""rosbot_base""&gt;
    &lt;xacro:inertial_matrix_cuboid mass=""${base_box_mass}"" box_length=""${base_box_length}"" box_width=""${base_box_width}""/&gt;
    &lt;collision name=""rosbot_collision""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""0  0  0""/&gt;
      &lt;geometry&gt;
        &lt;box size=""${base_box_length} ${base_box_width} ${base_box_height}""/&gt;
      &lt;/geometry&gt;
    &lt;/collision&gt;
    &lt;visual name=""rosbot_visual""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""0  0  0""/&gt;
      &lt;geometry&gt;
        &lt;box size=""${base_box_length} ${base_box_width} ${base_box_height}""/&gt;
      &lt;/geometry&gt;
      &lt;material name=""blue""/&gt;
    &lt;/visual&gt;
&lt;/link&gt;

&lt;!-- base_link and its fixed joint --&gt;
&lt;joint name=""joint_fix"" type=""fixed""&gt;
    &lt;parent link=""base_link""/&gt;
    &lt;child link=""rosbot_base""/&gt;
&lt;/joint&gt;

&lt;!-- A swiveling base on which next arm will sit --&gt;
&lt;link name=""rosbot_swivel_base""&gt;
    &lt;xacro:inertial_matrix_cylinder mass=""${swivel_arm_mass}"" arm_length=""${swivel_arm_length}"" arm_radius=""${swivel_arm_radius}""/&gt;
    &lt;collision name=""rosbot_collision""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""0   0  ${swivel_arm_length/2}""/&gt;
      &lt;geometry&gt;
        &lt;cylinder length=""${swivel_arm_length}"" radius=""${swivel_arm_radius}""/&gt;
      &lt;/geometry&gt;
    &lt;/collision&gt;
    &lt;visual name=""rosbot_visual""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""0   0  ${swivel_arm_length/2}""/&gt;
      &lt;geometry&gt;
        &lt;cylinder length=""${swivel_arm_length}"" radius=""${swivel_arm_radius}""/&gt;
      &lt;/geometry&gt;
      &lt;material name=""white""/&gt;
    &lt;/visual&gt;
&lt;/link&gt;

&lt;!-- The joint between swivel and base needs to be flush on the top face of rosbot_base  --&gt;
&lt;joint name=""rosbot_base_swivel_joint"" type=""revolute""&gt;
  &lt;parent link=""rosbot_base""/&gt;
  &lt;child link=""rosbot_swivel_base""/&gt;
  &lt;origin rpy=""0  0  0"" xyz=""0  0  ${base_box_height/2}""/&gt;
  &lt;axis xyz=""0  0  1""/&gt;
  &lt;limit effort=""100"" lower=""-1.57"" upper=""1.57"" velocity=""100""/&gt;
&lt;/joint&gt;

&lt;!-- A moving/manipulating arm1 --&gt;
&lt;link name=""rosbot_arm1""&gt;
    &lt;xacro:inertial_matrix_cylinder mass=""${arm_mass}"" arm_length=""${arm_length}""  arm_radius=""${arm_radius}""/&gt;
    &lt;collision name=""rosbot_collision""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""0   0   ${arm_length/2}""/&gt;
      &lt;geometry&gt;
        &lt;cylinder length=""${arm_length}"" radius=""${arm_radius}""/&gt; 
      &lt;/geometry&gt;
    &lt;/collision&gt;
    &lt;visual name=""rosbot_visual""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""0   0   ${arm_length/2}""/&gt;
      &lt;geometry&gt;
        &lt;cylinder length=""${arm_length}"" radius=""${arm_radius}""/&gt; 
      &lt;/geometry&gt;
      &lt;material name=""blue""/&gt;
    &lt;/visual&gt;
&lt;/link&gt;

&lt;!-- The joint between swivel and arm1 needs to be at the height of swivel link  --&gt;
&lt;joint name=""rosbot_swivel_arm1_joint"" type=""revolute""&gt;
  &lt;parent link=""rosbot_swivel_base""/&gt;
  &lt;child link=""rosbot_arm1""/&gt;
  &lt;origin rpy=""0  0  0"" xyz=""0  0  ${swivel_arm_length}""/&gt;
  &lt;axis xyz=""1  0  0""/&gt;
  &lt;limit effort=""100"" lower=""-1.57"" upper=""1.57"" velocity=""100""/&gt;
&lt;/joint&gt;

&lt;!-- A moving/manipulating arm2 --&gt;
&lt;link name=""rosbot_arm2""&gt;
    &lt;xacro:inertial_matrix_cylinder mass=""${arm_mass}"" arm_length=""${arm_length}""  arm_radius=""${arm_radius}""/&gt;
    &lt;collision name=""rosbot_collision""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""0   0   ${arm_length/2}""/&gt;
      &lt;geometry&gt;
        &lt;cylinder length=""${arm_length}"" radius=""${arm_radius}""/&gt; 
      &lt;/geometry&gt;
    &lt;/collision&gt;
    &lt;visual name=""rosbot_visual""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""0   0   ${arm_length/2}""/&gt;
      &lt;geometry&gt;
        &lt;cylinder length=""${arm_length}"" radius=""${arm_radius}""/&gt; 
      &lt;/geometry&gt;
      &lt;material name=""blue""/&gt;
    &lt;/visual&gt;
&lt;/link&gt;

&lt;!-- The joint between arm1 and arm2 needs to be at height of arm1  --&gt;
&lt;joint name=""rosbot_arm1_arm2_joint"" type=""revolute""&gt;
  &lt;parent link=""rosbot_arm1""/&gt;
  &lt;child link=""rosbot_arm2""/&gt;
  &lt;origin rpy=""0  0  0"" xyz=""0  0  ${arm_length}""/&gt;
  &lt;axis xyz=""1  0  0""/&gt;
  &lt;limit effort=""100"" lower=""-1.57"" upper=""1.57"" velocity=""100""/&gt;
&lt;/joint&gt;

&lt;!-- A gripper box, which holds the gripper joints --&gt;
&lt;link name=""rosbot_gripper_box""&gt;
    &lt;xacro:inertial_matrix_cuboid mass=""${gripper_box_mass}"" box_length=""${gripper_box_length}"" box_width=""${gripper_box_width}""/&gt;
    &lt;collision name=""rosbot_collision""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""0   0  ${gripper_box_height/2}""/&gt;
      &lt;geometry&gt;
        &lt;box size=""${gripper_box_length} ${gripper_box_width} ${gripper_box_height}""/&gt;
      &lt;/geometry&gt;
    &lt;/collision&gt;
    &lt;visual name=""rosbot_visual""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""0   0  ${gripper_box_height/2}""/&gt;
      &lt;geometry&gt;
        &lt;box size=""${gripper_box_length} ${gripper_box_width} ${gripper_box_height}""/&gt;
      &lt;/geometry&gt;
       &lt;material name=""cyan""/&gt;
    &lt;/visual&gt;
&lt;/link&gt;

&lt;!-- The joint between arm2 and gripper needs to be at height of arm2  --&gt;
&lt;joint name=""rosbot_arm2_gripper_joint"" type=""revolute""&gt;
  &lt;parent link=""rosbot_arm2""/&gt;
  &lt;child link=""rosbot_gripper_box""/&gt;
  &lt;origin rpy=""0  0  0"" xyz=""0  0  ${arm_length}""/&gt;
  &lt;axis xyz=""0  0  1""/&gt;
  &lt;limit effort=""100"" lower=""-1.57"" upper=""1.57"" velocity=""100""/&gt;
&lt;/joint&gt;

&lt;!-- The left gripper  --&gt;
&lt;link name=""rosbot_lgripper""&gt;
    &lt;xacro:inertial_matrix_cuboid mass=""${gripper_finger_mass}"" box_length=""${gripper_finger_length}"" box_width=""${gripper_finger_width}""/&gt;
    &lt;collision name=""rosbot_collision""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""-0.0  0.20  ${gripper_finger_height/2}""/&gt;
      &lt;geometry&gt;
        &lt;box size=""${gripper_finger_length} ${gripper_finger_width} ${gripper_finger_height}""/&gt;
      &lt;/geometry&gt;
    &lt;/collision&gt;
    &lt;visual name=""rosbot_visual""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""-0.0  0.20  ${gripper_finger_height/2}""/&gt;
      &lt;geometry&gt;
        &lt;box size=""${gripper_finger_length} ${gripper_finger_width} ${gripper_finger_height}""/&gt;
      &lt;/geometry&gt;
      &lt;material name=""green""/&gt;
    &lt;/visual&gt;
  &lt;/link&gt;

&lt;!-- The joint between gripper box and gripper needs to be at origin/slightly higher than origin of gripper box  --&gt;
&lt;joint name=""rosbot_lgripper_joint"" type=""prismatic""&gt;
    &lt;parent link=""rosbot_gripper_box""/&gt;
    &lt;child link=""rosbot_lgripper""/&gt;
    &lt;origin rpy=""0  0  0"" xyz=""-0.2  0.2  0.02""/&gt;
    &lt;axis xyz=""1  0  0""/&gt;
    &lt;limit effort=""100"" lower=""0"" upper=""0.14"" velocity=""100""/&gt;
&lt;/joint&gt;

&lt;!-- The right gripper  --&gt;
&lt;link name=""rosbot_rgripper""&gt;
    &lt;xacro:inertial_matrix_cuboid mass=""${gripper_finger_mass}"" box_length=""${gripper_finger_length}"" box_width=""${gripper_finger_width}""/&gt;
    &lt;collision name=""rosbot_collision""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""-0.0  0.20  ${gripper_finger_height/2}""/&gt;
      &lt;geometry&gt;
        &lt;box size=""${gripper_finger_length} ${gripper_finger_width} ${gripper_finger_height}""/&gt;
      &lt;/geometry&gt;
    &lt;/collision&gt;
    &lt;visual name=""rosbot_visual""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""-0.0  0.20  ${gripper_finger_height/2}""/&gt;
      &lt;geometry&gt;
        &lt;box size=""${gripper_finger_length} ${gripper_finger_width} ${gripper_finger_height}""/&gt;
      &lt;/geometry&gt;
      &lt;material name=""green""/&gt;
    &lt;/visual&gt;
  &lt;/link&gt;

&lt;!-- The joint between gripper box and gripper needs to be at origin or slightly higher than origin of gripper box  --&gt;
&lt;joint name=""rosbot_rgripper_joint"" type=""prismatic""&gt;
    &lt;parent link=""rosbot_gripper_box""/&gt;
    &lt;child link=""rosbot_rgripper""/&gt;
    &lt;origin rpy=""0  0  0"" xyz=""0.2  0.2  0.02""/&gt;
    &lt;axis xyz=""1  0  0""/&gt;
    &lt;limit effort=""100"" lower=""-0.14"" upper=""0"" velocity=""100""/&gt;
&lt;/joint&gt;

&lt;/robot&gt;
</code></pre>

<p>When I edit the model in Gazebo and then toggle the static flag, it seems to be stable.</p>
",2018-02-15 07:17:00,,1035,1,0,1,,9333030.0,India,2/8/2018 12:17,91.0,48805293.0,"<p>I could solve this issue by adding the friction and damping elements to the URDF file.</p>

<pre><code>&lt;xacro:property name=""damping_value"" value=""10"" /&gt;
&lt;xacro:property name=""friction_value"" value=""0.1"" /&gt;
</code></pre>

<p>Sample usage of the property in joint is given below,</p>

<pre><code>&lt;!-- The joint between swivel and base needs to be flush on the top face of rosbot_base  --&gt;
&lt;joint name=""rosbot_base_swivel_joint"" type=""revolute""&gt;
  &lt;parent link=""rosbot_base""/&gt;
  &lt;child link=""rosbot_swivel_base""/&gt;
  &lt;origin rpy=""0  0  0"" xyz=""0  0  ${base_box_height/2}""/&gt;
  &lt;axis xyz=""0  0  1""/&gt;
  &lt;limit effort=""100"" lower=""-1.57"" upper=""1.57"" velocity=""100""/&gt;
  &lt;dynamics damping=""${damping_value}"" friction=""${friction_value}""/&gt;
&lt;/joint&gt;
</code></pre>
",9333030.0,0.0,0.0,,
3211,52668674,"For a robot in a maze, infer path from detected edges of walls",|python|opencv|computer-vision|robotics|,"<p>I am working on Robot Vision for navigating in a maze. I am pretty new to OpenCV and so far I have managed to read a test image of a maze as seen from the robot's eye view, detect the edges using Canny edge detection, focus on a Region of Interest and using HoughLinesP Transform detect where the walls meet the floor and draw a blue line.</p>

<p><a href=""https://i.stack.imgur.com/ST8QW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ST8QW.jpg"" alt=""Blue lines inferred in the maze""></a></p>

<p>What I need to do now is calculate the center point between the two blue lines which are closer to the robot's camera and then do the same for the two lines which are more in the center of the image. Afterwards I want to connect the two center points to obtain a line.</p>

<p>Next step is to get the robot to follow this superimposed line.</p>

<p>Any help would be great :-)</p>

<p>Attached is an image created with my current script.</p>

<p>Here is my code:</p>

<pre><code># Original ideas abd code from 
# https://towardsdatascience.com/finding-driving-lane-line-live-with-opencv-f17c266f15db
# Testing edge detection for maze

import cv2
import numpy as np

image = cv2.imread(""/home/pi/opencv/maze_test_images/maze1.png"")
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
kernel_size = 5
blur_gray = cv2.GaussianBlur(gray,(kernel_size,kernel_size),0)
low_threshold = 50
high_threshold = 150

edges = cv2.Canny(blur_gray, low_threshold, high_threshold)

# create a mask of the edges image using cv2.filpoly()
mask = np.zeros_like(edges)
ignore_mask_color = 255

# define the Region of Interest (ROI) - source code sets as a trapezoid for roads
imshape = image.shape

vertices = np.array([[(0,imshape[0]),(100, 420), (1590, 420),(imshape[1],imshape[0])]], dtype=np.int32)

cv2.fillPoly(mask, vertices, ignore_mask_color)
masked_edges = cv2.bitwise_and(edges, mask)

# mybasic ROI bounded by a blue rectangle

#ROI = cv2.rectangle(image,(0,420),(1689,839),(0,255,0),3)

# define the Hough Transform parameters
rho = 2 # distance resolution in pixels of the Hough grid
theta = np.pi/180 # angular resolution in radians of the Hough grid
threshold = 15     # minimum number of votes (intersections in Hough grid cell)
min_line_length = 40 #minimum number of pixels making up a line
max_line_gap = 30    # maximum gap in pixels between connectable line segments

# make a blank the same size as the original image to draw on
line_image = np.copy(image)*0 

# run Hough on edge detected image
lines = cv2.HoughLinesP(masked_edges, rho, theta, threshold, np.array([]),min_line_length, max_line_gap)

for line in lines:
        for x1,y1,x2,y2 in line:
            cv2.line(line_image,(x1,y1),(x2,y2),(255,0,0),10)

# draw the line on the original image 
lines_edges = cv2.addWeighted(image, 0.8, line_image, 1, 0)
#return lines_edges

coord = np.where(np.all(lines_edges == (255,0,0), axis=-1))
print zip(coord[0], coord[1])


cv2.imshow(""original"", image)
cv2.waitKey(0)

cv2.imshow(""edges"", edges)
cv2.waitKey(0)

cv2.imshow(""detected"", lines_edges)
cv2.waitKey(0)

cv2.imwrite(""lanes_detected.jpg"", lines_edges)
cv2.destroyAllWindows()
</code></pre>
",2018-10-05 15:13:00,,1234,0,3,3,,10462374.0,"England, UK",10/5/2018 14:53,11.0,,,,,,92267931.0,I have added my code. What I am trying to do is be able to plot a line central to the walls of the maze (The lines now highlighted in blue) and get the robot to follow this central line. Just like a line follower but with a virtual line.
3227,53307599,Difference between Evolutionary Strategies and Reinforcement Learning?,|deep-learning|reinforcement-learning|robotics|evolutionary-algorithm|,"<p>I am learning about the approach employed in Reinforcement Learning for robotics and I came across the concept of Evolutionary Strategies. But I couldn't understand how  RL and ES are different. Can anyone please explain?</p>
",2018-11-14 19:36:00,53348627.0,6077,3,0,12,0.0,9761439.0,,5/8/2018 22:25,67.0,53348627.0,"<p>To my understanding, I know of two main ones.</p>

<p><strong>1)</strong> Reinforcement learning uses the concept of one agent, and the agent learns by interacting with the environment in different ways.  In evolutionary algorithms, they usually start with many ""agents"" and only the ""strong ones survive"" (the agents with characteristics that yield the lowest loss).</p>

<p><strong>2)</strong> Reinforcement learning agent(s) learns both positive and negative actions, but evolutionary algorithms only learns the optimal, and the negative or suboptimal solution information are discarded and lost.  </p>

<p><strong><em>Example</em></strong></p>

<p>You want to build an algorithm to regulate the temperature in the room.</p>

<p>The room is 15 C, and you want it to be 23 C. </p>

<p>Using Reinforcement learning, the agent will try a bunch of different actions to increase and decrease the temperature.  Eventually, it learns that increasing the temperature yields a good reward.  But it also learns that reducing the temperature will yield a bad reward.</p>

<p>For evolutionary algorithms, it initiates with a bunch of random agents that all have a preprogrammed set of actions it is going to do.  Then the agents that has the ""increase temperature"" action survives, and moves onto the next generation.  Eventually, only agents that increase the temperature survive and are deemed the best solution.  However, the algorithm does not know what happens if you decrease the temperature.</p>

<p><strong>TL;DR:</strong> RL is usually one agent, trying different actions, and learning and remembering all info (positive or negative).  EM uses many agents that guess many actions, only the agents that have the optimal actions survive.  Basically a brute force way to solve a problem.</p>
",9191460.0,16.0,0.0,,
3248,53683579,ROS Kinetic Installation Error on OSX Mojave,|python|installation|homebrew|ros|robotics|,"<p>During the process of installing ROS kinetic on my mac. I've been trying to resolve dependencies using the following commands:</p>

<pre><code>$ cd ~/ros_catkin_ws
$ rosinstall_generator ros_comm --rosdistro kinetic --deps --wet-only --tar &gt; kinetic-ros_comm-wet.rosinstall
$ wstool init -j8 src kinetic-ros_comm-wet.rosinstall

$ rosdep install --from-paths src --ignore-src --rosdistro kinetic -y # resolves dependancies
</code></pre>

<p>I get the following error:</p>

<pre><code>Error: No available formula with the name ""gtest"" 

ERROR: Rosdep experienced an internal error.
Please go to the rosdep page [1] and file a bug report with the message below.
[1] : http://www.ros.org/wiki/rosdep

rosdep version: 0.13.0

Bad installer [homebrew]: Error while parsing brew info for 'gtest'
 * Output of `brew info gtest --json=v1`:

 * Error while parsing:
 Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/site-packages/rosdep2/platforms/osx.py"", line 203, in is_installed
    pkg_info = json.loads(std_out)
  File ""/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/__init__.py"", line 339, in loads
    return _default_decoder.decode(s)
  File ""/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/decoder.py"", line 364, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/decoder.py"", line 382, in raw_decode
    raise ValueError(""No JSON object could be decoded"")
ValueError: No JSON object could be decoded
</code></pre>

<p>I've tried uninstalling and reinstalling several things, but I'm not sure what the actual issue is and how I can fix this.</p>
",2018-12-08 14:38:00,,775,1,3,2,,7426971.0,,1/16/2017 18:45,7.0,54100441.0,"<p>As also specified in the <a href=""http://wiki.ros.org/kinetic/Installation/OSX/Homebrew/Source"" rel=""nofollow noreferrer"">guide</a>, <em>Google Mock</em> is not compatible with OS X so you should skip it adding <code>--skip-keys google-mock</code> to <code>rosdep install</code></p>
",4922045.0,1.0,0.0,94290114.0,"Tried it and got: `Error: No available formula with the name ""gtest"" `"
3135,50750013,Jackal simulation in ROS kinetic - getting the error: bash: cd: jackal_ws: No such file or directory,|bash|simulation|ros|robotics|,"<p>I'm trying to simulate the Jackal via ROS Kinetic on Gazebo following this tutorial, <a href=""https://gist.github.com/vfdev-5/57a0171d8f5697831dc8d374839bca12"" rel=""nofollow noreferrer"">https://gist.github.com/vfdev-5/57a0171d8f5697831dc8d374839bca12</a></p>

<p>I have done the following steps:</p>

<pre><code>sudo apt-get install ros-kinetic-robot-localization ros-kinetic-controller-manager ros-kinetic-joint-state-controller ros-kinetic-diff-drive-controller ros-kinetic-gazebo-ros ros-kinetic-gazebo-ros-control ros-kinetic-gazebo-plugins             ros-kinetic-lms1xx ros-kinetic-pointgrey-camera-description ros-kinetic-roslint ros-kinetic-amcl ros-kinetic-gmapping      ros-kinetic-map-server ros-kinetic-move-base ros-kinetic-urdf ros-kinetic-xacro ros-kinetic-message-runtime ros-kinetic-topic-tools ros-kinetic-teleop-twist-joy    
</code></pre>

<p>and</p>

<pre><code>mkdir -p jackal_ws/src; cd jackal_ws/src; catkin_init_workspace
git clone https://github.com/jackal/jackal.git
git clone https://github.com/jackal/jackal_simulator.git
git clone https://github.com/jackal/jackal_desktop.git
git clone https://github.com/ros-visualization/interactive_marker_twist_server.git
</code></pre>

<p>After the above step, I got: </p>

<pre><code>bash: cd: jackal_ws: No such file or directory
</code></pre>

<p>Then, I did the next step below:</p>

<pre><code>cd jackal_ws; catkin_make; source devel/setup.bash
</code></pre>

<p>After the above step, I got: </p>

<pre><code>bash: cd: jackal_ws: No such file or directory  
Base path: /home/USER/jackal_ws/src  
The specified source space ""/home/USER/jackal_ws/src/src"" does not exist   
bash: devel/setup.bash: No such file or directory
</code></pre>

<p>What have I done wrong?</p>

<hr>

<p>I redid the steps separately, and here is what I get in the terminal: </p>

<p>user@user:~$ sudo apt-get install ros-kinetic-robot-localization ros-kinetic-controller-manager ros-kinetic-joint-state-controller ros-kinetic-diff-drive-controller ros-kinetic-gazebo-ros ros-kinetic-gazebo-ros-control ros-kinetic-gazebo-plugins             ros-kinetic-lms1xx ros-kinetic-pointgrey-camera-description ros-kinetic-roslint ros-kinetic-amcl ros-kinetic-gmapping      ros-kinetic-map-server ros-kinetic-move-base ros-kinetic-urdf ros-kinetic-xacro ros-kinetic-message-runtime ros-kinetic-topic-tools ros-kinetic-teleop-twist-joy</p>

<p>[sudo] password for user: </p>

<p>E: Could not get lock /var/lib/dpkg/lock - open (11: Resource temporarily unavailable)</p>

<p>E: Unable to lock the administration directory (/var/lib/dpkg/), is another process using it?</p>

<p>user@user:~$ sudo rm /var/lib/dpkg/lock</p>

<p>user@user:~$ sudo dpkg --configure -a</p>

<p>dpkg: error: dpkg status database is locked by another process</p>

<p>user@user:~$ sudo apt-get install ros-kinetic-robot-localization ros-kinetic-controller-manager ros-kinetic-joint-state-controller ros-kinetic-diff-drive-controller ros-kinetic-gazebo-ros ros-kinetic-gazebo-ros-control ros-kinetic-gazebo-plugins             ros-kinetic-lms1xx ros-kinetic-pointgrey-camera-description ros-kinetic-roslint ros-kinetic-amcl ros-kinetic-gmapping      ros-kinetic-map-server ros-kinetic-move-base ros-kinetic-urdf ros-kinetic-xacro ros-kinetic-message-runtime ros-kinetic-topic-tools ros-kinetic-teleop-twist-joy</p>

<p>E: Could not get lock /var/lib/dpkg/lock - open (11: Resource temporarily unavailable)</p>

<p>E: Unable to lock the administration directory (/var/lib/dpkg/), is another process using it?</p>

<p>user@user:~$ mkdir -p jackal_ws/src</p>

<p>user@user:~$ cd jackal_ws/src</p>

<p>user@user:~/jackal_ws/src$ catkin_init_workspace</p>

<p>File ""/home/user/jackal_ws/src/CMakeLists.txt"" already </p>

<p>existsuser@user:~/jackal_ws/src$ git clone </p>

<p><a href=""https://github.com/jackal/jackal"" rel=""nofollow noreferrer"">https://github.com/jackal/jackal</a>. </p>

<p>fatal: destination path 'jackal' already exists and is not an empty directory.</p>

<p>user@user:~/jackal_ws/src$ git clone </p>

<p><a href=""https://github.com/jackal/jackal_simulator.git"" rel=""nofollow noreferrer"">https://github.com/jackal/jackal_simulator.git</a></p>

<p>fatal: destination path 'jackal_simulator' already exists and is not an empty directory.</p>

<p>user@user:~/jackal_ws/src$ git clone </p>

<p><a href=""https://github.com/jackal/jackal_desktop.git"" rel=""nofollow noreferrer"">https://github.com/jackal/jackal_desktop.git</a></p>

<p>fatal: destination path 'jackal_desktop' already exists and is not an empty directory.</p>

<p>user@user:~/jackal_ws/src$ git clone <a href=""https://github.com/ros-visualization/interactive_marker_twist_server.git"" rel=""nofollow noreferrer"">https://github.com/ros-visualization/interactive_marker_twist_server.git</a></p>

<p>fatal: destination path 'interactive_marker_twist_server' already exists and is not an empty directory.</p>

<p>user@user:~/jackal_ws/src$ cd jackal_ws</p>

<p>bash: cd: jackal_ws: No such file or directory</p>
",2018-06-07 20:54:00,,467,0,7,0,,9841724.0,,5/24/2018 14:49,8.0,,,,,,88533311.0,"@thatotherguy, I redid the steps, but after the line (cd jackal_ws), I get the error: bash: cd: jackal_ws: No such file or directory."
3058,49612231,Collision avoidance system - image interpretation,|computer-vision|robotics|,"<p><br>
I am refining an algorithm for collision avoidance I wrote for an ASV using a monocular camera.<br></p>

<p>In order to have a simple yet effective system, the algorithm relies on edge detection using a <em>Canny</em> filter, after proper erosion/dilation to remove noise and shading spots.<br>
The final image is a black and white frame where, starting from the bottom, the first contour found and the upper part gets coloured in white while the rest is black: so black can be interpreted as ""free space"" whereas white as ""obstacle"".</p>

<p>The rover should use this reactive collision avoidance system to steer accordingly, avoiding close obstacles.</p>

<p>Now I am having some problems to decide how to actually find the way to go.</p>

<p>This is my approach so far.<br>
The frame is split in columns whose width is exactly 1 relative bearing, then columns are grouped in three super-columns (corresponding to the directions <em>turn left, go on, turn right</em>) and the robot steers on the direction where the most black space is found.</p>

<p>Do you have any other idea on how to find where to steer/proceed given the output of this algorithm? <br>Thank you</p>

<p><strong>Image processing example</strong></p>

<ol>
<li>initial frame</li>
</ol>

<p><a href=""https://i.stack.imgur.com/w41gc.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w41gc.jpg"" alt=""initial frame""></a></p>

<ol start=""2"">
<li>filtering</li>
</ol>

<p><a href=""https://i.stack.imgur.com/kMOyL.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kMOyL.jpg"" alt=""filtering""></a></p>

<ol start=""3"">
<li>occupancy frame</li>
</ol>

<p><a href=""https://i.stack.imgur.com/p1kVK.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p1kVK.jpg"" alt=""occupancy frame""></a></p>

<ol start=""4"">
<li>column divisions</li>
</ol>

<p><a href=""https://i.stack.imgur.com/vMbIA.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vMbIA.jpg"" alt=""column divisions""></a></p>

<ol start=""5"">
<li>command -> steer left</li>
</ol>
",2018-04-02 13:27:00,,70,0,0,1,0.0,7123392.0,"Rome, Metropolitan City of Rome, Italy",11/6/2016 19:13,48.0,,,,,,,
3149,51063970,Python fast calculating and slow Serial writing : Multithread or Multiprocess,|python|multithreading|multiprocessing|robotics|,"<p>I have a robotics project, basically a path tracking problem.
In PC, a reference generation algorithm is implemented in Python3.65. The algorithm takes Indoor GPS data and use these continuously-updated data to calculate the reference path for a robot-car. Of course, the algorithm runs in a 
<strong><em>while True:
   ....</em></strong>
framework.
The algorithm can work well only if the sampling frequency is quite high, say 0.001s. 
However, the problem is that, after calculating the reference path, the path info needs to be written to Serial port of the PC, byte by byte, via Serial.write(). 
This serial.write() function is still a for loop. And this writing process is very slow (more than 0.02s for 16 bytes). If the for loop is included in the while True: framework, something like</p>

<hr>

<pre><code>while True:
  Data = Ref_generation()
  Bytes_Full = Float_2_Bytes_List(Data)
  for ele in Bytes_Full:
    Serial.write(ele)  # VERY SLOW!!!
  sleep(0.001)
</code></pre>

<hr>

<p>Then, the Data can not be calculated correctly since the cycle period is much longer than 0.001s. </p>

<p>In a nutshell, how can I separate the fast calculating algorithm from the slow serial.wtite()? I tried multithreads, but not works. </p>

<p>Anyhelp will be appreciated, thanks a lot!</p>
",2018-06-27 13:24:00,51064156.0,1686,2,0,1,0.0,9438864.0,,3/3/2018 16:26,35.0,51064062.0,"<p>You don't need to leverage multiple cpu cores, all you want is to <strong>wait</strong> for the serial port... Your CPU will be idle, <strong>waiting</strong>... Spawning new threads/processes to just <strong>wait</strong> is a waste...</p>

<p>That's why you should try to use some asynchronous IO solution.</p>

<p>Example, use <a href=""https://github.com/pyserial/pyserial-asyncio"" rel=""nofollow noreferrer"">https://github.com/pyserial/pyserial-asyncio</a> or <a href=""https://twistedmatrix.com/documents/16.1.0/api/twisted.internet.serialport.SerialPort.html"" rel=""nofollow noreferrer"">https://twistedmatrix.com/documents/16.1.0/api/twisted.internet.serialport.SerialPort.html</a> </p>

<p>These asynchronous frameworks allow you to register events and have your function called automatically when they finish, <strong>all in a single thread/process</strong>.</p>

<p>They also allow you to schedule events in the time you want. </p>
",17160.0,1.0,5.0,,
3152,51089252,Using serializing an object in python for use with an XBee,|python|serialization|deserialization|ros|robotics|,"<p>For a project I'm working on, I'm supposed to use XBee radio modules, which isn't super important, except that I have to read and write to their serial port in order to use them. I'm currently working with Python and ROS, so I'm attempting to send TransformStamped messages over the XBees.</p>

<p>My question is, unless I'm misunderstanding how Serial.read() and Serial.write() work, how can I tell how many bytes to read? I was planning on using Pickle to serialize the data into a string, and then sending that over the serial ports. Is there a better way that I've overlooked? Is there some sort of loop that would work to read data until the end of the pickled string is read?</p>
",2018-06-28 18:27:00,51144344.0,193,1,0,1,,7901933.0,"Denver, CO, United States",4/21/2017 14:11,15.0,51144344.0,"<p>The short answer is, serial.read() cannot tell you how many bytes to read. Either you have some prior knowledge as to how long the message is, or the data you send has some means of denoting the boundaries between messages. </p>

<p>Hint; knowing how long a message is is not enough, you also need to know whereabouts in the received byte stream a message has actually started. You don't know for sure that the bytes received are exactly aligned with the sent bytes: you may not have started the receiver before the transmitter, so they can be out of step. </p>

<p>With any serialisation one has to ask, is it self delimiting, or not? Google Protocol buffers are not. I don't think Pickle is either. ASN.1 BER is, at least to some extent. So is XML.</p>

<p>The point is that XBee modules are (assuming you're using the ones from Digi) just unreliable byte transports, so whatever you put through them has to be delimited in some way so that the receiving end knows when it has a complete message. Thus if you pickle or Google Protocol Buf your message, you need some other way of framing the serialised data so that the receiving end knows it has a complete message (i.e. it's seen the beginning <em>and</em> end). This can be as simple as some byte pattern (e.g. 0xffaaccee00112233) used to denote the end of one message and the beginning of the next, chosen so as to be unlikely to occur in the sent messages themselves. Your code at the receiving end would read and discard data until is saw that pattern, would then read subsequent data into a buffer until it saw that pattern again, and only then would it attempt to de-pickle / de-GPB the data back into an object. </p>

<p>With ASN.1 BER, the data stream itself incorporates effectively the same thing, saving you the effort. It uses tags, values and length fields to tell its decoders about the incoming data, and if the incoming data makes no sense to the decoder in comparison to the original schema, incorrectly framed data is easily ignored.</p>

<p>This kind of problem also exists on tcp sockets, though at least with those delivery is more or less guaranteed (the first bytes you receive are the first bytes sent). A Digimesh connection does not quite reach the same level of guaranteed delivery as a tcp socket, so something else is needed (like a framing byte pattern) for the receiving application to know that it is synchronised with the sender. </p>
",2147218.0,1.0,1.0,,
3122,50158870,How to get started with robotics having no idea what it is,|robotics|,"<p>I wanted to learn robotics, Actually i want to build a robot not very complex but atleast that makes sense of what it does. So basically im a cs student who knows only about programming and nothing about microcontroller or anything remotely close to that particular subject. I know little about electronics but never dug deep into that. </p>

<p>So i need an advice on how to start on my journey of learning robotics where i have no idea about the above mentioned fields </p>

<p>Ive seen in lot of article where u have to learn maths (obviously) and adruino. I've  seen them telling to use adruino ide and even use python language or c/c++
But atleast for now what would be the difderence between these two. Does adruino directly compile the code to machine level language or it is just that its easier to do so in adruino</p>

<p>Thanks for going through the post</p>
",2018-05-03 15:33:00,50184942.0,259,2,0,-2,,9376844.0,"Bangalore, Karnataka, India",2/18/2018 12:59,11.0,50184942.0,"<p>In order to start with Robotics, You should know the fundamentals of machine learning that are</p>

<ol>
<li><a href=""https://medium.com/@equipintelligence/computer-vision-giving-vision-to-machines-fd777c1c678c"" rel=""nofollow noreferrer"">Computer Vision </a></li>
<li><a href=""https://medium.com/@equipintelligence/bayes-text-classification-machine-learning-algorithms-915d320abcdb"" rel=""nofollow noreferrer"">Natural language processing </a></li>
<li><a href=""https://medium.com/@equipintelligence/artificial-neural-networks-mapping-the-human-brain-2e0bd4a93160"" rel=""nofollow noreferrer"">Artificial Neural networks </a></li>
<li>Deep understanding about learning       methods</li>
</ol>

<p>The above points refer to the software which controls the robot. </p>
",,0.0,1.0,,
3232,53314997,solve a coupled ODE of euler angels Rotation vector of a body,|matlab|robotics|,"<p>i'm trying to solve coupled ODEs by using matlab ode45 function:</p>

<p><a href=""https://i.stack.imgur.com/gRVvu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gRVvu.png"" alt=""enter image description here""></a></p>

<p>Here is my function called 'Rot' to describe these ODE's for using matlab ode45.</p>

<pre><code>function omega= Rot(t,y)
omega(2,1)=(0.03*sin(3*t)*((cos(Y(1)))^2)+0.002*t^3*sin(y(1)))...
/-((cos(Y(1)))^2)+((sin(Y(1)))^2);
omega(1,1)=((0.002*t^2-omega(2,1)*sin(y(1)))...
/-cos(y(3))*sin(y(2)))*cos(y(2))+0.01*t^2+0.3*t;
omega(3,1)=(0.002*t^2-omega(2,1)*sin(y(1)))...
/-cos(y(3))*sin(y(2));
</code></pre>

<p>but I'm getting ""Not enough input arguments."" error.</p>
",2018-11-15 08:15:00,,142,1,7,1,,10259947.0,"Tunis, Tunisia",8/22/2018 13:23,9.0,53342309.0,"<p>OK, so by expressing <code>theta_dot</code> as a function of the other variables in equation (3) and injecting the result in equation (2), I get (pseudo-code):</p>

<p><code>phi_dot = (0.03*sin(psi)*sin(3*t) - 0.002*t^2 * cos(psi)) / (sin(theta)*(cos(psi))^2 + sin(theta) * sin(psi) * sin(phi))</code></p>

<p>So makes that the first equation of your ODE file as it only depends on time and the state vector.</p>

<p>Then your second equation in the ODE file is:</p>

<p><code>psi_dot = -phi_dot * cos(theta) + 0.01*t^2 + 0.3*t</code></p>

<p>which is OK because you've calcuated <code>phi_dot</code> in the previous equation.</p>

<p>And finally the last equation in your ODE file:</p>

<p><code>theta_dot = (-0.03*sin(3*t) + phi_dot * sin(theta) * sin(phi)) / cos(psi);</code></p>

<p>which is also OK because you have calculated <code>phi_dot</code> in your first equation.</p>

<p>You can then pass this to the ODE solver and it should work. (do check my maths though)</p>
",2257388.0,0.0,0.0,93517212.0,Yes. It is because of that.
3104,49842838,Multiple systems sharing resources on multiple SoC's,|raspberry-pi|mpi|distributed-computing|hpc|robotics|,"<p>I have some Raspberry Pi's from previous projects/learning and I would like to pool their resources to make adifferential drive robot.</p>

<p>Two Pi's would have one camera each for avision system, one connected to an Arduino to read analog sensors, one for driving motors, and the last pi is the ""control"" and hosting a user interface (web app). Nothing really special here! But I would like to be able to share the resources of all the Pi's for improved performance...</p>

<p>My thoughts on sharing resources is one of two approaches:</p>

<p>1) Use distributed memcached as a RAM cluster and run each sub systemon one CPUonly to avoid data races.</p>

<p>or</p>

<p>2) Use a messaging layer to distribute processing on all CPU.</p>

<p>To avoid a lot of headache, I thought I could use MPI since it does a lot of heaving lifting when it comes to messaging.However I can't seem to find any examples of any robotics projcets using MPI.</p>

<p>It looks like MPI is <em>simplest</em> to design when it's for supervised learning, or genomics (samecode and large data sets).</p>

<p>In my case, each sub system runs very different code from the other. But for example, the vision system runs the same code on a stream of hundred/thousand images. So why not use MPI for the vision, and let the ""contorl"" schedule when its starts / stops.</p>

<p>Then use its output as input for the next system, which also runs the same code, so can be paralleled.</p>

<p>So my question is:</p>

<blockquote>
  <p>Is there a reason why MPI is not a common approach for things like
  this in Robotics? If so, why and what is a good alternative?</p>
</blockquote>

<p>There's a CUDA-MPI for GPU's so maybe this approach is not too far fetched?</p>
",2018-04-15 14:06:00,,78,0,3,0,,1158977.0,"Boston, MA, USA",1/19/2012 16:38,663.0,,,,,,86701939.0,"I agree! It's a course of action I am excited about, but that doesn't make it the right one"
3166,51346929,"How to convert 2D(x,y) coordinates to 3D(x,y,z) coordinates using MATLAB?",|matlab|image-processing|computer-vision|camera-calibration|robotics|,"<p>I am trying to automate an robotic arm using MATLAB. So the thing is, camera will be mounted on the base of robotic arm. It will capture the snapshots of the camera frames and do image processing on it and it will detect the target(object) and find the pixel coordinate for it. After it is done, this pixel coordinates should be <strong>mapped to real world x-y-z metric coordinates?</strong> and this real world coordinates(x,y,z) would serve as parameters for inverse kinematics function which would give the value of theta so that servos can move.</p>

<p>I'm stuck  here,  this pixel coordinates should be <strong>mapped to real world x-y-z metric coordinates?</strong> i dont know how to do it. nor getting any ideas, how to proceed it? 
Anyone having any lead, please share it!!</p>

<p>PS anyone of you guyz thinks that for automation of robotic arm should i use MATLAB or something else?. Coz this all code would be uploaded on raspberry pi 3 using ROS environment.</p>

<p>BEST REGARDS</p>

<p>hitesh kumar</p>
",2018-07-15 09:09:00,,2439,1,5,0,0.0,8740644.0,India,10/8/2017 11:14,17.0,51698370.0,"<p>To calculate 3D world point for the given pixel in an image, you need depth information (should use 3D camera like Kinect ..etc). One you have depth information and camera intrinsics and extrinsics you can convert 2D pixels to 3D world coordinates and vice versa.</p>

<p>Below equation for calcualting X,Y,Z in world coordinates.</p>

<p><a href=""https://i.stack.imgur.com/wPOhs.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wPOhs.gif"" alt=""enter image description here""></a></p>

<p>Technically <code>Perspective projection</code> is what camera does in converting 3D world to 2D and below equation represents this projection. </p>

<p><a href=""https://i.stack.imgur.com/2P1jS.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2P1jS.jpg"" alt=""enter image description here""></a></p>

<p>Above image is from <a href=""https://www.cc.gatech.edu/classes/AY2016/cs4476_fall/results/proj3/html/agartia3/index.html"" rel=""nofollow noreferrer"">here</a></p>
",1595504.0,1.0,0.0,89666909.0,"@seralouk I have seen them but some says that you need estimate camera intrinsics, extrinsics, and lens distortion parameters."
3047,49353430,running python code using single key presses,|python|raspberry-pi|robot|,"<p>Currently building Petrol-powered RC car controlled by a raspberry pi and 16ch adafruit servo controller Pi hat. Pretty novice query from a beginner but how can simple Python commands be carried out by a single key press. E.g. Holding the ""w"" key on a keyboard to run ""pwm.setPWM(0, 0, servoMax)"". (In order for the servo to push the throttle to move the vehicle forward). What follows is the code currently used:</p>

<pre><code>#!/usr/bin/python

from Adafruit_PWM_Servo_Driver import PWM
import time

pwm = PWM(0x40)

servoMin = 150
servoMax = 600

def setServoPulse(channel, pulse):
 pulseLength = 1000000
 pulseLength /= 60
 print ""%d us per period"" % pulseLength
 pulseLength /= 4096
 print ""%d us per bit"" % pulseLength
 pulse *= 1000
 pulse /= pulseLength
 pwm.setPWM(channel, 0, pulse)

pwm.setPWMFreq(60)
While (True): 
 pwm.setPWM(0, 0, servoMin)   #throttle servo set to off position -should be default 
 pwm.setPWM(0, 0, servoMAX)   #throttle servo set on -to be run by ""W"" key
 pwm.setPWM(1, 0, servoMin)   #steering servo left -by holding ""A"" key
 pwm.setPWM(1, 0, servoMax)   #steering servo right -by holding ""D"" key
</code></pre>

<p>I would assume the answer involves If and ElseIf commands, but I really would just like to run a program then input() keyboard presses to run the code.</p>
",2018-03-18 22:23:00,,442,2,2,-1,0.0,9513500.0,,3/18/2018 21:37,3.0,49407640.0,"<p>1)you can first of all make a infinite while loop.</p>

<p>2) after take input via row input</p>

<p>3) then after apply condition for which keyword is found then which function is called</p>

<p>4) now call the function if condition is true.</p>
",9219170.0,0.0,0.0,85707265.0,Have you seen this solution SO already ? [link](https://stackoverflow.com/questions/15855168/create-a-raw-input-with-commands-inside-a-python-script) also.. there are a few of these questions already answered asking for similar things and people seem to mention the python [cmd](https://docs.python.org/2/library/cmd.html). Ive never used it before but it might be what your looking for.
3072,49674179,Understanding Inverse Kinematics pybullet,|robotics|bulletphysics|inverse-kinematics|,"<p>I'm trying to do <strong>cartesian control</strong> with a simulated PR2 robot in <strong>pybullet</strong>.
In pybullet, the function <em>calculateInverseKinematics(...)</em> optionally takes joint lower limits, upper limits, joint ranges and rest poses in order to do null space control. </p>

<p>First of all, what practical benefit do you get using null space control instead of ""regular"" inverse kinematics? </p>

<p>Secondly, why do you need to specify joint ranges, isn't that fully determined by the lower and upper limits? What is the range of a continuous joint? </p>

<p>What exactly are rest poses? Is it just the initial pose before the robot starts to do a task?</p>
",2018-04-05 13:55:00,,6633,1,0,6,0.0,3925668.0,,8/9/2014 18:44,8.0,49769256.0,"<p>There are often many solutions to the Inverse Kinematics problem. Using the null space allows you to influence the IK solution, for example closer to a rest pose.</p>

<p>By default, the PyBullet IK doesn't use the limits from the URDF file, hence you can explicitly specify the desired ranges for the IK solution. A continuous joint has the full 360 degree range.</p>

<p>Check the <a href=""https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA/edit#"" rel=""nofollow noreferrer"">PyBullet</a> user manual and there are several examples how to use inverse kinematics with PyBullet:</p>

<p><a href=""https://github.com/bulletphysics/bullet3/tree/master/examples/pybullet/examples"" rel=""nofollow noreferrer"">https://github.com/bulletphysics/bullet3/tree/master/examples/pybullet/examples</a>
(just use git checkout <a href=""https://github.com/bulletphysics/bullet3"" rel=""nofollow noreferrer"">https://github.com/bulletphysics/bullet3</a> and go to examples/pybullet/examples)</p>

<p>There is also an additional PyBullet IK example for the Sawyer robot here:
<a href=""https://github.com/erwincoumans/pybullet_robots"" rel=""nofollow noreferrer"">https://github.com/erwincoumans/pybullet_robots</a></p>
",295157.0,1.0,1.0,,
2982,48110189,how to get wheel encoder count from turtlebot's /odom topic,|ros|robotics|,"<p>turtlebot's odom topic gives:</p>

<pre><code>header:    seq: 406289   stamp: 
    secs: 4392
    nsecs: 160000000   frame_id: odom child_frame_id: base_footprint pose:   pose: 
    position: 
      x: 1.56701645246e-05
      y: -9.82132735628e-06
      z: 0.0
    orientation: 
      x: 0.0
      y: 0.0
      z: -0.548275342929
      w: 0.836297882537   covariance: [0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1000000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1000000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1000000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05] twist:    twist: 
    linear: 
      x: -2.67171244095e-06
      y: 0.0
      z: 0.0
    angular: 
      x: 0.0
      y: 0.0
      z: -0.000185729678152   covariance: [0.0, 0.0, 0.0, 0.0, 0.0,
0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
</code></pre>

<p>I'd like to find the wheel encoder count from this data. How does turtlebot use wheel encoder values to compute positions? Where can I find the code that does this?</p>
",2018-01-05 08:56:00,,288,0,0,1,,5192123.0,United States,8/5/2015 3:38,418.0,,,,,,,
3193,52084828,Improving controls of Raspberry Pi RC robot,|python|raspberry-pi|robotics|,"<p>I'm currently working on a basic Raspberry pi project and I need help with improving the controls for it.</p>

<p>This is my current code:</p>

<pre><code>import RPi.GPIO as GPIO
import curses

GPIO.setmode(GPIO.BOARD)
GPIO.setup(7,GPIO.OUT)
GPIO.setup(11,GPIO.OUT)
GPIO.setup(13,GPIO.OUT)
GPIO.setup(15,GPIO.OUT)

screen = curses.initscr()
curses.noecho()
curses.cbreak()
screen.keypad(True)

try:
    while True:
        char = screen.getch()
        if char == curses.KEY_UP:
            GPIO.output(11,True)
            GPIO.output(13,True)
        elif char == ord('s'): 
            GPIO.output(7,False)
            GPIO.output(11,False)
            GPIO.output(13,False)
            GPIO.output(15,False)
            break
        elif char == curses.KEY_DOWN:
            GPIO.output(7,True)
            GPIO.output(15,True)
        elif char == curses.KEY_RIGHT:
            GPIO.output(7,True)
            GPIO.output(13,True)
        elif char == curses.KEY_LEFT:
            GPIO.output(11,True)
            GPIO.output(15,True)
        elif char == 10:
            GPIO.output(7,False)
            GPIO.output(11,False)
            GPIO.output(13,False)
            GPIO.output(15,False)
finally:
    curses.nocbreak()
    screen.keypad(False)
    curses.echo()
    curses.endwin()
    GPIO.cleanup()
</code></pre>

<p>The problem that I have is that it requires me to press the enter key to stop the motors and change direction and the way that I would like to have it set up is so that, for example, when the up key is <strong>pressed and held</strong> the motor moves forward and <strong>once the key is released the motors stop.</strong></p>
",2018-08-29 19:30:00,,169,1,0,0,,10291731.0,,8/29/2018 19:09,1.0,52414697.0,"<p>The problem with your code is it only changes the GPIO when a button is pushed. This is because <code>getch()</code> What you want is to </p>

<p>In order to get the behavior you want you need to have <code>getch()</code> return a value even when no button is pressed. There are two ways to do this</p>

<ol>
<li><strong>nodelay</strong>: this makes getch() return immediately, whether or not a key is pressed, if no key is pressed it will return <em>curses.ERR</em></li>
<li><strong>halfdelay</strong>: this makes getch() wait upto a set amount of time, if no key is pressed in that time period it returns <em>curses.ERR</em></li>
</ol>

<p>The similarity is both will return <em>curses.ERR</em> when no button is pushed, so you need to add that case to your if statement</p>

<pre><code>#this will shut off the GPIO when nothing is pushed
elif char == curses.ERR:
    GPIO.output(7,False)
    GPIO.output(11,False)
    GPIO.output(13,False)
    GPIO.output(15,False)
</code></pre>

<p>But that code won't work until you do the setup. Replace the <strong>cbreak</strong> mode with whichever mode we are using</p>

<h1>Method 1: nodelay</h1>

<p>The advantage of this method is that it is nearly instantaneous. As soon as you let go of a key the bot should stop. 
Disadvantage, this may consume a lot of processing power. Since it will execute the loop very quickly, set GPIOs very quickly, read key quickly, etc it will do many cycles while not achieving much real work</p>

<pre><code>screen = curses.initscr()
curses.noecho()
curses.nodelay(True)
screen.keypad(True)
</code></pre>

<h1>Method 2: halfdelay</h1>

<p>The advantage of this method is it should lower the processor usage compared to nodelay method. 
The disadvantage is there is a delay after you release a key. So after  you release the key the bot would continue executing the previous command for some time (in my sample code .7 seconds) before it stops</p>

<pre><code>#the delay units are tenths of seconds, or deciseconds
#this is a .7 second delay 7*0.10sec = 0.7 seconds
delay_in_deciseconds = 7 

screen = curses.initscr()
curses.noecho()
curses.halfdelay(delay_in_deciseconds)
screen.keypad(True)
</code></pre>
",2705382.0,0.0,0.0,,
3108,49880973,Room coverage in Answer Set Programming,|robotics|answer-set-programming|clingo|,"<p>I'm currently developing an Answer Set Programming problem, consisting in a robot that is needed to cover a room avoiding obstacles and reach a Goal point when all the room is covered.
My idea was to transform the room map into asp predicates,in the form of room/3, being the parameters:</p>

<ul>
<li>X:x coord</li>
<li>Y:y coord</li>
<li>V:Value of the point in the room, being 0(initial point),1(point to cover),2(Obstacle),3(Goal point)</li>
</ul>

<p>One of the criteria that the program must meet is to cover every point with a value of 1,which can be achieved with a constraint, but I do not know how to model the robot movement. My idea was to use a predicate of the form move/1,with up,down,left or right.</p>

<p>Can anybody help me guiding me in how to model this problem?</p>

<pre><code>    void map_to_asp(std::ofstream&amp; file,std::vector&lt;std::vector&lt;char&gt;&gt;&amp; room)
{
  std::cout &lt;&lt; room.size() &lt;&lt; "","" &lt;&lt; room[0].size() &lt;&lt; std::endl;
  for(int i = 0; i &lt; room.size(); i++)
  {
    for(int j = 0;j &lt; room[0].size(); j++)
    {
      switch(room[i][j])
      {
        case '@':
        file &lt;&lt; ""initial("" &lt;&lt; i+1 &lt;&lt; "","" &lt;&lt; j+1 &lt;&lt; "").\n"";
        break;
        case '.':
        file &lt;&lt; ""toClean("" &lt;&lt; i+1 &lt;&lt; "","" &lt;&lt; j+1 &lt;&lt; "").\n"";
        break;
        case '#':
        file &lt;&lt; ""obstacle("" &lt;&lt; i+1 &lt;&lt; "","" &lt;&lt; j+1 &lt;&lt; "").\n"";
        break;
        case 'X':
        file &lt;&lt; ""goal("" &lt;&lt; i+1 &lt;&lt; "","" &lt;&lt; j+1 &lt;&lt; "").\n"";
        break;
      }
    }
  }
}
</code></pre>

<p>Thank you in advance.</p>
",2018-04-17 14:37:00,,227,1,2,1,,9659283.0,,4/17/2018 14:25,2.0,50087368.0,"<p>If your goal is to have a model for each possible path, a simple way to go is to make an iterative progression in the graph.</p>

<p>We need first to define all positions we can move in (we are actually building a graph before solving any problem):</p>

<pre><code>position(X,Y,S):- room(X,Y,S) ; not S=2.
</code></pre>

<p>Now we decide where we can go from any position (edges of the graph):</p>

<pre><code>edge((X,Y),(I,J)):- position(X,Y,_) ; position(I,J,_) ; |X-I|=0..1 ; |Y-J|=0..1 ; |X-I|+|Y-J|=1..2 .
</code></pre>

<p>Note that we consider that the graph is undirected (not necessarily true, if there is a slide in your room for instance).
Let's define some constants:</p>

<pre><code>#const start_pos=(1,1).
#const goal=(5,5).
#const path_maxlen=100.
</code></pre>

<p>We obviously start at the starting point:</p>

<pre><code>path(1,start_pos).
</code></pre>

<p>And now, we recursively indicate that there is a next way to go, with a limit to avoid too useless solutions.</p>

<pre><code>0{path(N+1,E): path(N,S), edge(S,E), S!=goal}1:- path(N,_) ; N&lt;path_maxlen.
</code></pre>

<p>We have to avoid all useless paths.</p>

<pre><code>% a path that do not join the end is illegal.
:- path(N,E) ; not path(N+1,_) ; not E=goal.

% a path must go by all milestone (example of milestone: milestone(2,14)).
:- not path(_,(X,Y)): milestone(X,Y).
</code></pre>

<p>We want the shortest path:</p>

<pre><code>last_step(N):- path(N,_) ; not path(N+1,_).
#minimize{N: last_step(N)}.
</code></pre>

<p>The full code is available <a href=""https://github.com/Aluriak/learning-ASP/blob/master/path-search.lp"" rel=""nofollow noreferrer"">here</a>.</p>

<hr>

<p>As a side-notes:</p>

<ul>
<li>since we don't use them, you could (should) take rid of all room/3 that describe an obstacle.</li>
<li>you could also make you goal point artificial (out of the room, but the real goad is linked to it) in order to allow your path to pass by the real goal, without stopping. Using that, you can achieve support for multiple goal.</li>
</ul>
",3077939.0,0.0,0.0,86778187.0,Do you have an example of what you have done so far?
2980,48109985,ros gazebo skipped loading plugin with error,|ros|robotics|,"<p>When I run my gazebo, I get the following error:</p>

<pre><code>[ERROR] [1515141508.242475977]: Skipped loading plugin with error: XML Document '/opt/ros/kinetic/share/gmapping/nodelet_plugins.xml' has no Root Element. This likely means the XML is malformed or missing..
[ERROR] [1515141508.249164933]: Skipped loading plugin with error: XML Document '/opt/ros/kinetic/share/gmapping/nodelet_plugins.xml' has no Root Element. This likely means the XML is malformed or missing..
</code></pre>

<p>Even with the errors, gazebo seems to work fine but I would like to fix it just to be safe. However, I'm not sure what the error messages mean and how I can fix it. </p>
",2018-01-05 08:41:00,,2009,2,0,0,,5192123.0,United States,8/5/2015 3:38,418.0,48337790.0,"<p>Can you verify if there is any ""nodelet_plugins.xml"" file is present in the /opt/ros/kinetic/share/gmapping directory?</p>
",9238248.0,0.0,0.0,,
3010,48598961,Explaining environments in Roboschool Half-Cheetah,|environment|robotics|reinforcement-learning|openai-gym|,"<p>I have some questions regarding the roboschool Half-Cheetah.</p>

<ol>
<li><p>I see that the observation space for Half-Cheetah is 26. Can anyone tell me what is each value for?- I only counted 18. (also, some of the values seem to remain 0 for all timesteps)</p></li>
<li><p>In the half_cheetah.xml under roboschool/mujoco_assets, there is the following comment:</p>

<p>Cheetah Model</p></li>
</ol>

<p>The state space is populated with joints in the order that they are
defined in this file. The actuators also operate on joints.</p>

<pre><code>State-Space (name/joint/parameter):
    - rootx     slider      position (m)
    - rootz     slider      position (m)
    - rooty     hinge       angle (rad)
    - bthigh    hinge       angle (rad)
    - bshin     hinge       angle (rad)
    - bfoot     hinge       angle (rad)
    - fthigh    hinge       angle (rad)
    - fshin     hinge       angle (rad)
    - ffoot     hinge       angle (rad)
    - rootx     slider      velocity (m/s)
    - rootz     slider      velocity (m/s)
    - rooty     hinge       angular velocity (rad/s)
    - bthigh    hinge       angular velocity (rad/s)
    - bshin     hinge       angular velocity (rad/s)
    - bfoot     hinge       angular velocity (rad/s)
    - fthigh    hinge       angular velocity (rad/s)
    - fshin     hinge       angular velocity (rad/s)
    - ffoot     hinge       angular velocity (rad/s)


Actuators (name/actuator/parameter):
    - bthigh    hinge       torque (N m)
    - bshin     hinge       torque (N m)
    - bfoot     hinge       torque (N m)
    - fthigh    hinge       torque (N m)
    - fshin     hinge       torque (N m)
    - ffoot     hinge       torque (N m)
</code></pre>

<p>Could you please confirm to me if the order presented here is the same with the order they appear in the observation matrix? If so, should I take the values that are always 0 into account?</p>

<p>Thank you.</p>
",2018-02-03 15:27:00,,1239,0,2,4,0.0,7151522.0,,11/13/2016 1:01,3.0,,,,,,92680869.0,"Looks like you got some answers on Github, maybe you can create an answer with your findings so others know: https://github.com/openai/roboschool/issues/142"
3604,58982694,Turning Issue With Vex Robot C++,|c++|robotics|,"<p>When we use the joystick with axises 1 and 2 to turn, when we turn left the robot turns right and when we turn right the robot turns left. We've tried switching different values to make some negative to reverse this but nothing seems to work. We've also tried making the left motor reversed instead of the right motor and it corrected the problem but the forward and backward are switched. </p>

<pre><code>  while (true) { 
      int rightSpeed= (Controller1.Axis3.position(vex::pct) + 
                              (Controller1.Axis1.position(vex::pct) + Controller1.Axis4.position(vex::pct)));

      int leftSpeed= (Controller1.Axis1.position(vex::pct) - 
                              (Controller1.Axis2.position(vex::pct) + Controller1.Axis3.position(vex::pct)));

      if (leftSpeed&gt;15||leftSpeed&lt;-15){
          RightMotor.spin(vex::directionType::fwd, leftSpeed, vex::velocityUnits::pct);   
      }

      if (rightSpeed&gt;15||rightSpeed&lt;-15){
           LeftMotor.spin(vex::directionType::fwd, rightSpeed, vex::velocityUnits::pct);
           RightMotor.spin(vex::directionType::fwd, -leftSpeed, vex::velocityUnits::pct);    
      }  
  }
</code></pre>
",2019-11-21 19:47:00,,126,0,2,0,,11857443.0,,7/30/2019 11:58,5.0,,,,,,104656027.0,The best option to get a good result would be to go to the vex forums (vexforums.com) and there is a wider range of people there who know the answers to the questions
3611,59101547,How to pass a function as an argument to other function? My code is given below,|python|function|class|object|robotics|,"<p>this is the code for RRT algorithm </p>

<p>import sys
import pygame
import random, math
from math import sqrt, atan2, cos, sin
from pygame.locals import *</p>

<p>class RRT(object):</p>

<pre><code>x = 0
y = 0
X_dimension = 0
Y_dimension = 0
Window_size = 0
EPS         = 0
Max_nodes   = 0 
nodes       = list()
K_ESCAPE    = True
KEYUP       = True
QUIT        = True

def __init__(self,x,y):
    self.x = x
    self.y = y

#parameters
    self.X_dimension = 1280                                #length of the window
    self.Y_dimension = 1280                                  #breadth of the window
    self.Window_size = [self.X_dimension, self.Y_dimension]  #Window size
    self.EPS         =  7000                                #EPSILON or Incremental Distance 
    self.Max_nodes   = 100                                  #maximum number of nodes
    self.nodes       = list()
    self.QUIT        = QUIT
    self.KEYUP       = KEYUP
    self.K_ESCAPE    = K_ESCAPE


#function for calculating euclidean distance
def Calculate_Distance(self,x,y):

    x = [10,20]
    y = [15,30]  
    return sqrt((x[0]-y[0])*(x[0]-y[0])+(x[1]-y[1])*(x[1]-y[1]))

    pass 

def Initiate_Sampling(self,x,y):

    self.EPS = 7000 

    if Calculate_Distance(x,y) &lt; 7000:
        return y
    else:
        theta = atan2(y[1]-x[1], y[0]-x[0])
        return x[0] + self.EPS*cos(theta), x[1] + self.EPS*sin(theta)

#Function for displaying the output
def Start_The_Game(self):

    pygame.init()

    screen = pygame.display.set_node(Window_size)

    caption = pygame.display.set_caption(""performing RRT"")

    white = 255, 240, 200
    black = 20, 20, 40
    screen.fill(black)

#Main Function
def Node_Generation(self, nodes):
    self.nodes      = []
    self.QUIT       = QUIT
    self.KEYUP      = KEYUP
    self.K_ESCAPE   = K_ESCAPE

    #nodes.append(X_dimension/2.0, Y_dimension/2.0)

    nodes.append(0.0, 0.0)
    pygame.init()

for i in range(Max_nodes):
    rand = random.random()*640.0, random.random()*480.0
    nn = nodes[0]

for p in nodes:
    if dist(p,rand) &lt; dist(nn,rand):
        nn = p
        newnode = step_from_to(nn,rand)
        nodes.append(newnode)
        pygame.draw.line(screen,white,nn,newnode)
        pygame.display.update()
        print (i, ""  "", nodes)

for j in pygame.event.get():
    if j.type == QUIT or (j.type == KEYUP and j.key == K_ESCAPE):
        pygame.quit()
        sys.exit(""GAME OVER"")
</code></pre>

<p>path = RRT(10,15)</p>

<p>path.Calculate_Distance(10,15)</p>

<p>path.Initiate_Sampling(path.Calculate_Distance(10,15))</p>

<p>path.Start_The_Game()</p>

<p>path.Node_Generation()        </p>

#

<p>My query - I want to pass Calculate_Distance function as an argument to the Initiate_Sampling function to compare it with EPS.</p>
",2019-11-29 08:52:00,59101588.0,35,1,0,0,,12455645.0,,11/29/2019 8:47,7.0,59101588.0,"<p>You can access member function via <code>self</code> the same way you would access other class members within the class. i.e. </p>

<pre><code>self.Calculate_Distance
</code></pre>
",9006027.0,0.0,0.0,,
3285,54606363,Simple rotation of a rectangle along a curve in matlab,|matlab|matlab-figure|robotics|,"<p>I'm trying to rotate a rectangle(polyshape) in matlab to orient itself along the curve(a set of points). So far this is my code.</p>

<pre><code>l=2;w=1;xc=-1;yc=2;
xvs= [xc+w/2 xc+w/2 xc-w/2 xc-w/2];
yvs= [yc-l/2 yc+l/2 yc+l/2 yc-l/2];
ax = gca;
polyin = polyshape(xvs,yvs); % %w/2,h/2% polyin = rectangle('Position',[-0.1 -0.1 0.2, 0.4]);
k=2;
% t = acos((y(k-1)*x(k-1)+y(k)*x(k))/(norm([x(k-1) y(k-1)])*norm([x(k) y(k)])));
t = atan(y(k)/x(k));
polyout = translate(polyin,[-xc -yc]);
polyout = translate(polyout,[x(1) y(1)]);
polyout = rotate(polyout, t, [x(2) y(2)]);
plot(polyout);
axis([-4 4 0 50]);
prev_t = atan(y(2)/x(2));
for k=2:length(x)
    cla();
    t = atan((y(k)-y(k-1))/(x(k)-x(k-1)));
%     t = acos((y(k-1)*x(k-1)+y(k)*x(k))/(norm([x(k-1) y(k-1)])*norm([x(k) y(k)])));
%     t = atan((y(k-1))/(x(k-1)));
    polyout=translate(polyout,x(k)-x(k-1),y(k)-y(k-1));
    [t3,t4]=centroid(polyout);
    t-prev_t
    polyout=rotate(polyout,rad2deg(t-prev_t) ,[t3, t4] );%, );
    prev_t = t;
    plot(polyout);
    hold on;
    plot(x(1:k),y(1:k));
    hold on; 
    quiver(x(k), y(k), 1, 1);
    axis([-4 4 0 50]);
    drawnow;
end
</code></pre>

<p>x,y is listed below</p>

<pre><code>x = [-1 -1.00972272933410   -1.01870478051530   -1.02695805761983   -1.03449438259277   -1.04132549543428   -1.04746305438566   -1.05291863611549   -1.05770373590578   -1.06182976783809   -1.06530806497965   -1.06814987956949   -1.07036638320459   -1.07196866702597   -1.07296774190485   -1.07337453862878   -1.07319990808774   -1.07245462146029   -1.07114937039970   -1.06929476722009   -1.06690134508251   -1.06397955818112   -1.06053978192930   -1.05659231314578   -1.05214737024076   -1.04721509340205   -1.04180554478121   -1.03592870867963   -1.02959449173473   -1.02281272310602   -1.01559315466128   -1.00794546116266   -0.999879240452816  -0.991404013641053  -0.982529225289427  -0.973264243598892  -0.963618360595424  -0.953600792316151  -0.943220678995479  -0.932487085251223  -0.921409000270737  -0.909995337997038  -0.898254937314939  -0.886196562237176  -0.873828902090537  -0.861160571701991  -0.848200111584816  -0.834955988124727  -0.821436593766009  -0.807650247197639  -0.793605193539421  -0.779309604528111  -0.764771578703544  -0.749999141594771  -0.735000245906179  -0.719782771703621  -0.704354526600550  -0.688723245944142  -0.672896593001429  -0.656882159145424  -0.640687464041251  -0.624319955832276  -0.607787011326233  -0.591095936181355  -0.574253965092497  -0.557268261977274  -0.540145920162182  -0.522893962568731  -0.505519341899570  -0.488028940824620  -0.470429572167200  -0.452727979090155  -0.434930835281988  -0.417044745142986  -0.399076243971348  -0.381031798149319  -0.362917805329311  -0.344740594620036  -0.326506426772637  -0.308221494366812  -0.289891921996946  -0.271523766458236  -0.253123016932825  -0.234695595175927  -0.216247355701956  -0.197784085970655  -0.179311506573228  -0.160835271418462  -0.142360967918861  -0.123894117176775  -0.105440174170524  -0.0870045279405308 -0.0685925017754495 -0.0502093533982922 -0.0318602751525592 -0.0135503941883670 0.00471522735142232 0.0229315921450729  0.0410937675058457  0.0591968851958715  0.0772361412400196  0.0952067957397721  0.113104172687091   0.130923659778296   0.148660708227926   0.166310832582623   0.183869610534992   0.201332682737479   0.218695752616237   0.235954586185006   0.253105011858973   0.270142920268653   0.287064264073754   0.303865057777052   0.320541377538260   0.337089360987901   0.353505207041178   0.369785175711844   0.385925587926079   0.401922825336353   0.417773330135305   0.433473604869608   0.449020212253846   0.464409774984379   0.479638975553221   0.494704556061906   0.509603318035363   0.524332122235782   0.538887888476493   0.553267595435832   0.567468280471012   0.581487039431996   0.595321026475369   0.608967453878208   0.622423591851952   0.635686768356277   0.648754368912964   0.661623836419772   0.674292670964308   0.686758429637899   0.699018726349463   0.711071231639382   0.722913672493370   0.734543832156346   0.745959549946308   0.757158721068200   0.768139296427783   0.778899282445511   0.789436740870399   0.799749788593894   0.809836597463747   0.819695394097884   0.829324459698281   0.838722129864827   0.847886794409204   0.856816897168752   0.865510935820345   0.873967461694257   0.882185079588040   0.890162447580389   0.897898276845016   0.905391331464524   0.912640428244272   0.919644436526252   0.926402278002956   0.932912926531251   0.939175407946249   0.945188799875176   0.950952231551246   0.956464883627534   0.961725987990839   0.966734827575568   0.971490736177594   0.975993098268136   0.980241348807631   0.984234973059598   0.987973506404514   0.991456534153687   0.994683691363123   0.997654662647400   1.00036918199354    1.00282703257487    1.00502804656492    1.00697210495126    1.00865913734939    1.01008912181662    1.01126208466593    1.01217810027981    1.01283729092421    1.01323982656233    1.01338592466854    1.01327585004223    1.01290991462170    1.01228847729801    1.01141194372885    1.01028076615244    1.00889544320138    1.00725651971651    1.00536458656082    1.00322028043328    1.00082428368272    0.998177324121733   0.995280174840507   0.992133654020716   0.988738624749389   0.985095994832775   0.981206716610224   0.977071786768050   0.972692246153409   0.968069179588162   0.963203715682755   0.958097026650084   0.952750328119371   0.947164878950031   0.941341981045545   0.935282979167332   0.928989260748622   0.922462255708321   0.915703436264890   0.908714316750211   0.901496453423459   0.894051444284976   0.886380928890139   0.878486588163234   0.870370144211326   0.862033360138129   0.853478039857879   0.844706027909207   0.835719209269005   0.826519509166304   0.817108892896138   0.807489365633420   0.797662972246815   0.787631797112606   0.777397963928569   0.766963635527843   0.756331013692799   0.745502338968920   0.734479890478659   0.723265985735320   0.711862980456928   0.700273268380098   0.688499281073906   0.676543487753763   0.664408395095284   0.652096547048159   0.639610524650029   0.626952945840348   0.614126465274265   0.601133774136485   0.587977599955151   0.574660706415706   0.561185893174769   0.547555995674006   0.533773884953999   0.519842467468121   0.505764684896401   0.491543513959404   0.477181966232097   0.462683087957718   0.448049959861653   0.433285696965303   0.418393448399959   0.403376397220668   0.388237760220109   0.372980787742464   0.357608763497285   0.342125004373370   0.326532860252634   0.310835713823975   0.295036980397154   0.279140107716656   0.263148575775570   0.247065896629457   0.230895614210219   0.214641304139976   0.198306573544930   0.181895060869242   0.165410435688903   0.148856398525599   0.132236680660593   0.115555043948583   0.0988152806315876  0.0820212131528061  0.0651766939704940  0.0482856053718364  0.0313518592868137  0.0143793971020798  -0.00262781052517255    -0.0196657638533383 -0.0367304342425269 -0.0538177643406945 -0.0709236682697698 -0.0880440318117838 -0.105174712595000  -0.122311540280039  -0.139450316746014  -0.156586816276652  -0.173716785746427  -0.190835944806689  -0.207939986071788  -0.225024575305211  -0.242085351605702  -0.259117927593395  -0.276117889595944  -0.293080797834649  -0.310002186610585  -0.326877564490733  -0.343702414494106  -0.360472194277880  -0.377182336323518  -0.393828248122908  -0.410405312364482  -0.426908887119349  -0.443334306027426  -0.459676878483560  -0.475931889823665  -0.492094601510844  -0.508160251321521  -0.524124053531569  -0.539981199102440  -0.555726855867290  -0.571356168717112  -0.586864259786862  -0.602246228641589  -0.617497152462564  -0.632612086233408  -0.647586062926218  -0.662414093687702  -0.677091168025301  -0.691612253993325  -0.705972298379074  -0.720166226888972  -0.734188944334692  -0.748035334819290  -0.761700261923330  -0.775178568891010  -0.788465078816297  -0.801554594829052  -0.814441900281157  -0.827121758932650  -0.839588915137848  -0.851838094031477  -0.863864001714800  -0.875661325441750  -0.887224733805055  -0.898548876922365  -0.909628386622385  -0.920457876631001  -0.931031942757411  -0.941345163080250  -0.951392098133724  -0.961167291093732  -0.970665267964000  -0.979880537762209  -0.988807592706122  -0.997440908399714  -1.00577494401930   -1.01380414249966   -1.02152293072018   -1.02892571969097   -1.03600690473899   -1.04276086569418   -1.04918196707561   -1.05526455827758   -1.06100297375575   -1.06639153321331   -1.07142454178703   -1.07609629023348   -1.08040105511509   -1.08433309898632   -1.08788667057976   -1.09105600499228   -1.09383532387113   -1.09621883560011   -1.09820073548567   -1.09977520594304   -1.10093641668239   -1.10167852489489   -1.10199567543892   -1.10188200102614   -1.10133162240765   -1.10033864856010   -1.09889717687183   -1.09700129332900   -1.09464507270172   -1.09182257873014   -1.08852786431066   -1.08475497168197   -1.08049793261124   -1.07575076858022   -1.07050749097139   -1.06476210125405   -1.05850859117050   -1.05174094292213   -1.04445312935557   -1.03663911414879   -1.02829285199726   -1.01940828880009   -1.00997936184609   -1]    

y = [2  2.26499544506307    2.52345285165919    2.77551012741366    3.02130356526704    3.26096785177083    3.49463607538307    3.72243973476395    3.94450874707147    4.16097145625704    4.37195464136115    4.57758352480892    4.77798178070582    4.97327154313322    5.16357341444407    5.34900647355850    5.52968828425946    5.70573490348834    5.87726088964058    6.04437931086135    6.20720175334112    6.36583832961133    6.52039768683998    6.67098701512728    6.81771205580129    6.96067710971352    7.09998504553457    7.23573730804975    7.36803392645471    7.49697352265110    7.62265331954214    7.74516914932828    7.86461546180284    7.98108533264758    8.09467047172843    8.20546123139099    8.31354661475628    8.41901428401626    8.52195056872954    8.62244047411698    8.72056768935728    8.81641459588266    8.91006227567448    9.00159051955883    9.09107783550219    9.17860145690705    9.26423735090754    9.34806022666506    9.43014354366389    9.51055952000682    9.58937914071082    9.66667216600261    9.74250713961429    9.81695139707904    9.89007107402665    9.96193111447923    10.0325952791468    10.1021261537228    10.1705851571801    10.2380325500660    10.3045274427986    10.3701278039618    10.4348904686013    10.4988711465199    10.5621244305736    10.6247038049668    10.6866616535481    10.7480492681059    10.8089168566642    10.8693135517778    10.9292874188284    10.9888854643200    11.0481536441744    11.1071368720271    11.1658790275228    11.2244229646110    11.2828105198417    11.3410825206610    11.3992787937066    11.4574381731036    11.5155985087602    11.5737966746629    11.6320685771728    11.6904491633205    11.7489724291023    11.8076714277756    11.8665782781543    11.9257241729050    11.9851393868420    12.0448532852234    12.1048943320464    12.1652900983432    12.2260672704765    12.2872516584350    12.3488682041292    12.4109409896870    12.4734932457494    12.5365473597658    12.6001248842901    12.6642465452759    12.7289322503726    12.7942010972203    12.8600713817463    12.9265606064601    12.9936854887492    13.0614619691748    13.1299052197674    13.1990296523224    13.2688489266957    13.3393759590994    13.4106229303974    13.4826012944010    13.5553217861645    13.6287944302809    13.7030285491776    13.7780327714119    13.8538150399664    13.9303826205452    14.0077421098691    14.0858994439713    14.1648599064931    14.2446281369795    14.3252081391748    14.4066032893182    14.4888163444396    14.5718494506550    14.6557041514622    14.7403813960366    14.8258815475266    14.9122043913492    14.9993491434859    15.0873144587782    15.1760984392230    15.2656986422686    15.3561120891100    15.4473352729848    15.5393641674687    15.6321942347710    15.7258204340304    15.8202372296108    15.9154385993964    16.0114180430878    16.1081685904975    16.2056828098453    16.3039528160545    16.4029702790468    16.5027264320383    16.6032120798353    16.7044176071297    16.8063329867944    16.9089477881795    17.0122511854074    17.1162319656689    17.2208785375182    17.3261789391692    17.4321208467907    17.5386915828022    17.6458781241694    17.7536671107000    17.8620448533392    17.9709973424653    18.0805102561854    18.1905689686311    18.3011585582540    18.4122638161214    18.5238692542118    18.6359591137108    18.7485173733063    18.8615277574846    18.9749737448257    19.0888385762990    19.2031052635592    19.3177565972414    19.4327751552571    19.5481433110899    19.6638432420907    19.7798569377739    19.8961662081124    20.0127526918339    20.1295978647159    20.2466830478817    20.3639894160961    20.4814980060606    20.5991897247094    20.7170453575050    20.8350455767337    20.9531709498011    21.0714019475281    21.1897189524464    21.3081022670938    21.4265321223103    21.5449886855333    21.6634520690937    21.7819023385110    21.9003195207895    22.0186836127132    22.1369745891423    22.2551724113081    22.3732570351088    22.4912084194057    22.6090065343178    22.7266313695183    22.8440629425299    22.9612813070203    23.0782665610982    23.1949988556084    23.3114584024278    23.4276254827613    23.5434804554365    23.6590037652004    23.7741759510141    23.8889776543494    24.0033896274833    24.1173927417946    24.2309679960590    24.3440965247448    24.4567596063089    24.5689386714917    24.6806153116135    24.7917712868696    24.9023885346262    25.0124491777157    25.1219355327329    25.2308301183300    25.3391156635127    25.4467751159356    25.5537916501978    25.6601486761387    25.7658298471334    25.8708190683886    25.9751005052380    26.0786585914379    26.1814780374631    26.2835438388023    26.3848412842538    26.4853559642211    26.5850737790085    26.6839809471169    26.7820640135392    26.8793098580560    26.9757057035313    27.0712391242082    27.1658980540041    27.2596707948069    27.3525460247703    27.4445128066094    27.5355605958967    27.6256792493570    27.7148590331637    27.8030906312344    27.8903651535260    27.9766741443309    28.0620095905721    28.1463639300994    28.2297300599846    28.3121013448172    28.3934716250003    28.4738352250458    28.5531869618704    28.6315221530909    28.7088366253202    28.7851267224627    28.8603893140098    28.9346218033358    29.0078221359933    29.0799888080092    29.1511208741798    29.2212179563668    29.2902802517928    29.3583085413370    29.4253041978307    29.4912691943530    29.5562061125265    29.6201181508128    29.6830091328082    29.7448835155393    29.8057463977587    29.8656035282405    29.9244613140760    29.9823268289694    30.0392078215333    30.0951127235842    30.1500506584387    30.2040314492083    30.2570656270958    30.3091644396903    30.3603398592634    30.4106045910644    30.4599720816159    30.5084565270100    30.5560728812031    30.6028368643122    30.6487649709102    30.6938744783217    30.7381834549183    30.7817107684148    30.8244760941642    30.8664999234537    30.9078035718003    30.9484091872463    30.9883397586550    31.0276191240064    31.0662719786928    31.1043238838141    31.1418012744739    31.1787314680751    31.2151426726150    31.2510639949816    31.2865254492487    31.3215579649719    31.3561933954840    31.3904645261906    31.4244050828661    31.4580497399487    31.4914341288367    31.5245948461837    31.5575694621942    31.5903965289195    31.6231155885532    31.6557671817267    31.6883928558052    31.7210351731827    31.7537377195783    31.7865451123314    31.8195030086975    31.8526581141437    31.8860581906445    31.9197520649774    31.9537896370183    31.9882218880374    32.0231008889946    32.0584798088355    32.0944129227864    32.1309556206506    32.1681644151037    32.2060969499891    32.2448120086139    32.2843695220444    32.3248305774017    32.3662574261575    32.4087134924294    32.4522633812768    32.4969728869964    32.5429090014181    32.5901399222001    32.6387350611250    32.6887650523952    32.7403017609286    32.7934182906543    32.8481889928080    32.9046894742278    32.9629966056499    33.0231885300040    33.0853446707091    33.1495457399690    33.2158737470682    33.2844120066671    33.3552451470980    33.4284591186606    33.5041412019175    33.5823800159901    33.6632655268539    33.7468890556344    33.8333432869025    33.9227222769705    34.0151214621873    34.1106376672341    34.2093691134204    34.3114154269792    34.4168776473628    34.5258582355384    34.6384610822839    34.7547915164830    34.8749563134217    34.9990637030830    35.1272233784432    35.2595465037671    35.3961457229040    35.5371351675831    35.6826304657089    35.8327487496574    35.9876086645714    36.1473303766559    36.3120355814743    36.4818475122433    36.6568909481293    36.8372922225435    37.0231792314376    37.2146814415997    37.4119298989495    37.6150572368345    37.8241976843249    38.0394870745098    38.2610628527928    38.4890640851872    38.7236314666120    38.9649073291876    39.2130356505310    39.4681620620517    39.7304338572476    40]
</code></pre>

<p>However the rectangle doesn't orient itself exactly towards the curve in question, it still has a offset.Any help is appreciated. Thanks. </p>
",2019-02-09 12:48:00,54647022.0,55,1,2,0,,961682.0,,9/23/2011 17:34,134.0,54647022.0,"<p>Hi i'm posting the solution to the question here. Thanks to anyone who took time to take a look at it. </p>

<pre><code>      for k=2:length(x)
        %%%Plot road
        rl=-2; rr=0; w=4; l=100;
        yroad = 0:l;
        xroad = repmat(rl+w/2,l+1);
        plot(xroad, yroad, 'w--','LineWidth',1.5);
        hold on;
        rectangle('Position',[rl,rr,w,l],'FaceColor',[0 0 0 0.9]);
        axis([-4 4 0 50]);
        hold on;
        %%%%%
        set(gca,'children',flipud(get(gca,'children')))
        %%%time annotation
        str = strcat('t=',string(t));
        % annotation('textbox',[0.8 0.8 .5 .5],'String',str,'FitBoxToText','on');
        text(1,47,str,'Color','w')
        ratio = diff(get(gca, 'YLim'))/diff(get(gca, 'XLim'));
        phi = atan2(y(k) - y(k - 1), (x(k) - x(k - 1))*ratio);
        [x_rect_rot, y_rect_rot]=get_rectangle(phi, x(k), y(k));
        plot(polyshape(x_rect_rot, y_rect_rot),'FaceColor','c','FaceAlpha',0.85);
        hold on;
        plot(x(1:k), y(1:k));
        hold on;  
        drawnow;
      end

    function[xa,ya] =get_rectangle(phi,xc,yc,varargin)
        if length(varargin)&lt;2
            h = 5;
            w = 3;
        else 
            h = varargin{4};
            w = varargin{5};
        end
        x_rect = [-h, h, h, -h]/2;
        y_rect = [-w, -w, w, w]/2;
        % Consider aspect ratio of the axis
        ratio = diff(get(gca, 'YLim'))/diff(get(gca, 'XLim'));            

        % Calculate rotated rectangle
        x_rect_rot = x_rect*cos(phi) - y_rect*sin(phi);
        y_rect_rot = x_rect*sin(phi) + y_rect*cos(phi);

        % Incorporate ratio
        x_rect_rot = x_rect_rot/ratio;

        % Calculate offset
        xa = x_rect_rot + xc;
        ya = y_rect_rot + yc;

    end
</code></pre>
",961682.0,0.0,0.0,96085079.0,"I wanted to indicate the direction of next point to move to, which i wrote wrong here in the question, this question is answered in a discord chatroom, i'll put the answer here, once again thank you very much for your time ViG :)"
3364,55360519,"Obtaining dimensions for Stewart Platform for arm, base and top plate",|robotics|servo|kinematics|,"<p>I am in a need to calculate the Stewart platform dimensions for an application where the Stewart platform top plate can be, consisting of a width of 19Inches maximum, and length 19Inches. Also, the height should not exceed, 25Inches. The Stewart platform should be able to withstand 100kg. The maximum tilt angle expected to achieve is 20Degrees. This Stewart platform operates using 6 servo motors at each leg. </p>

<p>The following website was referred to obtain the calculation so far.
<a href=""https://memememememememe.me/post/stewart-platform-math/"" rel=""nofollow noreferrer"">https://memememememememe.me/post/stewart-platform-math/</a></p>
",2019-03-26 15:13:00,,449,0,2,0,,11261153.0,,3/26/2019 14:57,1.0,,,,,,97445102.0,please show us what you did so far? maybe show us a [mcve] of what you are trying to achieve.
3577,58739227,roblem with ROS control and Gazebo,|ros|robotics|gazebo-simu|,"<p>I have a problem with controlling a URDF that I exported from SolidWorks. (Ubuntu 16.04 , Kinetic, Gazebo 7.x) I followed this tutorial and I wanted to implemented on my robot. All the controllers are starting correctly so as the Gazebo simulation also the Node publish the data correctly I have checked it with echo-ing the topic and with different values for the data. Is there a chance not working because the PID values ?</p>

<p>All the transmissions look like this :</p>

<pre class=""lang-xml prettyprint-override""><code>&lt;transmission name=""tran1""&gt;
    &lt;type&gt;transmission_interface/SimpleTransmission&lt;/type&gt;
    &lt;joint name=""Joint_1""&gt;
      &lt;hardwareInterface&gt;hardware_interface/EffortJointInterface&lt;/hardwareInterface&gt;
    &lt;/joint&gt;
    &lt;actuator name=""motor1""&gt;
      &lt;hardwareInterface&gt;hardware_interface/EffortJointInterface&lt;/hardwareInterface&gt;
      &lt;mechanicalReduction&gt;1&lt;/mechanicalReduction&gt;
    &lt;/actuator&gt;
&lt;/transmission&gt;  
</code></pre>

<p>The controller is like this (for all joints) :</p>

<pre><code>joint_state_controller:
    type: joint_state_controller/JointStateController
    publish_rate: 50
joint1_position_controller:
    type: effort_controllers/JointPositionController
    joint: Joint_1
    pid: {p: 100.0, i: 0.01, d: 10.0}
</code></pre>

<p>And I have this node:</p>

<pre class=""lang-py prettyprint-override""><code>    rospy.init_node('ArmMovement')
    pub1=rospy.Publisher(""/rrbot/joint1_position_controller/command"",Float64,queue_size=10 )
    rate = rospy.Rate(50)
    ArmCor1= Float64()
    ArmCor1.data=0
    while not rospy.is_shutdown():
      pub1.publish(ArmCor1)
      rate.sleep()
</code></pre>

<p>Part of URDF for the Joint_1:</p>

<pre class=""lang-xml prettyprint-override""><code>&lt;joint name=""Joint_1"" type=""revolute""&gt;
    &lt;origin
      xyz=""0 0 -0.008""
      rpy=""1.5708 0 0"" /&gt;
    &lt;parent link=""base_link"" /&gt;
    &lt;child link=""Link_1"" /&gt;
    &lt;axis
      xyz=""0 1 0"" /&gt;
    &lt;limit
      lower=""0""
      upper=""3.14""
      effort=""0""
      velocity=""0"" /&gt;
&lt;/joint&gt;
</code></pre>
",2019-11-06 22:10:00,58827131.0,825,1,1,1,,12334653.0,,11/6/2019 22:03,12.0,58827131.0,"<p>Thank you guys for your help, it actually work after your comments. 
This was my Joint_1 :</p>

<pre><code> &lt;joint name=""Joint_1"" type=""revolute""&gt;
 &lt;origin
 xyz=""0 0 -0.008""
 rpy=""1.5708 0 0"" /&gt;
&lt;parent link=""base_link"" /&gt;
&lt;child link=""Link_1"" /&gt;
&lt;axis
xyz=""0 1 0"" /&gt;
&lt;limit
 lower=""0""
 upper=""3.14""
 effort=""0""
 velocity=""0"" /&gt;
&lt;/joint&gt;
</code></pre>

<p>I changed the limit section to this :</p>

<pre><code>&lt;limit
lower=""0""
upper=""3.14""
effort=""2""
velocity=""2.0"" /&gt;
</code></pre>

<p>I have other problems like a very annoying shiver (that comes from effort value I think) but It is not for this topic.</p>
",12334653.0,0.0,0.0,103770883.0,What does the [rostopic list](http://wiki.ros.org/rostopic#rostopic_list) console command return?
3433,57008332,OpenCV: Wrong result in calibrateHandEye function,|c++|opencv|computer-vision|camera-calibration|robotics|,"<p>I am working in a robot application, in which I have a camera fixed to a robot gripper. In order to calculate the matrix transformation between the camera and the gripper Hcg I am using the calibrateHandEye new function provided in the OpenCV version 4.1.0 </p>

<p>I had taken 10 pictures of the chessboard from the camera mounted in the gripper and at the same time I recorded the robot position. </p>

<p>The code I am working on:</p>

<pre><code>// My_handeye.cpp : This file contains the 'main' function. Program execution begins and ends there.
//
#include &lt;iostream&gt;
#include &lt;sstream&gt;
#include &lt;string&gt;
#include &lt;ctime&gt;
#include &lt;cstdio&gt;
#include ""pch.h""

#include &lt;opencv2/opencv.hpp&gt;
#include &lt;opencv2/core.hpp&gt;
#include &lt;opencv2/core/utility.hpp&gt;
#include &lt;opencv2/imgproc.hpp&gt;
#include &lt;opencv2/calib3d.hpp&gt;
#include &lt;opencv2/imgcodecs.hpp&gt;
#include &lt;opencv2/videoio.hpp&gt;
#include &lt;opencv2/highgui.hpp&gt;

using namespace cv;
using namespace std;

Mat eulerAnglesToRotationMatrix(Vec3f &amp;theta);
Vec3f rotationMatrixToEulerAngles(Mat &amp;R);
float rad2deg(float radian);
float deg2rad(float degree);

int main()
{

    // Camera calibration information

    std::vector&lt;double&gt; distortionCoefficients(5);  // camera distortion
    distortionCoefficients[0] = 2.4472856611074989e-01;
    distortionCoefficients[1] = -8.1042032574246325e-01;
    distortionCoefficients[2] = 0;
    distortionCoefficients[3] = 0;
    distortionCoefficients[4] = 7.8769462320821060e-01;

    double f_x = 1.3624172121852105e+03; // Focal length in x axis
    double f_y = 1.3624172121852105e+03; // Focal length in y axis (usually the same?)
    double c_x = 960; // Camera primary point x
    double c_y = 540; // Camera primary point y

    cv::Mat cameraMatrix(3, 3, CV_32FC1);
    cameraMatrix.at&lt;float&gt;(0, 0) = f_x;
    cameraMatrix.at&lt;float&gt;(0, 1) = 0.0;
    cameraMatrix.at&lt;float&gt;(0, 2) = c_x;
    cameraMatrix.at&lt;float&gt;(1, 0) = 0.0;
    cameraMatrix.at&lt;float&gt;(1, 1) = f_y;
    cameraMatrix.at&lt;float&gt;(1, 2) = c_y;
    cameraMatrix.at&lt;float&gt;(2, 0) = 0.0;
    cameraMatrix.at&lt;float&gt;(2, 1) = 0.0;
    cameraMatrix.at&lt;float&gt;(2, 2) = 1.0;

    Mat rvec(3, 1, CV_32F), tvec(3, 1, CV_32F);
    //

    std::vector&lt;Mat&gt; R_gripper2base;
    std::vector&lt;Mat&gt; t_gripper2base;
    std::vector&lt;Mat&gt; R_target2cam;
    std::vector&lt;Mat&gt; t_target2cam;
    Mat R_cam2gripper = (Mat_&lt;float&gt;(3, 3));
    Mat t_cam2gripper = (Mat_&lt;float&gt;(3, 1));

    vector&lt;String&gt; fn;
    glob(""images/*.bmp"", fn, false);

    vector&lt;Mat&gt; images;
    size_t num_images = fn.size(); //number of bmp files in images folder
    Size patternsize(6, 8); //number of centers
    vector&lt;Point2f&gt; centers; //this will be filled by the detected centers
    float cell_size = 30;
    vector&lt;Point3f&gt; obj_points;

    R_gripper2base.reserve(num_images);
    t_gripper2base.reserve(num_images);
    R_target2cam.reserve(num_images);
    t_target2cam.reserve(num_images);

    for (int i = 0; i &lt; patternsize.height; ++i)
        for (int j = 0; j &lt; patternsize.width; ++j)
            obj_points.push_back(Point3f(float(j*cell_size),
                float(i*cell_size), 0.f));

    for (size_t i = 0; i &lt; num_images; i++)
        images.push_back(imread(fn[i]));

    Mat frame;

    for (size_t i = 0; i &lt; num_images; i++)
    {
        frame = imread(fn[i]); //source image
        bool patternfound = findChessboardCorners(frame, patternsize, centers);
        if (patternfound)
        {
            drawChessboardCorners(frame, patternsize, Mat(centers), patternfound);
            //imshow(""window"", frame);
            //int key = cv::waitKey(0) &amp; 0xff;
            solvePnP(Mat(obj_points), Mat(centers), cameraMatrix, distortionCoefficients, rvec, tvec);

            Mat R;
            Rodrigues(rvec, R); // R is 3x3
            R_target2cam.push_back(R);
            t_target2cam.push_back(tvec);
            Mat T = Mat::eye(4, 4, R.type()); // T is 4x4
            T(Range(0, 3), Range(0, 3)) = R * 1; // copies R into T
            T(Range(0, 3), Range(3, 4)) = tvec * 1; // copies tvec into T

            cout &lt;&lt; ""T = "" &lt;&lt; endl &lt;&lt; "" "" &lt;&lt; T &lt;&lt; endl &lt;&lt; endl;

        }
        cout &lt;&lt; patternfound &lt;&lt; endl;
    }

    Vec3f theta_01{ deg2rad(-153.61), deg2rad(8.3),   deg2rad(-91.91) };
    Vec3f theta_02{ deg2rad(-166.71), deg2rad(3.04),  deg2rad(-93.31) };
    Vec3f theta_03{ deg2rad(-170.04), deg2rad(24.92), deg2rad(-88.29) };
    Vec3f theta_04{ deg2rad(-165.71), deg2rad(24.68), deg2rad(-84.85) };
    Vec3f theta_05{ deg2rad(-160.18), deg2rad(-15.94),deg2rad(-56.24) };
    Vec3f theta_06{ deg2rad(175.68),  deg2rad(10.95), deg2rad(180) };
    Vec3f theta_07{ deg2rad(175.73),  deg2rad(45.78), deg2rad(-179.92) };
    Vec3f theta_08{ deg2rad(-165.34), deg2rad(47.37), deg2rad(-166.25) };
    Vec3f theta_09{ deg2rad(-165.62), deg2rad(17.95), deg2rad(-166.17) };
    Vec3f theta_10{ deg2rad(-151.99), deg2rad(-14.59),deg2rad(-94.19) };

    Mat robot_rot_01 = eulerAnglesToRotationMatrix(theta_01);
    Mat robot_rot_02 = eulerAnglesToRotationMatrix(theta_02);
    Mat robot_rot_03 = eulerAnglesToRotationMatrix(theta_03);
    Mat robot_rot_04 = eulerAnglesToRotationMatrix(theta_04);
    Mat robot_rot_05 = eulerAnglesToRotationMatrix(theta_05);
    Mat robot_rot_06 = eulerAnglesToRotationMatrix(theta_06);
    Mat robot_rot_07 = eulerAnglesToRotationMatrix(theta_07);
    Mat robot_rot_08 = eulerAnglesToRotationMatrix(theta_08);
    Mat robot_rot_09 = eulerAnglesToRotationMatrix(theta_09);
    Mat robot_rot_10 = eulerAnglesToRotationMatrix(theta_10);

    const Mat robot_tr_01 = (Mat_&lt;float&gt;(3, 1) &lt;&lt; 781.2, 338.59, 903.48);
    const Mat robot_tr_02 = (Mat_&lt;float&gt;(3, 1) &lt;&lt; 867.65, 382.52, 884.42);
    const Mat robot_tr_03 = (Mat_&lt;float&gt;(3, 1) &lt;&lt; 856.91, 172.99, 964.61);
    const Mat robot_tr_04 = (Mat_&lt;float&gt;(3, 1) &lt;&lt; 748.81, 146.75, 1043.29);
    const Mat robot_tr_05 = (Mat_&lt;float&gt;(3, 1) &lt;&lt; 627.66, 554.08, 920.85);
    const Mat robot_tr_06 = (Mat_&lt;float&gt;(3, 1) &lt;&lt; 715.06, 195.96, 889.38);
    const Mat robot_tr_07 = (Mat_&lt;float&gt;(3, 1) &lt;&lt; 790.9, 196.29, 1117.38);
    const Mat robot_tr_08 = (Mat_&lt;float&gt;(3, 1) &lt;&lt; 743.5, 283.93, 1131.92);
    const Mat robot_tr_09 = (Mat_&lt;float&gt;(3, 1) &lt;&lt; 748.9, 288.19, 910.58);
    const Mat robot_tr_10 = (Mat_&lt;float&gt;(3, 1) &lt;&lt; 813.18, 400.44, 917.16);

    R_gripper2base.push_back(robot_rot_01);
    R_gripper2base.push_back(robot_rot_02);
    R_gripper2base.push_back(robot_rot_03);
    R_gripper2base.push_back(robot_rot_04);
    R_gripper2base.push_back(robot_rot_05);
    R_gripper2base.push_back(robot_rot_06);
    R_gripper2base.push_back(robot_rot_07);
    R_gripper2base.push_back(robot_rot_08);
    R_gripper2base.push_back(robot_rot_09);
    R_gripper2base.push_back(robot_rot_10);

    t_gripper2base.push_back(robot_tr_01);
    t_gripper2base.push_back(robot_tr_02);
    t_gripper2base.push_back(robot_tr_03);
    t_gripper2base.push_back(robot_tr_04);
    t_gripper2base.push_back(robot_tr_05);
    t_gripper2base.push_back(robot_tr_06);
    t_gripper2base.push_back(robot_tr_07);
    t_gripper2base.push_back(robot_tr_08);
    t_gripper2base.push_back(robot_tr_09);
    t_gripper2base.push_back(robot_tr_10);

    calibrateHandEye(R_gripper2base, t_gripper2base, R_target2cam, t_target2cam, R_cam2gripper, t_cam2gripper, CALIB_HAND_EYE_TSAI);

    Vec3f R_cam2gripper_r = rotationMatrixToEulerAngles(R_cam2gripper);

    cout &lt;&lt; ""R_cam2gripper = "" &lt;&lt; endl &lt;&lt; "" "" &lt;&lt; R_cam2gripper &lt;&lt; endl &lt;&lt; endl;
    cout &lt;&lt; ""R_cam2gripper_r = "" &lt;&lt; endl &lt;&lt; "" "" &lt;&lt; R_cam2gripper_r &lt;&lt; endl &lt;&lt; endl;
    cout &lt;&lt; ""t_cam2gripper = "" &lt;&lt; endl &lt;&lt; "" "" &lt;&lt; t_cam2gripper &lt;&lt; endl &lt;&lt; endl;
}

Mat eulerAnglesToRotationMatrix(Vec3f &amp;theta)
{
    // Calculate rotation about x axis
    Mat R_x = (Mat_&lt;double&gt;(3, 3) &lt;&lt;
        1, 0, 0,
        0, cos(theta[0]), -sin(theta[0]),
        0, sin(theta[0]), cos(theta[0])
        );

    // Calculate rotation about y axis
    Mat R_y = (Mat_&lt;double&gt;(3, 3) &lt;&lt;
        cos(theta[1]), 0, sin(theta[1]),
        0, 1, 0,
        -sin(theta[1]), 0, cos(theta[1])
        );

    // Calculate rotation about z axis
    Mat R_z = (Mat_&lt;double&gt;(3, 3) &lt;&lt;
        cos(theta[2]), -sin(theta[2]), 0,
        sin(theta[2]), cos(theta[2]), 0,
        0, 0, 1);


    // Combined rotation matrix
    Mat R = R_z * R_y * R_x;

    return R;

}

float rad2deg(float radian) {
    double pi = 3.14159;
    return(radian * (180 / pi));
}

float deg2rad(float degree) {
    double pi = 3.14159;
    return(degree * (pi / 180));
}

// Checks if a matrix is a valid rotation matrix.
bool isRotationMatrix(Mat &amp;R)
{
    Mat Rt;
    transpose(R, Rt);
    Mat shouldBeIdentity = Rt * R;
    Mat I = Mat::eye(3, 3, shouldBeIdentity.type());

    return  norm(I, shouldBeIdentity) &lt; 1e-6;

}

// Calculates rotation matrix to euler angles
// The result is the same as MATLAB except the order
// of the euler angles ( x and z are swapped ).
Vec3f rotationMatrixToEulerAngles(Mat &amp;R)
{

    assert(isRotationMatrix(R));

    float sy = sqrt(R.at&lt;double&gt;(0, 0) * R.at&lt;double&gt;(0, 0) + R.at&lt;double&gt;(1, 0) * R.at&lt;double&gt;(1, 0));

    bool singular = sy &lt; 1e-6; // If

    float x, y, z;
    if (!singular)
    {
        x = atan2(R.at&lt;double&gt;(2, 1), R.at&lt;double&gt;(2, 2));
        y = atan2(-R.at&lt;double&gt;(2, 0), sy);
        z = atan2(R.at&lt;double&gt;(1, 0), R.at&lt;double&gt;(0, 0));
    }
    else
    {
        x = atan2(-R.at&lt;double&gt;(1, 2), R.at&lt;double&gt;(1, 1));
        y = atan2(-R.at&lt;double&gt;(2, 0), sy);
        z = 0;
    }
    return Vec3f(x, y, z);

}
</code></pre>

<p>The result the function is giving me is the next one:</p>

<pre><code>R_cam2gripper =
 [0.3099803593003124, -0.8923086952824562, -0.3281727733547833;
 0.7129271761196039, 0.4465219155360299, -0.5406967916458927;
 0.6290047840821058, -0.0663579028402444, 0.7745641421680119]

R_cam2gripper_r =
 [-0.0854626, -0.680272, 1.16065]

t_cam2gripper =
 [-35.02063730299775;
 -74.80633768251272;
 -307.6725851251873]
</code></pre>

<p>I am getting  'good' results provided by other software. With them, the robot got to the exact points I am pointing in the camera (I have a 3D camera, from which I am getting the x, y, z from the camera world) so they are certainly correct, but I am having troubles to repeat the same result with the OpenCV function. </p>

<p>Sorry for the long introduction to my problem. Any understanding of why the solutions are not what is supposed to be? My guess is, that I have a problem understanding the angles or converting them but I couldn't find any way to solve this. Any hint will be well welcome!</p>
",2019-07-12 13:49:00,57383985.0,1417,1,0,0,,11775921.0,Chile,7/12/2019 13:44,16.0,57383985.0,"<p>I actually managed to solve this problem. The general idea was correct, but:</p>

<ol>
<li>I was not understanding correctly the vector rotation notation the robot was giving. It was necessary to multiply the actual values by a factor.</li>
<li>I created a new program that extracts directly from the robot and the pictures the matrixes that the algorithm requires and writes these values to a YML file.</li>
<li>The CALIB_HAND_EYE_TSAI method wasn't giving me correct values. But with the four others, the values seem to converge to the actual values</li>
</ol>

<p>Anyway, thank you for your help. I am stuck to get more precision in the algorithm, but that's for another question.</p>
",11775921.0,0.0,0.0,,
3273,54358550,Pepper Robot Connected But Show Offline On Connected Device,|android|android-studio|windows-7|robot|pepper|,"<p>I try to connect to the Pepper robot using my Windows 7 laptop. It is connected and I am able to view the Robot Viewer using Android Studio, but it shows ""offline"" when I want to deploy the Pepper robot.</p>

<p>I have tried:</p>

<ol>
<li>uninstall Vysor</li>
<li>kill adb.exe on my task manager</li>
<li>Restart the robot and my PC</li>
</ol>

<p>Any other solutions? </p>

<p>Screenshot showing the problem:</p>

<p><img src=""https://i.stack.imgur.com/vOfto.png"" alt=""Screenshot""></p>
",2019-01-25 03:25:00,,240,1,0,1,,6145865.0,,4/1/2016 15:55,5.0,54997410.0,"<p>Are your SDK platform-tools up-to-date? Also, have you activated the debug option on the tablet? Are you asked for your password when you connect the robot?  </p>

<p>Try to run <code>adb kill-server</code> by command line and then try again:  </p>

<p>1- Disconnect the robot<br>
2- Connect again (make sure you connect to the <strong>robot</strong> ip and not the tablet ip)<br>
3- Try to select target</p>
",4949074.0,0.0,0.0,,
3363,55352630,Robot Model drops off the floor after running the simulation,|gravity|robot|physics-engine|webots|,"<p>I imported a model in webots simulation from URDFs in ROS. The robot is a tricycle drive with 3 castor wheels. I have followed the wheel style as in the webots style guide and changed accordingly.
My problem is that when I run the simulation the robot's wheels drops off the floor and is not able to move. Just chassis is on the floor and the wheels hangs down.</p>
",2019-03-26 08:22:00,55352846.0,498,1,0,3,,5230316.0,"Bonn, Germany",8/15/2015 13:40,114.0,55352846.0,"<p>Your robot is probably too heavy for the physics configuration.</p>

<p>You can fix this by changing the fields values of the WorldInfo node (<a href=""https://www.cyberbotics.com/doc/reference/worldinfo"" rel=""nofollow noreferrer"">https://www.cyberbotics.com/doc/reference/worldinfo</a>).
Here are the important fields:</p>

<ul>
<li><strong>ERP</strong>: you probably want to increase the default value which works fine for small and light objects (try setting it to ~0.6)</li>
<li><strong>basicTimeStep</strong>: here you might decrease the default value to 16 (or even 8) this will make the simulation runs slightly slower because it computes more steps but much more stable.</li>
<li><strong>contactProperties</strong>: You probably need to add a contact property defining the properties of the contact between the floor and your robot's wheels (to increase the friction, and decrease the spongyness of the contact).</li>
</ul>

<p>Here is an example of contact properties:</p>

<pre><code>ContactProperties {
  material2 ""MyRobotWheelContactMaterial""
  coulombFriction [
    8
  ]
  softCFM 1e-5
}
</code></pre>
",8427891.0,2.0,2.0,,
3530,58056874,How to create an occupancy grid map in python similar to Matlab occupancy grid,|python|grid|robotics|,"<p>I am aiming to create an occupancy grid as following in Matlab:</p>

<p>(<a href=""https://au.mathworks.com/help/robotics/ug/occupancy-grids.html"" rel=""nofollow noreferrer"">https://au.mathworks.com/help/robotics/ug/occupancy-grids.html</a>)</p>

<pre><code>map = binaryOccupancyMap(10,10,5);
setOccupancy(map,[5 5], 1);
</code></pre>

<p>I have googled and got overwhelmed with Python's robotics algorithms. 
Could anyone please help to instruct a simple way to do in python?</p>
",2019-09-23 06:40:00,,2541,0,0,2,,10122673.0,Australia,7/23/2018 13:14,9.0,,,,,,,
3318,54949446,Java: duration of movement of Finch Robot,|java|eclipse|robot|finch|,"<p>I'm new to programming. I have code for my Finch robot that simply loops a zigzag section after the user inputs how many times it should loop for, but how do I input another question that asks how long each zigzag section should be?</p>

<p>For example, the first question I ask is how many zigzag sections the user wants to loop for, but I also want to ask how long each zigzag segment should be (how long each line should be before it turns the other way).</p>

<p>Code:</p>

<pre class=""lang-java prettyprint-override""><code>Finch myFinch = new Finch();
Scanner sc = new Scanner(System. in );

System.out.println(""Welcome! Complete the following entries"");
System.out.println(""Number of zigzag sections:  "");

int noOfTimes = sc.nextInt();

do {
    myFinch.setLED(Color.green);
    myFinch.buzz(600, 2250);
    myFinch.setWheelVelocities(180, 0, 750);
    myFinch.setWheelVelocities(100, 100, 1500);
    myFinch.setLED(Color.red);
    myFinch.buzz(600, 2350);
    myFinch.setWheelVelocities(0, 180, 850);
    myFinch.setWheelVelocities(180, 180, 1500);
    noOfTimes--;

} while ( noOfTimes &gt; 0 );

myFinch.quit();
System.exit(0);
</code></pre>
",2019-03-01 17:18:00,54951801.0,225,1,1,0,,11120121.0,"Leyton, London, UK",2/26/2019 15:11,16.0,54951801.0,"<p>Check java <code>Scanner</code> doccumentation <a href=""https://docs.oracle.com/javase/7/docs/api/java/util/Scanner.html"" rel=""nofollow noreferrer"">here</a></p>

<p>Code sample to accept multiple inputs</p>

<pre><code>import java.util.Scanner;

class GetInputFromUser
{
   public static void main(String args[])
   {
      int a;
      float b;
      String s;

      Scanner in = new Scanner(System.in);

      System.out.println(""Enter an integer"");
      a = in.nextInt();
      System.out.println(""You entered integer "" + a);

      System.out.println(""Enter a float"");
      b = in.nextFloat();
      System.out.println(""You entered float "" + b);  

      System.out.println(""Enter a string"");
      s = in.nextLine();
      System.out.println(""You entered string "" + s);
   }
}
</code></pre>
",3065198.0,0.0,0.0,96664163.0,"What have you tried? Can you see where the other values are asked for and then scanned in? What happens if you try it yourself? Unfortunately, SO is not a great tutorial site, so you are encouraged to figure this out yourself with the many tutorials you can find online."
3382,55965675,From camera calibration to picking up colored cubes with robot arm,|python|opencv|camera-calibration|robotics|coordinate-transformation|,"<p>I want to pick up colored cubes with a robot arm that I detect with a camera and OpenCV, in Python. I managed to detect the different colored cubes and I calibrated the camera with the checkerboard process.</p>

<p><strong>The setup:</strong></p>

<p><a href=""https://i.stack.imgur.com/dt0rH.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dt0rH.jpg"" alt=""Setup front""></a>
<a href=""https://i.stack.imgur.com/rpWsl.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rpWsl.jpg"" alt=""Setup top""></a></p>

<p><strong>Cube detection:</strong>
<a href=""https://i.stack.imgur.com/wYvNL.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wYvNL.jpg"" alt=""Cubes detected""></a></p>

<p>The problem is that I don't understand the rest of the process of getting the coordinates from the object with the camera and translating them to the robot arm for pick up.</p>

<p><strong>The following steps have been completed:</strong></p>

<ol>
<li><p>Separating color boundaries with HSV and drawing bounding boxes. So I have the pixel x, y of the object.</p></li>
<li><p>Calibrating the camera resulting in the following camera matrix and distortion coefficients: </p>

<p>Camera Matrix: [[1.42715609e+03 0.00000000e+00 9.13700651e+02]  [0.00000000e+00 1.43275509e+03 5.58917609e+02]  [0.00000000e+00
0.00000000e+00 1.00000000e+00]]</p>

<p>Distortion:[[ 0.03924722 -0.30622971  0.00124042 -0.00303094  0.49458539]]</p></li>
<li><p>Trying to figure out the next steps in the documentation of OpenCV</p></li>
</ol>

<p><strong>The result</strong></p>

<p>I read through the API documentation on this page: <a href=""https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html"" rel=""nofollow noreferrer"">https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html</a></p>

<p>But with my skill level I can't seem the extract the practical steps to take, in order to attain my goal.</p>

<p>My questions:</p>

<ol>
<li>How do I use the camera matrix and distortion coefficients to get coordinates of the object in the image frame?</li>
<li>How do I translate the coordinates on the image frame to the robot end-effector coordinates?</li>
<li>If I keep the camera on a fixed position. Does this mean I only have to do the calibration ones?</li>
</ol>

<p>******* <strong>edit:</strong> *******</p>

<p>I went for a different approach. I managed to solve the rotation and translation between two coordinate systems through SVD. But I made an error in thinking I could use pixel coordinates to translate from the camera coordinate system to that of the robot. I think you need to translate the u, v values first. </p>

<p><strong>How can I translate the pixel uv to world coordinates, so that I can use the code below to get translation and rotation to robot arm?</strong></p>

<p>Here's my code:</p>

<pre><code>#######################################################################
# Step 1: Input camera and world coordinates, and calculate centroids #
#######################################################################
print("""")

# Camera and robot to world coordinates
Pc = np.matrix([[604,119,0],[473,351,0], [730,329,0]])
print(""Camera points matrix: "")
print(Pc)
print("""")

Pr = np.matrix([[177,-38,0],[264,-93,0], [258,4.7,0]])
print(""Robot points matrix: "")
print(Pr)
print("""")

# Calculate centroids
Cc = Pc.mean(axis=0)
Cr = Pr.mean(axis=0)

print(""Centroid camera: "")
print(Cc)
print("""")
print(""Centroid robot: "")
print(Cr)
print("""")

# Pc and Pr - centroids of Pc and Pr
Pc_minus_Cc = np.subtract(Pc, Cc)
print(""Pc - centroidC: "")
print(Pc_minus_Cc)
print("""")

Pr_minus_Cr = np.subtract(Pr, Cr)
print(""Pr - centroidR: "")
print(Pr_minus_Cr)
print("""")

############################################################################
# Step 2: Calculate H, perform SVD and get rotation and translation matrix #
############################################################################

# Get H
print(""(Pr - centroidR) transposed: "")
print(Pr_minus_Cr.T)
print("""")
H = np.matmul(Pc_minus_Cc, Pr_minus_Cr.T)
print(""H: "")
print(H)
print("""")

# Perform SVD
u, s, v = np.linalg.svd(H)
print(""SVD result: "")
print(""u: "")
print("""")
print(u)
print("""")
print(""s: "")
print(s)
print("""")
print(""v: "")
print(v)
print("""")

# Calculate rotation matrix
R = np.matmul(v,u.T)
print(""Rotation matrix: "")
print(R)

# Calculate t
t = -R * Cc.T + Cr.T
print(""t: "")
print(t)
</code></pre>
",2019-05-03 08:07:00,,1696,1,3,0,0.0,4581914.0,,2/19/2015 0:32,39.0,55968784.0,"<p>to 1) you have already code which draws boxes around detected objects. So you have already coordinates in your matrix. If not you could do something like that.</p>

<pre><code>        for c in contours:
        if cv2.contourArea(c) &lt; self.min_area:
            continue
        # detected
        (x, y, w, h) = cv2.boundingRect(c)
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
</code></pre>

<p>and than x + w / 2 is your x in the matrix</p>

<p>to 2 ) you cannot directly you would need someorientation point so that the Arm 
would know where (x+y distance) starts your matrix in the world of the arm</p>

<p>to 3) calibartion depends always on your light conditions right ? so as long they don't change your calibartion should be fine. However it turns out the calibration is needed from time to time e.g. with usb cameras</p>
",3732793.0,0.0,4.0,98582618.0,"The camera matrix is related to the camera per se (focal length, image size, etc) I suggest [this article](http://ksimek.github.io/2013/08/13/intrinsic/). So in other words, for question 3, If you have the same camera/resolution, the calibration (in this case the camera matrix) is the same. For question 1 and 2, you need depth, or know points which you can get the depth out of it. You can take a look to [this](https://www.pyimagesearch.com/2015/01/19/find-distance-camera-objectmarker-using-python-opencv/) and [this](https://docs.opencv.org/3.3.0/dc/d2c/tutorial_real_time_pose.html)"
3446,57123050,Spatial toolbox robot model,|matlab|simulation|spatial|robotics|,"<p>I want to model these D-H parameters in spatial toolbox:</p>

<pre><code>Link:          alpha,      a,        theta,      d
Link 1 :        -90        0        theta1*      d1
Link 2 :          0        a2       theta2*      0
Link 3 :          0        a3       theta3*      0
</code></pre>

<p>This is the code I have tried:</p>

<pre><code>n=3;
rob.NB = n;
rob.parent = [0:n-1];
rob.jtype = { 'R', 'R', 'R' }

l2=0.28;    %link length
l3=0.2;     %link length
d1=0.05;    %link offset


rob.Xtree{1} = rotx(pi)*xlt([0,0,0]);
rob.Xtree{2} = rotz(0)*xlt([l2 0 0]);
rob.Xtree{3} = xlt([l3 0 0]);

ax1=0.03; ay1=0.03; az1=0.03;
ax2=0.28; ay2=0.05; az2=0.05;
ax3=0.2; ay3=0.05; az3=0.05;

rob.I{1} = mcI( 1, [0 0 -0.02], 1/12*[ay1^2+az1^2 0 0; 0 ax1^2+az1^2 0; 0 0 ax1^2+ay1^2] )
rob.I{2} = mcI( 4, [0.14 0 0], 4/12*[ay2^2+az2^2 0 0; 0 ax2^2+az2^2 0; 0 0 ax2^2+ay2^2] )
rob.I{3} = mcI( 3, [0.1 0 0], 3/12*[ay3^2+az3^2 0 0; 0 ax3^2+az3^2 0; 0 0 ax3^2+ay3^2] )

rob.appearance.base = ...
  { 'box', [-0.2 -0.3 -0.2; 0.2 0.3 -0.07] };


rob.appearance.body{1} = ...
    { 'box', [0 -0.07 -0.04; 0.05 0.07 0.04], ...
      'cyl', [0 -0.07 0; 0 0.07 0], 0.06 };

rob.appearance.body{2} = ...
    { 'box', [0 -0.07 -0.04; 0.28 0.07 0.04], ...
      'cyl', [0 0 -0.07; 0 0 0.07], 0.06 };

rob.appearance.body{3} = ...
    { 'box', [0 -0.07 -0.04; 0.2 0.07 0.04], ...
      'cyl', [0 0 -0.07; 0 0 0.07], 0.06 };

showmotion(rob)
</code></pre>

<p>But this is what I get, with motion only along 1 axis: </p>

<p>showmotion robot model</p>

<p><a href=""https://i.stack.imgur.com/R5GVv.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/R5GVv.jpg"" alt=""""></a></p>

<p>how to get the model right?</p>
",2019-07-20 08:32:00,57141051.0,45,1,0,0,,11713385.0,,6/28/2019 11:48,15.0,57141051.0,"<p>I changed my code and now it works properly.</p>

<pre><code>rob.Xtree{1} = rotx(1.57) * xlt([0 0 0]);
rob.Xtree{2} = roty(1.57) * xlt([0.15,0,0]);
rob.Xtree{3} = xlt([0.34 0 0]);
</code></pre>
",11713385.0,0.0,0.0,,
3587,58836294,Create graph from GVD (edges and vertices),|c++|opencv|computer-vision|graph-theory|robotics|,"<p>Given a binary image of a map with obstacles (black pixels), the objective is to create a Voronoi diagram graph. So far, this is done using the brushfire algorithm, from which the GVD can be extracted using the Laplacian operator (i.e. the gradient); the process is demonstrated below and the C++ code can be found in <a href=""https://github.com/martinandrovich/rb-pro5/blob/45be4b23db1b910f8ac6b0facc864d5b0ca3ea95/src/modules/geometry.h#L365"" rel=""nofollow noreferrer"">this file</a>.</p>

<p><a href=""https://i.stack.imgur.com/5pwfE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5pwfE.png"" alt=""gvd process""></a></p>

<p>The goal is now to use the GVD (image 3) to create an adjacency graph, being basically a graph-like data structure with the Voronoi vertices as nodes and edges being the paths between nodes.</p>

<p>My question is, what is the most optimal way of doing this? So far, I have detected vertices of the GVD using the <code>cv::findContours</code> method as shown in the last image, and the idea is to now remove any near-duplicate vertices and try connecting the nodes using the black pixels and a line iterator.</p>

<p>Is there a better way?</p>
",2019-11-13 11:39:00,,270,0,0,1,,1658105.0,Denmark,9/9/2012 12:39,77.0,,,,,,,
3542,58584411,Java: Instance of Class B in Class A where Class B uses variables declared in Class A,|java|class|robotics|,"<p>AutoRedBuilding class declares instance variables leftMotor, rightMotor and foundationServo. RobotMover class contains methods that use those instance variables and I would like to call the RobotMover methods in AutoRedBuilding. </p>

<p>Specifically, the instance variables declared in AutoRedBuilding are actuators like motors and servos. RobotMover is a class that contains methods which move the actuators in a certain way such as driving forward and turning. I would like to somehow call the RobotMover methods in AutoRedBuilding. </p>

<p>I've tried making the AutoRedBuilding variables public, then creating an instance of RobotMover in AutoRedBuilding but that didn't work (code below). Example error message (I get the same error message for all the actuator variables): </p>

<pre><code>RobotMover.java - cannot find symbol
symbol: leftMotor
</code></pre>

<pre><code>// Copyright (c) 2019 Terrace BroBots. All rights reserved.

package org.firstinspires.ftc.teamcode;

import com.qualcomm.robotcore.eventloop.opmode.Autonomous;
import com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;
import com.qualcomm.robotcore.hardware.DcMotor;
import com.qualcomm.robotcore.hardware.Servo;

@Autonomous(name=""Autonomous: Red Building Zone"", group=""SkyStone"")

public class AutoRedBuilding extends LinearOpMode {

    // declare hardware variables
    public DcMotor leftMotor;
    public DcMotor rightMotor;
    public Servo foundationServo;

    @Override
    public void runOpMode() {
        // initialise hardware variables
        leftMotor = hardwareMap.get(DcMotor.class, ""leftMotor"");
        rightMotor = hardwareMap.get(DcMotor.class, ""rightMotor"");
        foundationServo = hardwareMap.get(Servo.class, ""foundationServo"");

        // wait for game to start
        waitForStart();

        /* 
        THIS IS WHERE RobotMover CLASS METHODS ARE CALLED
        */
        RobotMover mover = new RobotMover();
        mover.driveForward(10, 1);
        mover.driveReverse(10, 1);
        mover.turnRight90();
        mover.turnLeft90();
        mover.clipFoundation();
        mover.unclipFoundation();
    }
}

</code></pre>

<pre><code>// Copyright (c) 2019 Terrace BroBots. All rights reserved.

import java.lang.Math; 

public class RobotMover {

    // unit conversion rates
    private double robotWidthCm = 0;
    private double wheelRadiusCm = 0;
    private double degree90ToCm = (2 * Math.PI * robotWidthCm) / 4;
    private double cmToTicks = 2240 / (2 * Math.PI * wheelRadiusCm);

    public void setMotorPowers(double leftPower, double rightPower) {
        leftMotor.setPower(leftPower);
        rightMotor.setPower(rightPower);
    }

    public void driveMotorDistances(double cmLeftDistance, double cmRightDistance, double power) {
        // reset encoders
        leftMotor.setMode(DcMotor.RunMode.STOP_AND_RESET_ENCODER);
        rightMotor.setMode(DcMotor.RunMode.STOP_AND_RESET_ENCODER);

        // convert cm to ticks and set target position
        int tickLeftDistance = (int) Math.round(cmLeftDistance * cmToTicks);
        int tickRightDistance = (int) Math.round(cmRightDistance * cmToTicks);
        leftMotor.setTargetPosition(tickLeftDistance);
        rightMotor.setTargetPosition(tickRightDistance);

        // drive until position is reached 
        setMotorPowers(power, power);
        while(leftMotor.isBusy() &amp;&amp; rightMotor.isBusy()) {}
        setMotorPowers(0, 0);
    }

    /*
    THESE ARE THE METHODS CALLED IN AutoRedBuilding CLASS
    */

    public void driveForward(double cmDistance, double power) {
        driveMotorDistances(cmDistance, cmDistance, power);
    }

    public void driveReverse(double cmDistance, double power) {
        driveMotorDistances(-cmDistance, -cmDistance, power);
    }

    public void turnRight90() {
        driveMotorDistances(degree90ToCm, 0, 0.8);
    }

    public void turnLeft90() {
        driveMotorDistances(0, degree90ToCm, 0.8);
    }

    public void clipFoundation() {
        foundationServo.setPosition(0.5);
    }

    public void unclipFoundation() {
        foundationServo.setPosition(0);
    }
}
</code></pre>
",2019-10-27 23:58:00,,45,1,3,0,,9227818.0,,1/17/2018 5:58,16.0,58584496.0,"<p>If you want <code>RobotMover</code> to know about these variables then they need to be passed to this class either in the constructor or as <code>setter</code> methods</p>

<p>e.g.</p>

<pre><code>RobotMover mover = new RobotMover(DCMotor left, DCMotor right); 
</code></pre>

<p>The <code>RobotMover</code> will need to have corresponding fields </p>
",2310289.0,0.0,0.0,103482962.0,"*but that didn't work* - why do you mean? Do you have some errors, if so please share."
3447,57132401,How to run a robot's controller in multi processes or multi threads in webots?,|robotics|webots|,"<p>I want to have a controller that somehow runs 3 processes to run the robot's code.</p>

<p>I am trying to simulate a humanoid soccer robot in webots . To run our robot's code, we run 3 processes. One for the servomotors' power management , another one for image processing and communications and the last one for motion control.</p>

<p>Now I want to have a controller making me somehow able to simulate something like this or at least similar to it. Does anyone have any idea how I can do this?</p>
",2019-07-21 10:41:00,57140469.0,748,1,0,1,,7892779.0,,4/20/2017 0:43,6.0,57140469.0,"<p>Good news: the Webots API is thread safe :-)</p>

<p>Generally speaking, I would not recommend to use multi-threads, because programming threads is a big source of issues. So, if you have any possibility to merge your threads into a single-threaded application, it's the way to go!</p>

<p>If you would like to go in this direction, the best solution is certainly to create a single controller running your 3 threads, and synchronize them with the main thread (thread 0).</p>

<p>The tricky part is to deal correctly with the time management and the simulation steps. A solution could be to set the <a href=""https://cyberbotics.com/doc/reference/robot"" rel=""nofollow noreferrer""><code>Robot.synchronization</code></a> field to FALSE and to use the main thread to call the <code>wb_robot_step(duration)</code> function every <code>duration</code> time (real time).</p>
",2210777.0,3.0,4.0,,
3612,59170947,Getting the pygame window but no output,|python|algorithm|pygame|robotics|,"<p>I am trying to write an RRT path planning algorithm in python. Although the code executes without error, the result is a plain pygame window with no output. </p>

<pre><code>import sys, random, math, pygame
from math import sqrt,cos,sin,atan2
from pygame.locals import *
pygame.init()

class RRT(object):

X_dimension = 0
Y_dimension = 0
Window_size = 0
EPS         = 0
Max_nodes   = 0 
nodes       = list()
K_ESCAPE    = True
KEYUP       = True
QUIT        = True

def __init__(self,x,y):
    self.x = [10,20,40,50,60,0]
    self.y = [15,30,0,54,75,68]  

#parameters
    self.X_dimension = 1280                                  #length of the window
    self.Y_dimension = 1280                                  #breadth of the window
    self.Window_size = [self.X_dimension, self.Y_dimension]  #Window size
    self.EPS         =  7000                                 #EPSILON or Incremental Distance 
    self.Max_nodes   = 100                                   #maximum number of nodes
    self.QUIT        = QUIT
    self.KEYUP       = KEYUP
    self.K_ESCAPE    = K_ESCAPE


#function for calculating euclidean distance
def Calculate_Distance(self):
    x= self.x
    y=self.y

    return sqrt((x[5]-y[5])*(x[5]-y[5])+(x[4]-y[4])*(x[4]-y[4]))

#Function for calculating all the possible points
def Initiate_Sampling(self):

    self.EPS = 7000 
    y = self.y
    x = self.x

    if self.Calculate_Distance() &lt; 70:
        return y, x
    else:
        theta = atan2(y[1]-x[1], y[0]-x[0])
        return x[0] + self.EPS*cos(theta), x[1] + self.EPS*sin(theta)

#Function for displaying the output
def Start_The_Game(self):

    pygame.init()

    screen = pygame.display.set_mode(self.Window_size)
    caption = pygame.display.set_caption(""performing RRT"")
    white = 255, 240, 200
    black = 20, 20, 40
    screen.fill(black) 

    return('GAME BEGINS')   


#Main Function
def Node_Generation(self, nodes=True):
    self.nodes      = nodes
    self.QUIT       = QUIT
    self.KEYUP      = KEYUP
    self.K_ESCAPE   = K_ESCAPE

    nodes = [(5.0,5.25),(7.0,7.25),(8,8.25)]

    #nodes.append(X_dimension/2.0, Y_dimension/2.0)        
    nodes.append((0.0,0.0))

for i in range(Max_nodes):
    rand = random.random()*640.0, random.random()*480.0
    nn = self.nodes[0]

for p in nodes:
    if self.Calculate_Distance(p,rand) &lt; self.Calculate_Distance(nn,rand):
        nn = p
        newnode = step_from_to(nn,rand)
        nodes.append(newnode)
        pygame.draw.line(screen,white,nn,newnode)
        pygame.display.update()
        #print (i, ""  "", nodes)

for event in pygame.event.get():
    if event.type == QUIT or (event.type == KEYUP and event.key == K_ESCAPE):
        pygame.quit()
        sys.exit(""GAME OVER"")



path = RRT(0,0)#write the starting nodes
path.Calculate_Distance()
path.Initiate_Sampling()
path.Start_The_Game()
path.Node_Generation()    
</code></pre>

<p>The above results in a plain pygame window but no output.</p>
",2019-12-04 07:30:00,,93,0,3,0,,,,,,,,,,,104565837.0,I will fix the indentation. Also the node is a list object. Apart from the indentation is there any logical or syntactical error?
3336,55151675,My team's robot keeps spinning for a random amount of time and we don't know why,|java|methods|robotics|,"<p>We are using the modern robotics gyro.
We program in java.
We are using the base code provided by FTC for the First Tech Challenge.
Here is our gyro code as well as the turning code including the code that is supposed to stop the robot.</p>

<p>gyro = hardwareMap.get(ModernRoboticsI2cGyro.class, ""gyro"");</p>

<p>ModernRoboticsI2cGyro gyro;</p>

<p>private static final double GYRO_TOLERANCE = 4.00; // degrees
  private static final long GYRO_DELAY = 10; //ms</p>

<pre><code>public void turn(double angle, double power)
{

    Hw.frontLeftDrive.setMode(DcMotor.RunMode.RUN_USING_ENCODER);
    Hw.frontRightDrive.setMode(DcMotor.RunMode.RUN_USING_ENCODER);
    Hw.backLeftDrive.setMode(DcMotor.RunMode.RUN_USING_ENCODER);
    Hw.backRightDrive.setMode(DcMotor.RunMode.RUN_USING_ENCODER);

    // Sets angle above 0 and below 360 to prevent errors when plugging values in.
    angle = angle % 360;
    if(angle == 0 || power == 0) // if someone plugs in 0 for power or the angle, it just stops the statement and continues
        return;
    power = Math.abs(power);
    if(power &gt; 0.5) power = 0.5; // and this sets power to half power if anyone plugs in a higher value than half

    if(Math.abs(angle) &gt; 180.0) power = -power; //sets power sign (+ or -) to sign of angle

    //gets current angle and adds desired change to it
    float currentAngle = Hw.gyro.getHeading();
    float targetAngle = (float) (currentAngle + angle);


    // give robot some boundaries when checking gyro to prevent infinite checking of accuracy
    double tolerance = GYRO_TOLERANCE * Math.abs(power);
    long compensatedDelay = (long)(GYRO_DELAY / Math.abs(power));

    //make sure angle is over 0 or under 361
    if (targetAngle &gt; 360)
        targetAngle -= 360;
    else if (targetAngle &lt; 0)
        targetAngle += 360;

    double maxLimit = targetAngle + tolerance;
    double minLimit = targetAngle - tolerance;

    //set power states of motors
    Hw.frontLeftDrive.setPower(power);
    Hw.frontRightDrive.setPower(-power);
    Hw.backLeftDrive.setPower(power);
    Hw.backRightDrive.setPower(-power);

    // repeatedly check position for best accuracy until it falls into the set boundaries
    double current = Hw.gyro.getHeading();
    **while(current &gt; maxLimit || current &lt; minLimit){
        sleep(compensatedDelay);
        current = Hw.gyro.getHeading();
    }**
    //stops motors
    Hw.frontLeftDrive.setPower(0);
    Hw.frontRightDrive.setPower(0);
    Hw.backLeftDrive.setPower(0);
    Hw.backRightDrive.setPower(0);
}
</code></pre>
",2019-03-13 21:46:00,,90,0,5,0,,,,,,,,,,,97044373.0,"hmm either it hops over the tolerance(during sleep) or adding tollerance lead you out of the gyro.getHeading() range(probably both). I see other parts that look confusing. Why don't adjust in the while the power. So you need some initial power(depending on the angle) and the closer you get to target, the slower you turn"
3415,56867596,Is it possible to use G1ANT Studio to automate outlook?,|robotics|rpa|g1ant|,"<p>Greetings fellow software engineers,</p>

<p>I've extremely new to RPA and I'm looking forward to using some of the best tools. The first task that I want to robotize/automate involves heavy use of outlook. I've seen that G1ANT gives the biggest opportunities to use programming languages (C#) and I want to know is it possible to use it to outlook automation or is there any templates/solutions to do it?</p>

<p>Thanks a lot!</p>
",2019-07-03 10:02:00,56871204.0,152,2,0,1,,11733698.0,"Warsaw, Poland",7/3/2019 9:54,46.0,56871204.0,"<p>@NeedHelpAsap, G1ANT offers a number of commands for working with Outlook. If you have already installed the latest G1ANT developer version, open it and look on the left side of the workspace for the ""Addons"" window (if you don't see it, go to the ""View"" menu at the top and click on ""Addons"". You should see this, or similar:</p>

<p><a href=""https://i.stack.imgur.com/lXxh3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lXxh3.png"" alt=""Addon window""></a></p>

<p>Check the ""msoffice"" box as shown. You'll see a list of commands in the window below. The Outlook-related commands portion should look very much like this:</p>

<p><a href=""https://i.stack.imgur.com/SMmtC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SMmtC.png"" alt=""List of commands""></a></p>

<p>Double-clicking any of these commands will bring up its help. In the help, in addition to a description of the command and its arguments, there is a tab for the Manual page for that command. You can also view the <a href=""https://manual.g1ant.com/"" rel=""nofollow noreferrer"">G1ANT.Robot Manual page</a> online. </p>

<p>Hope this gets you started.</p>

<p>Regards,
burque505</p>
",6457212.0,3.0,0.0,,
3293,54790336,Control Webots from external Python IDE,|python|robotics|webots|,"<p>Is it possible to control Webots from external Python IDE (like pyCharm) ? I would appreciate if there is an example showing how to do so and location of modules to be added. Thanks</p>
",2019-02-20 15:52:00,,667,1,6,4,,2974823.0,"6th of October City, Egypt",11/9/2013 20:25,89.0,57497084.0,"<p>Yes, there is some new documentation on how to run Webots robot controllers within PyCharm <a href=""https://cyberbotics.com/doc/guide/using-pycharm-with-webots"" rel=""nofollow noreferrer"">here</a>.</p>
",810268.0,1.0,0.0,96412579.0,You are correct @FabienRohrer. I am working on Windows and I hope to see a short example of external python script for moving a model and if possible getting images from the scene. Thanks a lot
3321,55081981,How to include a folder of libraries in C?,|c|include|libraries|clion|robotics|,"<p>I'm trying to include a folder that contains a combination of around 60 .h and .hpp files. This folder contains libraries for programming robots with a <a href=""https://www.kipr.org/kiss-institute-for-practical-robotics/hardware-software"" rel=""nofollow noreferrer"">Wallaby</a> (a mini-computer-like device) for Botball competition. <code>include</code> is located in the same place as <code>main.c</code> (inside <code>code</code>). Up until now, this is what my header for including libraries looks like:</p>

<pre><code>#include ""../code/include/accel.h""
</code></pre>

<p>Just like <code>accel.h</code>, I have 60 other .h and .hpp files inside <code>include</code>. So, coming to my question, do I need to type out all the 60 header lines? or is there a way to include the <code>include</code> folder. </p>

<p>I'm using Clion for this project, if I can't include the folder itself, does anyone know of a shortcut in Clion to include all the files in <code>include</code>.</p>

<p>I was also thinking of using some sort of placeholder for the folder name and only specify the file type. So, for example: <code>#include ""../code/include/(generic placeholder name).h""</code>. I have no clue if something like this exists.</p>

<p>I would also request you to keep in mind that I'm a beginner to programming, so please keep your answers simple.</p>

<p>This is just for some extra info:<br>
The Wallaby is a mini computer to which you would connect your sensors, motors, servos and cameras in order to control a robot for the Botball competition. Usually, one can connect to the Wallaby either via Wifi Direct or a cable and write programs on it directly through an online interface (not entirely sure of the word for it, but you just type in an IP address in your browser and it brings up an interface where you can make projects and code). All the code written in that interface saves directly onto the Wallaby. Here the default include statement is <code>#include &lt;kipr/botball.h&gt;</code>, so I'm assuming that <code>botball.h</code> (which is located on the Wallaby's storage) has all those 60 libraries consolidated in it. I got the include folder that I'm using from <a href=""https://github.com/kipr/libwallaby"" rel=""nofollow noreferrer"">GitHub</a>. This link was provided to me by one of the Botball organisers. So the main point in me trying to download the library is so that I can write and successfully compile code even when I'm not connected to the Wallaby. Hope this provides some relevant context. </p>

<p>Thank you for your answers!</p>
",2019-03-09 21:10:00,,402,2,5,0,,11176928.0,,3/9/2019 19:27,5.0,55082150.0,"<p>CLion is an IDE for Clang and GCC.  These compilers are instructed to search paths for include files by specifying <code>-I&lt;path&gt;</code> command line arguments.  Any number may be specified, and they are searched in the order given, and the first match found is the file that gets included.</p>

<p>I am not familiar with CLion specifically but no doubt it has a dialog somewhere where you can set header file search paths.</p>

<hr>

<p>Edit:  It seems that CLion may not make this so straightforward.  I understand that you have to add then via CMake: <a href=""https://cmake.org/cmake/help/v3.0/command/include_directories.html#command:include_directories"" rel=""nofollow noreferrer"">https://cmake.org/cmake/help/v3.0/command/include_directories.html#command:include_directories</a>, but after that, the IDE will not recognise the header in the editor and will warn you of unrecognised files and will not provide code comprehension features.  I believe it will build nonetheless.</p>
",168986.0,0.0,0.0,96909757.0,What do you want to achieve? Less typing? Faster something? Smaller something? More maintainability? More readability?
3315,54935240,Robot reverses at high speed before following path given by ros nav stack,|localization|navigation|ros|robotics|motion-planning|,"<p>I am using ros nav stack in conjunction with google cartographer (Mapping and localization) to navigate the robot through a known map. Right now, the robot follows the path generated with acceptable accuracy. But,often, once the path has been generated, the robot reverses at the highest speed set in the params file (escape_velocity parameter), and then starts to move forward correctly on the genrated path.</p>

<p>I have attached images of all my param file: 1.<a href=""https://i.stack.imgur.com/fEzXV.png"" rel=""nofollow noreferrer"">Praram Files-1</a> 2. <a href=""https://i.stack.imgur.com/G3wKO.png"" rel=""nofollow noreferrer"">Param Files-2</a>. The name of each parameter file is mentioned at the top. But to avoid confusion, they are in the order:</p>

<p>A. Param Files-1:
   1. Global Costmap Params  2. Local Costmap Params  3. Common Costmap Params 
   4. Global Plnner Params  5. Local Planner Params</p>

<p>B. Param Files-2: Move Base Params</p>

<p>This is a link to a video of how it looks on rviz. <a href=""https://vimeo.com/320040685"" rel=""nofollow noreferrer"">https://vimeo.com/320040685</a></p>

<p>The thinner line in green is the plan generated by ros nav stack. The thicker line seen later in the video is the actual robot movement. You can see that the robot first reverses and then starts moving forward.</p>

<p>I am new to this forum so please let me know if I need to give anymore data for anyone to answer this</p>

<p>Has anyone else has this problem? Will appreciate any tips on fixing this! Thanks in advance!</p>

<p>P.S: I am using ROS Indigo on Ubuntu 14.04</p>
",2019-02-28 22:25:00,,626,2,0,0,,11133212.0,"MI, USA",2/28/2019 22:08,1.0,54947869.0,"<p>This is a list of observation that may help you to find the problem:</p>

<ol>
<li><p>Your robot seem not sure of is position at the beginning. Try to improve that to get a better navigation.</p></li>
<li><p>(I think it's your problem) Your robot start in the inflation layer of the local costmap. I don't see your local planner file but by default the robot will avoid to be in this place. 
Have a look at  : <a href=""http://wiki.ros.org/costmap_2d/hydro/inflation"" rel=""nofollow noreferrer"">Inflation Costmap Plugin</a></p></li>
<li><p>Check if your robot radius, obstacle avoidance and all that config match to your robot.</p></li>
<li><p>Navfn is quite old, it should works on your project but it might be nice to use global planner. Another local planner could also be tried.</p></li>
</ol>

<p>Hope this help.</p>

<p>Have a nice day! </p>
",8508485.0,0.0,1.0,,
3606,58997045,run background loop or callbacks concurrently with qt C++ application?,|c++|qt|callback|robotics|,"<p>I'm writing a qt5 application in C++ to control my robotic contraption. I'm using a Raspberry Pi running Ubuntu 18.04 to accomplish this. The program will never run on anything but Linux. I'm not too familiar with qt, but it seems like the common way to run a qt application is by returning </p>

<pre><code>int QApplication::exec()
</code></pre>

<p>in the main function. However, I need to run non-GUI-related code concurrently. For example, I need a callback to be executed every time my encoders change state. This callback has nothing to do with user input or the GUI. How can I make the main function receptive to the callback while it is running the qt application window at the same time?</p>

<p>The documentation says: </p>

<pre><code>To make your application perform idle processing, i.e., executing a special function whenever
there are no pending events, use a QTimer with 0 timeout. More advanced idle processing schemes 
can be achieved using processEvents().
</code></pre>

<p>I don't really understand how either of these methods are applicable to my situation. Should either of these schemes be used, or should I use another technique? A simple example would help me a lot.</p>

<p>Edit: I'm adding this due to the comments below:</p>

<p>I'd rather not sample in intervals, because it would most likely miss some changes in state completely. I'm reading the data output from an AM26C32 chip via input on my gpio pins of the Pi. Essentially, every change in state needs to execute a callback. Since there are 40,000 states per motor revolution, I cannot have periodic checks -- the callbacks need to be executed immediately. The encoders (sensors) are incremental and not absolute. Also, I'm using the pigpio library for gpio handling.</p>

<p>Edit 2: After a bit more reading, I think I'll need to use QThread with event loops. The Raspberry Pi 3B+ has 4 cores; if I run the GUI on one thread, and each (of 2) encoder on another thread, this may work. Does anyone have experience with this? Am I on the right track here?</p>
",2019-11-22 15:13:00,58998285.0,941,1,4,0,,6928068.0,,10/5/2016 18:12,14.0,58998285.0,"<p>It seems to me this is more about the design of the program to handle events, probably realtime asyou are talking about robotic control</p>

<p>As callback in qt application thread => Execution blocked by GUI events</p>

<p>In background thread => Mostly free from interruption from GUI events, as you can have control on the priority. See: <a href=""http://man7.org/linux/man-pages/man3/pthread_setschedparam.3.html"" rel=""nofollow noreferrer"">http://man7.org/linux/man-pages/man3/pthread_setschedparam.3.html</a></p>

<p>IMO handling in a background thread is a better options. Even if you are using Qt you can still use other threading / timer implementation.</p>

<p>The Frequency Counter 1 example from pigpio is still applicable</p>

<p>If you need communication between the callbacks and GUI, you can use a wait-free queue (e.g. <a href=""https://github.com/Taymindis/wfqueue"" rel=""nofollow noreferrer"">https://github.com/Taymindis/wfqueue</a> ) to pass message between them, while not being blocked by the progress of GUI thread</p>

<p>At the end you have to ensure the GPIO callback is can be completed in each interval.</p>
",2829853.0,0.0,8.0,104245202.0,"A `QTimer` (interval configurable in milliseconds) performs a periodic callback of a signal handler. If you have to read data periodically it might be sufficient but two additional requirements have to be matched: 1) Reading may not block. 2) Reading may not last too long. It's important that signal handlers return to event loop ASAP. (While a signal is handled the event loop is blocked.) For a blocking read, I see no alternative than to do it in an extra thread."
3412,56806576,Forward and Inverse Kinematics for robot MATLAB,|matlab|simulink|robotics|inverse-kinematics|kinematics|,"<p>Hope you are doing well.</p>

<p>I am verifying the output of my forward kinematics through inverse kinematics and the results are not as desired. As the output of my inverse kinematics is not coming out to be the same as the input of forward kinematics.</p>

<p>The D-H parameters of manipulator is given as: </p>

<p>Link: alpha, a, theta, d</p>

<p>Link 1 :         -90            0        theta1*        d1</p>

<p>Link 2 :           0            a2       theta2*         0</p>

<p>Link 3 :           0            a3       theta3*         0</p>

<p>Functions used are:</p>

<p>Inverse kinematics</p>

<pre><code>function q = inv_kinematics(ph)
%input in radians: [ph1 ph2 ph3] = [0.1 0.2 0.4]
l1 = 0.05;
l2 = 0.28;
l3 = 0.2;
d1 = 0.03;
ph1 = ph(1);
ph2 = ph(2);
ph3 = ph(3);
r=sqrt(ph1^2+(ph3-d1)^2);
alpha=acos((r^2+l2^2-l3^2)/(2*r*l2))
q = zeros(3,1);
q1=atan2(ph2,ph1);
q2=atan2(ph1,ph3-d1)-alpha;
q3=atan2(ph1-l2*sin(q2),ph3-d1-l2*cos(q2))-q2;
q=[q1;q2;q3];
%output: q = [1.107 -0.265 1.314]
end
</code></pre>

<p>Forward kinematics:</p>

<pre><code>function ph  = forward_kinematics(q)
%input: [q1 q2 q3] = [1.107 -0.265 1.314]
l2 = 0.28;
l3 = 0.2;
d1 = 0.03;
q1 = q(1);
q2 = q(2);
q3 = q(3);
ph = zeros(3,1);
ph1 = l2*cos(q1)*cos(q2)+l3*cos(q1)*cos(q2+q3);
ph2 = l2*sin(q1)*cos(q2)+l3*sin(q1)*cos(q2+q3);
ph3 = d1-l2*sin(q2)-l3*sin(q2+q3);
ph=[ph1;ph2;ph3];
%output: p = [0.1655 0.3308 -0.07005] 
end
</code></pre>
",2019-06-28 12:00:00,,1195,0,3,0,,11713385.0,,6/28/2019 11:48,15.0,,,,,,100238852.0,"Very few (if any) people here are going to download a random model from someone they don't know.  As part of your question please provide a value for `q` (presumably a 3-element numeric vector) showing what your first function gives for `ph`, and then what your second function gives for `q`."
3283,54373318,"VSCode Java Project: Class file has wrong version 55.0, should be 52.0, Gradle Build Failing",|java|visual-studio-code|robotics|,"<p>I have a Java project in VSCode that is failing to build via Gradle and WPILib (FRC code).</p>

<p><a href=""https://i.stack.imgur.com/lPJZM.png"" rel=""nofollow noreferrer""> This is a screenshot of my terminal while trying to build.</a>
I've seen replies that say it's a problem with the JAVA_PATH variable, but that can't be it because this code is failing to build on multiple devices. It seems to be a problem with the software itself, has anyone had this problem before or have suggestions about how to fix it? I've checked my vendor libraries, all of them are up to date (the only one relevant here is REV Robotics, and that's the correct version.</p>
",2019-01-25 21:56:00,,1170,1,1,1,0.0,9276670.0,,1/27/2018 14:23,22.0,54373519.0,"<p>This errors say, that the classes inside SparkMax-java-1.0.27.jar were compiled with a newer Java compiler (Java 11) and your Gradle Build compiles with a Java 8 compiler. You have to set your Gradle build to compile with Java 11.</p>

<p>Should be build.gradle</p>

<pre><code>apply plugin: 'java'
sourceCompatibility = 11
targetCompatibility = 11
</code></pre>
",10519477.0,1.0,0.0,95561201.0,"Possible duplicate of [Class file has wrong version 52.0, should be 50.0](https://stackoverflow.com/questions/28180915/class-file-has-wrong-version-52-0-should-be-50-0)"
3267,54147301,Applying PID Controller for trajectory planning algorithm,|linear-algebra|robotics|pid-controller|,"<p>I am developing a path planning algorithm for a Segway robot. After running the algorithm path to the goal is obtained in form of <code>(x, y, theta)</code> coordinates. The hardware and sensor noises causing much trouble in following the path and error is accumulating. I thought of applying PID Control. I need to set the steering angle to the direction of movement by calculating the cross track error. </p>

<p>1) How to calculate the CTE with original co-ordinate <code>(x1, y1, theta1)</code> and current position <code>(x1', y1', theta1')</code>?</p>

<p>2) How to select values for <i>K<sub>p</sub></i>, <i>K<sub>i</sub></i> and <i>K<sub>d</sub></i> for this scenario?</p>

<p>Additional information: Development environment is Android studio.</p>
",2019-01-11 13:15:00,54184184.0,577,1,2,0,,10579805.0,Germany,10/30/2018 10:23,32.0,54184184.0,"<p>You can find the <i>K<sub>p</sub></i>, <i>K<sub>i</sub></i>, <i>K<sub>d</sub></i> values either experimentally or with a method like <a href=""https://en.wikipedia.org/wiki/Ziegler%E2%80%93Nichols_method"" rel=""nofollow noreferrer"">Ziegler-Nichols Method</a> which would be more accurate.</p>

<p>Experimentally, you can try:</p>

<ol>
<li>Set all gains to 0.</li>
<li>Increase <i>K<sub>d</sub></i> until the system oscillates.</li>
<li>Reduce <i>K<sub>d</sub></i> by a factor of 2-4.</li>
<li>Set <i>K<sub>p</sub></i> to about 1% of <i>K<sub>d</sub></i>.</li>
<li>Increase <i>K<sub>p</sub></i> until oscillations start.</li>
<li>Decrease <i>K<sub>p</sub></i> by a factor of 2-4.</li>
<li>Set <i>K<sub>i</sub></i> to about 1% of <i>K<sub>p</sub></i>.</li>
<li>Increase <i>K<sub>i</sub></i> until oscillations start.</li>
<li>Decrease <i>K<sub>i</sub></i> by a factor of 2-4.</li>
</ol>
",3636858.0,1.0,0.0,95392439.0,Cross track error
3928,65206902,How to use the iot lab platform and how to choose the right nodes,|iot|robotics|riot-os|,"<p>I want to create an Iot-project where I am gonna have to create a watering system.
I am going to use Riot-OS on an ESP32 or ESP8266 that will interact with a water pump, n-mosfet, power supply and a humidity sensor.
At the same time I will have a Linux webserver running accepting the data from this controller.</p>
<p>Until my hardware arrives I want to test the code using iot-lab. However, I do not know what nodes I have to use and why. How do I choose the correct ones?
Do I also need an .elf file for the sensor, like in this tutorial <a href=""https://www.iot-lab.info/earn/"" rel=""nofollow noreferrer"">https://www.iot-lab.info/earn/</a> is being used for the lamp ?</p>
<p>Also, regarding if I use the ssh keygen command and then the copy paste and the connect to the experiment is the only thing that I need to do in order to run my code properly??</p>
<p>Is there a good tutorial that explains those things because I did not find any ?</p>
",2020-12-08 21:06:00,,109,1,5,0,,,,,,65343909.0,"<p>If you are going to use IoT lab, you only can use the <code>Pycom FiPy</code>.</p>
<blockquote>
<p>Until my hardware arrives I want to test the code using iot-lab.
However, I do not know what nodes I have to use and why. How do I
choose the correct ones? Do I also need an .elf file for the sensor,
like in this tutorial <a href=""https://www.iot-lab.info/earn/"" rel=""nofollow noreferrer"">https://www.iot-lab.info/earn/</a> is being used for
the lamp ?</p>
</blockquote>
<p>Just follow the <a href=""https://doc.riot-os.org/getting-started.html"" rel=""nofollow noreferrer"">getting started guide.</a> That should do the job.
<code>make flash</code> is generating the elf and flashs it. I recommend the target <code>native</code> for development. <code>BOARD=native make flash term</code> in order to also see the serial output. It has a proper IP stack (when configured with <code>tapsetup</code>) and works well enough for development. At least for the networking part. When you need to interact with the hardware, you have to use the actual hardware, of course. IoTLab is only useful, if you have big routing simulations etc. IoTLab also doesn't have the water pump etc. you need.</p>
<blockquote>
<p>Is there a good tutorial that explains those things because I did not find any ?</p>
</blockquote>
<p>Checkout the <a href=""https://github.com/RIOT-OS/RIOT/tree/master/examples"" rel=""nofollow noreferrer"">RIOT OS examples</a>.
<code>gnrc networking</code> and <code>saul</code> are interesting for you, I guess.
You should also read the <a href=""https://riot-os.org/api/group__drivers__saul.html"" rel=""nofollow noreferrer"">SAUL documentation</a>.</p>
",4094489.0,0.0,0.0,115321406.0,"Are you sure they didn't mean that you're supposed to program riot in Linux, and then flash it on your esp32? With a Linux server running alongside of it?"
3739,63019490,Error with few arguments while running from ROS a python file,|python|pycharm|ros|robotics|,"<p>I am new to ROS and I am working with a Jaco 2 robotic arm (model 7 DOF)
Now I am trying to do some stuff to understand better how exactly it works</p>
<p>I found some code here:</p>
<p>[https://github.com/Kinovarobotics/kinova-ros/blob/master/kinova_demo/nodes/kinova_demo/pose_action_client.py][1]</p>
<p>to move the arm but I when I tried to run it from termninal with command : python JacoTutorial.py</p>
<p>I had a weird error that says:</p>
<p>**</p>
<pre><code>JacoTutorial.py [-h] [-r] [-v]
                       kinova_robotType [unit] [pose_value [pose_value ...]]
JacoTutorial.py: error: too few arguments
</code></pre>
<p>**</p>
<p>The problem is that because Pycharm doesnt recognize rospy and all ros libraries I cannot also debug it to see where the problem with the few argument exists...</p>
<p>I thought that maybe the code is only for 6 DOF Jaco but it doesnt seem to have different arguments for different models.</p>
<p>Does anybody face that error in the past ?</p>
",2020-07-21 17:03:00,,150,1,0,0,,12737914.0,,1/18/2020 15:16,7.0,63059513.0,"<p>It does not look like there is an error with the actual program, you need to pass arguments in the command line to the program to configure it correctly.</p>
<p>If you run <code>python JacoTutorial.py -h</code> (<code>-h</code> is an argument!), it will display the help prompt with all of the arguments you can pass on the command line. Looks like the only required argument is <code>kinova_robotType</code>.</p>
",8316278.0,0.0,0.0,,
3727,62553569,problem while installing rock-robotics package in ubuntu 18.04 LTS,|installation|ubuntu-18.04|robotics|corba|,"<p>I've followed the instalation guide proposed at &quot;<a href=""https://www.rock-robotics.org/documentation/installation.html"" rel=""nofollow noreferrer"">https://www.rock-robotics.org/documentation/installation.html</a>&quot; .I'm having an error while installing &quot;rock-robotics&quot; I tried at first in my lighter distribution &quot;Lubuntu&quot; and now I'm trying in &quot;Ubuntu 18.04 LTS&quot; and it happens again, I'm getting crazy with this and the very problem is that I need this package to develop my degree final thesis. Please I need help on this matter, at the end you can see the full output when I run $sudo sh bootstrap.sh (I tried both of the bootstraps suggested and both of them give me the same problem) and here you have the <strong>output lines of the error message</strong>:</p>
<pre><code>configuring CMake for base/orogen/std   configured CMake for tools/pocolog_cpp   ERROR: got an error processing base/orogen/std, waiting for pending jobs to end   updated environment Command failed base/orogen/std(/home/emi/rock-robotics/base/orogen/std): failed in configure phase
    'cmake -DCMAKE_INSTALL_PREFIX=/home/emi/rock-robotics/install -DCMAKE_MODULE_PATH= -DCMAKE_PREFIX_PATH=/home/emi/rock-robotics/install;/home/emi/rock-robotics/tools/orogen
-DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DCMAKE_BUILD_TYPE=Release -DROCK_TEST_ENABLED=OFF /home/emi/rock-robotics/base/orogen/std' returned status 1
    see /home/emi/rock-robotics/install/log/base/orogen/std-configure.log for details
    last 10 lines are:
    -- Checking for module 'orocos-rtt-corba-gnulinux&gt;=2.1.0'
    --   No package 'orocos-rtt-corba-gnulinux' found
    CMake Error at .orogen/config/FindOrocosCORBA.cmake:8 (MESSAGE):
      RTT has not been built with CORBA support
    Call Stack (most recent call first):
      .orogen/typekit/transports/corba/CMakeLists.txt:4 (find_package)
   
   
    -- Configuring incomplete, errors occurred!
    See also &quot;/home/emi/rock-robotics/base/orogen/std/build/CMakeFiles/CMakeOutput.log&quot;.
</code></pre>
<p><strong>FULL-OUTPUT</strong></p>
<pre><code>--2020-06-24 12:02:16--  http://rock-robotics.org/master/autoproj_bootstrap
Resolviendo rock-robotics.org (rock-robotics.org)... 37.17.224.128
Conectando con rock-robotics.org (rock-robotics.org)[37.17.224.128]:80... conectado.
Peticin HTTP enviada, esperando respuesta... 301 Moved Permanently
Ubicacin: https://rock-robotics.org/master/autoproj_bootstrap [siguiente]
--2020-06-24 12:02:17--  https://rock-robotics.org/master/autoproj_bootstrap
Conectando con rock-robotics.org (rock-robotics.org)[37.17.224.128]:443... conectado.
Peticin HTTP enviada, esperando respuesta... 301 Moved Permanently
Ubicacin: https://www.rock-robotics.org/master/autoproj_bootstrap [siguiente]
--2020-06-24 12:02:17--  https://www.rock-robotics.org/master/autoproj_bootstrap
Resolviendo www.rock-robotics.org (www.rock-robotics.org)... 185.199.110.153, 185.199.111.153, 185.199.109.153, ...
Conectando con www.rock-robotics.org (www.rock-robotics.org)[185.199.110.153]:443... conectado.
Peticin HTTP enviada, esperando respuesta... 200 OK
Longitud: 30078 (29K) [application/octet-stream]
Guardando como: autoproj_bootstrap

autoproj_bootstrap  100%[===================&gt;]  29,37K  --.-KB/s    en 0,03s   

2020-06-24 12:02:17 (906 KB/s) - autoproj_bootstrap guardado [30078/30078]

Which protocol do you want to use to access rock-core/buildconf.git on github.com? [git|ssh|http] (default: http) 
Detected 'gem' to be /usr/bin/gem2.5
Detected bundler at /home/emi/.local/share/autoproj/gems/ruby/2.5.0/bin/bundle
Installing autoproj in /home/emi/.local/share/autoproj/gems/ruby/2.5.0
Don't run Bundler as root. Bundler can ask for sudo if it is needed, and
installing your bundle as root will break this application for all non-root
users on this machine.
[DEPRECATED] The --binstubs option will be removed in favor of `bundle binstubs`
Fetching gem metadata from https://rubygems.org/......
Fetching gem metadata from https://rubygems.org/.
Resolving dependencies...
Using rake 12.3.3
Using equatable 0.5.0
Using tty-color 0.4.3
Using pastel 0.7.2
Using tty-cursor 0.5.0
Using necromancer 0.4.0
Using timers 4.3.0
Using tty-screen 0.6.5
Using wisper 2.0.1
Using tty-reader 0.2.0
Using tty-prompt 0.15.0
Using facets 3.1.0
Using utilrb 3.0.1
Using autobuild 1.20.0
Using backports 3.18.1
Using bundler 2.1.4
Using concurrent-ruby 1.0.5
Using ffi 1.13.1
Using rb-inotify 0.10.1
Using thor 0.20.3
Using tty-spinner 0.8.0
Using xdg 2.2.5
Using autoproj 2.12.1
Bundle complete! 2 Gemfile dependencies, 23 gems now installed.
Bundled gems are installed into `/home/emi/.local/share/autoproj/gems`
starting the newly installed autoproj for stage2 install
saving temporary env.sh and .autoproj/env.sh
running 'autoproj envsh' to generate a proper env.sh
[DEPRECATED] `Bundler.with_clean_env` has been deprecated in favor of `Bundler.with_unbundled_env`. If you instead want the environment before bundler was originally loaded, use `Bundler.with_original_env` (called at /home/emi/rock-robotics/.autoproj/bin/autoproj:8)
  Which prepackaged software (a.k.a. 'osdeps') should autoproj install automatically (all, none or a comma-separated list of: os gem pip) ?
    The software packages that autoproj will have to build may require other
    prepackaged softwares (a.k.a. OS dependencies) to be installed (RubyGems
    packages, packages from your operating system/distribution, ...). Autoproj
    is able to install those automatically for you.
    
    Advanced users may want to control this behaviour. Additionally, the
    installation of some packages require administration rights, which you may
    not have. This option is meant to allow you to control autoproj's behaviour
    while handling OS dependencies.
    
    * if you say &quot;all&quot;, it will install all packages automatically.
      This requires root access thru 'sudo'
    * if you say &quot;pip&quot;, only the Python packages will be installed.
      Installing these packages does not require root access.
    * if you say &quot;gem&quot;, only the Ruby packages will be installed.
      Installing these packages does not require root access.
    * if you say &quot;os&quot;, only the OS-provided packages will be installed.
      Installing these packages requires root access.
    * if you say &quot;none&quot;, autoproj will not do anything related to the
      OS dependencies.
    
    Finally, you can provide a comma-separated list of pip gem and os.
    
    As any configuration value, the mode can be changed anytime by calling
      autoproj reconfigure
    
    Finally, the &quot;autoproj osdeps&quot; command will give you the necessary information
    about the OS packages that you will need to install manually.
    
    So, what do you want ? (all, none or a comma-separated list of: os gem pip) [all] 
  Would you like autoproj to keep apt packages up-to-date? [yes] 
  updated environment
running 'autoproj osdeps' to re-install missing gems
[DEPRECATED] `Bundler.with_clean_env` has been deprecated in favor of `Bundler.with_unbundled_env`. If you instead want the environment before bundler was originally loaded, use `Bundler.with_original_env` (called at /home/emi/rock-robotics/.autoproj/bin/autoproj:8)
  updated environment
Command finished successfully at 2020-06-24 12:02:30 +0200 (took 1 sec)
The current directory is not empty, continue bootstrapping anyway ? [yes] 
  checked out autoproj main configuration


autoproj bootstrap successfully finished

To further use autoproj and the installed software, you
must add the following line at the bottom of your .bashrc:
source /home/emi/rock-robotics/env.sh

WARNING: autoproj will not work until your restart all
your consoles, or run the following in them:
$ source /home/emi/rock-robotics/env.sh

To import and build the packages, you can now run
aup
amake

The resulting software is installed in
/home/emi/rock-robotics/install

  How should I interact with github.com (git, http, ssh)
    If you give one value, it's going to be the method used for all access
    If you give multiple values, comma-separated, the first one will be
    used for pulling and the second one for pushing. An optional third value
    will be used to pull from private repositories (the same than pushing is
    used by default) [http,ssh] 
  operating system: ubuntu,debian - 18.04,18.04.4,lts,bionic,beaver
  updating bundler
  updating autoproj
  bundler: connected to https://rubygems.org/
  already up-to-date autoproj main configuration
  checking out git:https://github.com/rock-core/package_set.git interactive=false push_to=git@github.com:/rock-core/package_set.git repository_id=github:/rock-c  checked out git:https://github.com/rock-core/package_set.git interactive=false push_to=git@github.com:/rock-core/package_set.git repository_id=github:/rock-core/package_set.git retry_count=10
  Which flavor of Rock do you want to use ?
    Stay with the default ('master') if you want to use Rock on the most recent
    distributions (Ubuntu 16.04 and later). Use 'stable' only for 
    now officially unsupported distributions (Ubuntu 14.04) [master] 
  Do you want to activate python? [no] 
  checking out git:https://github.com/rock-core/rock-package_set.git interactive=false push_to=git@github.com:/rock-core/rock-package_set.git repository_id=gith  checked out git:https://github.com/rock-core/rock-package_set.git interactive=false push_to=git@github.com:/rock-core/rock-package_set.git repository_id=github:/rock-core/rock-package_set.git retry_count=10
  checking out git:https://github.com/rock-tutorials/tutorials-package_set.git interactive=false push_to=git@github.com:/rock-tutorials/tutorials-package_set.gi  checked out git:https://github.com/rock-tutorials/tutorials-package_set.git interactive=false push_to=git@github.com:/rock-tutorials/tutorials-package_set.git repository_id=github:/rock-tutorials/tutorials-package_set.git retry_count=10
  checking out git:https://github.com/orocos-toolchain/autoproj.git interactive=false push_to=git@github.com:/orocos-toolchain/autoproj.git repository_id=github  checked out git:https://github.com/orocos-toolchain/autoproj.git interactive=false push_to=git@github.com:/orocos-toolchain/autoproj.git repository_id=github:/orocos-toolchain/autoproj.git retry_count=10
  WARN: osdeps definition for cmake, previously defined in /home/emi/.local/share/autoproj/gems/ruby/2.5.0/gems/autoproj-2.12.1/lib/autoproj/default.osdeps overridden by /home/emi/rock-robotics/autoproj/remotes/rock.core/rock.osdeps:
  WARN:   resp. apt-dpkg: cmake
  WARN:         osdep: build-essential
  WARN:   and   apt-dpkg: cmake
  Do you need compatibility with OCL ? (yes or no)
    New Rock users that don't need backward compatibility with legacy Orocos components
    probably want to say 'no'. Otherwise, say 'yes'.
    Saying 'yes' will significantly impact compilation time and the size of the resulting binaries
    Please answer 'yes' or 'no' [no] 
  the target operating system for Orocos/RTT (gnulinux, xenomai, or macosx) [gnulinux] 
  which CORBA implementation should the RTT use ?
    Answer &quot;none&quot; to disable CORBA, otherwise pick either tao or omniorb [omniorb] &quot;none&quot;
invalid value: invalid value '&quot;none&quot;', accepted values are 'none', 'tao', 'omniorb' (without the quotes)
  which CORBA implementation should the RTT use ?
    Answer &quot;none&quot; to disable CORBA, otherwise pick either tao or omniorb [omniorb] none
  checked out base/templates/cmake_vizkit_widget
  checked out base/orogen/std
  checked out base/console_bridge
  checked out base/numeric
  checked out base/logging
  checked out base/templates/bundle
  checked out base/templates/vizkit3d_plugin
  checked out base/templates/ruby_lib
  checked out base/orogen/types
  checked out base/templates/cmake_lib
  checked out base/scripts
  checked out base/cmake
  checked out bundles/rock
  checked out drivers/orogen/aggregator
  checked out bundles/common_models
  checked out drivers/orogen/iodrivers_base
  checked out drivers/aggregator
  checked out drivers/iodrivers_base
  checked out drivers/orogen/transformer
  checked out drivers/transformer
  checked out base/types
  checked out perception/frame_helper
  checked out perception/jpeg_conversion
  checked out gui/osgviz
  checked out tools/log_tools
  checked out gui/rock_webapp
  checked out gui/vizkit3d
  checked out tools/logger
  tools/class_loader: checking out branch indigo-devel
  checked out gui/rock_widget_collection
  checked out tools/class_loader
  checked out tools/orogen_metadata
  checked out gui/vizkit
  checked out tools/pocolog2msgpack
  checked out tools/pocolog_cpp
  checked out tools/pocolog
  checked out tools/telemetry
  checked out base/templates/doc
  checked out tools/rest_api
  checked out rtt_typelib
  checked out utilrb
  checked out tools/orocos.rb
  checked out orogen
  checked out tools/metaruby
  checkout of tools/syskit failed, deleting the source directory /home/emi/rock-robotics/tools/syskit and retrying (1/10)
  checkout of tools/msgpack-c failed, deleting the source directory /home/emi/rock-robotics/tools/msgpack-c and retrying (1/10)
  checked out tools/syskit
  tools/msgpack-c: resetting branch master to 83a82e3eb512b18d4149cabb7eb43c7e8bc081af
  checked out tools/msgpack-c
  WARN: tools/msgpack-c from rock.core does not have a manifest
  checkout of tools/service_discovery failed, deleting the source directory /home/emi/rock-robotics/tools/service_discovery and retrying (1/10)
  checkout of typelib failed, deleting the source directory /home/emi/rock-robotics/tools/typelib and retrying (1/10)
  checkout of tools/roby failed, deleting the source directory /home/emi/rock-robotics/tools/roby and retrying (1/10)
  checkout of external/sisl failed, deleting the source directory /home/emi/rock-robotics/external/sisl and retrying (1/10)
  checked out typelib
  typelib: using the castxml importer
  checked out tools/service_discovery
  checked out external/sisl
  checked out tools/roby
  checkout of rtt failed, deleting the source directory /home/emi/rock-robotics/tools/rtt and retrying (1/10)
  checked out rtt
  building initial autoproj import log, this may take a while
  bundler: connected to https://rubygems.org/
  updated environment
Command finished successfully at 2020-06-24 12:08:47 +0200 (took 6 mins 9 secs)
  bundler: connected to https://rubygems.org/
  updated environment
Command finished successfully at 2020-06-24 12:08:52 +0200 (took 3 secs)
  operating system: ubuntu,debian - 18.04,18.04.4,lts,bionic,beaver
  WARN: osdeps definition for cmake, previously defined in /home/emi/.local/share/autoproj/gems/ruby/2.5.0/gems/autoproj-2.12.1/lib/autoproj/default.osdeps overridden by /home/emi/rock-robotics/autoproj/remotes/rock.core/rock.osdeps:
  WARN:   resp. apt-dpkg: cmake
  WARN:         osdep: build-essential
  WARN:   and   apt-dpkg: cmake
  WARN: tools/msgpack-c from rock.core does not have a manifest
  typelib: using the castxml importer
  configured CMake for tools/msgpack-c
  built tools/msgpack-c
  installed tools/msgpack-c
  configured CMake for rtt
  built rtt (10 warnings)
  set up Ruby package utilrb
  installed rtt
  configured CMake for external/sisl
  set up Ruby package tools/metaruby
  configured CMake for typelib
  set up Ruby package tools/roby
  built typelib (2 warnings)
  set up Ruby package base/scripts
  installed typelib
  built external/sisl (165 warnings)
  set up Ruby package tools/pocolog
  configured CMake for rtt_typelib
  installed external/sisl
  built rtt_typelib
  configured CMake for base/cmake
  installed rtt_typelib
  built base/cmake
  set up Ruby package orogen
  installed base/cmake
  set up Ruby package tools/log_tools
  configured CMake for tools/orogen_metadata
  configured CMake for gui/osgviz
  built tools/orogen_metadata
  configured CMake for base/logging
  built gui/osgviz (6 warnings)
  installed tools/orogen_metadata
  installed gui/osgviz
  built base/logging
  configured CMake for gui/vizkit3d
  built gui/vizkit3d (7 warnings)
  installed base/logging
  installed gui/vizkit3d
  configured CMake for tools/service_discovery
  configured CMake for base/console_bridge
  configured CMake for base/types
  built tools/service_discovery
  installed tools/service_discovery
  built base/types (3 warnings)
  built base/console_bridge
  generated oroGen base/orogen/std
  installed base/types
  installed base/console_bridge
  configuring CMake for base/orogen/std
  configured CMake for tools/pocolog_cpp
  ERROR: got an error processing base/orogen/std, waiting for pending jobs to end
  updated environment
Command failed
base/orogen/std(/home/emi/rock-robotics/base/orogen/std): failed in configure phase
    'cmake -DCMAKE_INSTALL_PREFIX=/home/emi/rock-robotics/install -DCMAKE_MODULE_PATH= -DCMAKE_PREFIX_PATH=/home/emi/rock-robotics/install;/home/emi/rock-robotics/tools/orogen -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DCMAKE_BUILD_TYPE=Release -DROCK_TEST_ENABLED=OFF /home/emi/rock-robotics/base/orogen/std' returned status 1
    see /home/emi/rock-robotics/install/log/base/orogen/std-configure.log for details
    last 10 lines are:

    -- Checking for module 'orocos-rtt-corba-gnulinux&gt;=2.1.0'
    --   No package 'orocos-rtt-corba-gnulinux' found
    CMake Error at .orogen/config/FindOrocosCORBA.cmake:8 (MESSAGE):
      RTT has not been built with CORBA support
    Call Stack (most recent call first):
      .orogen/typekit/transports/corba/CMakeLists.txt:4 (find_package)
    
    
    -- Configuring incomplete, errors occurred!
    See also &quot;/home/emi/rock-robotics/base/orogen/std/build/CMakeFiles/CMakeOutput.log&quot;.
</code></pre>
",2020-06-24 11:03:00,63002782.0,322,1,0,0,,13805178.0,,6/24/2020 10:38,28.0,63002782.0,"<p>Finaly I solved it following some advices of the rock-robotics team. I used another bootstrap.sh and also accepted the corba support, the complete process was:</p>
<pre><code>mkdir rock-workspace
cd rock-workspace
wget https://www.rock-robotics.org/bootstrap.sh
sh bootstrap.sh
</code></pre>
<p>Then the answers to the critical questions while bootstrapping:</p>
<pre><code>Do you need compatibility with OCL ? (yes or no)
New Rock users that don't need backward compatibility with legacy Orocos components
probably want to say 'no'. Otherwise, say 'yes'.
Saying 'yes' will significantly impact compilation time and the size of the resulting binaries
Please answer 'yes' or 'no' [no] no
the target operating system for Orocos/RTT (gnulinux, xenomai, or macosx) [gnulinux] gnulinux
which CORBA implementation should the RTT use ?
Answer &quot;none&quot; to disable CORBA, otherwise pick either tao or omniorb [omniorb] omniorb
</code></pre>
<p>so the answers were <strong>no</strong>, <strong>gnulinux</strong> and <strong>omniorb</strong></p>
",13805178.0,0.0,0.0,,
3707,61838032,Coordinate axis transformation for robot manipulation,|python|linear-algebra|ros|robotics|coordinate-transformation|,"<p>This is a fairly generic robotics based Linear Algebra question, and I'm looking for an algorithm based mathematical approach to solve my problem and get an understanding, rather than a strictly ROS based answer.</p>

<p>I am trying out a custom picking of an object using a Robotic Arm. I have a separate perception module that detects objects and estimates the pose for grasping. It, however is in the camera frame and follows image processing convention of coordinate frames, i.e. with right: </p>

<blockquote>
  <p>+x-axis, forward: +z-axis, down: +y-axis</p>
</blockquote>

<p>From this perception module, I get two values - 3x3 Rotation matrix and 1x3 translation vector. 
As an example, say <code>T1</code></p>

<pre><code>Tra: [0.09014122 0.16243269 0.6211668 ]
Rot: [[ 0.          0.03210089 -0.99948463]
[ 0.          0.99948463  0.03210089]
[ 1.         -0.          0.        ]]
</code></pre>

<p>(i.e. I have to grasp at that location and in that orientation)
My robot base to camera transform is understandably in the right hand coordinate system. Here is an example of the same, say <code>T2</code></p>

<pre><code>translation: 
  x: 0.0564581200121
  y: 0.318823912978
  z: 0.452250135698
rotation: 
  x: -0.6954818376
  y: 0.693982204231
  z: -0.13156524004
  w: 0.13184954074
</code></pre>

<p>I am using <code>scipy.spatial.transform</code> to convert my poses from one format to another, so the actual implementation can handle any format of pose.</p>

<p>Now, to get the pose of the object from the robot, is a simple transformation T2 times T1. However, T1 follows a different convention from T2.</p>

<p>How would I go about this ? A detailed explanation using this example will be highly appreciated! I am trying to understand from scratch, hence I would prefer to arrive at a transformation matrix on my own to apply to the above ones to get the final pose.</p>

<p>This question probably belongs to Mathematics Stack Exchange and ROS as well, but as I mentioned, I am trying to approach it analytically.</p>
",2020-05-16 14:03:00,61865294.0,475,1,0,1,,9926472.0,,6/11/2018 17:23,124.0,61865294.0,"<p>If you want to get the pose of the object by multiplying T2*T1, both T1, T2 must be in the homogeneous coordinates, where T1, T2 contain rotation matrix and translation. And the size of T1, T2 should be 4x4 for 3D.</p>

<p><a href=""https://i.stack.imgur.com/ppa6R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ppa6R.png"" alt=""enter image description here""></a></p>

<p>In your case, the orientation of T2 is represented in Quaternion. Convert this into rotation matrix and create a homogeneous matrix using rotation matrix, translation.</p>
",1595504.0,0.0,3.0,,
3646,60412326,What is the good choice for NodeJS Robotics Raspberry pi or Arduino?,|javascript|node.js|arduino|raspberry-pi|robotics|,"<p>Actually, I'm very new to NodeJS and Robotics. I want to know which device is good for Nodejs Robotics before buying it. </p>

<p>Thanks</p>
",2020-02-26 10:55:00,,78,1,1,0,0.0,8101376.0,"Dhaka Division, Bangladesh",6/2/2017 5:42,8.0,60412429.0,"<p>I've used both. For the Arduino I needed to use Firmata and drive with an RPi, so I'd say a RPi.</p>

<p>A Raspberry Pi Zero is close to the size of an Arduino.</p>
",1758461.0,2.0,0.0,106870231.0,You can compare them here http://johnny-five.io/platform-support/
3934,65387147,Raspberry Pi connected to Motor Controller- Getting error in code,|python|gpio|robotics|,"<pre><code>*Hi, I am getting this code error:
*blink.py:25: RuntimeWarning: This channel is already in use, continuing anyway. Use GPIO.setwarnings(False) to disable warnings.*
GPIO.setup(ENA1,GPIO.OUT)
Traceback (most recent call last):
File &quot;/home/pi/my_python_programs/blink.py&quot;, line 27, in &lt;module&gt;
GPIO.output(IN1,GPIO.LOW)
RuntimeError: The GPIO channel has not been set up as an OUTPUT*
</code></pre>
<p><strong>It is in reference to this code I made to control a motor controller from the raspberry pi. Here is my code. I am wondering what the error is:</strong></p>
<pre><code>#ImportedLibraries
import RPi.GPIO as GPIO
from time import sleep
#Motor1
PWR= 17
ENA1 = 33
IN1 = 31
IN2 = 29
GND = 39
#Motor2
PWR = 1
ENA2 = 32
IN3 = 18
IN4 = 16
GND = 34
#SetMode
GPIO.setmode(GPIO.BOARD)
#Motor1
GPIO.setup(ENA1,GPIO.OUT)
PWM1=GPIO.PWM(ENA1,100)
GPIO.output(IN1,GPIO.LOW)
GPIO.output(IN2,GPIO.LOW)
#Motor2
GPIO.setup(ENA2,GPIO.OUT)
PWM2=GPIO.PWM(ENA2,100)
GPIO.output(IN3,GPIO.LOW)
GPIO.output(IN4,GPIO.LOW)
PWM1.start(10)
</code></pre>
",2020-12-21 03:14:00,,201,1,1,0,,14862630.0,,12/21/2020 2:43,1.0,65512574.0,"<p>It seems that the setup is not been doing very well, it fails in line 25.</p>
<p>If you want to control a motor you will need a H bridge, like L293D or L298, and then use the gpiozero library, look for it in the net.</p>
<p>Try to use this code:</p>
<pre><code>from gpiozero import Motor
from time import sleep

motor = Motor(forward=4, backward=14)

while True:
    motor.forward()
    sleep(5)
    motor.backward()
    sleep(5)
</code></pre>
",14402174.0,0.0,0.0,115601022.0,I would ask this at https://raspberrypi.stackexchange.com
3909,64552488,OPC UA Robotics - Schema,|robotics|opc-ua|,"<p>The OPC UA Companion Spec for Robotics defines <code>new Enumeration DataTypes</code></p>
<ul>
<li>AxisMotionProfileEnumeration (used under AxisType)</li>
<li>ExecutionModeEnumeration (used under TaskControlType)</li>
<li>MotionDeviceCategoryEnumeration (used under MotionDeviceType)</li>
<li>OperationalModeEnumeration (used under SafetyStateType)</li>
</ul>
<p>However, <code>no official bsd file</code> was released as part of the OPC UA Robotics Schema. Any reason why?</p>
",2020-10-27 10:28:00,,103,1,0,1,,10842865.0,,12/28/2018 11:52,4.0,65253265.0,"<p>Just adding the <a href=""https://github.com/OPCFoundation/UA-Nodeset/issues/75#issuecomment-719587316"" rel=""nofollow noreferrer"">answer</a> of OPC Foundation, also here:</p>
<blockquote>
<p>The BSD is embedded in the NodeSet as Base64 data.
It can be extracted with a copy and paste.
We have requested that all specification writers provide the BSD when their spec is
released.
It will take a bit of time for current specifications to catch up.</p>
</blockquote>
",9474615.0,0.0,0.0,,
3859,63809575,Publish on topic for a certain period of time,|time|ros|robot|publisher|subscriber|,"<p>I have a ROS Node where i subscribe to a Topic and then publish to another topic on the following way :</p>
<pre><code>#include ...

//Ros Nodehandle + Publisher

//Callback Function definition

int main (int argc, char** argv){
   //Initialisation

   // Pub
   pub = nh-&gt;advertise&lt;Messagetype&gt;(&quot;Topic2&quot;, 1);
 
   //Sub
   ros::Subscriber sub = nh-&gt;subscribe(&quot;Topic1&quot;, 1, sub_cb);

   ros::spin();

   return 0;
}

void sub_cb(){
    //Write the msg i want to publish

    pub.publish(msg);

}
</code></pre>
<p>I wanted to publish the message for 15 seconds for example. I tried a solution with <strong>Ros::Time</strong> and <strong>Ros::Duration</strong> . But the fact that i have a publisher in my callback function didn't allow me to do that.</p>
<p>Is there a way to do it even is my <strong>publish event</strong> is in my callback function ? If not, any solution would work, the main thing that my subscriber and my publisher are on the same node.</p>
",2020-09-09 10:18:00,,1273,1,5,0,,14247194.0,,9/9/2020 10:04,1.0,63821535.0,"<p>Like I said in the comments, I think this is just a logic question, nothing really specific to ROS. Here is one of several possible solutions:</p>
<pre class=""lang-cpp prettyprint-override""><code>#include &quot;ros/ros.h&quot;
#include &quot;std_msgs/String.h&quot;

ros::Publisher pub;
ros::Time begin;

void sub_cb(const std_msgs::StringConstPtr&amp; str) {
  std_msgs::String msg;
  msg.data = &quot;hello world&quot;;
  ros::Time now = ros::Time::now();
  if (now.sec - begin.sec &lt; 15) { // stop publishing after 15 seconds
    std::cout &lt;&lt; &quot;.&quot; &lt;&lt; std::endl;
    pub.publish(msg);
  } else {
    std::cout &lt;&lt; &quot;stopped&quot; &lt;&lt; std::endl;  // just for debugging
  }
}

int main (int argc, char** argv){
  ros::init(argc, argv, &quot;test&quot;);
  ros::NodeHandle nh;
  pub = nh.advertise&lt;std_msgs::String&gt;(&quot;Topic2&quot;, 1);
  ros::Subscriber sub = nh.subscribe(&quot;Topic1&quot;, 1, sub_cb);
  begin = ros::Time::now();
  ros::spin();
  return 0;
}
</code></pre>
",1087119.0,1.0,1.0,112846729.0,Create a timer in the global scope; set it in main; check it in `sub_cb` before calling `publish`.
3713,62043150,Robotics: What are the advantages of cartesian paths in MoveIt?,|ros|robotics|moveit|,"<p>I have some experience in C++ programming but I am a bit new to robotics. I have to create robot functionality for a customer, where the robot arm moves in between several objects to reach a given pose goal for its end-effector (gripper). </p>

<p>I have to use the MoveIt interface for motion planning. This interface can also compute Cartesian paths. If I understand correctly, cartesian paths are just a colletion of waypoint poses that the end-effector should reach sequentially.</p>

<p>My first question is, what is the advantage/disadvantage of executing a cartesian path, rather than moving to several waypoints manually one by one? </p>

<p>My second question is, would a motion planner first plan <strong>all</strong> the waypoints and then start executing them them one by one. Or will a motion planner repeatedly plan and execute for each waypoint in the cartesian path? </p>

<p>The reason that I would like to know this is because I think that moving the end-effector in steps (or waypoints) would improve the chances of finding a trajectory for a goal that is very difficult to reach due to many objects being in the environment. </p>

<p>Thank you in advance, </p>

<p>Dirk</p>
",2020-05-27 12:28:00,,699,0,0,1,,13627136.0,,5/27/2020 12:15,5.0,,,,,,,
3730,62662082,Receiving Data From Universal Robot and Decoding,|python|python-3.x|decode|robot|,"<p>I am working on a project where we are looking to get some data from a universal robot such as position and force data and then store that data in a text file for later reference. We can receive the data just fine, but turning it into readable coordinates is an issue. An example data string is below:</p>
<p>b'\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x80\xbf\x00\x00\x80\xbf\x00\x00\x80\xbf\x00\x00\x80\xbf\x00\x00\x80\xbf\x00\x00\x80\xbf\x00\x00\xc0?\x00\x00\x16C\x00\x00\xc0?\x00\x00\x16C\x00\x00\x00?\xcd\xcc\xcc&gt;\x00\x00\x96C\x00\x00\xc8A\x1e\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x88\xfb\x7f?\xd0M&gt;&lt;\xc0G\x9e:tNT?\r\x11\x07\xbc\xb9\xfd\x7f?~\xa0\xa1:\x03\x02+?\x16\xeb\x7f\xbf#\xce\xcc\xbc9\xdfl\xbbq\xc3\x8a&gt;i\x19T&lt;\xf3\xf9\x7f\xbf\xb4k\x87\xbb-&gt;\xc2&gt;\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x80?\xdb\x0f\xc9@\xa7\xdcU@\xa7\xdcU@\xa7\xdcU@\xa7\xdcU@\xa7\xdcU@\xa7\xdcU@\xfe\xff\xff\xff\xfe\xff\xff\xff\xfe\xff\xff\xff\xfe\xff\xff\xff\xfe\xff\xff\xff\xff\xff\xff\xff\xecb\xc7@\xecb\xc7@\xecb\xc7@\</p>
<p>*not entire string received</p>
<p>At first I thought it was hex so I tried the code:</p>
<pre><code>packet_12 = packet_12.encode('hex')
x = str(packet_12)
x = struct.unpack('!d', packet_12.decode('hex'))[0]
all_data.write(&quot;X=&quot;, x * 1000)
</code></pre>
<p>But to no avail. I tried several different decoding methods using codecs and .encode, but none worked. I found on a different post here the two code blocks below:</p>
<pre><code>y = codecs.decode(packet_12, 'utf-8', errors='ignore')


packet_12 = s.recv(8)
z = str(packet_12)
x = ''.join('%02x' % ord(c) for c in packet_12)
</code></pre>
<p>Neither worked for my application. Finally I tried saving the entire sting in a .txt file and opening it with python and decoding it with the code below, but again nothing seemed to happen.</p>
<pre><code>with io.open('C:\\Users\\myuser\\Desktop\\decode.txt', 'r', encoding='utf8') as f:
    text = f.read()

with io.open('C:\\Users\\myuser\\Desktop\\decode', 'w', encoding='utf8') as f:
    f.write(text)
</code></pre>
<p>I am aware I might be missing something incredibly simple such as using the wrong decoding type or I might even have jibberish as the robot output, but any help is appreciated.</p>
",2020-06-30 16:34:00,,698,1,1,0,,13806555.0,,6/24/2020 14:27,6.0,63545041.0,"<p>The easiest way to receive data from the robot with python is to use Universal Robots' <a href=""https://www.universal-robots.com/articles/ur/real-time-data-exchange-rtde-guide/"" rel=""nofollow noreferrer"">Real-Time-Data-Exchange Interface</a>. They offer some python examples for receiving and sending data.
Check out my GitHub repo for an example code which is based on the official code from UR:
<a href=""https://github.com/jonenfabian/Read_Data_From_Universal_Robots"" rel=""nofollow noreferrer"">https://github.com/jonenfabian/Read_Data_From_Universal_Robots</a></p>
",13318092.0,0.0,1.0,110813879.0,"It looks like binary data, so the struct module is probably the way to go.  But you need to know the format of the data being sent to unpack it properly; ideally this information should be found in the product's documentation."
3651,60465046,FRC problem with getting to run a motor for a certain amount of time,|java|robotics|,"<p>So I'm trying to have the code run the motor for a certain amount of time based on what the REV color sensor V3 senses. I've tried many things. but in the end it just gets stuck in an infinite loop.
I don't know how to fix this thing I don't know if there is a way to do this rather then what I'm trying to do. </p>

<pre><code>private void redDetect(){
  long t= System.currentTimeMillis();

  if (buttons[9]){
    long endg = t+15000; 
    while (System.currentTimeMillis() &lt; endg) {
      trenchMotor.set(0.6);
      break;
    }
  }
  if (buttons[10]){
    long endy = t+10000; 
    while (System.currentTimeMillis() &lt; endy) {
      trenchMotor.set(0.6);
      break;
    }
  }
    if (buttons[11]){
      long endr = t+7000; 
      while (System.currentTimeMillis() &lt; endr) {
        trenchMotor.set(0.6);
        break;
      }
    }
      if (buttons[12]){
        long endb = t+5000; 
        while (System.currentTimeMillis() &lt; endb) {
          trenchMotor.set(0.6);
          break;
        }
    } 
} 

private void blueDetect(){
  long t= System.currentTimeMillis();

  if (buttons[9]){
    long endg = t+15000; 
    while (System.currentTimeMillis() &lt; endg) {
      trenchMotor.set(0.6);
      break;
    }
  }
  if (buttons[10]){
    long endy = t+10000; 
    while (System.currentTimeMillis() &lt; endy) {
      trenchMotor.set(0.6);
      break;
    }
  }
    if (buttons[11]){
      long endr = t+7000; 
      while (System.currentTimeMillis() &lt; endr) {
        trenchMotor.set(0.6);
        break;
      }
    }
      if (buttons[12]){
        long endb = t+5000; 
        while (System.currentTimeMillis() &lt; endb) {
          trenchMotor.set(0.6);
          break;
        }
    } 
} 

private void greenDetect(){
int endg = 10;

  if (buttons[9]){
    while (endg &gt; 0) {
      trenchMotor.set(0.6);
      endg --; 

      break;
    }
  }
  if (buttons[10]){
    int endy = 1000;
    while (endy &gt; 0) {
      trenchMotor.set(0.6);
      endy--;
      break;
    }
  }
    if (buttons[11]){
      int endr = 7000; 
      while (endr &gt; 0) {
        trenchMotor.set(0.6);
        endr--;
        break;
      }
    }
      if (buttons[12]){
        int endb = 5000; 
        while (endb &gt; 0) {
          trenchMotor.set(0.6);
          endb--;
          break;
        }
    } 
} 

private void yellowDetect(){
  long t= System.currentTimeMillis();

  if (buttons[9]){
    long endg = t+15000; 
    while (System.currentTimeMillis() &lt; endg) {
      trenchMotor.set(0.6);
      break;
    }
  }
  if (buttons[10]){
    long endy = t+10000; 
    while (System.currentTimeMillis() &lt; endy) {
      trenchMotor.set(0.6);
      break;
    }
  }
    if (buttons[11]){
      long endr = t+7000; 
      while (System.currentTimeMillis() &lt; endr) {
        trenchMotor.set(0.6);
        break;
      }
    }
      if (buttons[12]){
        long endb = t+5000; 
        while (System.currentTimeMillis() &lt; endb) {
          trenchMotor.set(0.6);
          break;
        }
    } 
} 


</code></pre>

<pre><code>
if(detectedColor.red &gt; detectedColor.green &amp;&amp; detectedColor.red &gt; detectedColor.blue) {
      redDetect();
    }

    if(detectedColor.blue &gt; detectedColor.green &amp;&amp; detectedColor.blue &gt; detectedColor.red) {
      blueDetect();
    }

    if(detectedColor.green &gt; detectedColor.blue &amp;&amp; detectedColor.green &gt; detectedColor.red) {
      greenDetect();
    }
</code></pre>
",2020-02-29 12:15:00,,89,0,3,0,0.0,12940002.0,,2/21/2020 16:57,15.0,,,,,,106996714.0,"Hello Arda. Consider updating the code you posted to reflect the modifications you made. 
Also, could you explain what the ""buttons"" array used in your conditions is about?"
3698,61409469,How to make contour mapping of orientation angles plot,|matplotlib|plot|robotics|,"<p>I am developing a robotic system and found this interesting plot in a paper:</p>

<p><a href=""https://i.stack.imgur.com/xRNRj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xRNRj.png"" alt=""enter image description here""></a></p>

<p>The paper is ""A Comparison of Robot Wrist Implementations for the iCub Humanoid"".
The plot depicts the coupling of the two degrees of freedom of a robotic wrist. I wanted to do something similar for my application but I have no idea where to start and the paper doesn't explain how its done.
If anyone has done something similar in the past, I would be very grateful for any inputs.</p>
",2020-04-24 13:25:00,61411294.0,50,1,0,0,,11267799.0,"Zrich, Schweiz",3/27/2019 17:04,14.0,61411294.0,"<p>This looks like a demonstration of what <code>meshgrid</code> does.  Note here that what would normally be <code>Z</code> in the contour plot is now either <code>X</code> or <code>Y</code>.  </p>

<p><a href=""https://i.stack.imgur.com/p45vA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p45vA.png"" alt=""enter image description here""></a></p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt

x = np.arange(-90, 91, 15)
X, Y = np.meshgrid(x, x)

fig, ax = plt.subplots(figsize=(5,5))
cs = ax.contour(X, Y, X, colors=['blue'], levels=x) # Z = X
ax.clabel(cs, inline=1, fontsize=7)
cs = ax.contour(X, Y, Y, colors=['red'], levels=x) # Z = Y
ax.clabel(cs, inline=1, fontsize=7)
</code></pre>
",102302.0,1.0,0.0,,
3706,61711549,Solving kidnapped robot problem using particle or kalman filter,|ros|robotics|,"<p>I'm doing some research on navigation algorithms in ROS and I want to test kidnapped robot problem in gazebo. Looking on internet I saw the two solutions are particle and kalman filter. I know that amcl already implements particle filter and you can use kalman filter with this <a href=""http://wiki.ros.org/robot_pose_ekf"" rel=""nofollow noreferrer"">package</a>, but the problem with them is that amcl needs robot's initial position. So my question is does amcl realy solve the kidnapped robot problem and are there any other methods for solving this issue? </p>
",2020-05-10 11:56:00,61726423.0,928,1,0,0,,13381031.0,"Zagreb, Hrvatska",4/22/2020 12:23,26.0,61726423.0,"<p>AMCL doesn't need initial pose. When the initial pose is not given, it will initialize the particles uniformly across the map. After moving the robot enough distance, particle filter will converge to correct pose.</p>

<p>AMCL solves kidnapped robot problem by adding random particles. When the robot is kidnapped, number of random particles added will increase. Of the random particles, which are near the actual pose of the robot get highest weight and upon resampling, more particle will be added near the correct pose. After few sensor updates and resampling, pf will converge to actual pose of the robot.</p>

<p>There are many solutions proposed for kidnapped robot problem in research. Most of them use additional setup or additional sensors.</p>
",1595504.0,0.0,0.0,,
3742,63202897,Arduino IDE is not showing any port macOS,|macos|arduino|arduino-uno|robotics|arduino-ide|,"<p>My <strong>Arduino Uno</strong> is plugged in with mac USB port. But Arduino IDE is not showing any port.
(macOS- Catalina)</p>
<p><a href=""https://i.stack.imgur.com/arPha.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/arPha.png"" alt=""enter image description here"" /></a></p>
",2020-08-01 07:37:00,67665010.0,41213,11,7,17,,8400582.0,,8/1/2017 15:22,69.0,64007180.0,"<p>This problem can occur due to two main reasons.<br></p>
<ol>
<li>Your computer does not recognize your Arduino board<br></li>
<li>Your Arduino Board is damaged</li>
</ol>
<p>Sometimes your computer does not recognize your arduino board. You can check it by opening <strong>Device Manager &gt; Other Devices</strong> If a device named Arduino Uno is there, probably it is due to the driver issue.<br>
You can solve it by manually configuring the driver <a href=""https://www.arduino.cc/en/Guide/DriverInstallation/"" rel=""nofollow noreferrer"">from here.</a><br>
<br>
If the Arduino Board is still not recognized, it can be due to a damaged cable or the Arduino board is damaged. Try using a different cable to connect it to your computer. If the board is still not recognized, the Arduino board might be damaged, You may have to repair the damaged board by giving it to a shop or may have to buy a new board which seems the easier way.</p>
",14301325.0,0.0,3.0,111812682.0,"No, it didn't work."
3639,60183911,Blue Prism - Not able to spy elements - Browser Firefox,|automation|robotics|blueprism|rpa|,"<p>I have an issue with spy Browser mode in Firefox.
I have designed a new RPA process on Dev machine where I have spy in Browser mode (in firefox) web page.
On my Dev machine the Browser mode is working good and the process runs very good.
The issue is on production machine where the modeler don't see the elements which have been spied on dev machine.</p>

<p>Actions so far done.
We have set all settings regarding Firefox to the same like on dev machine using the BP guide,
We have installed the same Firefox extension on prod like on dev machine.
We have set the same internet options.</p>

<p>Non of the actions have helped us to be able to spy elements in Firefox on prod machine.</p>

<p>My questions to the experts community:)
What else could have impact on the BP - Browser mode which is stopping BP to see elements?
What virtual machine settings need to be set/or what to check?</p>

<p>We have BP Version 6:
Application Manager 6.4.2.10610
.Net Framework 4.7
Firefox version 72.0.2
Blue Prism Browser Extension version 6.4.2.10610vycoxormiz (updated 30. Jan. 2020)</p>

<p>Thank you for your help!</p>
",2020-02-12 08:20:00,60274369.0,3299,1,0,2,,5739124.0,"Berlin, Deutschland",1/2/2016 21:31,34.0,60274369.0,"<p>I found the solution.</p>

<p>During investigation I came on this link</p>

<p>[<a href=""https://superuser.com/questions/719875/google-chrome-always-says-google-chrome-was-not-shut-down-properly][1]"">https://superuser.com/questions/719875/google-chrome-always-says-google-chrome-was-not-shut-down-properly][1]</a></p>

<p>In the location ""%UserProfile%\AppData\Local\Google\Chrome\User Data\Default\Preferences</p>

<p>I have changed the ""exit_type"": ""normal"" to ""exit_type"": ""standard"" and immediate the </p>

<p>Browser mode on production machine was working and we could run the process.</p>

<p>It seems that the chrome when it wasn't shut down properly has impact on Blue Prism.</p>

<p>Adding update.
I have noticed that each time Chrome opens it changes in the Preferences file the status to Normal.
I have fixed the issue in that way, that I have added additional logic to my solution:
1. I have copied the Preferences file to a different location and changed in the file the status to Standard.
2. I have added additional logic to my process where bot copy the file from the new location and replace the file in the Chrome location each time he runs.</p>

<p>Hope this will help others with similar issue.</p>

<p>Regards! </p>
",5739124.0,1.0,0.0,,
3728,62592629,What is this error ? It's an digital assistant,|python|stream|pyaudio|robotics|errno|,"<p>This is my code:</p>
<pre><code>import speech_recognition
import pyttsx3
from datetime import date, datetime

robot_ear = speech_recognition.Recognizer()
robot_mouth = pyttsx3.init()
robot_brain = &quot;&quot;

while True:
    with speech_recognition.Microphone() as mic:
        print(&quot;Robot: I'm Listening&quot;)
        audio = robot_ear.listen(mic)

    print(&quot;Robot:...&quot;)

    try:
        you = robot_ear.recognize_google(audio)
    except:
        you = &quot;&quot;
    print (&quot;You: &quot; + you)

    you = &quot;hello&quot;

    if you == &quot;&quot;:
        robot_brain = &quot;I can't hear you, try again!&quot;
    elif&quot;hello&quot; in you:
        robot_brain = &quot;Hello Huan&quot;
    elif &quot;today&quot; in you:
        today = date.today()
        robot_brain = today.strftime(&quot;%B %d, %Y&quot;)
    elif &quot;time&quot; in you:
        now = datetime.now()
        robot_brain = now.strftime(&quot;%H hours %M minutes %S seconds&quot;)
    elif &quot;president&quot; in you:
        robot_brain = &quot;Donald Trump&quot;
    elif &quot;bye&quot; in you:
        robot_brain = &quot;Bye Duong Gia Huan&quot;
        print(&quot;Robot: &quot; + robot_brain)
        robot_mouth.say(robot_brain)
        robot_mouth.runAndWait()
        break
    else:
        robot_brain = &quot;I'm fine, Thank you, and you ?&quot;

    print(&quot;Robot: &quot; + robot_brain)
    robot_mouth.say(robot_brain)
    robot_mouth.runAndWait()
</code></pre>
<p>when I run it, It's have an error like this :</p>
<pre><code>Robot: I'm Listening
Traceback (most recent call last):
  File &quot;trolyao.py&quot;, line 12, in &lt;module&gt;
    audio = robot_ear.listen(mic)
  File &quot;C:\Users\huana\AppData\Local\Programs\Python\Python38\lib\site-packages\speech_recognition\__init__.py&quot;, line 652, in listen
    buffer = source.stream.read(source.CHUNK)
  File &quot;C:\Users\huana\AppData\Local\Programs\Python\Python38\lib\site-packages\speech_recognition\__init__.py&quot;, line 161, in read
    return self.pyaudio_stream.read(size, exception_on_overflow=False)
  File &quot;C:\Users\huana\AppData\Local\Programs\Python\Python38\lib\site-packages\pyaudio.py&quot;, line 608, in read
    return pa.read_stream(self._stream, num_frames, exception_on_overflow)
OSError: [Errno -9999] Unanticipated host error

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;trolyao.py&quot;, line 12, in &lt;module&gt;
    audio = robot_ear.listen(mic)
  File &quot;C:\Users\huana\AppData\Local\Programs\Python\Python38\lib\site-packages\speech_recognition\__init__.py&quot;, line 151, in __exit__
    self.stream.close()
  File &quot;C:\Users\huana\AppData\Local\Programs\Python\Python38\lib\site-packages\speech_recognition\__init__.py&quot;, line 166, in close
    if not self.pyaudio_stream.is_stopped():
  File &quot;C:\Users\huana\AppData\Local\Programs\Python\Python38\lib\site-packages\pyaudio.py&quot;, line 543, in is_stopped
    return pa.is_stream_stopped(self._stream)
OSError: [Errno -9988] Stream closed
</code></pre>
<p>Help me fix it !!!I'm a begginer of Python !
<strong>Help me fix it, pls</strong></p>
<blockquote>
<p>Can you help me ?
I don't know what it is ?
I think we should fix the audio with the microphone, Right ?
<em>ntroduction to Python Programming language. Python is developed by Guido van Rossum. Guido van Rossum started implementing Python in 1989. Python is a very simple programming language so even if you are new to programming, you can learn python without facing any issues.</em></p>
</blockquote>
",2020-06-26 10:17:00,,381,1,0,-2,0.0,13795538.0,,6/23/2020 0:02,4.0,62592770.0,"<pre><code>r = sr.Recognizer()
with sr.Microphone() as source:
    r.pause_threshold = 1
    r.adjust_for_ambient_noise(source, duration=1)
    audio = r.listen(source)
try:
    print(&quot;Recognizing...&quot;)
    query = r.recognize_google(audio, language='en-us')
except Exception as e:
    print(&quot;Say that again please...&quot;)
    return &quot;None&quot;
return query
</code></pre>
<p>I don't know the reason behind the error  but i think you need add the above code. I guess it is not recognizing audio clearly so try to adjust the ambient noise. I hope it helps. I am a beginner too. Peace</p>
",13754401.0,0.0,0.0,,
3854,63766021,Callback fuction is not getting called,|c++|c++14|ros|robotics|gazebo-simu|,"<p>I am trying to implement ROS GotoGoal in c++, here is the code</p>
<pre class=""lang-cpp prettyprint-override""><code>#include &quot;ros/ros.h&quot;
#include &quot;geometry_msgs/Twist.h&quot;
#include &quot;geometry_msgs/Pose2D.h&quot;
#include &quot;turtlesim/Pose.h&quot;

class Turtle {

public :
  Turtle(int argc,char** argv){
    ros::init(argc,argv,&quot;mover&quot;);
    ros::NodeHandle n;
    pose = turtlesim::Pose();
    pub = n.advertise&lt;geometry_msgs::Twist&gt;(&quot;/turtle1/cmd_vel&quot;, 100);
    sub = n.subscribe(&quot;/turtle1/pose&quot;, 100, &amp;Turtle::Update, this);
  }

  void Update(const turtlesim::Pose::ConstPtr&amp; msg){
    ROS_INFO(&quot;Pose recieved : x = %f y = %f\n&quot;, msg-&gt;x, msg-&gt;y );
    pose = *msg;
  }

  void move2goal(){
    turtlesim::Pose goalPose= turtlesim::Pose() ;
    
    std::cout&lt;&lt;&quot;Enter goal x : &quot;&lt;&lt;&quot; &quot;;
    std::cin&gt;&gt;goalPose.x ;
    std::cout&lt;&lt;&quot;Enter goal y : &quot;&lt;&lt;&quot; &quot;;
    std::cin&gt;&gt;goalPose.y ;
    
    float d ;
    std::cout&lt;&lt;&quot;Enter distance tolerance d : &quot;&lt;&lt;&quot; &quot;;
    std::cin&gt;&gt;d ;
    
    auto vel_msg = geometry_msgs::Twist() ;
    ros::Rate loop_rate(2.0);
    while(distance(goalPose)&gt;=d &amp;&amp; ros::ok()){
      vel_msg.linear.x = linear_velocity(goalPose,1.5); 
      vel_msg.linear.y =0 ;
      vel_msg.linear.z = 0 ;
      
      vel_msg.angular.x = 0 ;
      vel_msg.angular.y= 0 ;
      vel_msg.angular.z = angular_velocity(goalPose,6) ;
      
      pub.publish(vel_msg) ;
      loop_rate.sleep() ; 
 ROS_INFO(&quot;current : %f %f\n&quot;,pose.x,pose.y) ;  
 }
    vel_msg.angular.z=0 ;
    vel_msg.linear.x =0 ;
    pub.publish(vel_msg) ;
    ros::spin();
  }

  ros::Publisher pub;
  ros::Subscriber sub;
  turtlesim::Pose pose;
  int ch = 0;
};

int main(int argc, char** argv) {
  Turtle turtle = Turtle(argc,argv);
  turtle.move2goal();
  return 0;
}
</code></pre>
<p>But the <strong>Update</strong> callback function is not getting called and the turtle is moving in a circle as pose is not getting updated. I tried using ROS_INFO for debugging the issue but nothing worked.
What am I doing wrong here?
<strong>Note</strong>: implementations of few functions have been removed from the code snippet due to stackoverflow's policy.</p>
<p>[Output][1]
[1]:https://i.stack.imgur.com/eL9Sr.png</p>
",2020-09-06 15:48:00,63769117.0,1334,1,5,0,,14133727.0,,8/19/2020 20:47,4.0,63769117.0,"<p>I think you misunderstood what the sleep does. Unlike <code>spin</code> it doesn't actually perform all the ROS communication events. It's just a convenience for accurate sleep. See <a href=""https://stackoverflow.com/questions/23227024/difference-between-spin-and-rate-sleep-in-ros"">Difference between spin and rate.sleep in ROS</a>.</p>
<p>Fortunately the fix is really easy, just add a <code>spinOnce</code>:</p>
<pre><code>while( distance(goalPose) &gt;= d &amp;&amp; ros::ok()) {
  // (..)
  ros::spinOnce();
  loop_rate.sleep();
}
</code></pre>
",1087119.0,1.0,2.0,112772470.0,"cin statements are there for getting the goal location (x,y) from the console. I added a debug statement in Update callback function and one at the end of the loop"
3821,63271230,Can you dynamically create variables based on rows in an Excel file in C#?,|c#|dynamic|robotics|,"<p>I am trying to make a C# program that takes coordinates from an excel file in the sense that each column is x,y,z and r, respectively, and each row is a different point. I would like to be able to create variables in the format point0, point1, etc. depending on how many rows there are.</p>
<p>As of right now I am reading each cell into an Array, then manually creating points from that array. In this case there are 4 rows and 4 points (points 0 to 3). This works for now but I have to imagine there is a much easier way of doing this or at least something more dynamic. 4 points is not a big deal but there could be many more.</p>
<pre><code>        int rows = 4;

        for(int i = 0; i &lt; 4; i++)
        {
           for(int j = 0; j &lt; rows; j++)
           {
               points[i,j] = excel.ReadCell(i, j);
           }
        }
      
        for(int i = 0; i &lt; 4; i++)
        {
            point0[0, i] = points[0, i];
        }

        for (int i = 0; i &lt; 4; i++)
        {
            point1[1, i] = points[1, i];
        }

        for (int i = 0; i &lt; 4; i++)
        {
            point2[2, i] = points[2, i];
        }

        for (int i = 0; i &lt; 4; i++)
        {
            point3[3, i] = points[3, i];
        }
</code></pre>
<p>Even condensing the set of loops where the points are manually created would save time, I am just not sure if there is a way to say something such as</p>
<pre><code>       for(int i = 0; i &lt; rows; i++)
       {
           for(int j = 0; j &lt; cols; j++)
           {
               point+&quot;i&quot;[i,j] = points[i,j]
           }
       }
</code></pre>
<p>Where the ith iteration is concatenated to the variable name.</p>
<p>Any help would be greatly appreciated, and I am open to all recommendations (I am pretty new to C# if you can't tell)</p>
",2020-08-05 18:16:00,63272484.0,381,1,2,0,,14055717.0,,8/5/2020 17:56,2.0,63272484.0,"<p>I would define a class of what the combination of the four points mean.  Not based on the row or column that they're stored on but what they actually represent.</p>
<pre><code>public class Shape
{
    public int Start { get; set; }
    public int End { get; set; }
    public int Mean { get; set; }
    public int Median { get; set; }
}
</code></pre>
<p>Then as you loop through each row you can Create and add all four points at the same time.  Some thing like this.</p>
<p>public List GetShapesFromExcel()
{
var list = new List();</p>
<pre><code>        int StartColumn = 'x';
        int EndColumn = 'y';
        int MeanColumn = 'z';
        int MedianColumn = 'r';

        foreach (var row in workSheet)
        {
            var shape = new Shape()
            {
                Start = excel.ReadCell(row, StartColumn);
                End = excel.ReadCell(row, EndColumn);
                Mean = excel.ReadCell(row, MeanColumn);
                Median = excel.ReadCell(row, MedianColumn);
            };
        
        }
    return list;
    }
</code></pre>
<p>I'm taking a wild stab in the dark on what you're data actually represents but I would take the time to go ahead and spin that up into a real object so that it's easier to reason about as you're writing the code.</p>
<p>variables like I &amp; J  Don't save enough time in this case to be useful.</p>
<p>last suggestion is to go ahead and check out the package EPPlus. that package has the ability to turn rows into structured classes</p>
<p>check this out to get started.
<a href=""https://stackoverflow.com/questions/33436525/how-to-parse-excel-rows-back-to-types-using-epplus"">How to parse excel rows back to types using EPPlus</a></p>
",3808982.0,0.0,0.0,111882898.0,"Rather than store these in variables whose names get larger numbers added, you can just store them in a list."
3899,64457784,Pointer gives abnormal values after deferencing,|c++|pointers|robotics|,"<p>I am trying to replicate a big C++ library. It has the following structure of code</p>
<p>RobotRunner.h</p>
<pre><code>class RobotRunner
{
public:
    VectorNavData* vectorNavData;
};
</code></pre>
<p>RobotRunner.cpp</p>
<pre><code>void RobotRunner::run()
{
printf(&quot;Quaternion[0]: %f \n&quot;, vectorNavData-&gt;quat[0]); //Output: 243235653487854 - Abnormal values
}
</code></pre>
<p>SimulationBridge.h</p>
<pre><code>class SimulationBridge
{
private:
    VectorNavData _vectorNavData; // Actual value of vectornavdata from the robot is stored here
    
    RobotRunner* _robotRunner = nullptr; //Pointer to the RobotRunner Object
}
</code></pre>
<p>SimulationBridge.cpp</p>
<pre><code>void SimulationBridge::init()
{
_robotRunner = new RobotRunner();

printf(&quot;Quaternion[0]: %f \n&quot;, _vectorNavData.quat[0]); // Output: 0.43 - Normal and expected
_robotRunner-&gt;vectorNavData = &amp;_vectorNavData;
}

void SimulationBridge::run()
{
_robotRunner-&gt;run();
}

//This function runs continuously and updates the _vectorNavData in a separate thread
void SimulationBridge::readIMU()
{
    while(true)
    { 
        //_lowState stores the values of different robot parameters at a given time

        _vectorNavData.accelerometer[0] = _lowState.imu.accelerometer[0];
        _vectorNavData.accelerometer[1] = _lowState.imu.accelerometer[1];
        _vectorNavData.accelerometer[2] = _lowState.imu.accelerometer[2];

        _vectorNavData.quat[0] = _lowState.imu.quaternion[1];
        _vectorNavData.quat[1] = _lowState.imu.quaternion[2];
        _vectorNavData.quat[2] = _lowState.imu.quaternion[3];
        _vectorNavData.quat[3] = _lowState.imu.quaternion[0];

        _vectorNavData.gyro[0] = _lowState.imu.gyroscope[0];
        _vectorNavData.gyro[1] = _lowState.imu.gyroscope[1];
        _vectorNavData.gyro[2] = _lowState.imu.gyroscope[2];
    }
}
</code></pre>
<p>VectorNavData is a struct which stores the details about the orientation of the robot. It has the following definition</p>
<pre><code>struct VectorNavData {
  Vec3&lt;float&gt; accelerometer;
  Vec3&lt;float&gt; gyro;
  Quat&lt;float&gt; quat;
};
</code></pre>
<p>I have included only the necessary part of the code here for brevity.</p>
<p><strong>Code Explanation:</strong></p>
<p>SimulationBridge class communicates with the robot in the simulation. It takes in vectorNavData and stores it in the member variable _vectorNavData. SimulationBridge also contains the pointer to the RobotRunner class as one of it's member. I am allocating the address of _vectorNavData object to the pointer _robotRunner-&gt;vectorNavData (check SimulationBridge.cpp). Inside the RobotRunner class I deference this pointer and use the values in other parts of the code.</p>
<p><strong>Problem:</strong></p>
<p>If I print the vectorNavData inside the SimulationBridge.cpp the values seems to be normal. But after assigning the pointer of the same object to the robot runner, if I print the values there the values seems to be abnormally high. My question is, is this way of using pointers for dynamic allocation recommended? If not what is the best alternative way I can use?</p>
<p>Another important point to note is, I am compiling the code with CMake and &quot;-O3&quot; optimization flag is set to the CMAKE_CXX_FLAGS. If I remove this flag, the code sorta works fine for the above object pointer but I am still getting similar error for another object pointer in another part of the code.  I have not included that here because it's pretty complex to describe the code structure and the problem essentially is the same.</p>
",2020-10-21 06:26:00,,82,0,10,1,,9971625.0,,6/21/2018 7:32,10.0,,,,,,113978439.0,Sorry! I have edited the post for better clarity. I hope this is clear now. @Someprogrammerdude
3886,64299715,Is there a way to simplify repetitive addition in x-drive?,|c++|robotics|,"<p>I wrote the C++ code below for controlling an x-drive, and it looks really repetitive, I feel like there should be a way to simplify, but I can't think of any that would keep it looking readable. Any ideas would be appreciated!</p>
<pre class=""lang-cpp prettyprint-override""><code>int lefty_analog = master.get_analog(ANALOG_LEFT_Y);
int leftx_analog = master.get_analog(ANALOG_LEFT_X);
int rightx_analog = master.get_analog(ANALOG_RIGHT_X);

rightf_motor = lefty_analog - leftx_analog - rightx_analog;
rightb_motor = lefty_analog + leftx_analog - rightx_analog;
leftf_motor = -lefty_analog - leftx_analog - rightx_analog;
leftb_motor = -lefty_analog + leftx_analog - rightx_analog;
</code></pre>
",2020-10-11 00:59:00,,41,0,3,0,,14428374.0,,10/11/2020 0:50,2.0,,,,,,113702661.0,You should only worry about this if this is in a tight loop. See how compilers optimize it: https://gcc.godbolt.org/z/KffP13
3922,65099688,Need help resolving Arduino Code error [-Woverflow] in gimbal code. I am not sure how to resolve or if to disable yaw,|c++|c|arduino|robotics|gyroscope|,"<p>This code is not mine, but found on <a href=""https://howtomechatronics.com/projects/diy-arduino-gimbal-self-stabilizing-platform/"" rel=""nofollow noreferrer"">How To Mechatronics</a>.</p>
<p>I am working on an Arduino gimbal and am using this code. It brings up an error, which I will paste at the bottom.</p>
<p>I searched this sort of error and it seems it is because it has an output that is negative but is not defined to come out as negative or may be too large.</p>
<p>I am not quite sure what to change or how to change this in order to function. I also have a problem with the yaw motor, which I believe may be fried because my brother connected it to a 12 V battery and it is only supposed to be 5 V.</p>
<p>I am sure I can disable the yaw (although not sure if this would solve the other issue), but I don't know which lines to code out in order to do so.</p>
<pre><code>/*
  DIY Gimbal - MPU6050 Arduino Tutorial
  by Dejan, www.HowToMechatronics.com
  Code based on the MPU6050_DMP6 example from the i2cdevlib library by Jeff Rowberg:
  https://github.com/jrowberg/i2cdevlib
*/
// I2Cdev and MPU6050 must be installed as libraries, or else the .cpp/.h files
// for both classes must be in the include path of your project
#include &quot;I2Cdev.h&quot;

#include &quot;MPU6050_6Axis_MotionApps20.h&quot;
//#include &quot;MPU6050.h&quot; // not necessary if using MotionApps include file

// Arduino Wire library is required if I2Cdev I2CDEV_ARDUINO_WIRE implementation
// is used in I2Cdev.h
#if I2CDEV_IMPLEMENTATION == I2CDEV_ARDUINO_WIRE
#include &quot;Wire.h&quot;
#endif
#include &lt;Servo.h&gt;
// class default I2C address is 0x68
// specific I2C addresses may be passed as a parameter here
// AD0 low = 0x68 (default for SparkFun breakout and InvenSense evaluation board)
// AD0 high = 0x69
MPU6050 mpu;
//MPU6050 mpu(0x69); // &lt;-- use for AD0 high

// Define the 3 servo motors
Servo servo0;
Servo servo1;
Servo servo2;
float correct;
int j = 0;

#define OUTPUT_READABLE_YAWPITCHROLL

#define INTERRUPT_PIN 2  // use pin 2 on Arduino Uno &amp; most boards

bool blinkState = false;

// MPU control/status vars
bool dmpReady = false;  // set true if DMP init was successful
uint8_t mpuIntStatus;   // holds actual interrupt status byte from MPU
uint8_t devStatus;      // return status after each device operation (0 = success, !0 = error)
uint16_t packetSize;    // expected DMP packet size (default is 42 bytes)
uint16_t fifoCount;     // count of all bytes currently in FIFO
uint8_t fifoBuffer[64]; // FIFO storage buffer

// orientation/motion vars
Quaternion q;           // [w, x, y, z]         quaternion container
VectorInt16 aa;         // [x, y, z]            accel sensor measurements
VectorInt16 aaReal;     // [x, y, z]            gravity-free accel sensor measurements
VectorInt16 aaWorld;    // [x, y, z]            world-frame accel sensor measurements
VectorFloat gravity;    // [x, y, z]            gravity vector
float euler[3];         // [psi, theta, phi]    Euler angle container
float ypr[3];           // [yaw, pitch, roll]   yaw/pitch/roll container and gravity vector

// packet structure for InvenSense teapot demo
uint8_t teapotPacket[14] = { '$', 0x02, 0, 0, 0, 0, 0, 0, 0, 0, 0x00, 0x00, '\r', '\n' };



// ================================================================
// ===               INTERRUPT DETECTION ROUTINE                ===
// ================================================================

volatile bool mpuInterrupt = false;     // indicates whether MPU interrupt pin has gone high
void dmpDataReady() {
  mpuInterrupt = true;
}

// ================================================================
// ===                      INITIAL SETUP                       ===
// ================================================================

void setup() {
  // join I2C bus (I2Cdev library doesn't do this automatically)
#if I2CDEV_IMPLEMENTATION == I2CDEV_ARDUINO_WIRE
  Wire.begin();
  Wire.setClock(400000); // 400kHz I2C clock. Comment this line if having compilation difficulties
#elif I2CDEV_IMPLEMENTATION == I2CDEV_BUILTIN_FASTWIRE
  Fastwire::setup(400, true);
#endif

  // initialize serial communication
  // (115200 chosen because it is required for Teapot Demo output, but it's
  // really up to you depending on your project)
  Serial.begin(38400);
  while (!Serial); // wait for Leonardo enumeration, others continue immediately

  // initialize device
  //Serial.println(F(&quot;Initializing I2C devices...&quot;));
  mpu.initialize();
  pinMode(INTERRUPT_PIN, INPUT);
  devStatus = mpu.dmpInitialize();
  // supply your own gyro offsets here, scaled for min sensitivity
  mpu.setXGyroOffset(17);
  mpu.setYGyroOffset(-69);
  mpu.setZGyroOffset(27);
  mpu.setZAccelOffset(1551); // 1688 factory default for my test chip

  // make sure it worked (returns 0 if so)
  if (devStatus == 0) {
    // turn on the DMP, now that it's ready
    // Serial.println(F(&quot;Enabling DMP...&quot;));
    mpu.CalibrateAccel(6);
    mpu.CalibrateGyro(6);
    mpu.PrintActiveOffsets();
    mpu.setDMPEnabled(true);

    attachInterrupt(digitalPinToInterrupt(INTERRUPT_PIN), dmpDataReady, RISING);
    mpuIntStatus = mpu.getIntStatus();

    // set our DMP Ready flag so the main loop() function knows it's okay to use it
    //Serial.println(F(&quot;DMP ready! Waiting for first interrupt...&quot;));
    dmpReady = true;

    // get expected DMP packet size for later comparison
    packetSize = mpu.dmpGetFIFOPacketSize();
  } else {
    // ERROR!
    // 1 = initial memory load failed
    // 2 = DMP configuration updates failed
    // (if it's going to break, usually the code will be 1)
    // Serial.print(F(&quot;DMP Initialization failed (code &quot;));
    //Serial.print(devStatus);
    //Serial.println(F(&quot;)&quot;));
  }

  // Define the pins to which the 3 servo motors are connected
  servo0.attach(10);
  servo1.attach(9);
  servo2.attach(8);
}
// ================================================================
// ===                    MAIN PROGRAM LOOP                     ===
// ================================================================

void loop() {
  // if programming failed, don't try to do anything
  if (!dmpReady) return;

  // wait for MPU interrupt or extra packet(s) available
  while (!mpuInterrupt &amp;&amp; fifoCount &lt; packetSize) {
    if (mpuInterrupt &amp;&amp; fifoCount &lt; packetSize) {
      // try to get out of the infinite loop
      fifoCount = mpu.getFIFOCount();
    }
  }

  // reset interrupt flag and get INT_STATUS byte
  mpuInterrupt = false;
  mpuIntStatus = mpu.getIntStatus();

  // get current FIFO count
  fifoCount = mpu.getFIFOCount();

  // check for overflow (this should never happen unless our code is too inefficient)
  if ((mpuIntStatus &amp; _BV(MPU6050_INTERRUPT_FIFO_OFLOW_BIT)) || fifoCount &gt;= 1024) {
    // reset so we can continue cleanly
    mpu.resetFIFO();
    fifoCount = mpu.getFIFOCount();
    Serial.println(F(&quot;FIFO overflow!&quot;));

    // otherwise, check for DMP data ready interrupt (this should happen frequently)
  } else if (mpuIntStatus &amp; _BV(MPU6050_INTERRUPT_DMP_INT_BIT)) {
    // wait for correct available data length, should be a VERY short wait
    while (fifoCount &lt; packetSize) fifoCount = mpu.getFIFOCount();

    // read a packet from FIFO
    mpu.getFIFOBytes(fifoBuffer, packetSize);

    // track FIFO count here in case there is &gt; 1 packet available
    // (this lets us immediately read more without waiting for an interrupt)
    fifoCount -= packetSize;

    // Get Yaw, Pitch and Roll values
#ifdef OUTPUT_READABLE_YAWPITCHROLL
    mpu.dmpGetQuaternion(&amp;q, fifoBuffer);
    mpu.dmpGetGravity(&amp;gravity, &amp;q);
    mpu.dmpGetYawPitchRoll(ypr, &amp;q, &amp;gravity);

    // Yaw, Pitch, Roll values - Radians to degrees
    ypr[0] = ypr[0] * 180 / M_PI;
    ypr[1] = ypr[1] * 180 / M_PI;
    ypr[2] = ypr[2] * 180 / M_PI;
    
    // Skip 300 readings (self-calibration process)
    if (j &lt;= 300) {
      correct = ypr[0]; // Yaw starts at random value, so we capture last value after 300 readings
      j++;
    }
    // After 300 readings
    else {
      ypr[0] = ypr[0] - correct; // Set the Yaw to 0 deg - subtract  the last random Yaw value from the currrent value to make the Yaw 0 degrees
      // Map the values of the MPU6050 sensor from -90 to 90 to values suatable for the servo control from 0 to 180
      int servo0Value = map(ypr[0], -90, 90, 0, 180);
      int servo1Value = map(ypr[1], -90, 90, 0, 180);
      int servo2Value = map(ypr[2], -90, 90, 180, 0);
      
      // Control the servos according to the MPU6050 orientation
      servo0.write(servo0Value);
      servo1.write(servo1Value);
      servo2.write(servo2Value);
    }
#endif
  }
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/6Pnpf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6Pnpf.png"" alt=""enter image description here"" /></a></p>
<p>See image for error code. There is only one exit condition, so I believe this is the only issue.</p>
<p>the arrow in the error code (~~^~~~~~~) points at (2*16384);</p>
",2020-12-01 22:45:00,65100114.0,200,1,2,1,,14745165.0,,12/1/2020 22:27,3.0,65100114.0,"<p>This is a warning for an int overflow in the MPU6050 library code, not in your code.</p>
<p>On Github, an <a href=""https://github.com/jrowberg/i2cdevlib/issues/380"" rel=""nofollow noreferrer"">issue</a> was raised about this some time ago, which also has the fix in the same posting.</p>
<p>Another solution suggested in the <a href=""https://github.com/jrowberg/i2cdevlib/issues/380#issuecomment-408547936"" rel=""nofollow noreferrer"">comments there</a> to get rid of this warning is to simply change the &quot;16384&quot; to &quot;16384L&quot; in the library code.</p>
<p>Note that i2cdevlib has 247 open issues; I don't think the owner/maintainer will fix this particular problem any time soon.</p>
",12570891.0,0.0,2.0,115091302.0,"On Arduino, `int` is 16-bit. `2*16384` is too large to fit in a 16-bit signed int. I have no idea why authors of this library didn't use the constant 32768 there, perhaps you want to ask them?"
3738,62913510,How to get Arduino Robot token?,|arduino|robotics|,"<p><a href=""https://i.stack.imgur.com/Ch7Fr.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ch7Fr.jpg"" alt=""enter image description here"" /></a>Yesterday I built a robot called <strong>Miro</strong>. I want to use Open Roberta but I have to connect the robot using a token, but I don't know how to get that token.
<strong>note</strong>: I'm using Windows 10 enterprise as my os.</p>
<p><a href=""https://i.stack.imgur.com/YH1Q7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YH1Q7.png"" alt=""enter image description here"" /></a></p>
<p>I really need some help with that.
Thank you!</p>
",2020-07-15 11:07:00,,36,1,0,-1,,12097240.0,,9/20/2019 19:35,18.0,62915953.0,"<p>When I go to <a href=""https://lab.open-roberta.org/"" rel=""nofollow noreferrer"">https://lab.open-roberta.org/</a> the very first thing I see is a pop-up that says:</p>
<blockquote>
<p>Would you like to get started, but do not know exactly how? We will
show you the first steps in an interactive tutorial.</p>
</blockquote>
<p>I've never used this service, but I would think that would be the place to start.</p>
",7994837.0,0.0,0.0,,
3626,59645243,simulating a 6 DOF robot in matlab robotic toolbox,|matlab|robotics|,"<p>I am new with matlab and its robotic toolbox. I am trying to simulate a simple model of a 6 DOF manipulator in matlab but i got this error. Here is my code:</p>

<pre><code>clc

startup_rvc;  
syms th1 th2 th3 th4 th5 
%//////////robot  d-h////////////
%%%%L= Link([ th d a alpha 'joint type'])%%%%
L(1) = Link([th1 0.1519 0 -pi/2 ]);
L(2) = Link([th2 0.1198 0.24365 0 ]);
L(3) = Link([th3 -0.0925 0.21325 0 ]);
L(4) = Link([th4 0.08505 0 -pi/2 ]);
L(5)= Link([th5 0.08535 0 pi/2]);
L(6)= Link([th6 0 0 0]);

robot = SerialLink(L,'name','surgicalarm');
q=[0 0 0 0 0];
robot.plot(q)
robot.teach();
</code></pre>

<p>but when I run this code I got this error:</p>

<pre><code>Error using SerialLink/plot (line 205)
Insufficient columns in q

Error in surgicalarm (line 16)
robot.plot(q)
</code></pre>

<p>Can anyone help me to fix this? Thanks.</p>
",2020-01-08 11:49:00,,796,1,0,0,,10546023.0,,10/23/2018 11:50,3.0,59800548.0,"<p>Since your robot has 6 DOF, I would expect <code>q</code> also have 6 columns instead of 5. </p>

<p>Try with <code>q = [0 0 0 0 0 0]</code> in your code.</p>
",6018272.0,2.0,0.0,,
3927,65203283,improve remote control app command latency,|flutter|ros|latency|robotics|remote-control|,"<p>Is there a way to increase joystick command speed from my Flutter app to my robot? Right now Im using wifi (which has other data streaming back to the tablet) and there is a lag between commands and robot motion. This is not an issue when I use a wireless 2.4GHz Logitech controller.</p>
<p><a href=""https://pub.dev/packages/control_pad"" rel=""nofollow noreferrer"">https://pub.dev/packages/control_pad</a></p>
",2020-12-08 16:51:00,,107,0,1,1,,13132730.0,US,3/27/2020 2:56,20.0,,,,,,115352406.0,"Normally, the latency of wifi is very low, because of the high throughput. You could write your own sockets for the commands which speeds the whole process or you can use bluetooth aswell."
3994,66827788,How to get homography matrix from gps information,|python|opencv|gps|robotics|drone.io|,"<p>I am working a gps-denied UAV localization project. I am reading this paper <strong>GPS-Denied UAV Localization using Pre-existing Satellite Imagery</strong>. In this paper, they try to align a UAV frame sequentially with a satellite map using an homography. The first homography is estimated using the GPS information stored in the first UAV frame. In the code, I don't see any information about this part. I am wondering if someone can explain this or point me to some reference that can help.</p>
",2021-03-27 03:39:00,,189,0,3,0,,2130515.0,,3/4/2013 4:57,385.0,,,,,,118146986.0,"@Micka, Yes. based on the paper `We extract the GPS-aligned satellite map from Google Earth Pro. The UAV imagery was captured in April, 2013, and the satellite imagery in May, 2012. High-accuracy RTK GPS (latitude, longitude, and altitude) is included in the metadata of each UAV frame in the dataset`"
4000,66999588,"Given error of distance, and the error of angle, How would I set a percentage of motion attributed to angular motion, and the net speed",|math|robotics|,"<p>So I have a robot with a mecanum drive, meaning it can move in any direction. I am trying to program it to move to a point and reach that point at a given angle, all while moving in a straight line. So far I've gotten the robot to reach that point, I've gotten it to stay on the straight line, but I cant get it to reach the correct angle(without having it turn and then move). To simplify the movement, I have a function that I creatively called Move, Move takes in three floats:</p>
<p>The first is what angle the robot moves to relative to the robots angle, but ignore this one as I got it working</p>
<p>the second(angle%) determines the percentage of motion that is given to the angular motion, so when it is given 1, the robot will only be turning, and when it is given 0, it won't turn at all. At 0.5 it will be turning a bit, and moving towards the destination at the same time.</p>
<p>The last one is the speed the robot moves at ranging from 0-127. For this I will probably use two PID loops, one for the angle error, and one for the distance error, then add both up.</p>
<p>So my problem is finding an algorithm to find what I should set angle% to, in order to arrive at a point facing the right way. There are a few 'properties that angle% must have: if distance = 0, and angle error &gt; 0, angle% should = 1. And if distance &gt; 0 and angle error = 0. Also when the angle err is 180deg, angle % should = 1.</p>
<p>I know the solution is going to be really simple but I can't wrap my head around this one.</p>
",2021-04-08 07:56:00,,207,0,1,1,,12996244.0,,3/2/2020 22:35,2.0,,,,,,120535401.0,"how many joint your robot has ?  what are the minimum/maximum angle of rotation of each joint?   Of course, every single point in the work area can be reached in various ways (angles combinations). Perhaps a solution may be to calculate all possible rotations of all joints and save the achievable point for each combination of rotations. Once, forever. So with a database system ask for a particular point and get all the solutions for that point. In the end choose the most convenient solution for that point."
4056,68632492,Can ROS Gmaping be as good with big maps as Google's Cartographer?,|ros|robotics|slam|amr|,"<p>I am trying to determine, if ros gmaping (<a href=""http://wiki.ros.org/gmapping"" rel=""nofollow noreferrer"">http://wiki.ros.org/gmapping</a>) could be effectively and reliably used to map out large maps. So far I did not have any success. On larger maps (100x100 m) sometimes comes out curved.</p>
<p>To find other methods of mapping, I started dabbling in Google's Cartographer (<a href=""https://google-cartographer-ros.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">https://google-cartographer-ros.readthedocs.io/en/latest/</a>), and after a short while, it behaves much more reliably and precisely, but its much more time-consuming.</p>
<p>So in summary what calibration methods should I use to tune my gmaping procedure ?</p>
<p>My gmaping.launch :</p>
<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;launch&gt;

  &lt;!-- Gmapping --&gt;
  &lt;node pkg=&quot;gmapping&quot; type=&quot;slam_gmapping&quot; name=&quot;slam_gmapping&quot; output=&quot;screen&quot;&gt;
    &lt;remap from=&quot;scan&quot; to=&quot;/scan_front&quot;/&gt;
    &lt;param name=&quot;base_frame&quot; value=&quot;/base_footprint&quot;/&gt;
    &lt;param name=&quot;odom_frame&quot; value=&quot;/odom&quot;/&gt;
    &lt;param name=&quot;map_frame&quot; value=&quot;/map&quot;/&gt;

    &lt;param name=&quot;map_udpate_interval&quot; value=&quot;1.0&quot;/&gt;
    &lt;param name=&quot;maxUrange&quot; value=&quot;15.0&quot;/&gt;
    &lt;param name=&quot;sigma&quot; value=&quot;0.05&quot;/&gt;
    &lt;param name=&quot;kernelSize&quot; value=&quot;1&quot;/&gt;
    &lt;param name=&quot;lstep&quot; value=&quot;0.05&quot;/&gt;
    &lt;param name=&quot;astep&quot; value=&quot;0.05&quot;/&gt;
    &lt;param name=&quot;iterations&quot; value=&quot;10&quot;/&gt;
    &lt;param name=&quot;lsigma&quot; value=&quot;0.075&quot;/&gt;
    &lt;param name=&quot;ogain&quot; value=&quot;3.0&quot;/&gt;
    &lt;param name=&quot;lskip&quot; value=&quot;0&quot;/&gt;
    &lt;param name=&quot;srr&quot; value=&quot;0.01&quot;/&gt;
    &lt;param name=&quot;srt&quot; value=&quot;0.02&quot;/&gt;
    &lt;param name=&quot;str&quot; value=&quot;0.01&quot;/&gt;
    &lt;param name=&quot;stt&quot; value=&quot;0.02&quot;/&gt;
    &lt;param name=&quot;linearUpdate&quot; value=&quot;0.5&quot;/&gt;
    &lt;param name=&quot;angularUpdate&quot; value=&quot;0.5&quot;/&gt;
    &lt;param name=&quot;temporalUpdate&quot; value=&quot;-1.0&quot;/&gt;
    &lt;param name=&quot;resampleThreshold&quot; value=&quot;0.5&quot;/&gt;
    &lt;param name=&quot;particles&quot; value=&quot;100&quot;/&gt;
    &lt;param name=&quot;xmin&quot; value=&quot;-1.0&quot;/&gt;
    &lt;param name=&quot;ymin&quot; value=&quot;-1.0&quot;/&gt;
    &lt;param name=&quot;xmax&quot; value=&quot;1.0&quot;/&gt;
    &lt;param name=&quot;ymax&quot; value=&quot;1.0&quot;/&gt;
    &lt;param name=&quot;delta&quot; value=&quot;0.02&quot;/&gt;
    &lt;param name=&quot;llsamplerange&quot; value=&quot;0.01&quot;/&gt;
    &lt;param name=&quot;llsamplestep&quot; value=&quot;0.01&quot;/&gt;
    &lt;param name=&quot;lasamplerange&quot; value=&quot;0.005&quot;/&gt;
    &lt;param name=&quot;lasamplestep&quot; value=&quot;0.005&quot;/&gt;
    &lt;param name=&quot;inverted_laser&quot; value=&quot;true&quot;/&gt;
  &lt;/node&gt;

&lt;/launch&gt;
</code></pre>
",2021-08-03 08:10:00,,487,2,0,1,,16583236.0,,8/3/2021 7:06,1.0,68685350.0,"<p>a specific parameter that will help the output is <code>minimumScore</code> - this is the accepted match value between scans. the higher the value the higher the quality of the end map</p>
<p>or change the values associated with resampling and increase the number of scans taken i.e. using 1 scan every half a second will give better results than one scan every 10 seconds</p>
<p>otherwise you just need to <em>play</em> with the parameter values of the package until you see good results</p>
",9377091.0,0.0,0.0,,
4128,69587803,How to set SolverId when setting verbosity in IK solver options in Drake toolbox?,|robotics|drake|,"<pre><code> drake::solvers::SolverOptions options;
    options.SetOption(drake::solvers::**?**, &quot;verbose&quot;, {0, 1});   //{0,1} for verbose, {0,0} for no verbosity
    const auto result = Solve(ik.prog(), {}, options);
    const auto q_sol = result.GetSolution(ik.q());
</code></pre>
<p>What do I set the  SolverId to for solving the Inverse Kinematics nlp problem?</p>
",2021-10-15 16:22:00,69588140.0,80,1,0,0,,15412613.0,,3/17/2021 5:33,10.0,69588140.0,"<p>You have two options here:</p>
<ol>
<li>Set the option for the specific solver you use. You can know which solver is invoked by checking the <code>result</code>
<pre class=""lang-cc prettyprint-override""><code>std::cout &lt;&lt; result.get_solver_id().name() &lt;&lt; &quot;\n&quot;;
</code></pre>
if it prints &quot;IPOPT&quot;, then you can do <code>options.SetOption(drake::solvers::IpoptSolver::id(), ...)</code>.</li>
<li>Another (and better) solution is to set the common solver options
<pre class=""lang-cc prettyprint-override""><code>options.SetOption(CommonSolverOption::kPrintToConsole, 1);
</code></pre>
which will print the output information to the console for any solver that supports console printing. You can also do <code>options.SetOption(CommonSolverOption::kPrintFileName, &quot;output.txt&quot;)</code> which will print the output to <code>output.txt</code> file.</li>
</ol>
",1973861.0,0.0,3.0,,
4185,70326276,How to concatenate matrices with Math.Net. How to call for a particular row or column with Math.Net?,|c#|matrix|concatenation|robotics|mathnet-numerics|,"<p>How can I call for a particular row or column?</p>
<p>Lets say I have this 8 x 6 matrix and want to call only one row or one column and assign that to a new variable, How to go about this in c#.</p>
<p>Here is a piece of the code:</p>
<pre><code>//The Eight Solutions as one matrix
            Matrix&lt;double&gt; eightsols = DenseMatrix.OfArray(new double[,]
            {
            {theta1_1 * Degrees, theta2_1 * Degrees,  theta3_1 * Degrees, theta4_1 * Degrees, theta5_1 * Degrees, theta6_1 * Degrees},
            {theta1_1 * Degrees, theta2_2 * Degrees,  theta3_2 * Degrees, theta4_2 * Degrees, theta5_2 * Degrees, theta6_2 * Degrees},
            {theta1_2 * Degrees, theta2_3 * Degrees,  theta3_1 * Degrees, theta4_3 * Degrees, theta5_3 * Degrees, theta6_3 * Degrees},
            {theta1_2 * Degrees, theta2_4 * Degrees,  theta3_2 * Degrees, theta4_4 * Degrees, theta5_4 * Degrees, theta6_4 * Degrees},
            {theta1_1 * Degrees, theta2_1 * Degrees,  theta3_1 * Degrees, (theta4_1*Degrees) + Math.PI, -theta5_1 * Degrees, (theta6_1*Degrees) + Math.PI},
            {theta1_1 * Degrees, theta2_2 * Degrees,  theta3_2 * Degrees, (theta4_2*Degrees) + Math.PI, -theta5_2 * Degrees, (theta6_2*Degrees) + Math.PI},
            {theta1_2 * Degrees, theta2_3 * Degrees,  theta3_1 * Degrees, (theta4_3*Degrees) + Math.PI, -theta5_3 * Degrees, (theta6_3*Degrees) + Math.PI},
            {theta1_2 * Degrees, theta2_4 * Degrees,  theta3_2 * Degrees, (theta4_4*Degrees) + Math.PI, -theta5_4 * Degrees, (theta6_4*Degrees) + Math.PI}
            });
            Console.WriteLine(&quot;eightsols: &quot; + eightsols);
</code></pre>
<p><strong>Now, how do I get one of these Rows or column and assign to a variable?</strong></p>
<p>Secondly, Lets say I coded it differently and want to combine or concatenate a set of 1x6 matrix as an one 8x6, how can I do such in c#? I know how to do it in MATLAB, but getting a lot of errors when trying to rewrite my program in c#. <strong>Does anyone knows where to find a good documentation or book for MathNet.Numerics other than their website?</strong></p>
<p>Here is a potion of the code:</p>
<pre><code>//Solutions 1 to 4
            Matrix&lt;double&gt; Sol1 = DenseMatrix.OfArray(new double[,]
             {
             {theta1_1 * Degrees, theta2_1 * Degrees,  theta3_1 * Degrees, theta4_1 * Degrees, theta5_1 * Degrees, theta6_1 * Degrees }
             });
            Console.WriteLine(&quot;\nSol1: &quot; + Sol1);

            Matrix&lt;double&gt; Sol2 = DenseMatrix.OfArray(new double[,]
             {
             {theta1_1 * Degrees, theta2_2 * Degrees,  theta3_2 * Degrees, theta4_2 * Degrees, theta5_2 * Degrees, theta6_2 * Degrees }
             });
            Console.WriteLine(&quot;\nSol2: &quot; + Sol2);
</code></pre>
",2021-12-12 18:01:00,,235,1,0,1,,16920008.0,,9/15/2021 14:58,6.0,70363091.0,"<p>I decided to stick with the 1 x 6 matrices as is then place the equations in an 8 x 6 matrix.</p>
<pre class=""lang-cs prettyprint-override""><code>//Inverse kinematics solutions test 
//Solutions 1 to 4
Matrix&lt;double&gt; Sol1 = DenseMatrix.OfArray(new double[,]
{
    {theta1_1 * Degrees, theta2_1 * Degrees,  theta3_1 * Degrees, theta4_1 * Degrees, theta5_1 * Degrees, theta6_1 * Degrees }
});
Console.WriteLine(&quot;\nSol1: &quot; + Sol1);

Matrix&lt;double&gt; Sol2 = DenseMatrix.OfArray(new double[,]
{
    {theta1_1 * Degrees, theta2_2 * Degrees,  theta3_2 * Degrees, theta4_2 * Degrees, theta5_2 * Degrees, theta6_2 * Degrees }
});
Console.WriteLine(&quot;\nSol2: &quot; + Sol2);

Matrix&lt;double&gt; Sol3 = DenseMatrix.OfArray(new double[,]
{
    {theta1_2 * Degrees, theta2_3 * Degrees,  theta3_1 * Degrees, theta4_3 * Degrees, theta5_3 * Degrees, theta6_3 * Degrees }
});
Console.WriteLine(&quot;\nSol3: &quot; + Sol3);

Matrix&lt;double&gt; Sol4 = DenseMatrix.OfArray(new double[,]
{
    {theta1_2 * Degrees, theta2_4 * Degrees,  theta3_2 * Degrees, theta4_4 * Degrees, theta5_4 * Degrees, theta6_4 * Degrees }
});
Console.WriteLine(&quot;\nSol4: &quot; + Sol4);

// Solutions 5 to 8
Matrix&lt;double&gt; Sol5 = DenseMatrix.OfArray(new double[,]
{
    {theta1_1 * Degrees, theta2_1 * Degrees,  theta3_1 * Degrees, (theta4_1*Degrees) + Math.PI, -theta5_1 * Degrees, (theta6_1*Degrees) + Math.PI}
});
Console.WriteLine(&quot;\nSol5: &quot; + Sol5);

Matrix&lt;double&gt; Sol6 = DenseMatrix.OfArray(new double[,]
{
    {theta1_1 * Degrees, theta2_2 * Degrees,  theta3_2 * Degrees, (theta4_2*Degrees) + Math.PI, -theta5_2 * Degrees, (theta6_2*Degrees) + Math.PI}
});
Console.WriteLine(&quot;\nSol6: &quot; + Sol6);

Matrix&lt;double&gt; Sol7 = DenseMatrix.OfArray(new double[,]
{
    {theta1_2 * Degrees, theta2_3 * Degrees,  theta3_1 * Degrees, (theta4_3*Degrees) + Math.PI, -theta5_3 * Degrees, (theta6_3*Degrees) + Math.PI}
});
Console.WriteLine(&quot;\nSol7: &quot; + Sol7);

Matrix&lt;double&gt; Sol8 = DenseMatrix.OfArray(new double[,]
{
    {theta1_2 * Degrees, theta2_4 * Degrees,  theta3_2 * Degrees, (theta4_4*Degrees) + Math.PI, -theta5_4 * Degrees, (theta6_4*Degrees) + Math.PI}
});
Console.WriteLine(&quot;\nSol8: &quot; + Sol8);

//The Eight Solutions as one matrix
Matrix&lt;double&gt; eightsols = DenseMatrix.OfArray(new double[,]
{
    {theta1_1 * Degrees, theta2_1 * Degrees,  theta3_1 * Degrees, theta4_1 * Degrees, theta5_1 * Degrees, theta6_1 * Degrees},
    {theta1_1 * Degrees, theta2_2 * Degrees,  theta3_2 * Degrees, theta4_2 * Degrees, theta5_2 * Degrees, theta6_2 * Degrees},
    {theta1_2 * Degrees, theta2_3 * Degrees,  theta3_1 * Degrees, theta4_3 * Degrees, theta5_3 * Degrees, theta6_3 * Degrees},
    {theta1_2 * Degrees, theta2_4 * Degrees,  theta3_2 * Degrees, theta4_4 * Degrees, theta5_4 * Degrees, theta6_4 * Degrees},
    {theta1_1 * Degrees, theta2_1 * Degrees,  theta3_1 * Degrees, (theta4_1*Degrees) + Math.PI, -theta5_1 * Degrees, (theta6_1*Degrees) + Math.PI},
    {theta1_1 * Degrees, theta2_2 * Degrees,  theta3_2 * Degrees, (theta4_2*Degrees) + Math.PI, -theta5_2 * Degrees, (theta6_2*Degrees) + Math.PI},
    {theta1_2 * Degrees, theta2_3 * Degrees,  theta3_1 * Degrees, (theta4_3*Degrees) + Math.PI, -theta5_3 * Degrees, (theta6_3*Degrees) + Math.PI},
    {theta1_2 * Degrees, theta2_4 * Degrees,  theta3_2 * Degrees, (theta4_4*Degrees) + Math.PI, -theta5_4 * Degrees, (theta6_4*Degrees) + Math.PI}
});
Console.WriteLine(&quot;eightsols: &quot; + eightsols);

Console.ReadLine();
</code></pre>
<p><strong>Results (Got what I needed, most importantly) see photo</strong>
<a href=""https://i.stack.imgur.com/bCXxp.png"" rel=""nofollow noreferrer"">Click to view Results</a></p>
",16920008.0,0.0,0.0,,
4011,67159164,denavit hartenberg 6dof moveo inverse kinematic robot arm,|arduino|raspberry-pi|robotics|inverse-kinematics|,"<p>I need your help.
I cant get the denavit hartenberg matrix right. (for this robot: <a href=""https://github.com/BCN3D/BCN3D-Moveo"" rel=""nofollow noreferrer"">https://github.com/BCN3D/BCN3D-Moveo</a>)
My robotic arm has 6 dof (normal one has only 5) but I dont get how to configure the theta and alpha variable for it.
Current matrix looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>joint</th>
<th>d</th>
<th>r</th>
<th>alpha</th>
<th>theta</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>232.0</td>
<td>0</td>
<td>?</td>
<td>?</td>
</tr>
<tr>
<td>2</td>
<td>0</td>
<td>223.0</td>
<td>?</td>
<td>?</td>
</tr>
<tr>
<td>3</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>?</td>
</tr>
<tr>
<td>4</td>
<td>224.0</td>
<td>0</td>
<td>?</td>
<td>?</td>
</tr>
<tr>
<td>5</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>?</td>
</tr>
<tr>
<td>6</td>
<td>175.0</td>
<td>0</td>
<td>?</td>
<td>?</td>
</tr>
</tbody>
</table>
</div>
<p>(If this table doesn't look right click <a href=""https://i.stack.imgur.com/9mUJN.png"" rel=""nofollow noreferrer"">here</a>)
The robotic arm is looking straight in the air while being in the home position.</p>
<p>How does the denavit-hartenberg matrix look like?</p>
<p>More pictures:
<a href=""https://www.bcn3d.com/bcn3d-moveo-the-future-of-learning/"" rel=""nofollow noreferrer"">https://www.bcn3d.com/bcn3d-moveo-the-future-of-learning/</a></p>
",2021-04-19 09:13:00,67176874.0,1242,1,4,1,,15685971.0,,4/18/2021 23:58,6.0,67176874.0,"<p>DH Parameters allow us to fill in the elements of our transformation matrices according to a schema. This schema has some limitations to it, which sometimes calls for clever tricks to get by any issues - but more on that in a minute.</p>
<p>First off, about the parameters themseleves.<br/></p>
<ul>
<li><code>d</code> is the distance between two frames <strong>i</strong> and <strong>(i-1)</strong> along the <strong>z</strong> axis of <strong>(i-1)</strong>.</li>
<li><code>a</code> - or <code>r</code> in your case - is the distance between two frames <strong>i</strong> and <strong>(i-1)</strong> along the <strong>x</strong> axis of <strong>i</strong>.</li>
<li><code>theta</code> is the angle between the <strong>x</strong> axes of <strong>i</strong> and <strong>(i-1)</strong> about the positive <strong>z</strong> axis of the <strong>(i-1)</strong> frame</li>
<li><code>alpha</code> is the angle between the <strong>z</strong> axes about the newly rotated x axis <strong>after</strong> the rotation of <code>theta</code> has been applied</li>
</ul>
<p>Furthermore, DH notation presupposes the following about the axes of the coordinate frames:</p>
<ul>
<li>the z-axis always points along the axis of actuation (that is, rotation in your case).</li>
<li>the x-axis of the frame <strong>i</strong> has to intersect the z-axis of the frame <strong>(i-1)</strong></li>
<li>the y-axis is set such that the frame forms a right-hand coordinate system</li>
</ul>
<p>Below is an image of your system in the home pose with coordinate frames applied according to DH notation.</p>
<p><a href=""https://i.stack.imgur.com/J2ts0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/J2ts0.png"" alt=""home pose kinematic chain"" /></a></p>
<p>As you might notice, DH Notation does not allow for a displacement between the rotation and torison joint frames. This is not a problem, since mathematically it does not make a difference on where the rotation occurrs. The curved lines denote that the relevant frames are placed in the same position for notation purposes.</p>
<p><a href=""https://i.stack.imgur.com/loXDv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/loXDv.png"" alt=""table snapshot"" /></a></p>
<p>Now the only thing you have to do is think about where the rotations of your joints might be inserted into the DH-table as well as the displacement beteween the rotational frames (l1 through 4).</p>
<p>You may then insert the DH Parameters into the DH Matrix for each frame and use these transformations for your kinematic calculations.</p>
<p>For future questions, you might want to think about posting them at the <a href=""https://robotics.stackexchange.com/"">Robotics Stack Exchange</a> site, it will probably be easier to get an answer there very quick.</p>
",10493834.0,2.0,2.0,118726837.0,I fixed your table.
3950,65759206,Modeling segway robot in simscape multibody,|matlab|simulink|robotics|simscape|,"<p>Hello guys I am trying to model a self balancing segway robot in Simscape multibody, but there is a problem that I can't see the effect of gravity on my model as I run it. I have checked the direction of the gravity and the mass of my bodies but it still does not work. The inputs of the system are the revolute joints torques which are going to be connected to a controller.</p>
<p><a href=""https://i.stack.imgur.com/m9P3L.png"" rel=""nofollow noreferrer"">The simscape model</a></p>
<p><a href=""https://i.stack.imgur.com/cKPK6.png"" rel=""nofollow noreferrer"">The robot configuration</a></p>
",2021-01-17 09:50:00,,213,0,2,0,,15023096.0,,1/17/2021 9:38,4.0,,,,,,116266251.0,"I am not 100% sure, since I do not know the contents of the 'wheels connecting rod' block, but I suspect the issue lies with the prismatic joint. This does not allow for rotation of the connector rod, so the pendulum cannot rotate. Is that indeed the case?"
4166,70197548,How can I find angle between two turtles(agents) in a network in netlogo simulator?,|netlogo|simulator|angle|robotics|agent-based-modeling|,"<p>In a formation robots are linked with eachother,number of robots in a neighbourhood may vary. If one robot have 5 neighbours how can I find the angle of that one robot with its other neighbour?</p>
",2021-12-02 10:09:00,70198741.0,174,1,0,0,,16331927.0,"Paris, France",6/28/2021 7:28,10.0,70198741.0,"<p><em>(Following a comment, I replaced the sequence of</em> &lt;<code>face</code> <em>+ read</em> <code>heading</code>&gt; <em>with just using</em> <code>towards</code>, <em>wich I had overlooked as an option. For some reason the comment I am referring to has been deleted quickly so I don't know who gave the suggestion, but I read enough of it from the cell notification)</em></p>
<p>In NetLogo it is often possible to use turtles' <code>heading</code> to know degrees.</p>
<p>Since your agents are linked, a first thought could be to use <code>link-heading</code>, which directly <a href=""https://ccl.northwestern.edu/netlogo/docs/dictionary.html#link-heading"" rel=""nofollow noreferrer"">reports the heading in degrees from <em>end1</em> to <em>end2</em></a>.</p>
<p>However note that this might not be ideal: using <code>link-heading</code> will work spotlessly only if you are interested in knowing the heading from <em>end1</em> to <em>end2</em>, which means:</p>
<ul>
<li>If your links are <strong>directed</strong>, it reports the heading from the source to the target;</li>
<li>If your links are <strong>undirected</strong>, it reports the heading <a href=""https://ccl.northwestern.edu/netlogo/docs/programming.html#links"" rel=""nofollow noreferrer"">from the older turtle to the younger turtle</a>.</li>
</ul>
<p>If that's something that you are interested in, fine. But it might not be so! For example, if you have undirected links and are interested in knowing the angle from <code>turtle 1</code> to <code>turtle 0</code>, using <code>link-heading</code> will give you the wrong value:</p>
<pre><code>to setup
  clear-all

  create-turtles 2 [
   setxy random-xcor random-ycor
   set color black
   set label who
  ]

  ask turtle 0 [
   create-link-with turtle 1 
  ]
end


to go
  ask link 0 1 [
   show link-heading 
  ]
end
</code></pre>
<p><a href=""https://i.stack.imgur.com/azD0e.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/azD0e.png"" alt=""enter image description here"" /></a></p>
<p>... while we know, by looking at the two turtles' positions, that the degrees from <code>turtle 1</code> to <code>turtle 0</code> must be in the vicinity of 45.</p>
<p>An approach that better fits all possible cases is to directly look into the <code>heading</code> of the turtle you are interested in, regardless of the nature or direction of the link. You can let your reference turtle <code>face</code> the target turtle, and then read <code>heading</code> of the reference turtle. Or better: you can directly use <code>towards</code>, which reports just the same information but without having to make turtles actually change their heading. Copy and run the code below to see how this approach always gives the right answer!</p>
<pre><code>to setup
  clear-all

  create-turtles 5 [
   setxy random-xcor random-ycor
   set color black
   set label who
  ]
end


to go
  let reference sort turtles
  foreach reference [
   r -&gt;
   ask r [
     print &quot;---------------------------------------------------------------------------------------------------------------------------------------------&quot;
     let targets sort other turtles
     foreach targets [
       t -&gt;
       let direction (towards t)
       type &quot;I am &quot; type self type &quot;. The NetLogo angle between me and &quot; type t type &quot; is &quot; type direction type &quot;, while the normal mathematical angle is &quot; print heading-to-angle direction  
      ]
     print &quot;---------------------------------------------------------------------------------------------------------------------------------------------&quot;
    ] 
  ]
end


to-report heading-to-angle [ h ]
  report (90 - h) mod 360
end
</code></pre>
<p>In your case, the target group (that I have set just as <code>other turtles</code> in my brief example above) could be based on the actual links and so be constructed as <code>(list link-neighbors)</code> or <code>sort link-neighbors</code> (because if you want to use <code>foreach</code>, the agentset must be passed as a list - <a href=""https://ccl.northwestern.edu/netlogo/docs/dictionary.html#foreach"" rel=""nofollow noreferrer"">see here</a>).</p>
<p><strong>Update</strong>: I actually ended up also making a toy model that represents your case more closely, i.e. with links and using <code>link-neighbors</code>. See below:</p>
<pre><code>to setup
  clear-all

  create-turtles 100 [
   move-to one-of patches with [not any? turtles-here]
   set color black
   set label who
  ]

  ask n-of 2 turtles [
   create-links-with n-of 5 other turtles 
  ]
end


to go
  let reference-turtles sort turtles with [count my-links &gt; 2]
  foreach reference-turtles [
   r -&gt;
   ask r [
     print &quot;-----------------------------------------------------------------------------------------------------------------------------------------------&quot;
     let targets sort link-neighbors
     foreach targets [
       t -&gt;
       let direction (towards t)
       type &quot;I am &quot; type self type &quot;. The NetLogo angle between me and &quot; type t type &quot; is &quot; type direction type &quot;, while the normal mathematical angle is &quot; print heading-to-angle direction
      ]
     print &quot;-----------------------------------------------------------------------------------------------------------------------------------------------&quot;
    ] 
  ]
end


to-report heading-to-angle [ h ]
  report (90 - h) mod 360
end
</code></pre>
<p>Final note: you surely noticed the <code>heading-to-angle</code> procedure, taken directly from the <code>atan</code> entry <a href=""https://ccl.northwestern.edu/netlogo/docs/dictionary.html#atan"" rel=""nofollow noreferrer"">here</a>. It is a useful way to convert degrees expressed in the NetLogo geometry (where North is 0 and East is 90) to degrees expressed in the usual mathematical way (where North is 90 and East is 0). I don't know what degrees you're interested in, so it's worth to leave this hint here.</p>
",12391423.0,4.0,0.0,,
3982,66732022,Almathswig error when trying to use almath to program NAO robot,|python|importerror|robotics|,"<p>Hi There Stackoverflow,</p>
<p>I am trying to program a NAO robot using Python. I would like to access some of the motion features of the NAO which require the &quot;almath&quot; module which I believe is installed as a part of the naoqi python sdk.</p>
<p>The naoqi python sdk is successfully installed on my machine and I have no problem importing &quot;naoqi&quot; into any of my scripts. However when trying to run any motion related scripts that require &quot;almath&quot; I run into the error seen in the second image. I am not sure why this module cannot be found when I have installed the naoqi library. The almathswig error is not well documented online so I thought I would put up a question to see if anyone can point my in the right track.</p>
<p>The error I am faced with is:</p>
<pre><code>Traceback (most recent call last):
  File &quot;c:/Users/Zachary Ringer/Desktop/Python/stackexchangeexample.py&quot;, line 4, in &lt;module&gt;
    import almath as m # python's wrapping of almath
  File &quot;C:\Python27\Lib\site-packages\pythonNaoqi\lib\almath.py&quot;, line 28, in &lt;module&gt;
    from almathswig import *
  File &quot;C:\Python27\Lib\site-packages\pythonNaoqi\lib\almathswig.py&quot;, line 26, in &lt;module&gt;
    _almathswig = swig_import_helper()
  File &quot;C:\Python27\Lib\site-packages\pythonNaoqi\lib\almathswig.py&quot;, line 18, in swig_import_helper
    import _almathswig
ImportError: No module named _almathswig
</code></pre>
<p>Within my naoqi library, the almath swig module is present
[1]: <a href=""https://i.stack.imgur.com/BdhFU.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/BdhFU.png</a></p>
<p>Please help, why can't python find it?</p>
",2021-03-21 11:59:00,67379741.0,881,3,0,0,,15444742.0,,3/21/2021 11:46,5.0,67379741.0,"<p>I have experienced the same while working with Almath. However, could you directly install the chorographe Version 2.8.6.X from <em><a href=""https://developer.softbankrobotics.com/nao6/downloads/nao6-downloads-windows"" rel=""nofollow noreferrer"">https://developer.softbankrobotics.com/nao6/downloads/nao6-downloads-windows</a></em>.</p>
<p>After installing it you can right click on the canvas and do select <strong>Create New Box</strong> then <em><strong>&quot;python&quot;</strong></em></p>
<p>In General Description write anything like Test then OK.</p>
<p>Double click on the Box and remove everything, then</p>
<p>import almath</p>
<p>After connecting the nodes run it,</p>
<p>Are you getting the same error.</p>
<p>If yes then you can 2.5 version.</p>
<p>There is some bug in the latest version. It is not working on my laptop as well. However Version 2.5 is working.</p>
",12195145.0,0.0,2.0,,
3990,66786292,multiprocessing - blanket process termination,|python|python-multiprocessing|robotics|terminate|kill-process|,"<p>I'm building a GUI to control a robot in Python.
The GUI has a few buttons, each one executes a function that loops indefinitely.
It's like a roomba, where the &quot;clean kitchen&quot; function makes it continually clean until interrupted.</p>
<p>In order to keep the GUI interactive, I'm executing the function in a separate process using multiprocessing.</p>
<p>I've got a stop function that I call that returns the robot to home, and kills the child process (otherwise the child process would reach the next line, and the robot would start turning around when it's left the kitchen and gone home). The stop function runs in the main/parent process as it doesn't loop.</p>
<p>I've got a GUI button which calls Stop, and I'll also call it whenever I start a new process.</p>
<p>My processes are started like this:</p>
<pre><code>from file1 import kitchen
from file2 import bedroom

    if event == &quot;Clean kitchen&quot;:
       stoptour()
       p = multiprocessing.Process(target=kitchen,args=(s,),daemon=True)
       p.start()

    if event == &quot;Clean bedroom&quot;:
       stoptour()
       p = multiprocessing.Process(target=bedroom,args=(s,),daemon=True)
       p.start()
</code></pre>
<p>The argument being passed is just the socket that the script is using to connect to the robot.
My stop function is:</p>
<pre><code>def stoptour():
   p.terminate()
   p.kill()
   s.send(bytes.fromhex(XXXX)) #command to send the stop signal to the robot
   p.join()
</code></pre>
<p>This all runs without error and the robot stops, but then starts up again (because the child process is still running). I confirmed this by adding to the stop function:</p>
<pre><code>if p.is_alive:
        print('Error, still not dead')
    else:
        print('Success, its dead')
</code></pre>
<p>Every time it prints &quot;Error, still not dead&quot;...
Why are p.kill and p.terminate not working? Is something spawning more child processes?
Is there a way to write my <code>stoptour()</code> function so that it kills any and all child processes completely indiscriminately?</p>
<hr />
<p>Edited to show the code:</p>
<pre><code>import socket
import PySimpleGUI as sg
import multiprocessing
import time

from file1 import room1
from file2 import room2
from file3 import room3

#Define the GUI window
layout = [[sg.Button(&quot;Room 1&quot;)],
          [sg.Text(&quot;Start cleaning of room1&quot;)],
          [sg.Button(&quot;Room 2&quot;)],
          [sg.Text(&quot;Start cleaning of room2&quot;)],
          [sg.Button(&quot;Room 3&quot;)],
          [sg.Text(&quot;Start cleaning room3&quot;)],
          [sg.Button(&quot;Stop&quot;)],
          [sg.Text(&quot;Stop what you're doing&quot;)]]

# Create the window
window = sg.Window(&quot;Robot Utility&quot;, layout)

#Setup TCP Connection &amp; Connect
TCP_IP = '192.168.1.100' #IP
TCP_port = 2222 #Port
s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) #Setup TCP connection
s.connect((TCP_IP, TCP_port)) #Connect

#Define p so I can define stop function
if __name__ == '__main__':
    p = multiprocessing.Process(target=room1, args=(s,), daemon=True)
    p.start()
    p.terminate()
    p.kill()
    p.join()

#Define stop function
def stoptour():
    s.send(bytes.fromhex('longhexkey'))
    p.terminate()
    p.kill()
    p.join()
    s.send(bytes.fromhex('longhexkey')) #No harm stopping twice...
    if p.is_alive:
        print('Error, still not dead')
    else:
        print('Success, its dead')
stoptour()

#GUI event loop
while True:
    event, values = window.read()
    if event == &quot;Room 1&quot;:
        if __name__ == '__main__':
            stoptour()
            p = multiprocessing.Process(target=room1, args=(s,), daemon=True)
            p.start()

        
    if event == &quot;Room 2&quot;:
        if __name__ == '__main__':
            stoptour()
            p = multiprocessing.Process(target=room2, args=(s,), daemon=True)
            p.start()

    if event == &quot;Room 3&quot;:
        if __name__ == '__main__':
            stoptour()
            p = multiprocessing.Process(target=room3, args=(s,), daemon=True)
            p.start()
        
    if event == &quot;Stop&quot;:
        stoptour()
        sg.popup(&quot;Tour stopped&quot;)
    
    if event == sg.WIN_CLOSED:
        stoptour()
        s.close()
        print('Closing Program')
        break

window.close()
</code></pre>
",2021-03-24 17:30:00,,93,0,4,0,,15460390.0,,3/23/2021 11:35,2.0,,,,,,118058852.0,"Note `p.kill` doesn't call `kill`, you need to do `p.kill()`"
3949,65703124,Convert Euler angle between camera & robot coordinate system,|python|opencv|robotics|euler-angles|aruco|,"<p>My problem is simple, but yet confusing as I personally have no experience in angles and angles conversion yet.</p>
<p>Basically, I need to locate the position of an object attached with single AruCo marker then send the 3d coordinate and pose of that object (the marker) to the robot. Note that the robot model I use is an industrial one manufactured by ABB, and the 3d coordinate I sent already been converted to Robot Coordinate System.</p>
<p>Put aside the problem of coordinate, I solved it using Stereo Cameras. However, I found the pose problem to be so difficult, especially when convert the pose of AruCo marker w.r.t camera to the robot coordinate system. The images below represent the two-coordinate system, one for camera and one for the robot.</p>
<p><a href=""https://i.stack.imgur.com/hwpG3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hwpG3.png"" alt=""Coordinate System"" /></a></p>
<p>The angle I collected from AruCo Marker were converted to Euler Angles, the methods were applied from OpenCV library here:</p>
<pre><code>def PoseCalculate(rvec, tvec, marker):
    rmat = cv2.Rodrigues(rvec)[0]
    P = np.concatenate((rmat, np.reshape(tvec, (rmat.shape[0], 1))), axis=1)
    euler = -cv2.decomposeProjectionMatrix(P)[6]
    eul = euler_angles_radians
    yaw = eul[1, 0] 
    pitch = eul[0, 0]
    roll = eul[2, 0]
    return (pitch, yaw, roll)
</code></pre>
<p>The result are three angles that represent pose of the marker. Pitch represents the rotation when the marker rotate around X axis (camera), Yaw for the Y axis (camera) and Roll for the Z axis (camera as well.)
So, how I can convert these three angles to the robot coordinate system?</p>
<p><a href=""https://i.stack.imgur.com/qqbi7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qqbi7.png"" alt=""Sample Angles Collected"" /></a></p>
<p>Thanks for reading this long question and wish all of you be healthy in new year 2021!</p>
",2021-01-13 13:39:00,,418,0,0,2,0.0,11414471.0,,4/26/2019 7:33,8.0,,,,,,,
3972,66222390,Communicating with MiR200 Robot using REST API,|python|json|robotics|rest|,"<p>I have been trying to control the Mir 200 robot using REST API in python. I have programmed the mission in Mir to work only if the PLC register value changes, so I give the following request in python but I get a 405 error. Can anybody help me with the syntax? I try to change the PLC register 41 value to 5.</p>
<pre class=""lang-py prettyprint-override""><code>register = {&quot;value&quot;: 5}
PLCregister = requests.post(host + 'registers/41', json = register, headers = headers)
print(PLCregister)
</code></pre>
",2021-02-16 10:09:00,,1762,2,2,0,0.0,14643608.0,,11/15/2020 17:11,12.0,66405743.0,"<p>I have never worked with the MiR200 or any of their robots (they look cool and I'm a little jealous) but it looks like instead of a <code>POST</code> request, you can try a <code>PUT</code> request to modify the value. I'm going off this pdf: <a href=""https://www.mobile-industrial-robots.com/media/2214/mir_robot_rest_api_200.pdf"" rel=""nofollow noreferrer"">https://www.mobile-industrial-robots.com/media/2214/mir_robot_rest_api_200.pdf</a></p>
<p>The 405 error means the request method (<code>POST</code> in this case) is not allowed, maybe it has been deprecated.</p>
<p>You can try this:</p>
<pre class=""lang-py prettyprint-override""><code>register = {'value': 5}
plc_register = requests.put(host + 'registers/41', json=register, headers=headers)
print(plc_register)
</code></pre>
<p><code>POST</code> requests are generally used to tell the server to create data, not modify it. You can see they kinda mention this in the MiR 2.0.X documentation:</p>
<p><code>POST /registers/{id}</code></p>
<blockquote>
<p>Modify the value of the PLC register with the specified ID. Registers
1 to 100 are integers and registers 101-200 are float. <code>Even though this is not a standard use of the POST call it has been included for compatibility purposes</code></p>
</blockquote>
<p>So, maybe the <code>POST</code> method is not in your robot's software, just speculating.</p>
",12442479.0,0.0,0.0,117398153.0,"`405` means not allowed, I'd take a look at the documentation for the MIR-200 (that's what anybody answering the question would have to do for you). You should update the question tags to make sure you get the right people viewing too."
3960,65929472,How to use Drake with deep reinforcement learning,|machine-learning|deep-learning|simulation|robotics|drake|,"<p>Does drake have a pipeline/platform with which I can implement deep reinforcement learning algorithms?</p>
",2021-01-28 00:43:00,,195,1,0,0,,15095362.0,,1/28/2021 0:39,3.0,65930488.0,"<p>Drake is frequently used as an environment in open-source deep RL frameworks; it should plug in nicely.  You can find some examples by searching the web, and would welcome more public examples.</p>
<p>(We will likely make a tutorial for it on the main drake site once we move the tutorials to their own repo so that they can have additional dependencies).</p>
",9510020.0,0.0,0.0,,
4068,69113962,Wiiuse library - how to calculate the quaternion from the wm->exp.mp.angle_rate_gyro like the DSU Controller test,|quaternions|robotics|euler-angles|wiimote|wiiuse|,"<p>I currently have the wiiuse lib op and running with the motion plus output the <code>angle_rate</code> from the gyro. Now i want this to give me the output in angles either euler representation or best of with quaternions, and i am a little stuck here. any  solutions code examples that could point me in the way of how to calculate these?</p>
<p>I have an example of wiimoteHook running with a DSU controller test that gives output the quaternion output and is exactly what i want to give further to my program.</p>
<p>my program i am working on is that i am trying to make the wii remote hold by a person with a position system using ultra sound that gives me a coordinate(x,y,z) in a world frame then I want the wiimote to give me the rotation in that point to teach a 6 axis robot a tool center point that in the end would imitate movement of the remote.</p>
<p>I hope that somebody can guide me in the way of getting the rotations from the wiimote.</p>
<p>Thanks in advance.</p>
",2021-09-09 07:27:00,,113,0,7,0,,16867345.0,,9/9/2021 7:08,2.0,,,,,,122349677.0,"Yes your are right at some point, but i can calculate the orientation based upon the anglerate and the sample time. im am almost there with a big inspiration from this site. https://x-io.co.uk/open-source-imu-and-ahrs-algorithms/ and from this https://www.digikey.com/en/articles/apply-sensor-fusion-to-accelerometers-and-gyroscopes. now i have the quaternions and a rotation vector. next in my work i have to make my own calibration function since i have to calbrate over time becourse of the gyro is drifting"
4037,67972978,Is there an easy way to find out which of two frames is closer to the root in a Multibody plant?,|collision|robotics|drake|motion-planning|,"<p>I am working on recovering from collision. I have the names of bodies in collision and the frames associated with them and now I want to move the body/frame that is closer to the end effector to get out of collision, but I couldn't find a straightforward way to get this information from a <code>MultiBodyPlant</code>. I could construct another representation of the graph and search through it, but I was wondering if it is possible to maybe get this from <code>drake</code> instead?</p>
<p>The problem is that sometimes the robot ends up in collision with itself or the environment and I want to make a plan to recover it.
From the <code>QueryObject</code>, I am able to get a <code>vector&lt;SignedDistancePair&gt;</code> that gives me the geometry IDs of object instances collision, and unit vector pointing in the direction of fastest increase in collision depth
Then I use a <code>SceneGraphInspector</code> to get the corresponding frame IDs and then use the frame IDs to get the bodies in collision
For now I make the assumption that only two bodies are in collision
Now that I have the two bodies in collision, I want to find the one that is closer to the end effector and is therefore easier to move out of collision</p>
",2021-06-14 15:20:00,67973688.0,76,1,1,0,,3138875.0,"Zrich, Switzerland",12/27/2013 8:42,46.0,67973688.0,"<p>Wow.  I think you're right that we don't make this one easy (but we should).</p>
<p>For now, I would think you can call <code>MultibodyPlant::GetJointIndices()</code> and then loop the joints via <code>MultibodyPlant::get_joint()</code> to find the joint <code>Joint::child_body() == collision_body</code>, and then use <code>Joint::parent_body()</code>.  And we can open an issue if avoiding that (small?) linear search becomes important?</p>
",9510020.0,1.0,2.0,120168627.0,"Workflow question: Why are you doing this via indexing rather than using distances and gradients? (is it b/c distances / gradients are ill-conditioned when trying to recover from ""in-collision""?)"
4125,69570518,OpenCV Detecting points along curved lines,|python|opencv|line|curve|robotics|,"<p>I am new to openCV programming and I would like to do something which seems fairly simple in my mind but I haven't found any useful solutions.</p>
<p>I am building a robot that needs to follow lines on an image drawn by a user. Let's use <a href=""https://i.stack.imgur.com/uyB8G.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uyB8G.png"" alt=""line drawing"" /></a> as a reference.</p>
<p>Obviously, the line will need to be converted into points in order to navigate to. I have attempted using Houghlines but that seems to only work for straight lines. Using contours seems promising to me but the result I get from findContours is a set of points on either side of the line such as: <a href=""https://i.stack.imgur.com/yNqKB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yNqKB.png"" alt=""drawing with contours"" /></a>I am not surprised as this is the definition of a contour.</p>
<p>My question is, is there a way to detect points along the line and not on either side of it?</p>
<p>Thanks!</p>
",2021-10-14 12:14:00,,2263,0,3,3,,17078688.0,,10/5/2021 9:50,10.0,,,,,,122968873.0,"For what it's worth, I had this same question a couple of years ago and I've not come across a solution since. There are some aspects that make this a difficult question in general. E.g., if you have a closed loop, where should the start point be; if you have a Y-fork, should your function return a path for each possibility; if you have a noisy/broken path, how do you reconstruct it, especially problematic if there are neighbouring paths."
4015,67265287,Distance covered by wheels and the angle it turns by (physics and robotics),|c|rotation|physics|robotics|webots|,"<p>Let me try and describe what I'm working on.</p>
<p>I'm trying to code the controller for a vacuum cleaning robot. The entire project is done within the Webots platform. How it works is that when the robot hits a wall, it turns by a random angle and continues cleaning the house.</p>
<p>The issue: what I found was that the amount it turns by is inaccurate. If I specify for it to turn by 90 degrees, it only turns by 60. If I specify for it to turn by 360 degrees, it only turns by 240 or so (visual guesstimation).</p>
<p>I found a workaround - a temporary workaround. I found that multiplying the angle by a certain amount (1.2275) will turn the angle very close to the real angle specified in the question. So if I specify the angle to turn as 180, and multiply that with 1.2275, I get a value that is probably 175 or 177 (again from visual guesstimation).</p>
<p>The problem is that this error stacks up over time. So initially it might be an offset of just 2-3 degrees, which is nothing, but over 30 minutes of running the simulation it might build to relatively high numbers. This is unacceptable.</p>
<p>I asked a friend and he suggested that I scale the offset as the simulation progresses so that the offset scales with the error, but I feel this is a temporary workaround.</p>
<p>I would appreciate some help with the physics of it all as I don't really understand or know much about the mechanical field it works in. Any help would be appreciated.</p>
<p>Provided below is the code for the turn function.</p>
<pre><code>static void turn(double angle) {
    double l_offset = wb_position_sensor_get_value(left_position_sensor);
    double r_offset = wb_position_sensor_get_value(right_position_sensor);
    double orient;

    stop();

    angle = angle * ANGLE_OFFSET;   //the important part
    double neg = (angle &lt; 0.0) ? -1.0 : 1.0;

    step();

    wb_motor_set_velocity(left_motor,   neg * HALF_SPEED);
    wb_motor_set_velocity(right_motor, -neg * HALF_SPEED);

    do {
        double l  = wb_position_sensor_get_value(left_position_sensor) - l_offset;
        double r  = wb_position_sensor_get_value(right_position_sensor) - r_offset;
        double dl = l * WHEEL_RADIUS;  // distance covered by left wheel in meter
        double dr = r * WHEEL_RADIUS;  // distance covered by right wheel in meter

        orient = neg * (dl - dr) / AXLE_LENGTH; // delta orientation in radian

        step();
    } while (orient &lt; neg * angle);

    stop();
    step();
}
</code></pre>
<p>Let me provide some explanation for the code. <code>angle</code> obviously specifies the angle to turn the robot by.  The second line shows the workaround that I'm currently using. Reading through the next few lines you can see that the robot turns by moving the left wheel backwards, and the right wheel forwards. It continuously gets the distance covered by the wheels and checks for a condition. When that condition is true, it stops turning.</p>
<p>My guess is that there might be something wrong in <code>orient = neg * (dl - dr) / AXLE_LENGTH; // delta orientation in radian</code> this line. Any tips? Maybe someone who can verify if the physics/mechanics aspect of it is correct?</p>
",2021-04-26 10:50:00,,227,1,4,0,,12775244.0,"Chennai, Tamil Nadu, India",1/24/2020 12:21,4.0,67269337.0,"<p>Can you segment the cause of the discrepancy between the desired angle and the actual (visual) angle?  I seems diving in and fixing that would be a better use of time than patching a patch.</p>
",15762880.0,0.0,3.0,118896613.0,is your sample period infinite?
3959,65888385,How does a vision guided robot arm approach a target object to pick up and drop it off at a target location?,|python|opencv|computer-vision|robotics|,"<p>I have a work project currently where my company is using a 3D printer, a couple of servo motors, and a raspberry pi to produce a robotic arm. This robotic arm is meant to be visually-guided. I am task to research and code the software for the robot to be able to pick up a specific object and place it at a specific location. I need some help on how I could program the robot to pick up the object using visual guidance.</p>
<p>I've made a set-up where the top-down image will very likely look like the picture below. The objective is to pick up a die (those that you can find in a casino or in a game of monopoly) inside a clearly marked area of operation and place it on a piece of paper with an 'X' marked on it.
<a href=""https://i.stack.imgur.com/rXzt8.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rXzt8.jpg"" alt=""robotic-arm-sandbox"" /></a></p>
<p>This is what I've accomplished thus far:</p>
<ol>
<li>I am able to use OpenCV and Machine Learning to identify the die and the paper marked with 'X' and extract the coordinates in the image where the die is located (right now my camera is not located directly above the area of operations)</li>
<li>I am able to determine the orientation of the die (i.e. 30 degrees clockwise, 42 degrees anti-clockwise, etc) - this is important because the robotic arm uses a caliper-like clipper to pick up the die and I would not want it to hold the die insecurely by the corner</li>
<li>I am also able to code a program to move the robotic arm (something like servo1 to turn 30 degrees clockwise, servo2 to turn 40 degrees anti-clockwise, etc)</li>
</ol>
<p>However, I have no idea how I could code the robotic arm to pick up the die. The die and the X in the photo are for illustration purposes only, their location could change to any spot inside the area of operation.</p>
<p>What is really depressing is I can't come up with any strategy. Hopefully, someone can advise on some principles and recommend some strategies - I'm not asking for code because I am confident to code my own software. I also don't think the programming language is important at this point but if it helps, I am using Python for this project - are there any libraries for such a task already? I felt like I have searched the entire web but haven't found any helpful tutorials on this yet.</p>
<p>Also, if it's helpful - I come from a web-software developer background usually coding in HTML, CSS, and Javascript. Python was the first language I've managed to master to a competent degree before I started coding with web technologies. I have some experience in C during my high school but have not coded in C for more than 10 years already.</p>
<p>Thanks for any help.</p>
",2021-01-25 16:11:00,,325,0,0,3,0.0,3910616.0,Singapore,8/5/2014 13:47,76.0,,,,,,,
3964,66024645,I am getting error when i run this code to make raspberry pi face tracking robot with gui without any servo motor just to follow only people,|python|performance|raspberry-pi|computer-vision|robotics|,"<p>Please tell me what wrong here. It gives error like this</p>
<pre><code>if face == True:#i have used this to make if there is no face found then stop if face found go forward
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
</code></pre>
<p>The code is here:</p>
<pre><code>import RPi.GPIO as GPIO

import cv2

import carapp

import sys

vid = cv2.VideoCapture(0)

face_cascade = cv2.CascadeClassifier('/home/pi/harr cascade/haarcascade_frontalface_default.xml')


Motor1A = 21

Motor1B = 20

Motor2A = 16

Motor2B = 26

GPIO.setwarnings(False)

GPIO.setmode(GPIO.BCM)

GPIO.setup(Motor1A,GPIO.OUT)

GPIO.setup(Motor1B,GPIO.OUT)

GPIO.setup(Motor2A,GPIO.OUT)

GPIO.setup(Motor2B,GPIO.OUT)

def forward():

    print(&quot;GOING FORWARD&quot;)

    GPIO.output(Motor1A,GPIO.LOW)

    GPIO.output(Motor1B,GPIO.HIGH)

    GPIO.output(Motor2A,GPIO.LOW)

    GPIO.output(Motor2B,GPIO.HIGH)

def backward():

    print(&quot;GOING BACKWARD&quot;)

    GPIO.output(Motor1A,GPIO.HIGH)

    GPIO.output(Motor1B,GPIO.LOW)

    GPIO.output(Motor2A,GPIO.HIGH)

    GPIO.output(Motor2B,GPIO.LOW)

def Left():

    print(&quot;Going Left&quot;)

    GPIO.output(Motor1A,GPIO.HIGH)

    GPIO.output(Motor1B,GPIO.LOW)

    GPIO.output(Motor2A,GPIO.LOW)

    GPIO.output(Motor2B,GPIO.HIGH)

def Right():

    print(&quot;Going Right&quot;)

    GPIO.output(Motor1A,GPIO.LOW)

    GPIO.output(Motor1B,GPIO.HIGH)

    GPIO.output(Motor2A,GPIO.HIGH)

    GPIO.output(Motor2B,GPIO.LOW)

def stop():

    print(&quot;Stopping&quot;)

    GPIO.output(Motor1A,GPIO.LOW)

    GPIO.output(Motor1B,GPIO.LOW)

    GPIO.output(Motor2A,GPIO.LOW)

    GPIO.output(Motor2B,GPIO.LOW)
    

def cameo():
    while(True):
        _,img = vid.read()
        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
        face = face_cascade.detectMultiScale(gray,1.1,4)
        for (x,y,w,h) in face:
            cv2.rectangle(img,(x,y),(x+w,y+h),(50,20,70),3)
            
        if face == True:#i have used this to make if there is no face found then stop if face found go forward
            carapp.forward()
        else:
            carapp.stop()

        cv2.imshow('img',img)
        if cv2.waitKey(1) &amp; 0xff == ord('q'):
            break
            sys.exit()
    vid.release()
    cv2.destroyAllWindows()

import tkinter as tk

gui = tk.Tk()

gui.title(&quot;Car control&quot;)

gui.geometry(&quot;500x500&quot;)

lol = tk.Button(gui,text=&quot;Forward&quot;,bg=&quot;red&quot;,command=forward)

lol.grid(row=2,column=5)

bot = tk.Button(gui,text=&quot;Backward&quot;,bg=&quot;green&quot;,command=backward)

bot.grid(row=10,column=5)

ron = tk.Button(gui,text=&quot;Left&quot;,bg=&quot;orange&quot;,command=Left)

ron.grid(row=5,column=0)

bob = tk.Button(gui,text=&quot;Right&quot;,bg=&quot;yellow&quot;,command=Right)

bob.grid(row=5,column=10)

dol = tk.Button(gui,text=&quot;camera&quot;,bg=&quot;blue&quot;,command = cameo)

dol.grid(row=5,column=100)

sod = tk.Button(gui,text=&quot;stop&quot;,bg=&quot;cyan&quot;,command = stop)

sod.grid(row=5,column=5)

button = tk.Button(text = &quot;Click and Quit&quot;, command = sys.exit)

button.grid(row=15,column=10)

gui.mainloop()

#this product is copytright of shouryawadhwa aka @programmerShourya
</code></pre>
",2021-02-03 09:29:00,,52,1,2,0,,15112207.0,,1/30/2021 14:02,7.0,66025029.0,"<p>The return value from</p>
<pre><code>face_cascade = cv2.CascadeClassifier('/home/pi/harr cascade/haarcascade_frontalface_default.xml')
</code></pre>
<p>Is not a single True/False value - it's something that (from the error message) contains multiple values - perhaps that's something over which you can iterate, or alternately use the all, any functions to perform boolean tests over the collection.</p>
<p>Are the results even boolean?</p>
<p>Why don't you look inside of <code>face_cascade</code> and see what kind of data it holds? Once you know that, you'll be better equipped to deal with the result.</p>
<p>Also here:</p>
<pre><code>for (x,y,w,h) in face:
        cv2.rectangle(img,(x,y),(x+w,y+h),(50,20,70),3)
        
    if face == True:#i have used this to make if there is no face found then stop if face found go forward
        carapp.forward()
    else:
        carapp.stop() 
</code></pre>
<p>You start iterating over <code>face</code> in your for-loop, but then later you try to ask whether it's True or False.</p>
<p>Which is inconsistent.</p>
<p>What are the contents of x,y,w,h ? They look like they might describe bounding boxes, not what I'd imagine you'd sensibly be able to test with a True or False.</p>
<p>Are any of those boolean?</p>
<p>Also, some of this &quot;copyrighted code&quot; looks like you copy/pasted it from these docs:</p>
<p><a href=""https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html"" rel=""nofollow noreferrer"">https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html</a></p>
",4137061.0,0.0,1.0,116775053.0,Thnke for editing the code
3970,66120048,Is it possible to change array size? How do you do it?,|robotics|kuka-krl|,"<p>Is it possible to change array size at runtime in KUKA KRL programming language? Is it possible to mimic the behavior of List from C#?</p>
",2021-02-09 13:35:00,66177253.0,321,2,0,1,,11668410.0,"Kaunas, Lietuva",6/19/2019 6:22,24.0,66131651.0,"<p>No.  It  is  not  possible  :(</p>
",5871947.0,0.0,0.0,,
3953,65831042,Computed Torque Control with a Simscape Manipulator,|matlab|simulink|robotics|manipulators|simscape|,"<p>I'm trying to realize a Vertical Two-Arm Planar Manipulator (Double Pendulum like) using Simscape Multibody. I'm kinda newbie in Robotics field and I'm not even sure if I did well the following creation.
I realized this Manipulator using 2 Spherical Joints (Actuation: Torque Provided by input, abilitating Velocity Sensors) e 2 arms (1 meter-long Brick Blocks).
I don't exactly know how to realize the Computed Torque Control: I know the theory behind it but I couldn't find any examples on how to implement it in Simulink through Matlab Functions, especially in a case like this. How can I realize a PD controller from a System created on Simscape Multibody?</p>
<p><a href=""https://i.stack.imgur.com/dY9NH.png"" rel=""nofollow noreferrer"">Simscape Model I created</a></p>
<p><a href=""https://i.stack.imgur.com/eyxy3.png"" rel=""nofollow noreferrer"">The Results</a></p>
",2021-01-21 15:55:00,,340,0,6,1,,15053293.0,,1/21/2021 15:33,4.0,,,,,,116442170.0,"For Computed Torque control you have modeled the robot, and calculated what torque you have to put on each joint. Realizing this torque value can be done using a PD controller, as said previously. There are multiple ways to obtain the values for the P and D gain. A common method is trial-and-error in which the values are tweaked such that an acceptable performance is reached. Optimization is another method. The Ackermann method is different, since it is based on the pole-placement principle."
4028,67449652,How to plot a right triangle path in MATLAB,|matlab|robotics|,"<p>The following code is used to plot a circular path:</p>
<pre><code>% Head velocity
v = 1.5;
% Angular velocity
w = 1;      
% Radius
R = v/w;
% Time
dt = 0.1;
t = 0:dt:(2*pi*R/v);
% Initial mobile robot position vector
x0 = 0; y0 = 0; th0 = 0;
P = [];
P(:,1) = [x0 y0 th0]; 
% Loop for all time steps
for k = 1 : length(t)
    P(:,k+1) = P(:,k) + dt*[v*cos(P(3,k));v*sin(P(3,k));w];
    plot(P(1,1:k+1),P(2,1:k+1))
    pause(0.01)
    axis square
    axis([-2 2 -0.5 3.5])
end                         
</code></pre>
<p>, how can I manipulate the position vector to plot a right triangle path.</p>
<p>Thanks in advance.</p>
",2021-05-08 16:14:00,,203,0,3,0,0.0,15872705.0,,5/8/2021 16:01,2.0,,,,,,119259361.0,"@MatteoV
The plot needs to be in terms of `v*cos(...),v*sin(...)` , I think of making 3 ""for"" loops for each segment."
4146,69841218,Line following challenge - robot moves forward but ignores the line,|c++|robotics|,"<p>I am trying to solve a line following challenge with a 3Pi+ robot, using 3 IR sensors (array in my code) and although I get my array working, receive the right data in the serial monitor and set a threshold value for my sensors, I cannot seem to make the robot follow the line : the robot moves forward but ignores the line.</p>
<p>I am adding my code here:</p>
<pre><code>const int left_sensor_pin = A0;
const int right_sensor_pin = A3;
const int centre_sensor_pin = A2;

// store values into array
#define NB_LS_PINS 3
int ls_pin[NB_LS_PINS] = { left_sensor_pin, centre_sensor_pin, right_sensor_pin };
int which;

// store time
unsigned long start_time;
unsigned long elapsed_time [3];
unsigned long end_time_ls [3];
unsigned long set_motors;

bool done = false;

# define L_PWM_PIN 10
# define L_DIR_PIN 16
# define R_PWM_PIN 9
# define R_DIR_PIN 15

void setup() {
  Serial.begin (9600);
  pinMode (left_sensor_pin, INPUT);
  pinMode (right_sensor_pin, INPUT);
  pinMode (centre_sensor_pin, INPUT);
  delay(5000);

  pinMode(L_PWM_PIN, OUTPUT);
  pinMode(L_DIR_PIN, OUTPUT);
  pinMode(R_PWM_PIN, OUTPUT);
  pinMode(R_DIR_PIN, OUTPUT);

  digitalWrite(L_DIR_PIN, LOW);
  digitalWrite(R_DIR_PIN, LOW);

  analogWrite(L_PWM_PIN, 50);
  analogWrite(R_PWM_PIN, 50);
}

void loop() {

  //charge capacitor
  pinMode (left_sensor_pin, OUTPUT);
  pinMode (right_sensor_pin, OUTPUT);
  pinMode (centre_sensor_pin, OUTPUT);
  digitalWrite(left_sensor_pin, HIGH);
  digitalWrite(right_sensor_pin, HIGH);
  digitalWrite(centre_sensor_pin, HIGH);

  // Delay for capacitor to charge.
  delayMicroseconds(10);
  start_time = micros();

  pinMode (left_sensor_pin, INPUT);
  pinMode (right_sensor_pin, INPUT);
  pinMode (centre_sensor_pin, INPUT);

  bool pins_read [3] = {false, false, false};
  done = false;

  while ( done == false ) {

    for ( int which = 0; which &lt; NB_LS_PINS; which++) {

      if ( digitalRead (ls_pin[ which ]) == LOW &amp;&amp; pins_read [which] == false) { // need also to track whether pins are read, a single = is setting to a value and == compares values

        end_time_ls [which] = micros ();
        elapsed_time [which] = micros() - start_time;
        pins_read [which] = true;
      }

      if (pins_read [0] == true &amp;&amp; pins_read [1] == true &amp;&amp; pins_read [2] == true) {

        done = true;
      }
    }
  }

  elapsed_time[0] = end_time_ls [0] - start_time;
  elapsed_time[1] = end_time_ls [1] - start_time;
  elapsed_time[2] = end_time_ls [2] - start_time;

  Serial.println (elapsed_time [0]);
  Serial.println (elapsed_time [1]);
  Serial.println (elapsed_time [2]);
  Serial.println(&quot;------------------------------&quot;);

  //Motors

  if (elapsed_time [0] &lt; 10000 &amp;&amp; elapsed_time[1] &gt; 10000 &amp;&amp; elapsed_time[2]  &lt; 10000)
  {
    //go forward
    analogWrite( L_PWM_PIN, 100 );
    analogWrite( R_PWM_PIN, 100 );
  }

  else if (elapsed_time [0] &gt; 10000 &amp;&amp; elapsed_time[1] &lt; 10000 &amp;&amp; elapsed_time[2]  &lt; 10000)
  {
    // Turn right.
    analogWrite( L_PWM_PIN, 100 );
    analogWrite( R_PWM_PIN, 0 );
  }
  else if (elapsed_time [0] &lt; 10000 &amp;&amp; elapsed_time[1] &lt; 10000 &amp;&amp; elapsed_time[2]  &gt; 10000)
  {
    // turn left.
    analogWrite( L_PWM_PIN, 0 );
    analogWrite( R_PWM_PIN, 100 );
  }
}
</code></pre>
",2021-11-04 14:54:00,,284,0,4,1,,17328995.0,,11/4/2021 14:39,4.0,,,,,,123456714.0,"True, I may need to use =><, I've been adjusting the threshold value and adding more conditions and my robot is starting to make jerky moves ! But do you know how to write intervals in Arduino ? I would like to write something like if  20000<sensorsX<30000 then ... but I cannot find this info on  C++ websites"
4370,73547951,Does WebRTC make sense for low-latency streaming over local network?,|python|stream|webrtc|real-time|robotics|,"<p>I am developing a python application where a drone and a computer communicate over local network (wifi). My need is to stream the drone's camera to OpenCV-python on the computer with the lowest possible latency at the highest possible resolution.</p>
<p>Thus far I have been trying rather naive approaches over TCP that give okay-ish results, I get something like 0.1s or 0.2s latency for VGA format. It has a point for some use cases as it enables lossless transmission, but since the most common scenario is to aggressively control the drone in real time from the stream, I am aiming for something of much lower latency and hopefully higher resolution.</p>
<p>My advisor has recommended using WebRTC. I have done some research on the matter, found the <code>aiortc</code> library that implements WebRTC in python, but I am unsure this is the way to go for my use case as it seems to be more geared toward web developers.</p>
<p>I am a bit lost I think. Could you highlight the advantages of WebRTC in my application if any, or point me toward solutions that are more relevant for my problem please?</p>
<p>Thanks in advance!</p>
",2022-08-30 20:09:00,,595,1,0,3,,13224876.0,,4/5/2020 0:28,48.0,73550430.0,"<p>[1]<a href=""https://i.stack.imgur.com/TCbTV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TCbTV.png"" alt=""Protocol comparison"" /></a></p>
<p>rtc communicate peer to peer, I think you knew that. And if u use local network U will not need STUN server or TURN to connect two devices. That is make more decrease latency and code shorter. I'm not work with drone but I think your method stream had latency &lt; 0,2 is good.
<a href=""https://restream.io/blog/streaming-protocols/"" rel=""nofollow noreferrer"">fyi protocol campare</a></p>
",16970888.0,1.0,0.0,,
4373,73792169,Why do I get wrong results in Eye to Hand calibration with TSAI method?,|opencv|camera-calibration|robotics|,"<p>My goal is to find the position of a fixed camera near an ABB robot. To achieve this I use OpenCV with the <code>calibrateHandEye()</code> method.</p>
<p>My setup :</p>
<ul>
<li>A fixed camera relative to the robot base.</li>
<li>An Aruco chessboard securely attached on the robot gripper.</li>
</ul>
<p>What are the datas I use :</p>
<ul>
<li>The robot flange position in the base robot frame. For each point of view I get X,Y,Z in mm and  Q0, Q1, Q2, Q3 (the quaternions for the orientation)</li>
<li>The chessboard pose using <code>Cv2.SolvePnP()</code> for each point of view.</li>
</ul>
<p>My problem :</p>
<p>In the <code>calibrateHandEye()</code> method when using the <code>HandEyeCalibrationMethod.PARK</code> parameter the results look fine but with the <code>HandEyeCalibrationMethod.TSAI</code> parameters the results are totally wrong.
What is weird is that with the <code>TSAI</code> method the results are wrong only because of a specific point of view. If I remove the problematic point I get good results. I double checked this point and nothing looks wrong about it. The only particularity of this point is that Q3 does not have the same sign compare to the others.
What do I miss? Is there a concept I do not get about the HandEye calibration? Do I need to &quot;convert&quot; my quaternion when Q3 has a different sign?</p>
<p>The robot poses (X Y Z Q0 Q1 Q2 Q3):</p>
<pre><code>752.38  -445.96 638.27  0.49314 0.82816 0.07749 -0.25488
695.07  -422.2  608.08  0.59008 0.55626 0.29763 -0.50378
676.35  -398.17 536.42  0.24081 0.79788 0.39938 -0.38195
564.8   -485.81 496.16  0.15692 0.67193 0.57801 -0.43565
636.3   -585.09 785.91  0.35118 0.70842 0.31395 -0.5256
674.15  -511.81 773.14  0.57508 0.58545 0.14986 -0.55143
571.73  -357.76 475.48  0.45984 0.76823 0.35026 -0.27512
570.54  -689.04 642.31  0.54275 0.73657 0.1737  -0.3643
522.64  -715.58 524.38  0.56756 0.69228 0.40363 -0.18898
959.89  -356.79 270.37  0.21973 0.84652 0.08542 -0.47732
797.93  -704.56 556.47  0.32647 0.79281 -0.20858    -0.47049
719.35  -976.8  364.49  0.30796 0.6863  -0.01742    -0.65867
906.75  -815.7  442.98  0.28293 0.61452 0.28696 -0.67821
817.77  60.02   154.15  0.39718 0.51765 0.5831  -0.48403
601.56  -569.27 480.03  0.73969 0.62158 0.24355 -0.08473
564.14 -408.19 612.04 0.43702 0.75943 0.42312 -0.23074
213.07 -606.02 670.48 0.43765 0.18263 0.87376 0.10798
</code></pre>
<p>The chessboard poses (X Y Z Rz Ry Rz):</p>
<pre><code>-173.77009900744957 -27.139489979884726 721.9859325597129 -2.334719719539493 -0.6062024477154826 -0.28966237372137954
-162.65501847055066 -26.85185566224528 554.2767630115629 -3.0903465310356175 -0.2708982558600907 0.3291807930683886
-212.2479141822481 51.766662118465504 800.8059566577332 2.84556559822533 0.6680923622745605 1.0186319821149945
-86.09262298219434 126.5189807416448 819.6499137042784 2.4464190779627226 0.3475444809955279 0.9796705709434745
-21.898455311634194 89.93494845557872 450.972207473501 2.9168894650869452 0.7851694907456874 0.36096988692878934
-125.26779119062992 46.70400960098488 387.12034740949827 -2.890237270353615 -0.6482179109246602 0.49523655100783626
-206.47692825640706 101.15492953993694 853.5432013448462 -2.7845901323880615 -0.22557851509655572 -0.6048715250043196
95.71225425941525 150.04326736083527 632.9246651210359 -2.667387096309625 -0.49559063361443423 -0.0800873810963933
185.27210452976593 75.05506502382198 785.5205288697724 -2.736330698283884 0.2238771996238751 -0.47211319818633085
-377.8399022540909 -154.0271857570537 1048.9407456307115 -2.5031939821571854 -1.4073251338872734 -0.564144060449975
-97.9813803618571 -2.8982716572417337 780.3826436567616 -2.0175024432723028 -1.52111122787833 0.07978790180677371
206.26218058582174 76.52368830230974 839.4288605610492 -2.5926961439942273 -1.6274800425133538 0.23606702098023685
168.593405541405 -172.2903924334086 728.3609797990065 2.5826463269087228 0.9741779880929422 0.06773704115655899
-599.5692170589801 -208.7756878651145 1082.1873391173815 2.643714241537085 -0.05662133314429313 0.32661484396025725
42.774011707655745 -13.244986693290826 777.4026418514148 -2.3533206372842397 0.38523715542666004 0.0006794851359404635
-146.04506073796117 71.8778078758312 735.3038919876881 -2.7999700612696636 -0.05617005954652475 -0.7930410515888396
-93.2419492180757 102.4051550208101 689.1444785949833 1.8674927518469928 -1.9023073605673702 0.8138748033587386

</code></pre>
<p>My results :
Translation X,Y,Z of my camera relative to the robot : 543.69 -551.87 282.19 (totally wrong results)</p>
<p>With same datas except the last one : 518.87 -769.82 1273.86 (coherent results)</p>
<p>My code if it can help : (I use OpenCV in C# with the nugget OpenCvSharp but you can provide me python or C++ code)</p>
<ul>
<li>To convert quaternion to rotation matrix :</li>
</ul>
<pre><code>    private double[,] Quaternion2RotationMatrix(float q0, float q1, float q2, float q3)
        {
            double[,] matrix = new double[3, 3];
            matrix[0, 0] = 2 * (q0 * q0 + q1 * q1) - 1;
            matrix[0, 1] = 2 * (q1 * q2 - q0 * q3);
            matrix[0, 2] = 2 * (q1 * q3 + q0 * q2);
            matrix[1, 0] = 2 * (q1 * q2 + q0 * q3);
            matrix[1, 1] = 2 * (q0 * q0 + q2 * q2) - 1;
            matrix[1, 2] = 2 * (q2 * q3 - q0 * q1);
            matrix[2, 0] = 2 * (q1 * q3 - q0 * q2);
            matrix[2, 1] = 2 * (q2 * q3 + q0 * q1);
            matrix[2, 2] = 2 * (q0 * q0 + q3 * q3) - 1;    
            return matrix;
        }
</code></pre>
<ul>
<li>To convert a Mat object to a Array</li>
</ul>
<pre><code>    private double[,] ConvertMat2Array(Mat _mat, bool _isFloat = true)
        {
            double[,] result = new double[_mat.Rows, _mat.Cols];
            for (int i = 0; i &lt; _mat.Rows; i++)
            {
                for (int j = 0; j &lt; _mat.Cols; j++)
                {
                    if (_isFloat)
                        result[i, j] = _mat.At&lt;float&gt;(i, j);
                    else
                        result[i, j] = _mat.At&lt;double&gt;(i, j);
                }
            }
            return result;
        }
</code></pre>
<ul>
<li>My Pose3D class to store the robot coordinates</li>
</ul>
<pre><code>    public class Pose3D
    {
        public float X { get; set; }

        public float Y { get; set; }

        public float Z { get; set; }

        public float Q0 { get; set; }

        public float Q1 { get; set; }

        public float Q2 { get; set; }

        public float Q3 { get; set; }

        public Pose3D()
        {

        }

        public Pose3D(float _x, float _y, float _z, float _q0, float _q1, float _q2, float _q3) : this()
        {
            X = _x;
            Y = _y;
            Z = _z;
            Q0 = _q0;
            Q1 = _q1;
            Q2 = _q2;
            Q3 = _q3;
        }

    }
</code></pre>
<ul>
<li>And finally my method to estimate the camera position in the robot base frame :</li>
</ul>
<pre><code>    private (double[,], double[,]) HandEyeCalibration(Pose3D[] _gripper2base, double[][] _rTarget2cam, double[][] _tTarget2Cam)
        {
            try
            {
                var l = _gripper2base.Length;
                Mat[] R_base2gripper = new Mat[l];
                Mat[] t_base2gripper = new Mat[l];

                Mat[] R_target2cam = new Mat[l];
                Mat[] t_target2cam = new Mat[l];
                for (int i = 0; i &lt; l; i++)
                {
                    // Convert quaternion to rotation matrix
                    var R_gripper2base = Quaternion2RotationMatrix(_gripper2base[i].Q0, _gripper2base[i].Q1, _gripper2base[i].Q2, _gripper2base[i].Q3);
                    double[,] T_gripper2base = new double[4, 4];
                    for (int row = 0; row &lt; 3; row++)
                        for (int col = 0; col &lt; 3; col++)
                            T_gripper2base[row, col] = R_gripper2base[row, col];

                    T_gripper2base[0, 3] = _gripper2base[i].X;
                    T_gripper2base[1, 3] = _gripper2base[i].Y;
                    T_gripper2base[2, 3] = _gripper2base[i].Z;
                    T_gripper2base[3, 3] = 1;

                    // Eye to Hand configuration. Need to invert gripper2base to get base2gripper to find cam2base
                    Mat mat = new Mat(4, 4, MatType.CV_64FC1, T_gripper2base);
                    var invExpr = mat.Inv();
                    var inv = invExpr.ToMat();

                    // Extract rotation matrix and translation vector
                    R_base2gripper[i] = inv.SubMat(0, 3, 0, 3);
                    t_base2gripper[i] = inv.SubMat(0, 3, 3, 4);
                    Mat tmp_R_t2c = new Mat(3, 1, MatType.CV_64FC1, _rTarget2cam[i]);
                    Mat tmp_t_t2c = new Mat(3, 1, MatType.CV_64FC1, _tTarget2Cam[i]);

                    R_target2cam[i] = tmp_R_t2c;
                    t_target2cam[i] = tmp_t_t2c;

                }

                using Mat R_cam2base = new Mat();
                using Mat t_cam2base = new Mat();
                // Replace HandEyeCalibrationMethod.TSAI by HandEyeCalibrationMethod.PARK to change the method used.
                Cv2.CalibrateHandEye(R_base2gripper, t_base2gripper, R_target2cam, t_target2cam, R_cam2base, t_cam2base, HandEyeCalibrationMethod.TSAI);
                var rotationMatrix = ConvertMat2Array(R_cam2base, false);
                var translationMatrix = ConvertMat2Array(t_cam2base, false);

                return (rotationMatrix, translationMatrix);
            }
            catch (Exception ex)
            {
                return (null, null);
            }
        }
</code></pre>
<p>Thank you !</p>
",2022-09-20 20:14:00,,413,0,0,1,,13247582.0,"Montral, CANADA",4/7/2020 10:31,8.0,,,,,,,
4275,71661263,What's consistency mapping in SLAM?,|c++|computer-vision|point-cloud-library|robotics|slam|,"<p>I always see &quot;consistent mapping&quot; or &quot;map consistency&quot; in SLAM papers and articles, but I have no idea about what consistent map is.</p>
<p>I have found <a href=""https://stackoverflow.com/questions/21086082/what-is-consistency-map-confidence-map"">enter link description here</a>, but it did not solve my problem.
Furthermore, what is local consistentcy and global consistency?</p>
",2022-03-29 11:33:00,71688947.0,216,1,0,1,,17081429.0,,10/5/2021 15:32,8.0,71688947.0,"<p>During the mapping, the robot sequentially tries to locate landmarks or objects around it with precise coordinates, both locally and globally. The local consistency of the landmarks in each sequential operation means that their positions relative to the robot and the positions among themselves correspond to reality.</p>
<p>The map fragments created at each step are combined to form a meaningful global map. In the meantime, the landmarks determined in the previous step can also take place in the next step. During this merge, if the locations of the landmarks are very different from each other on both local maps, local maps cannot be combined and mapping cannot be made because there will be no consistency.</p>
<p>Consistency of both the local and global map is critical, especially in loop closure studies. If the map you created is not consistent, you cannot detect loop closure.</p>
<p>In summary, <em>the consistency of your map</em> means that the locations of the same landmarks that you detect at different stages are consistent with each other.</p>
",4717084.0,0.0,1.0,,
4445,74280132,How to get the transformation between two body frames in PyDrake,|python|transformation|robotics|drake|pose-estimation|,"<p>I have two frames at different joint locations of IIWA arm, using
<code>f1 = plant.GetFrameByName(&quot;iiwa_link_0&quot;, kuka_model) f2 = plant.GetFrameByName(&quot;iiwa_link_2&quot;, kuka_model)</code></p>
<p>I want to find the transformation between these two body frames(f1, f2).</p>
<p>Getting error when using f1.CalcPoseInBodyFrame() :</p>
<p><code>TypeError: CalcPoseInBodyFrame(): incompatible function arguments. The following argument types are supported: 1. (self: pydrake.multibody.tree.Frame_[float], context: pydrake.systems.framework.Context_[float]) -&gt; pydrake.math.RigidTransform_[float]</code></p>
<p>What is the correct way to approach this problem?</p>
<p>Thanks,
Sarvesh</p>
",2022-11-01 18:13:00,74280393.0,160,1,0,1,,4329907.0,"Irvine, California",12/5/2014 18:30,38.0,74280393.0,"<p>It would be helpful if you post the actual code giving you the error.</p>
<p>Worst case, you can do this:</p>
<pre><code>context = ... # assuming you have a context where things are posed.

f1 = plant.GetFrameByName(&quot;iiwa_link_0&quot;, kuka_model)
f2 = plant.GetFrameByName(&quot;iiwa_link_2&quot;, kuka_model)
X_F2F1 = f1.CalcPose(context, f2)
</code></pre>
",7686256.0,4.0,2.0,,
4453,74339346,DiagramBuilder: Cannot operate on ports of System plant until it has been registered using AddSystem,|robotics|drake|,"<p>I have an issue working with <code>DiagramBuilder</code> and <code>ManipulationStation</code> classes.
It appears to me, that c++ API and the python bindings work differently in my case.
C++ API behaves as expected, while the python bindings result in the runtime error:</p>
<p><code>DiagramBuilder: Cannot operate on ports of System plant until it has been registered using AddSystem</code></p>
<h2>How I use C++ API</h2>
<ul>
<li>In one of the <code>ManipulationStation::Setup...()</code> methods I inject a block of code, that adds an extra manipuland</li>
</ul>
<pre><code>const std::string sdf_path = FindResourceOrThrow(&quot;drake/examples/manipulation_station/models/bolt_n_nut.sdf&quot;);
RigidTransform&lt;double&gt; X_WC(RotationMatrix&lt;double&gt;::Identity(), Vector3d(0.0, -0.3, 0.1));
bolt_n_nut_ = internal::AddAndWeldModelFrom(sdf_path, &quot;nut_and_bolt&quot;, lant_-&gt;world_frame(), &quot;bolt&quot;, X_WC, plant_);
</code></pre>
<ul>
<li>I inject another block of code into the method <code>ManipulationStation::Finalize</code>:</li>
</ul>
<pre><code>auto zero_torque = builder.template AddSystem&lt;systems::ConstantVectorSource&lt;double&gt;&gt;(Eigen::VectorXd::Zero(plant_-&gt;num_velocities(bolt_n_nut_)));
builder.Connect(zero_torque-&gt;get_output_port(), plant_-&gt;get_actuation_input_port(bolt_n_nut_));
</code></pre>
<p>With these changes, the simulation runs as expected.</p>
<h2>How I use python bindings</h2>
<pre><code>plant = station.get_multibody_plant()
manipuland_path = get_manipuland_resource_path()
bolt_with_nut = Parser(plant=plant).AddModelFromFile(manipuland_path)
X_WC = RigidTransform(RotationMatrix.Identity(), [0.0, -0.3, 0.1])
plant.WeldFrames(plant.world_frame(), plant.GetFrameByName('bolt', bolt_with_nut), X_WC)
</code></pre>
<p>...</p>
<pre><code>station.Finalize()
zero_torque = builder.AddSystem(ConstantValueSource(AbstractValue.Make([0.])))
builder.Connect(zero_torque.get_output_port(), plant.get_actuation_input_port(bolt_with_nut_model))
</code></pre>
<p>This triggers a <code>RuntimeError</code> with a message as above; The port, which causes this error is <code>nut_and_bolt_actuation</code>.</p>
<p>My vague understanding of the problem is the (in) visibility of <code>nut_and_bolt</code> System, due to having two distinct <code>DiagramBuilder</code>s in a process: 1) a one is inside <code>ManipulationStation</code> 2) another is in the python code, that instantiates this <code>ManipulationStation</code> object.</p>
<p>Using <code>ManipulationStation</code> via python bindings is a preference for me, because that way I would've avoided depending on a custom build of drake library.</p>
<p>Thanks for your insight!</p>
",2022-11-06 20:06:00,74339555.0,75,1,0,2,,1912514.0,,12/18/2012 10:14,72.0,74339555.0,"<p>I agree with your assessment: you have two different <code>DiagramBuilder</code> objects here.  This does not have anything to due with C++ or Python; the <code>ManipulationStation</code> is itself a <code>Diagram</code> (created using its own <code>DiagramBuilder</code>), and you have a second <code>DiagramBuilder</code> (in either c++ or python) that is connecting the <code>ManipulationStation</code> together with other elements. You are trying to connect a system that is in the external diagram to a port that is in the internal diagram, but is not exposed.</p>
<p>The solution would be to have the <code>ManipulationStation</code> diagram expose the extra nut and bolt actuation port so that you can connect to it from the second builder.</p>
<p>If you prefer Python, I've switched my course to using a <a href=""https://github.com/RussTedrake/manipulation/blob/d1c4056e1029fb8fe451828a88cdf5b2868d7bcf/manipulation/scenarios.py#L476"" rel=""nofollow noreferrer"">completely python version of the manipulation station</a>. I find this version is much easier to adapt to different student projects.  (To be clear, the setup is in python, but at simulation time all of the elements are c++ and it doesn't call back to python; so the performance is almost identical.)</p>
",9510020.0,3.0,0.0,,
4444,74279453,Getting joint location(3D pose) w.r.t world of Kuka Iiwa arm in Drake,|python|transformation|robotics|drake|,"<p>I have a Kuka Iiwa 7 arm in the scene and I am interested in getting the current(real-time) 3D coordinates of the arm joints w.r.t world (or w.r.t robot base). I am able to get the current joint 1 DOF positions(thetas) but not the exact 3D coordinate representing the position of the joint in some frame(world or base).</p>
<p>I tried adding Triad for each link and tried get_pose_in_world() method for RigidBody_[float] type object of the frame, but received an error that the method does not exist for RigidBody_[float] class.</p>
<p>Note: I am using PyDrake for the project.</p>
<p>What is the correct way to approach this problem?</p>
",2022-11-01 17:12:00,74281479.0,149,1,0,1,,4329907.0,"Irvine, California",12/5/2014 18:30,38.0,74281479.0,"<p>When the parser creates a joint, it'll also crate a &quot;parent&quot; frame and a &quot;child&quot; frame. These two frames are then constrained by the joint such that in the &quot;zero configuration&quot; (say zero angle) the parent and child frames are coincident.</p>
<p>If you do happen to have the frame object, then you can call <code>Frame::CalcPoseInWorld()</code>. Now, to retrieve the frame, you need to know its name. For that you will call <code>GetFrameByName()</code>.</p>
<p>Say you want the pose of the parent frame. Here is the convention used to name frames:</p>
<ul>
<li>If the pose of the parent frame in the body frame is the identity (either a default or explicitly provided) then the parent frame IS the body frame and you can retrieve it with the body's name.</li>
<li>If the pose is not the identity, then a new frame is created (offset by that pose) and its name is &quot;parent_[body_name]&quot;, where <code>body_name</code> is the name of the parent body.</li>
</ul>
",1889975.0,1.0,0.0,,
4430,74131579,How can I get an accurate relative yaw angle from a 6DOF IMU like the LSM6DS3 with minimal gyro drift?,|accelerometer|robotics|gyroscope|kalman-filter|imu|,"<p>I currently have a platform that has a 6DOF IMU (the LSM6DS3) with no magnetometer. I want to get as accurate of a relative yaw angle reading as possible, avoiding or minimizing any gyro drift. I tried a simple approach of:</p>
<p>a) calculate the gyro angular rate zero-offset by computing the average gyro angular rate reading for a few seconds while my platform is known to be stationary.</p>
<p>b) read the angular rate while the LSM6DS3 reports a new reading is available (so, this should essentially be the output data rate I configured, i.e. 416Hz). I subtract this from the zero-offset calculated in (a) above to get the degrees/s angular rate and then integrate (multiply the time delta with the degrees/s angular rate and add to the current yaw angle).</p>
<p>Is this the best I can do to avoid gyro drift issues if I want to get as accurate of a relative yaw rating possible?</p>
<p>I looked a little bit at Kalman filters and Madgwick filters and Mahony filters but they don't seem to suggest the ability to improve yaw angle readings as it seems the accelerometer would not be useful to calculate the yaw angle? Is that correct?</p>
",2022-10-19 20:34:00,,360,0,0,1,,20286248.0,,10/19/2022 20:22,2.0,,,,,,,
4446,74284444,Hard Realtime C++ for Robot Control,|c++|embedded|real-time|robotics|,"<p><br />
I am trying to control a robot using a template-based controller class written in c++. Essentially I have a UDP connection setup with the robot to receive the state of the robot and send new torque commands to the robot. I receive new observations at a higher frequency (say 2000Hz) and my controller takes about 1ms (1000Hz) to calculate new torque commands to send to the robot. The problem I am facing is that I don't want my main code to wait to send the old torque commands while my controller is still calculating new commands to send. From what I understand I can use Ubuntu with RT-Linux kernel, multi-thread the code so that my getTorques() method runs in a different thread, set priorities for the process, and use mutexes and locks to avoid data race between the 2 threads, but I was hoping to learn what the best strategies to write hard-realtime code for such a problem are.</p>
<pre class=""lang-cpp prettyprint-override""><code>// main.cpp
#include &quot;CONTROLLER.h&quot;
#include &quot;llapi.h&quot;

void main{
    ...
    CONTROLLERclass obj;
    ...
    double new_observation;
    double u;
    ...
    while(communicating){
        get_newObs(new_observation); // Get new state of the robot (2000Hz)
        obj.getTorques(new_observation, u); // Takes about 1ms to calculate new torques
        send_newCommands(u); // Send the new torque commands to the robot
    }
    ...
}
</code></pre>
<p>Thanks in advance!</p>
",2022-11-02 04:45:00,,319,1,5,0,,15412613.0,,3/17/2021 5:33,10.0,74285362.0,"<p>Okay, so first of all, it sounds to me like you need to deal with the fact that you receive input at 2 KHz, but can only compute results at about 1 KHz.</p>
<p>Based on that, you're apparently going to have to discard roughly half the inputs, or else somehow (in a way that makes sense for your application) quickly combine the inputs that have arrived since the last time you processed the inputs.</p>
<p>But as the code is structured right now, you're going to fetch and process older and older inputs, so even though you're producing outputs at ~1 KHz, those outputs are constantly being based on older and older data.</p>
<p>For the moment, let's assume you want to receive inputs as fast as you can, and when you're ready to do so, you process the most recent input you've received, produce an output based on that input, and repeat.</p>
<p>In that case, you'd probably end up with something on this general order (using C++ threads and atomics for the moment):</p>
<pre class=""lang-cpp prettyprint-override""><code>std::atomic&lt;double&gt; new_observation;

std::thread receiver = [&amp;] { 
    double d;
    get_newObs(d);
    new_observation = d;
};

std::thread sender = [&amp;] {
    auto input = new_observation;
    auto u = get_torques(input);
    send_newCommands(u);
};
</code></pre>
<p>I've assumed that you'll always receive input faster than you can consume it, so the processing thread can always process whatever input is waiting, without receiving anything to indicate that the input has been updated since it was last processed. If that's wrong, things get a little more complex, but I'm not going to try to deal with that right now, since it sounds like it's unnecessary.</p>
<p>As far as the code itself goes, the only thing that may not be obvious is that instead of passing a reference to <code>new_input</code> to either of the existing functions, I've read new_input into variable local to the thread, then passed a reference to that.</p>
",179910.0,0.0,4.0,131183013.0,"@JeremyFriesner I see you mean something similar to Jerry's solution. But as my comment says, the object I am using, I am not sure if I can use it as an atomic."
4316,72090152,Optimizing algorithm for a robot picking order,|algorithm|optimization|robotics|plc|,"<p>I'm trying to optimize a pick and place problem based on the previous robot positions.
Let's assume that all the positions, which represents working stations, are numbered from <strong>0</strong> to <strong>5</strong> and there's a SCARA robot between them. Every position does need some <strong>fixed</strong> time to process that piece and the piece must go to all stations before we can call it done . A PLC controls all of this process so it knows when a piece is ready somewhere inside one of these stations and sends the robot there to pick it. <br> <br> It is also important to know that stations <strong>2,3,4</strong> do the same process so a part must go either to station <strong>2, 3</strong> or <strong>4</strong> and then to station <strong>5</strong>. So the first part comes to position <strong>0</strong> (position <strong>0</strong> generates parts), the robot picks that part and places it to position <strong>1</strong>. After a fixed time the robot takes that part and moves it to station <strong>2</strong>. Now the position <strong>1</strong> is empty so the robot takes a part from position <strong>0</strong> and puts it into position <strong>1</strong>. Every movement of the robot takes a small time but not 0, which affects the whole process cycle time. <br> I'm trying to include that robot movement time into the parts processing time so when a piece is ready inside a station, the robot should be right there ready to pick it and place it somewhere else.<br><br>
A real experiment based on 5 stations numbered 0 to 5 gives the following order of the positions:<br />
<br> <br> <code>0-1, 1-2, 0-1, 2-5, 1-2, 0-1, 1-3, 0-1, 2-5, 1-2, 0-1, 1-3</code> ...
<br>
<BR>
The positions can be grouped because once a part is picked (the digit before '-') i know where it will be placed (the digit after) .
<br>
How can I estimate the next picking point so that the robot can move itself there before the plc tells it to ?</p>
",2022-05-02 17:19:00,72091541.0,84,1,0,0,,14814898.0,,12/12/2020 19:37,4.0,72091541.0,"<p>I don't know if your robot has any resources for this... If it doesn't, you can probably do it on your PLC.</p>
<p>Since the time of your process in each station is fixed, I'm thinking of using 5 or 6 separate timers, one for each station. Once the robot leaves the part in place you could start the specific timer for that station. When the robot is idle, it consults the remaining time of each one and goes to the one with the shortest time to complete.</p>
<p>This could be improved if you have a way to calculate (or look up in a list) the travel time from the current point to the station that is about to end... for example, the robot is at station 0 and station 1 is at 1.0 s from finishing and station 5 is 0.99999s from finishing... it would probably be more efficient to go to position 1 (closer) instead of going to position 5 (far).</p>
<p>Obviously this won't work if you don't know how long the part will take to be available at one of the stations, but in that case if you use a buffer to calculate the average waiting time of a part at a station (in which case you would have a sensor or something to check), you could estimate that the part is about to be ready using timers as well.</p>
",9203951.0,1.0,0.0,,
4416,73880798,Drake: Integrate Mass Matrix and Bias Term in Optimization Problem,|c++|robotics|nonlinear-optimization|drake|dynamicparameters|,"<p>I am trying to implement Non Linear MPC for a 7-DOF manipulator in drake. To do this, in my constraints, I need to have dynamic parameters like the Mass matrix M(q) and the bias term C(q,q_dot)*q_dot, but those depend on the decision variables q, q_dot.</p>
<p>I tried the following</p>
<pre><code>    // finalize plant
    // create builder, diagram, context, plant context
    ...

    // formulate optimazation problem
    drake::solvers::MathematicalProgram prog;

    // create decision variables
    ...
    std::vector&lt;drake::solvers::VectorXDecisionVariable&gt; q_v;
    std::vector&lt;drake::solvers::VectorXDecisionVariable&gt; q_ddot;

    for (int i = 0; i &lt; H; i++) {
        q_v.push_back(prog.NewContinuousVariables&lt;14&gt;(state_var_name));
        q_ddot.push_back(prog.NewContinuousVariables&lt;7&gt;(input_var_name));
    }

    // add cost
    ...

    // add constraints
    ...
    for (int i = 0; i &lt; H; i++) {
        plant.SetPositionsAndVelocities(*plant_context, q_v[i]);
        plant.CalcMassMatrix(*plant_context, M);
        plant.CalcBiasTerm(*plant_context, C_q_dot);
    }

    ...
    
    for (int i = 0; i &lt; H; i++) {
        prog.AddConstraint( M * q_ddot[i] + C_q_dot + G &gt;= lb );
        prog.AddConstraint( M * q_ddot[i] + C_q_dot + G &lt;= ub );
    }

    // solve prog
    ...
    
</code></pre>
<p>The above code will not work, because <code>plant.SetPositionsAndVelocities(.)</code> doesn't accept symbolic variables.</p>
<p>Is there any way to integrate M,C in my ocp constraints ?</p>
",2022-09-28 11:41:00,73884081.0,85,1,0,2,,20109781.0,,9/28/2022 10:39,3.0,73884081.0,"<p>I think you want to impose the following nonlinear nonconvex constraint</p>
<pre><code>lb &lt;= M * qddot + C(q, v) + g(q) &lt;= ub
</code></pre>
<p>This constraint is non-convex. We will need to solve it through nonlinear optimization, and evaluate the constraint in every iteration of the nonlinear optimization. We can't do this evaluation using symbolic computation (it would be horribly slow with symbolic computation).</p>
<p>So you will need a constraint evaluator, something like this</p>
<pre class=""lang-cc prettyprint-override""><code>// This constraint takes [q;v;vdot] and evaluate
// M * vdot + C(q, v) + g(q)
class MyConstraint : public solvers::Constraint {
 public:
  MyConstraint(const MultibodyPlant&lt;AutoDiffXd&gt;&amp; plant, systems::Context&lt;AutoDiffXd&gt;* context, const Eigen::Ref&lt;const Eigen::VectorXd&gt;&amp; lb, const Eigen::Ref&lt;const Eigen::VectorXd&gt;&amp; ub) : solvers::Constraint(plant.num_velocitiex(), plant.num_positions() + 2 * plant.num_velocities(), lb, ub), plant_{plant}, context_{context} {
  ...
  }

 private:
  void DoEval(const Eigen::Ref&lt;const AutoDiffVecXd&gt;&amp; x, AutoDiffVecXd* y) const {
    ...
  }
  
  MultibodyPlant&lt;AutoDiffXd&gt; plant_;
  systems::Context&lt;AutoDiffXd&gt;* context_;
};

int main() {
...
// Construct the constraint and add it to every time instances
std::vector&lt;std::unique_ptr&lt;systems::Context&lt;AutoDiffXd&gt;&gt;&gt; plant_contexts;
for (int i = 0; i &lt; H; ++i) {
 
 plant_contexts.push_back(plant.CreateDefaultContext());
  prog.AddConstraint(std::make_shared&lt;MyConstraint&gt;(plant, plant_context[i], lb, ub), {q_v[i], qddot[i]});
}
}
</code></pre>
<p>You could refer to the class <a href=""https://github.com/RobotLocomotion/drake/blob/e20e8e60763a3989e241af5f11a36f93f80022b1/multibody/optimization/centroidal_momentum_constraint.h#L11-L21"" rel=""nofollow noreferrer"">CentroidalMomentumConstraint</a> on how to construct your own <code>MyConstraint</code> class.</p>
",1973861.0,2.0,3.0,,
4218,71122378,Reward Function for automated parking autonomous Robots,|python|reinforcement-learning|robotics|reward|,"<p>I'm implementing a reinforcement learning task, to solve a parking task for autonomous robots. So basically, the idea of the task is to start at a certain Point in front of the parking spot and drive to a pose without colliding with obstacles. The Agent has reached the goal if the a given Position and a Heading angle of the robot matches the goal pose.</p>
<p>I Have actually a lot of Problems shaping a Reward Function to solve this task. So I ask you guys, to help me with this. What I need a Reward Function depending on the following:</p>
<ul>
<li><strong>(Distance reward)</strong>
The closer the robot is to the target, the higher the reward</li>
<li><strong>(Orientation reward)</strong> The smaller the tolerance of the heading angle to the angle of the target position, the higher the reward</li>
<li><strong>(Speed reward)</strong> The slower the speed when approaching the target position, the higher the reward</li>
</ul>
<p>My current Reward function looks like this:</p>
<pre><code>    current_distance = self.get_euclidean_distance(current_position, desire_position)
    self.distance_reward = (-1)* current_distance/self.max_dist_to_targ

    # Heading Reward. 0: indicates vertical parking, -1: means reverse parking state
    heading_angle = abs(current_heading_angle - target_angle)
    if heading_angle &gt; 0 and heading_angle &lt; np.pi:
        self.heading_angle_reward = -1 * abs(heading_angle - np.pi/2) / np.pi
    else:
        self.heading_angle_reward = -2 * abs(heading_angle - (3*np.pi)/2) / (2*np.pi - 1)
        
    # Goal Reward
    self.goal_reward = 0
    if current_distance &lt;= cp.pose_tolerance and heading_angle &lt;= np.radians(cp.heading_angle_tolerance):
        self.goal_reward = 150

    # Collistion Penalty
    if not self.is_near_by_an_object2(beams_coords, current_trans_vel):
        self.collision_penalty = 0
    else:
        self.collision_penalty = -10

    reward = 10*((1 - w) * self.distance_reward + w * self.heading_angle_reward) +\
            10*self.goal_reward + 10* self.collision_penalty 
</code></pre>
<p>I would be very happy if someone can suggest me what is wrong with the function and how to implement the velocity reward!
Thank you guys.</p>
",2022-02-15 07:13:00,,151,0,3,0,,18210401.0,,2/15/2022 6:52,4.0,,,,,,125776886.0,Hi! Im really new on this topic.  Where do I find the MRE? the Reward is being return (I Edit it allreardy! I you tell me what do you need to better understandig I would provide you (I Guess) Thank you!
4341,72675794,Balance 2-wheeled robot without making it drift forward/backward,|c|matlab|controls|robotics|control-theory|,"<p>I'm trying to design a controller to balance a 2-wheels robot (around 13kg) and making it robust against external forces (e.g. if someone kicks it, it should not fall and not drift indefinitely forward/backward). I'm pretty experienced with most control techniques (LQR, Sliding Mode Control, PID etc), but I've seen online that most people use LQR for balancing 2-wheels robots, hence I'm going with LQR.</p>
<p>My problem is that, despite I'm able to make the robot not fall down, it rapidly starts going forward/backward indefinitely, and I don't how to make it mantain a certain position on the ground. What I want to achieve is that, when the robot gets kicked by an external force, it must be able to stop moving forward/backward while mantaining the balance (it's not necessary to mantain a position on the ground, I just want the robot to stop moving).
The measurements to which I have access from the sensors are: position on both wheels (x), velocity of both wheels (x_dot), angular position of robot (theta), angular velocity of robot (theta_dot).
Since now I tried 2 approaches:</p>
<ol>
<li>set all reference signals to 0 and try to tune the LQR gain. With this (simple) approach I'm not sure if the coefficients of gain K relative to x and theta should have same or opposite sign, because, if for example the robot gets kicked away from its reference for x, the wheels should move in the direction that makes the robot return to the 0 point, but this would make theta go in the opposite direction. When the robot gets kicked, I would like that first theta gets adjusted in order to brake the movement given by the external force, and then x_dot should go in the same direction as theta in order to stop the robot.</li>
<li>use best LQR gains that I could find empirically/with MATLAB and use some &quot;heuristic&quot; in order to, given the current state of the robot (x, x_dot, theta, theta_dot), choose the reference signals for the state variables. I tried the heuristic &quot;if x_dot goes forward/backward, then make theta inclide backward/forward&quot;, which makes the robot avoid drifting forward/backward in case there're no disturbances, but if I kick the robot it starts oscillating really fast until it falls (I tried to adjust the K gain of LQR to solve this problem, but I couldn't find any that solved it).</li>
</ol>
<p>Which approach would you suggest me to use? Should I implement some more sophisticated heuristics (any suggestion?) or should I just tune the LQR gain until I find the perfect one? Should I consider using an integrator (for controlling which states?) together with the LQR?</p>
",2022-06-19 09:43:00,72724753.0,1147,2,3,0,,10225193.0,"Ancona, Province of Ancona, Italy",8/14/2018 14:44,33.0,72723724.0,"<p>The type of sensory system, the computation unit on board etc. would definitely define the approach you are taking. Since you did not give more details regarding the setup, let us assume you have a IMU aligned with the body frame and you have a method of computing roll, pitch and yaw of the robot at a given moment. Also let the speed at which u can compute RPY, is at least twice the speed of the main system loop.</p>
<p>You may want to start by designing three independent PID controllers, each for the three axis with 0 degrees being the target state that you want to maintain. Quite a while back, I was able to make my quadrotor self-balance by constraining two axes and tuning one at a time. In your case, you would first make the PID responsible for one of the axis be able to bring the robot to neutral position for a range of external disturbance that you expect the system to face during operation. A PID won't be able to respond fast enough if you had say tuned for say 5 - 10 N force kicks but later subjected to 100 N kick.</p>
<p>Give this a try and maybe furnish the question with details regarding the robot, the type of wheels u are using and so forth.</p>
<p>Good luck.</p>
",14860459.0,0.0,0.0,128708477.0,"Hi, unfortunately the code is not open source :\ The robot is also custom made"
4195,70602541,I2C issue on Raspi 4 running Raspbian,|robotics|raspberry-pi4|,"<p>I am running a Raspi4 with a HAT board to attach Grove attached components (Adeept's Mars Rover Picar-B kit).  I have had it succesfully up and running with remote access a couple of times.  I believe my SD card was corrupted due to improper shutdown, so I re-imaged and started over again.  Now I am getting this error message:</p>
<p>RuntimeError: Could not determine default I2C bus for platform.</p>
<p>I have never seen this before.  Is this a hardware issue, or can it be fixed on the software end?</p>
",2022-01-06 04:38:00,,218,0,3,0,,17813826.0,,1/2/2022 7:41,10.0,,,,,,125242661.0,"thank you Bill, I will try there"
4225,71324192,"How to get an array into a 28 rows x 28 columns grid which can be used as an (x,y) coordinate system?",|python|c++|coordinates|simulation|robotics|,"<p>I am inexperienced at this level of Python but need to complete it for a team project.</p>
<p>I currently have 28 arrays consisting of 28 data points inside each array which will look similar to this but way longer:</p>
<pre><code>grid =[[&quot;c&quot;,&quot;c&quot;,&quot;c&quot;,&quot;c&quot;,&quot;c&quot;,&quot;c&quot;,&quot;o&quot;,&quot;o&quot;,&quot;-&quot;,&quot;-&quot;,&quot;o&quot;,&quot;-&quot;,&quot;-&quot;,&quot;o&quot;,&quot;-&quot;,&quot;o&quot;,&quot;o&quot;,&quot;-&quot;,&quot;o&quot;,&quot;o&quot;,&quot;o&quot;,&quot;-&quot;,&quot;-&quot;,&quot;-&quot;,&quot;o&quot;,&quot;o&quot;,&quot;o&quot;,&quot;o&quot;]]
</code></pre>
<p>My goal is to get this into a grid-looking format and then use each data point like an (x,y) coordinate system so I can move an object through them using vector equations. Basically simulating the motion I want a robot to follow.</p>
<p>Grid visual example:</p>
<pre><code>[ . . . . . . . . . . .] 
[ . . . . . . . . . . . ]
continues...
</code></pre>
<p>Any guidance will be much appreciated, please!</p>
<p>I welcome results in Python and C++.</p>
<p>Thank you!</p>
",2022-03-02 14:16:00,,130,2,3,-1,,18354441.0,,3/2/2022 13:39,11.0,71324585.0,"<p>Here is an example</p>
<pre><code>#include &lt;array&gt;
#include &lt;iostream&gt;

// create a reuable alias for an array of an array of values
// for this example it will be a 5x5 grid.
using grid_t = std::array&lt;std::array&lt;char, 5&gt;, 5&gt;;

// pass grid by const reference
// So C++ will not copy the grid (pass by value)
// and the const means show_grid can't modify the content.
void show_grid(const grid_t&amp; grid)
{
    // use a range based for loop to loop over the rows in the grid
    for (const auto&amp; row : grid)
    {
        // use another to loop over the characters in a row
        for (const auto c : row) std::cout &lt;&lt; c;
        std::cout &lt;&lt; &quot;\n&quot;;
    }
}

int main()
{
    // setup a grid
    grid_t grid
    { {
        { 'a', '-' ,'o', 'a', 'a' },
        { '-', 'o' ,'o', 'a', 'a' },
        { 'o', 'a' ,'-', 'a', 'o' },
        { 'o', 'a' ,'-', '-', 'o' },
        { 'a', '-' ,'o', '-', 'a' }
    } };

    show_grid(grid);
    return 0;
}
</code></pre>
",16649550.0,1.0,0.0,126072618.0,Search example - `python print grid site:stackoverflow.com`
4335,72596431,IKPy Problem: TypeError: __init__() missing 1 required positional argument: 'origin_translation',|python|robotics|,"<p>I am trying to use IKPy Library(v3.3.3) with Python 3.8 to program some demos for my robot arm on the Mujoco platform. However, When I try to do the Kinematic demo, there is something wrong that happened as below.
<a href=""https://i.stack.imgur.com/RDQST.png"" rel=""nofollow noreferrer"">enter image description here</a>
Here is my project code below:</p>
<p><strong>utils.py</strong></p>
<pre><code>from mujoco_py import MjViewer
import glfw

import numpy as np
import ikpy
from ikpy.chain import Chain
from ikpy.link import OriginLink, URDFLink

open_viewers = []  # a static list to keep track of all viewers


class MjViewerExtended(MjViewer):
    &quot;&quot;&quot; An extension of mujoco-py's MjViewer. MjViewerExtended does not
        terminate all other viewers and the python interpreter when closeing.
    &quot;&quot;&quot;

    def __init__(self, sim):
        glfw.init()  # make sure glfw is initialized
        super().__init__(sim)
        open_viewers.append(self)

    def close(self):
        &quot;&quot;&quot; Closes the viewers glfw window. To open a new one, create a new
            instance of MjViewerExtended
        &quot;&quot;&quot;
        # MjViewer only calls glfw.terminate() here killing glfw entierly.
        if glfw.window_should_close(self.window):
            return
        try:
            glfw.set_window_should_close(self.window, 1)
            glfw.destroy_window(self.window)
        except Exception:
            pass

        open_viewers.remove(self)
        if len(open_viewers) == 0:
            glfw.terminate()

    def key_callback(self, window, key, scancode, action, mods):
        if action == glfw.RELEASE and key == glfw.KEY_ESCAPE:
            self.close()
        else:
            super().key_callback(window, key, scancode, action, mods)


class Wam4IK(Chain):
    &quot;&quot;&quot; A basic kinamatic model of the MjWAM4 &quot;&quot;&quot;

    def __init__(self, tool_orientation=None,
                 tool_translation=None,  # x, y, z
                 base_orientation=None,  # x, y, z
                 base_translation=None):

        if base_translation is None:
            base_translation = [0, 0, 0.84]
        if base_orientation is None:
            base_orientation = [0, 0, np.pi / 2]
        if tool_translation is None:
            tool_translation = [0, 0, 0]
        if tool_orientation is None:
            tool_orientation = [0, 0, 0]

        links = [OriginLink(),
                 URDFLink(name=&quot;wam/links/base&quot;,
                          translation=base_translation,  # translation of frame
                          origin_orientation=base_orientation,  # orientation of frame
                          rotation=[0, 0, 0]
                          ),  # joint axis [0, 0, 0] -&gt; no joint
                 URDFLink(name=&quot;wam/links/shoulder_yaw&quot;,
                          translation=[0, 0, 0.16],
                          origin_orientation=[0, 0, 0],
                          rotation=[0, 0, 1]
                          ),
                 URDFLink(name=&quot;wam/links/shoulder_pitch&quot;,
                          translation=[0, 0, 0.186],
                          origin_orientation=[0, 0, 0],
                          rotation=[1, 0, 0]
                          ),
                 URDFLink(name=&quot;wam/links/shoulder_roll&quot;,
                          translation=[0, 0, 0],
                          origin_orientation=[0, 0, 0],
                          rotation=[0, 0, 1]
                          ),
                 URDFLink(name=&quot;wam/links/upper_arm&quot;,
                          translation=[0, -0.045, 0.550],
                          origin_orientation=[0, 0, 0],
                          rotation=[1, 0, 0]
                          ),
                 URDFLink(name=&quot;wam/links/tool_base_wo_plate&quot;,
                          translation=[0, 0.045, 0.350],
                          origin_orientation=[0, 0, 0],
                          rotation=[0, 0, 0]
                          ),
                 URDFLink(name=&quot;wam/links/tool_base_w_plate&quot;,
                          translation=[0, 0, 0.008],
                          origin_orientation=[0, 0, 0],
                          rotation=[0, 0, 0]
                          ),
                 URDFLink(name=&quot;wam/links/tool&quot;,
                          translation=tool_translation,
                          origin_orientation=tool_orientation,
                          rotation=[0, 0, 0]
                          )
                 ]

        self.all_joints = [False, False, True, True, True, True, False, False, False]
        self.active_joints = list(map(lambda x: x == 1, active_joints))
        self.active_links = [False, False, *self.active_joints, False, False, False]
        Chain.__init__(self, name='wam4',
                       active_links_mask=self.active_links,
                       links=links)

    def fk(self, joints, full_kinematics=False):
        joints = np.array([0, 0, *joints, 0, 0, 0])
        return Chain.forward_kinematics(self, joints, full_kinematics)

    def ik(self, target_position=None, target_orientation=None, orientation_mode=None, **kwargs):
        full = Chain.inverse_kinematics(self, target_position, target_orientation, orientation_mode, **kwargs)
        active = self.joints_from_links(full)
        return active

    def joints_from_links(self, joints):
        return np.compress(self.all_joints, joints, axis=0)
</code></pre>
<p><strong>kinematics.py</strong></p>
<pre><code>import numpy as np
from mujoco_robots.utils import Wam4IK

import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d

# from juggling_wams.envs import SingleArmOneBall
from mujoco_robots.robots import MjWam4


def plot_joints(chain, qs):
    fig = plt.figure()
    ax = fig.add_subplot(1, 1, 1, projection='3d', facecolor=&quot;1.0&quot;)
    for pos in qs:
        chain.plot([0, 0] + pos + [0, 0, 0], ax)
    plt.xlabel('x')
    plt.ylabel('y')
    plt.show()


def main():
    chain = Wam4IK(base_translation=[0, 0, 0.84],
                   base_orientation=[0, 0, np.pi / 2])

    links = ['wam/links/base',
             'wam/links/shoulder_yaw',
             'wam/links/shoulder_pitch',
             'wam/links/upper_arm',
             'wam/links/forearm',
             'wam/links/tool_base_wo_plate',
             'wam/links/tool_base_w_plate']

    x0 = np.array([0, 0, 0.84])
    q_test = [[0, 0, 0, 0], [1, 1, 1, 1]]

    robot = MjWam4(render=True, g_comp=True)
    for q in q_test:
        print(f'\n\ntesting for q={q}')
        robot.reset(pos=q)
        cart = chain.forward_kinematics([0, 0] + q + [0, 0, 0], full_kinematics=True)

        for i in range(7):
            print(f'\n{links[i][10:]}')
            mj_pos = robot.get_body_full_mat(links[i])[:3, 3] - x0
            ikpy_pos = cart[i + 1][:3, 3] - x0
            print(f'mj:   {mj_pos}')
            print(f'ikpy: {ikpy_pos}')
            print(f'diff: {mj_pos - ikpy_pos}')

    plot_joints(chain, q_test)

    # inverse kinematics
    x_des = [0.15, 0.86, 1.45]
    q = chain.active_from_full(chain.inverse_kinematics(x_des))
    robot.set_mocap_pos('endeff_des', x_des)
    robot.step(des_pos=q, n_steps=5000)


if __name__ == '__main__':
    main()
</code></pre>
<p>I think there is no problem with my environment setting and other python files. The problem I think should be happened in these two files. If you would like to see other files, I would upload them soon. Thanks!</p>
",2022-06-12 22:53:00,,149,1,1,1,,14809807.0,America,12/11/2020 19:08,6.0,72597389.0,"<p>You made several calls like this:</p>
<pre><code>                 URDFLink(name=&quot;wam/links/shoulder_yaw&quot;,
                          translation=[0, 0, 0.16],
                          origin_orientation=[0, 0, 0],
                          rotation=[0, 0, 1]
                          ),
</code></pre>
<p>Please be sure to specify the mandatory <code>origin_translation</code> argument.</p>
",8431111.0,0.0,8.0,128239421.0,"Welcome back to Stack Overflow. [Please do not upload images of errors when asking a question.](//meta.stackoverflow.com/q/285551) Instead, copy and paste the error message, formatted like code.  Also, please start by trying to read and understand error messages. For example, do you understand what a `positional argument` is? Did you try to [read the documentation](https://ikpy.readthedocs.io/en/latest/link.html#ikpy.link.URDFLink) for `URDFLink`, in order to understand what needs to be passed?"
4333,72386899,Arc length of curve from data points in Python,|python|numpy|scipy|sympy|robotics|,"<p>I'm working on a robot simulation and trying to calculate the robot's distance from the goal along some planned trajectory. The trajectory curves to avoid obstacles, and it is given by a list of coordinates. To find progress, I need to find the arc length from the current position to the goal. I'm familiar with the equation for arc length of a function: <a href=""https://i.stack.imgur.com/8VUtL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8VUtL.png"" alt=""enter image description here"" /></a></p>
<p>The approach I was planning to use was creating a polynomial approximation of the function of the trajectory from the data points using NumPy's polynomial.polyfit, then finding its derivative, squaring that, adding 1, taking the square root, and finally integrating. However, the square root of a polynomial doesn't always exist, so this method wouldn't always work.</p>
<p>Is there some better way to approach this? I'm familiar with numerical integration, but not sure if/how it can be applied to this problem.</p>
<p>EDIT: Figured out how to do this numerically, which is much faster. Compute numerical derivative using numpy.gradient/numpy.diff, plug each element in that derivative into sqrt(1 + (dy/dx)^2), then use numpy.trapz/scipy.integrate.simpson to compute integral numerically.</p>
",2022-05-26 05:02:00,,2143,1,1,0,,15918774.0,,5/13/2021 17:31,15.0,72388886.0,"<p>How will your robot move from one point to another?</p>
<p>If it is a straight line it suffices to do</p>
<p><code>np.sum(np.sqrt(np.diff(x)**2 + np.diff(y)**2))</code></p>
<p>If not you should first figure out what the path your robot will follow.
Then having those equations you can integrate analytically, or sampling points in the curve. For smooth paths the error on the size tends to be <code>O(1/n^2)</code> where n is the number of points you use in your interpolation.</p>
",12750353.0,1.0,3.0,127878550.0,[This](https://stackoverflow.com/questions/44962794/how-to-integrate-arc-lengths-using-python-numpy-and-scipy) Appears Relevant~
4451,74295544,Communicating using receiver node in webots,|robotics|receiver|webots|emitter|,"<p>I want to implement an emitter robot and a receiver robot in webots. I have written following code.</p>
<pre><code>#include &lt;webots/robot.h&gt;
#include &lt;webots/receiver.h&gt;

#include &lt;stdio.h&gt;
#include &lt;math.h&gt;

#define TIME_STEP 64

int main(int argc, char **argv) {
  /* necessary to initialize webots stuff */
  wb_robot_init();
  WbDeviceTag rx = wb_robot_get_device(&quot;receiver&quot;);
  wb_receiver_enable(rx, 64);
  printf(&quot;Receiver sampling period: %d&quot;,wb_receiver_get_sampling_period(rx));
  
  while (wb_robot_step(TIME_STEP) != -1) {
    
    if (wb_receiver_get_queue_length(rx) &gt; 0) {
      const char *message = wb_receiver_get_data(rx);
      const double *dir = wb_receiver_get_emitter_direction(rx);
      double signal = wb_receiver_get_signal_strength(rx);
      printf(&quot;received: %s (signal=%g, dir=[%g %g %g])\n&quot;,
             message, signal, dir[0], dir[1], dir[2]);
      wb_receiver_next_packet(rx);
    }
  };

  /* Enter your cleanup code here */

  /* This is necessary to cleanup webots resources */
  wb_robot_cleanup();

  return 0;
}

</code></pre>
<p>It compiles successfully. But when I execute it, it generate following result,</p>
<p>Error: wb_receiver_enable(): invalid device tag.
Error: wb_receiver_get_sampling_period(): invalid device tag.</p>
<p>How can I fix this error?</p>
<p>I want to receive the message emitted by emitter</p>
",2022-11-02 20:55:00,74306154.0,162,1,0,1,,14958157.0,"Kandy, Sri Lanka",1/7/2021 10:48,19.0,74306154.0,"<p>This error shows up because there is no device named <code>&quot;receiver&quot;</code> in your <code>Robot</code> node. What kind of robot are using? If it is a <code>Robot</code> node defined in the world file (<code>.wbt</code>), you should add to the <code>children</code> list of your <code>Robot</code> node a <code>Receiver</code> node named <code>&quot;receiver&quot;</code> (which is the default name). If you are using a PROTO-based robot, like <code>E-Puck</code> or <code>Nao</code>, you should check whether the proto file already contains a <code>Receiver</code> node and use its <code>name</code> instead of <code>&quot;receiver&quot;</code>. If it doesn't contain any <code>Receiver</code> node, you should be able to add a <code>Receiver</code> node in some <code>extensionSlot</code> of the robot.</p>
<p>The very same principle applies to the <code>Emitter</code> node as well.</p>
",810268.0,0.0,0.0,,
4438,74230826,"How to get Raw mouse signal (x, y or displacement values) using python?",|python|mouseevent|mouse|robotics|,"<p>If i use libraries like pynput, I'm getting values which are mapped to screen, this value maxes out based on screen resolution.</p>
<p>I do not want that, I'm using my mouse for a robotic project, I want the raw values from my mouse to calculate displacement.</p>
<p>Kindly let me know if you have any answers. thanks.</p>
<p>I tried pynput etc. It does not help.</p>
",2022-10-28 04:57:00,,68,0,2,0,,15304805.0,,3/1/2021 5:50,2.0,,,,,,131104731.0,"Hi thank you for your reply, I tried that, This is the program in my repo.

https://github.com/SujithChristopher/armbo/blob/master/mouse_delta/test.py

It seems to be inactive for a particular direction. 

Because, for example, if my reference is (0, 0), if I move my mouse to (10, 10), then my mouse delta is (10, 10).

but I cannot move to (-10, -10) because that is not allowed, since it is mapped to the screen. 

I think ""Mouse Delta"" is the right term, if you have any ideas, do suggest. Thanks."
4326,72317047,Arduino only executing one set of if-else,|arduino|robotics|motordriver|,"<p>This code if for a simple robot car.
I'm trying to control the robot with 4 geared motors and L289 driver and standard RC Tx/Rx.</p>
<p>I have used some print statements to debug any errors.</p>
<p>When I try to move the robot forward/backward, I can see serial monitor printing froward/backward, but the robot doesn't move.</p>
<p>When I try to move if left/right it works fine. On commenting the left-right moving statements in code the robot does move forward and backward but fails to do so with all the if else statements uncommented.
Here's the code.</p>
<pre><code>//Receiver pin
byte CH1_PIN = 9;
byte CH2_PIN = 10;

//Motor driver pins
int left_motor_pin1 = 4;
int left_motor_pin2 = 5;
int right_motor_pin1 = 6;
int right_motor_pin2 = 7;
void setup() {
  
  // put your setup code here, to run once:
  pinMode(CH1_PIN, INPUT);
  pinMode(CH2_PIN, INPUT);
  pinMode(left_motor_pin1, OUTPUT);
  pinMode(left_motor_pin2, OUTPUT);
  pinMode(right_motor_pin1, OUTPUT);
  pinMode(right_motor_pin2, OUTPUT);
  digitalWrite(left_motor_pin1, LOW);
  digitalWrite(left_motor_pin2, LOW);
  digitalWrite(right_motor_pin1, LOW);
  digitalWrite(right_motor_pin2, LOW);
  Serial.begin(115200);
}

void loop() {
  // put your main code here, to run repeatedly:
  int ch_1 = pulseIn(CH1_PIN, HIGH);
  int ch_2 = pulseIn(CH2_PIN, HIGH);


  drive(ch_1, ch_2);
  delay(5);

}


void drive(int move_left_right, int move_fwd_back) {


  // Set direction for moving forward

  if ( move_fwd_back &gt; 1700 ) {
    digitalWrite(left_motor_pin1, HIGH);
    digitalWrite(left_motor_pin2, LOW);
    digitalWrite(right_motor_pin1, HIGH);
    digitalWrite(right_motor_pin2, LOW);
    Serial.println(&quot;forward&quot;);
  }
  // Set direction for moving backwards.
  else if (move_fwd_back &lt; 1300) {
    digitalWrite(left_motor_pin1, LOW);
    digitalWrite(left_motor_pin2, HIGH);
    digitalWrite(right_motor_pin1, LOW);
    digitalWrite(right_motor_pin2, HIGH);
    Serial.println(&quot;reverse&quot;);
  }
  else {
    digitalWrite(left_motor_pin1, LOW);
    digitalWrite(left_motor_pin2, LOW);
    digitalWrite(right_motor_pin1, LOW);
    digitalWrite(right_motor_pin2, LOW);
    Serial.println(&quot;NONE&quot;);
  }

  // Set direction for moving left
  if ( move_left_right &lt; 1300 ) {
    digitalWrite(left_motor_pin1, HIGH);
    digitalWrite(left_motor_pin2, LOW);
    digitalWrite(right_motor_pin1, LOW);
    digitalWrite(right_motor_pin2, HIGH);
    Serial.println(&quot;left&quot;);
  }
  
  //set directionfor moving right
  else if (move_left_right &gt; 1700) {
    digitalWrite(left_motor_pin1, LOW);
    digitalWrite(left_motor_pin2, HIGH);
    digitalWrite(right_motor_pin1, HIGH);
    digitalWrite(right_motor_pin2, LOW);
    Serial.println(&quot;right&quot;);
  }
  else {
    digitalWrite(left_motor_pin1, LOW);
    digitalWrite(left_motor_pin2, LOW);
    digitalWrite(right_motor_pin1, LOW);
    digitalWrite(right_motor_pin2, LOW);
    Serial.println(&quot;NONE&quot;);
  }


}
</code></pre>
",2022-05-20 10:02:00,72318753.0,68,1,3,0,,14765788.0,India,12/4/2020 20:17,18.0,72318753.0,"<p>The issue is that you have two <code>if-else</code> conditions - both changing the same outputs. So the 2nd <code>if-else</code> condition will always override what the 1st one has done.</p>
<p>eg. if you want the motor to just move forward, the code would set the motors to both move forward - however, immediately afterwards, the code decides there is no left/right input so sets the motors to stop. This is so fast you don't see any movement in the motors.</p>
<p>To start with, I would change the code so that the decision regarding the left/right input is inside the else condition of the forward/backward condition. This would give the forward/backward input priority over the left/right input.</p>
<p>i.e.</p>
<pre><code>if ( move_fwd_back &gt; 1700 ) {
    digitalWrite(left_motor_pin1, HIGH);
    digitalWrite(left_motor_pin2, LOW);
    digitalWrite(right_motor_pin1, HIGH);
    digitalWrite(right_motor_pin2, LOW);
    Serial.println(&quot;forward&quot;);
}
// Set direction for moving backwards.
else if (move_fwd_back &lt; 1300) {
    digitalWrite(left_motor_pin1, LOW);
    digitalWrite(left_motor_pin2, HIGH);
    digitalWrite(right_motor_pin1, LOW);
    digitalWrite(right_motor_pin2, HIGH);
    Serial.println(&quot;reverse&quot;);
}
else {
    // Set direction for moving left
    if ( move_left_right &lt; 1300 ) {
        digitalWrite(left_motor_pin1, HIGH);
        digitalWrite(left_motor_pin2, LOW);
        digitalWrite(right_motor_pin1, LOW);
        digitalWrite(right_motor_pin2, HIGH);
        Serial.println(&quot;left&quot;);
    }
    //set directionfor moving right
    else if (move_left_right &gt; 1700) {
        digitalWrite(left_motor_pin1, LOW);
        digitalWrite(left_motor_pin2, HIGH);
        digitalWrite(right_motor_pin1, HIGH);
        digitalWrite(right_motor_pin2, LOW);
        Serial.println(&quot;right&quot;);
    }
    else {
        digitalWrite(left_motor_pin1, LOW);
        digitalWrite(left_motor_pin2, LOW);
        digitalWrite(right_motor_pin1, LOW);
        digitalWrite(right_motor_pin2, LOW);
        Serial.println(&quot;NONE&quot;);
    }
}
</code></pre>
",18127542.0,0.0,3.0,127760556.0,I don't know. But I would start with a table (on paper or Excel) with inputs as header and first column and fill in the required outputs.
4211,71033078,'module' object is not callable in python roboticstoolbox by Peter Corke,|python|plot|backend|robotics|,"<p>Good morning everybody, I'm currently using the python Roboticstoolbox written by Peter Corke but I have a problem when trying to make the noodlelike plot of my robotic arm. The same error appears when trying to run the provided example:</p>
<p><a href=""https://i.stack.imgur.com/LWRXA.png"" rel=""nofollow noreferrer"">Example code</a></p>
<p>The error is the following:</p>
<pre><code> pyplot = rtb.backends.PyPlot()  # create a PyPlot backend
</code></pre>
<p>TypeError: 'module' object is not callable</p>
<p>How can I make it work?</p>
",2022-02-08 11:24:00,,105,0,4,0,,18150939.0,Italy,2/8/2022 11:10,2.0,,,,,,125570178.0,"Where is that distribution `rtb` available? If running one of their examples produces an errors, I think it is best to open an issue with the project."
4440,74265400,"Project in Python for a robot moving in a 10 by 10 grid, with some extra requirements",|python|python-3.x|robotics|,"<p>I have a project to write code in Python that will control a robot's movements in a 10 by 10 grid. First i would like to point out that i am a beginner so it would be better for me if i can get simple lines of code that i can digest.</p>
<p>So, the project asks for:</p>
<p>A 10 by 10 grid, with the robot starting from the uppermost left position which is X(0,0).</p>
<p>Moving from X(0,0) down one tile will increase the value to (1,0) until (9,0) which is the downmost left corner, while moving from X(0,0) to the right will increase each time by (0,1) until (0,9) upper right corner. Moving from position (9,0) to the right will again be up to (9,9).</p>
<p>It will accept commands to move Up, down, left, right (u ,d ,l, r). Each command should be given together with and integer number that denotes the steps to the given direction (for ex. u5, or d2).</p>
<p>The user will give commands continuously until ENTER is pressed which will make the program exit.</p>
<p>After each command, the program must calculate the position of the robot and print out a message with it.</p>
<p>In the case that the user gives a command that cannot be executed or will make the robot go outside the grid, then an error message must appear. It must also give the error message in instances where X or Y =&gt; N.</p>
<p>Thanks to anyone who will take the time to help me!</p>
<p>I have not tried anything yet as i am in a loss of what to do and how.</p>
",2022-10-31 15:15:00,,242,1,0,-1,,20380430.0,,10/31/2022 15:02,3.0,74356725.0,"<pre><code>while True:
try:
    n = int(input(&quot;   grid:&quot;))
    n1 = n*n
    print(f&quot;     {n1}.     :&quot;, x, &quot;,&quot;, y)
except ValueError:
    print(&quot;    &quot;)
    continue

while True:
    try:
        move = input(&quot; :&quot;)
        if move[0] == 'r':
            if int(move[1:])+y &gt;= n1:
                print(&quot;!       .\n    :&quot;, x, &quot;,&quot;, y)
            else:
                y = y+int(move[1:])
                print(&quot;     :&quot;, x, &quot;,&quot;, y)

        if move[0] == 'l':
            if int(move[1:])-y &gt; 0:
                print(&quot;!       .\n    :&quot;, x, &quot;,&quot;, y)
            else:
                y = y-int(move[1:])
                print(&quot;     :&quot;, x, &quot;,&quot;, y)

        if move[0] == 'u':
            if int(move[1:])-x &gt; 0:
                print(&quot;!       .\n    :&quot;, x, &quot;,&quot;, y)
            else:
                x = x-int(move[1:])
                print(&quot;     :&quot;, x, &quot;,&quot;, y)

        if move[0] == 'd':
            if int(move[1:])+x &gt;= n1:
                print(&quot;!       .\n    :&quot;, x, &quot;,&quot;, y)
            else:
                x = x+int(move[1:])
                print(&quot;     :&quot;, x, &quot;,&quot;, y)
    except ValueError:
        print(&quot;!       .\n    :&quot;, x, &quot;,&quot;, y)
        continue
while move == &quot;&quot;:
    print(&quot; &quot;)
    exit()
</code></pre>
",20380430.0,0.0,1.0,,
4318,72200178,Arduino MQTT synchronicity,|arduino|wifi|mqtt|robotics|,"<p>I try to establish a MQTT-connection between Arduino WifiRev2  and
a python script on an ubuntu20-system.</p>
<p>For test purposes the arduino is connected with 3 ultrasonic sensors and a gyro sensor.</p>
<p>The mqtt-connection seems to work fine, arduino does publish the sensor data and
also receives commands from the python scripts.</p>
<p>But the sensor data are not sent synchronously.
The sensor sent first is received much more frequent (about faktor 4)
than the other sensor data.</p>
<p>The script is adopted from this example code:
<a href=""https://docs.arduino.cc/tutorials/uno-wifi-rev2/uno-wifi-r2-mqtt-device-to-device"" rel=""nofollow noreferrer"">https://docs.arduino.cc/tutorials/uno-wifi-rev2/uno-wifi-r2-mqtt-device-to-device</a></p>
<p>Here is the loop in my arduino script:</p>
<pre><code>const long interval = 30;
unsigned long previousMillis = 0;
void loop() {

  mqttClient.poll();

  unsigned long currentMillis = millis();

  if (currentMillis - previousMillis &gt;= interval) {
    // save the last time a message was sent
    previousMillis = currentMillis;

    //record random value from A0, A1 and A2
    //int Rvalue = analogRead(A0);
    int gyroSCL = digitalRead(gryoSCL_PIN);
    int gyroSDA = digitalRead(gryoSDA_PIN);
    int l_distance = getUSensDistance(USens_L_Trig_Pin,USens_L_Echo_Pin);
    int r_distance = getUSensDistance(USens_R_Trig_Pin,USens_R_Echo_Pin);
    int f_distance = getUSensDistance(USens_F_Trig_Pin,USens_F_Echo_Pin);
    int g_value = getGyro();

    mqttClient.beginMessage(topicL);
    mqttClient.print(l_distance);
    mqttClient.endMessage();

    mqttClient.beginMessage(topicR);
    mqttClient.print(r_distance);
    mqttClient.endMessage();

    mqttClient.beginMessage(topicF);
    mqttClient.print(f_distance);
    mqttClient.endMessage();

    mqttClient.beginMessage(topicG);
    mqttClient.print(g_value);
    mqttClient.endMessage();

    //Serial.println();
  }
}
</code></pre>
<p>The python script is based on this example:
<a href=""https://www.emqx.com/en/blog/how-to-use-mqtt-in-python"" rel=""nofollow noreferrer"">https://www.emqx.com/en/blog/how-to-use-mqtt-in-python</a></p>
<p>A print example from the python subscriber is like this:</p>
<pre><code>Connected to MQTT Broker!
Received `44` from `usens_l` topic (timeDiff: 678024
Received `25` from `usens_f` topic (timeDiff: 882899
Received `44` from `usens_l` topic (timeDiff: 87380
Received `49` from `usens_l` topic (timeDiff: 274183
Received `44` from `usens_l` topic (timeDiff: 501763
Received `44` from `usens_l` topic (timeDiff: 702241
Received `44` from `usens_l` topic (timeDiff: 911118
Received `44` from `usens_l` topic (timeDiff: 113206
Received `44` from `usens_l` topic (timeDiff: 316174
Received `45` from `usens_l` topic (timeDiff: 521477
Received `45` from `usens_l` topic (timeDiff: 725778
Received `45` from `usens_l` topic (timeDiff: 930363
Received `-1` from `gyro` topic (timeDiff: 135167
Received `45` from `usens_l` topic (timeDiff: 354054
Received `54` from `usens_l` topic (timeDiff: 647140
Received `42` from `usens_r` topic (timeDiff: 647306
Received `41` from `usens_r` topic (timeDiff: 852423
Received `55` from `usens_l` topic (timeDiff: 58828
Received `55` from `usens_l` topic (timeDiff: 261107
Received `55` from `usens_l` topic (timeDiff: 465823
Received `55` from `usens_l` topic (timeDiff: 671458
</code></pre>
<p>So obviously some sensor data are not sent (or not received).
I tried with different intervals, but in any case it seems
that usens_l is receveid much more frequent than the other data.</p>
<p>There could be a more simple way to send the data synchronously
but building a string or calculate to a single value to integrate
the data and send at once. Maybe this is the solution for
my current issue. But I would like to understand, why this
communication does not work as expected.</p>
<p>Thank you for some hints!</p>
",2022-05-11 11:23:00,,77,1,0,0,,4074395.0,,9/24/2014 11:15,52.0,72203580.0,"<p>O.k., I found the solution, it was far more easy than expected:</p>
<p>I just had to install a local mosquitto server (as ubuntu service), now the  log looks like this:</p>
<pre><code>Received `49` from `usens_l` topic (timeDiff: 305325
Received `82` from `usens_r` topic (timeDiff: 305449
Received `16` from `usens_f` topic (timeDiff: 307354
Received `-1` from `gyro` topic (timeDiff: 309461
Received `49` from `usens_l` topic (timeDiff: 334474
Received `82` from `usens_r` topic (timeDiff: 337135
Received `16` from `usens_f` topic (timeDiff: 337257
Received `-1` from `gyro` topic (timeDiff: 339283
</code></pre>
",4074395.0,0.0,0.0,,
4414,73821190,What are the key differences between jogging the robot in world mode and joint mode for both ABB and Fanuc robots?,|automation|robotics|,"<p>ROBOTIC MOVEMENT</p>
<p>Jogging of robots</p>
<p>What are the key differences between jogging the robot in world mode and joint mode for both ABB and Fanuc robots?</p>
",2022-09-22 22:16:00,,292,0,2,0,,15225039.0,"Thiruvalla, Kerala, India",2/17/2021 4:21,5.0,,,,,,130568205.0,Maybe the Robotics Community is a better place to ask: https://robotics.stackexchange.com/
4322,72271060,"how to create an arc path from 3 points(x, y, z) in plane?",|matlab|geometry|controls|tracking|robotics|,"<p>I want to create an arc trajectory cross over n=3 points P(n)=(x, y, z), I decided to draw a circle over 3 points in plane. so I have center, radius, theta (angle in x, y plane) and phi(angle around z axis), and I know the position of 3 points (x, y, z), How can I extract an arc between p1 , p2 and p3 from this circle? I implemented this program in MATLAB..
Thanks a lot.</p>
",2022-05-17 08:58:00,72271694.0,599,1,3,1,,17172183.0,,10/17/2021 4:51,11.0,72271694.0,"<p>This answer on math.stackexchange gives a nice simple formulation for finding the circle centre (and therefore the radius)</p>
<p><a href=""https://math.stackexchange.com/a/2755842/283393"">3D coordinates of circle center given three point on the circle. (@Sergio G.)</a></p>
<p>From this other helpful math.stackexchange answer we can define any point on that circle in terms of the centre and two (non-colinear) points from the original 3.</p>
<p><a href=""https://math.stackexchange.com/a/2375120/283393"">Parametric equation of a circle in 3D given center and two points on the circle? (@milbrandt)</a></p>
<p>Finally we need the 3 angles of your 3 points to define the arcs, which can be done with <code>atan2</code> and the component vectors created in the other steps.</p>
<p>The full commented code is below, which yields this plot, and functions to compute the circle angle for any 3D point, then the value on the circumference for any angle.</p>
<p><a href=""https://i.stack.imgur.com/4IZwI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4IZwI.png"" alt=""circle plot 3D"" /></a></p>
<pre><code>% Original points
p1 = [1;0;2];
p2 = [0;0;0];
p3 = [1;2;2];
P = [p1,p2,p3];

% Get circle definition and 3D planar normal to it
p0 = getCentre(p1,p2,p3);
r = norm( p0 - p1 );
[n0,idx] = getNormal(p0,p1,p2,p3);
% Vectors to describe the plane
q1 = P(:,idx(1));
q2 = p0 + cross(n0,(p1-p0).').';
% Function to compute circle point at given angle
fc = @(a) p0 + cos(a).*(q1-p0) + sin(a).*(q2-p0);
% Get angles of the original points for the circle formula
a1 = angleFromPoint(p0,p1,q1,q2);
a2 = angleFromPoint(p0,p2,q1,q2);
a3 = angleFromPoint(p0,p3,q1,q2);
% Plot
figure(1); clf; hold on;
args = {'markersize',20,'displayname'};
plot3( P(1,:), P(2,:), P(3,:), '.', args{:}, 'Original Points' ); 
plot3( p0(1), p0(2), p0(3), '.k', args{:}, 'Centre' );   
plotArc(fc,a1,a2); % plot arc from p1 to p2
plotArc(fc,a2,a3); % plot arc from p2 to p3
plotArc(fc,a3,a1); % plot arc from p3 to p1
grid on; legend show; view(-50,40);

function ang = angleFromPoint(p0,p,q1,q2)
    % Get the circle angle for point 'p'
    comp = @(a,b) dot(a,b)/norm(b);
    ang = atan2( comp(p-p0,q2-p0), comp(p-p0,q1-p0) );
end
function plotArc(fc,a,b)
    % Plot circle arc between angles 'a' and 'b' for circle function 'fc'
    while a &gt; b
        a = a - 2*pi; % ensure we always go from a to b
    end
    aa = linspace( a, b, 100 );
    c = fc(aa);
    plot3( c(1,:), c(2,:), c(3,:), '.r', 'markersize', 5, 'handlevisibility', 'off' );
end
function p0 = getCentre(p1,p2,p3)
    % Get centre of circle defined by 3D points 'p1','p2','p3'
    v1 = p2 - p1;
    v2 = p3 - p1;

    v11 = dot( v1.', v1 );
    v22 = dot( v2.', v2 );
    v12 = dot( v1.', v2 );

    b = 1/(2*(v11*v22-v12^2));
    k1 = b * v22 * (v11-v12);
    k2 = b * v11 * (v22-v12);

    p0 = p1 + k1*v1 + k2*v2;
end
function [n0,idx] = getNormal(p0,p1,p2,p3)
    % compute all 3 normals in case two points are colinear with centre
    n12 = cross((p1 - p0),(p2 - p0));
    n23 = cross((p3 - p0),(p2 - p0));
    n13 = cross((p3 - p0),(p1 - p0));

    n = [n12,n23,n13];
    n = n./sign(n(1,:));
    idx = find(~all(isnan(n)),2);
    n = n(:,idx(1));
    n0 = n / norm(n);
end
</code></pre>
",3978545.0,2.0,3.0,127682453.0,Could you maybe show what you already tried in matlab? Or what you have so far? Also what kind of output are you looking for? A function? A custom class? A vector of points on your arc? A graph?
4576,76081863,Docker Image with cuda and ROS2 on Ubuntu 22.04,|docker|ubuntu|dockerfile|robotics|ros2|,"<p>I am trying desperately to setup a Docker image based on nvidia/cudagl image and add ROS2 humble distro on it.  The reason is theat I need this ENV for future Gazebo simulations with the NVIDIA GPU Capabilities.</p>
<p><strong>The problem is that the official nvidia/cudagl image is based on ubuntu 20.04 and ROS2 Humble requires ubuntu 22.04 so I can't build the image properly</strong></p>
<p>anyone knows **how can I make a docker image that is based on cudagl-ubuntu 20.04  and modify it to be a layer on top of ubuntu 22.04 ** so I will get something like this in my image layers</p>
<p><a href=""https://i.stack.imgur.com/wfkiy.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>i basically followed this article its just that's iit a bit outdated <a href=""https://roboticseabass.com/2021/04/21/docker-and-ros/"" rel=""nofollow noreferrer"">text</a></p>
<p>If you have other suggestions  or can guide me to source code of simillar <strong>dockerfile</strong> it would be great
tnxxxx :))</p>
<p>I tried using this offcial Nvidia docker file and as my base image</p>
<pre><code>FROM nvidia/cudagl:11.4.2-base-ubuntu20.04
</code></pre>
<p>and adding ROS2 humble in source installation on ubuntu 22.04 (my machine)</p>
<p>but it doesnt work  also tried building cudagl image from bae of ubutu 22.04 image just to serve as base for further adding a layer of ros2 - but this also cann't be built</p>
<pre><code># Use Ubuntu 22.04 as the base image
FROM ubuntu:22.04 as base

FROM base as base-amd64

ENV NVARCH x86_64

ENV NVIDIA_REQUIRE_CUDA &quot;cuda&gt;=11.4 brand=tesla,driver&gt;=418,driver&lt;419 brand=tesla,driver&gt;=450,driver&lt;451&quot;
ENV NV_CUDA_CUDART_VERSION 11.4.108-1
ENV NV_CUDA_COMPAT_PACKAGE cuda-compat-11-4

FROM base as base-arm64

ENV NVARCH sbsa
ENV NVIDIA_REQUIRE_CUDA &quot;cuda&gt;=11.4&quot;
ENV NV_CUDA_CUDART_VERSION 11.4.108-1

FROM base-${TARGETARCH}

ARG TARGETARCH

LABEL maintainer &quot;NVIDIA CORPORATION &lt;cudatools@nvidia.com&gt;&quot;

RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \
    gnupg2 curl ca-certificates &amp;&amp; \
    curl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/${NVARCH}/3bf863cc.pub | apt-key add - &amp;&amp; \
    echo &quot;deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/${NVARCH} /&quot; &gt; /etc/apt/sources.list.d/cuda.list &amp;&amp; \
    apt-get purge --autoremove -y curl \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

ENV CUDA_VERSION 11.4.2

# For libraries in the cuda-compat-* package: https://docs.nvidia.com/cuda/eula/index.html#attachment-a
RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \
    cuda-cudart-11-4=${NV_CUDA_CUDART_VERSION} \
    ${NV_CUDA_COMPAT_PACKAGE} \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

# Required for nvidia-docker v1
RUN echo &quot;/usr/local/nvidia/lib&quot; &gt;&gt; /etc/ld.so.conf.d/nvidia.conf \
    &amp;&amp; echo &quot;/usr/local/nvidia/lib64&quot; &gt;&gt; /etc/ld.so.conf.d/nvidia.conf

ENV PATH /usr/local/nvidia/bin:/usr/local/cuda/bin:${PATH}
ENV LD_LIBRARY_PATH /usr/local/nvidia/lib:/usr/local/nvidia/lib64

# Install OpenGL packages
RUN dpkg --add-architecture i386 \
    &amp;&amp; apt-get update &amp;&amp; apt-get install -y --no-install-recommends \
        pkg-config \
        libglvnd-dev libglvnd-dev:i386 \
        libgl1-mesa-dev libgl1-mesa-dev:i386 \
        libegl1-mesa-dev libegl1-mesa-dev:i386 \
        libgles2-mesa-dev libgles2-mesa-dev:i386 \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

COPY NGC-DL-CONTAINER-LICENSE /

# nvidia-container-runtime
ENV NVIDIA_VISIBLE_DEVICES all
ENV NVIDIA_DRIVER_CAPABILITIES compute,utility

</code></pre>
",2023-04-22 20:47:00,,2275,1,1,2,,21709575.0,,4/22/2023 20:27,6.0,76339964.0,"<p>I have run into the same issue. The following solution is working under the WSL2 system with Ubuntu22.04 running Docker Desktop 4.19.0 (106363). I have not tested on real Ubuntu but I should have fewer problems than I encountered in finding this solution.</p>
<p>From <a href=""https://github.com/microsoft/WSL/issues/7507#issuecomment-950235017"" rel=""nofollow noreferrer"">this issue answer on Github</a> that redirects to <a href=""https://github.com/microsoft/wslg/blob/main/samples/container/Containers.md"" rel=""nofollow noreferrer"">this official MS guide here</a>, the following base Dockefile that I slightly modified to add the latest Cuda capabilities is capable of rendering OpenGL apps:</p>
<pre><code>FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04 as runtime

ARG DEBIAN_FRONTEND=noninteractive

# Uncomment the lines below to use a 3rd party repository
# to get the latest (unstable from mesa/main) mesa library version
RUN apt-get update &amp;&amp; apt install -y software-properties-common
RUN add-apt-repository ppa:oibaf/graphics-drivers -y

RUN apt update &amp;&amp; apt install -y \
    vainfo \
    mesa-va-drivers \
    mesa-utils

ENV LIBVA_DRIVER_NAME=d3d12
ENV LD_LIBRARY_PATH=/usr/lib/wsl/lib
CMD vainfo --display drm --device /dev/dri/card0
</code></pre>
<p>Then, you can install ROS2 from apt.
Regarding additional env vars:
<code>NVIDIA_VISIBLE_DEVICES</code> should be already set to all and <code>NVIDIA_DRIVER_CAPABILITIES</code> to <code>compute,utility</code>.</p>
<p>You may want to set:</p>
<pre><code>ENV NVIDIA_DRIVER_CAPABILITIES \
${NVIDIA_DRIVER_CAPABILITIES:+$NVIDIA_DRIVER_CAPABILITIES,}graphics,video
</code></pre>
<p>or</p>
<pre><code>ENV NVIDIA_DRIVER_CAPABILITIES all
</code></pre>
<p>Then with the following, you control the device for the hw acceleration:
Nvidia card (similarly for AMD):</p>
<pre><code>ENV MESA_D3D12_DEFAULT_ADAPTER_NAME=NVIDIA
</code></pre>
<p>or integrated Intel (if supported):</p>
<pre><code>ENV MESA_D3D12_DEFAULT_ADAPTER_NAME=Intel
</code></pre>
<p>or software, which actually for me gives faster FPS probably due to WSL2 (my system) not being optimized:</p>
<pre><code>ENV LIBGL_ALWAYS_SOFTWARE=1
</code></pre>
",5128680.0,1.0,0.0,134254448.0,"What system are you doing this on?  If Jetson then https://github.com/dusty-nv/jetson-containers/blob/master/Dockerfile.ros.humble has the code to build ros2 humble on an Ubuntu 20.04 desktop or base image.  You could merge that with cudagl or just use their image. It gets complicated though when you add other requirements (gazebo, navigation2 etc) for which there are no binaries."
4603,76351277,Trying to make a turtlebot circle another moving turtlebot in ROS,|python|geometry|ros|robotics|,"<p>I am trying to make one turtlebot named turtle2 to circle around a stationary turtlebot and then have another turtlebot turtle3 circle around turtle 2. Turtle2 circles around turtle1 with a radius of 2 and an angular velocity of 1 and turtle3 should circle around turtle2 with a radius of 0.5 and an angular velocity of 2. However I cannot figure out how to make turtle3 circle around turtle2 while also closing the distance caused by turtle2's movement. Here's my ROS code and a screenshot of what I get with my results to help you understand my goal. Any help would be appreciated.</p>
<pre><code>#!/usr/bin/env python  
import roslib
roslib.load_manifest('learning_tf')
import rospy
import math
import tf
import geometry_msgs.msg
import turtlesim.srv

if __name__ == '__main__':
    rospy.init_node('turtle_tf_listener')

    listener = tf.TransformListener()

    rospy.wait_for_service('spawn')
    spawner = rospy.ServiceProxy('spawn', turtlesim.srv.Spawn)

    # wait for the world to spawn
    rospy.sleep(0.3)

    (trans, rot) = listener.lookupTransform('/world', '/turtle1', rospy.Time(0))
    r2 = 2
    w2 = 1
    # spawn second turtle along the orbit of the first still turtle
    spawner(trans[0], trans[1] - r2, 0, 'turtle2')
    turtle_vel2 = rospy.Publisher('turtle2/cmd_vel', geometry_msgs.msg.Twist,queue_size=1)

    # wait for second turtle to spawn
    rospy.sleep(0.3)
    (trans, rot) = listener.lookupTransform('/world', '/turtle2', rospy.Time(0))
    r3 = 0.5
    w3 = 2
    # spawn third turtle along the orbit of the second turtle
    spawner(trans[0], trans[1] - r3, 0, 'turtle3')
    turtle_vel3 = rospy.Publisher('turtle3/cmd_vel', geometry_msgs.msg.Twist,queue_size=1)

    rate = rospy.Rate(10.0)
    while not rospy.is_shutdown():
        flag2 = True
        flag3 = True
        try:
            (trans, rot) = listener.lookupTransform('/world', '/turtle1', rospy.Time(0))
        except (tf.LookupException, tf.ConnectivityException, tf.ExtrapolationException):
            flag2 = False

        if(flag2):
            angular2 = w2
            linear2 = w2 * r2
            cmd2 = geometry_msgs.msg.Twist()
            cmd2.linear.x = linear2
            cmd2.angular.z = angular2
            turtle_vel2.publish(cmd2)
        
        try:
            (trans, rot) = listener.lookupTransform('/turtle3', '/turtle2', rospy.Time(0))
        except (tf.LookupException, tf.ConnectivityException, tf.ExtrapolationException):
            flag3 = False

        if(flag3):
            angular3 = w3
            linear3 = w3 * r3
            cmd3 = geometry_msgs.msg.Twist()
            cmd3.linear.x = linear3
            cmd3.angular.z = angular3
            turtle_vel3.publish(cmd3)

        rate.sleep()
</code></pre>
<p><a href=""https://i.stack.imgur.com/tccBV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tccBV.png"" alt=""enter image description here"" /></a></p>
",2023-05-28 11:34:00,77151076.0,106,1,0,0,,12607149.0,Greece,12/27/2019 11:11,16.0,77151076.0,"<p>That's how I would approach it:</p>
<ol>
<li>Get turtle2 position let's call it Pos2 which is vector (x2, y2)</li>
<li>Make a function which generates point which is circling around (0, 0) so it's something like SpinnerPos = (cx, cy) = DistanceToCenter * (cos(angle), sin(angle)) where angle is incremented every frame so the point (aka vector) would spin.
<a href=""https://i.stack.imgur.com/GQz3a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GQz3a.png"" alt=""enter image description here"" /></a></li>
<li>Combine both: Target = Pos2 + SpinnerPos. Now if drawn Target point would perfectly spin around turtle2</li>
<li>Now make turtle3 follow Target point by calculating distance to it (Pythagorean theorem) and adjusting it's orientation (trigonometry atan2()) and speed proportional to distance</li>
<li>After tuning parameters turtle3 should circle turtle2</li>
</ol>
",5031366.0,0.0,0.0,,
4618,76477450,Mobile Robot doesn't reach nav goal within goal tolerances in nav2,|robotics|ros2|,"<p>I have found no resources on this online (or I'm just that bad at searching) so I thought I could get this answered here.</p>
<p>I have a differential drive robot that I am simulating in nvidia's Isaac sim. I'm using the navigation2 stack in order for the mobile robot to navigate its surroundings and pick up objects within the environment. Everything works fine except for one minor detail.</p>
<p>I noticed in testing that the mobile robot does not completely reach the navigation goals that are set for it (i.e. the controller claims to have reached its goal when the base_frame of the robot fails to reach the set goal point).</p>
<p>Obviously this is generally fine and understandable since the controller must prevent the robot from oscillating around the goal point, but after checking positions several times I realized that the robot doesn't even reach the goal within the specified tolerance of the goal checker (currently set at 1mm).</p>
<p>Here is the navigation parameters file:</p>
<pre><code>amcl:
  ros__parameters:
    use_sim_time: True
    alpha1: 0.2
    alpha2: 0.2
    alpha3: 0.2
    alpha4: 0.2
    alpha5: 0.2
    base_frame_id: &quot;base_link&quot;
    beam_skip_distance: 0.5
    beam_skip_error_threshold: 0.9
    beam_skip_threshold: 0.3
    do_beamskip: false
    global_frame_id: &quot;map&quot;
    lambda_short: 0.1
    laser_likelihood_max_dist: 2.0
    laser_max_range: 100.0
    laser_min_range: 0.4
    laser_model_type: &quot;likelihood_field&quot;
    max_beams: 60
    max_particles: 2000
    min_particles: 500
    odom_frame_id: &quot;odom&quot;
    pf_err: 0.05
    pf_z: 0.99
    recovery_alpha_fast: 0.0
    recovery_alpha_slow: 0.0
    resample_interval: 1
    robot_model_type: &quot;nav2_amcl::DifferentialMotionModel&quot;
    save_pose_rate: 0.5
    sigma_hit: 0.2
    tf_broadcast: true
    transform_tolerance: 1.0
    update_min_a: 0.2
    update_min_d: 0.25
    z_hit: 0.5
    z_max: 0.05
    z_rand: 0.5
    z_short: 0.05
    scan_topic: scan
    map_topic: map
    set_initial_pose: true
    always_reset_initial_pose: false
    first_map_only: false
    initial_pose:
      x: 0.0
      y: 0.0
      z: 0.0
      yaw: 0.0

amcl_map_client:
  ros__parameters:
    use_sim_time: True

amcl_rclcpp_node:
  ros__parameters:
    use_sim_time: True
bt_navigator:
  ros__parameters:
    use_sim_time: True
    global_frame: map
    robot_base_frame: base_link
    odom_topic: /odom
    bt_loop_duration: 20
    default_server_timeout: 40
    # 'default_nav_through_poses_bt_xml' and 'default_nav_to_pose_bt_xml' are use defaults:
    # nav2_bt_navigator/navigate_to_pose_w_replanning_and_recovery.xml
    # nav2_bt_navigator/navigate_through_poses_w_replanning_and_recovery.xml
    # They can be set here or via a RewrittenYaml remap from a parent launch file to Nav2.
    plugin_lib_names:
    - nav2_compute_path_to_pose_action_bt_node
    - nav2_compute_path_through_poses_action_bt_node
    - nav2_smooth_path_action_bt_node
    - nav2_follow_path_action_bt_node
    - nav2_spin_action_bt_node
    - nav2_wait_action_bt_node
    - nav2_back_up_action_bt_node
    - nav2_drive_on_heading_bt_node
    - nav2_clear_costmap_service_bt_node
    - nav2_is_stuck_condition_bt_node
    - nav2_goal_reached_condition_bt_node
    - nav2_goal_updated_condition_bt_node
    - nav2_globally_updated_goal_condition_bt_node
    - nav2_is_path_valid_condition_bt_node
    - nav2_initial_pose_received_condition_bt_node
    - nav2_reinitialize_global_localization_service_bt_node
    - nav2_rate_controller_bt_node
    - nav2_distance_controller_bt_node
    - nav2_speed_controller_bt_node
    - nav2_truncate_path_action_bt_node
    - nav2_truncate_path_local_action_bt_node
    - nav2_goal_updater_node_bt_node
    - nav2_recovery_node_bt_node
    - nav2_pipeline_sequence_bt_node
    - nav2_round_robin_node_bt_node
    - nav2_transform_available_condition_bt_node
    - nav2_time_expired_condition_bt_node
    - nav2_path_expiring_timer_condition
    - nav2_distance_traveled_condition_bt_node
    - nav2_single_trigger_bt_node
    - nav2_is_battery_low_condition_bt_node
    - nav2_navigate_through_poses_action_bt_node
    - nav2_navigate_to_pose_action_bt_node
    - nav2_remove_passed_goals_action_bt_node
    - nav2_planner_selector_bt_node
    - nav2_controller_selector_bt_node
    - nav2_goal_checker_selector_bt_node
    - nav2_controller_cancel_bt_node
    - nav2_path_longer_on_approach_bt_node
    - nav2_wait_cancel_bt_node
    - nav2_spin_cancel_bt_node
    - nav2_back_up_cancel_bt_node
    - nav2_drive_on_heading_cancel_bt_node

bt_navigator_rclcpp_node:
  ros__parameters:
    use_sim_time: True

controller_server:
  ros__parameters:
    use_sim_time: True
    controller_frequency: 20.0
    min_x_velocity_threshold: 0.001
    min_y_velocity_threshold: 0.5
    min_theta_velocity_threshold: 0.001
    failure_tolerance: 0.3
    progress_checker_plugin: &quot;progress_checker&quot;
    goal_checker_plugin: [&quot;general_goal_checker&quot;]
    controller_plugins: [&quot;FollowPath&quot;]

    # Progress checker parameters
    progress_checker:
      plugin: &quot;nav2_controller::SimpleProgressChecker&quot;
      required_movement_radius: 0.5
      movement_time_allowance: 10.0
    # Goal checker parameters
    general_goal_checker:
      plugin: &quot;nav2_controller::SimpleGoalChecker&quot;
      xy_goal_tolerance: 0.001
      yaw_goal_tolerance: 0.001
      stateful: true
    FollowPath:
      # Rotation Shim Controller parameters
      plugin: &quot;nav2_rotation_shim_controller::RotationShimController&quot;
      primary_controller: &quot;nav2_mppi_controller::MPPIController&quot;
      angular_dist_threshold: 0.785
      forward_sampling_distance: 2.0
      rotate_to_heading_angular_vel: 1.8
      max_angular_accel: 3.2
      simulate_ahead_time: 1.0
      # DWB parameters
      time_steps: 56
      model_dt: 0.05
      batch_size: 2000
      vx_std: 0.2
      vy_std: 0.2
      wz_std: 0.4
      vx_max: 0.5
      vx_min: -0.35
      vy_max: 0.5
      wz_max: 1.9
      iteration_count: 1
      prune_distance: 1.7
      transform_tolerance: 0.1
      temperature: 0.3
      gamma: 0.015
      motion_model: &quot;DiffDrive&quot;
      visualize: false
      reset_period: 1.0 # (only in Humble)
      TrajectoryVisualizer:
        trajectory_step: 5
        time_step: 3
      AckermannConstrains:
        min_turning_r: 0.2
      critics: [&quot;ConstraintCritic&quot;, &quot;ObstaclesCritic&quot;, &quot;GoalCritic&quot;, &quot;GoalAngleCritic&quot;, &quot;PathAlignCritic&quot;, &quot;PathFollowCritic&quot;, &quot;PathAngleCritic&quot;, &quot;PreferForwardCritic&quot;]
      ConstraintCritic:
        enabled: true
        cost_power: 1
        cost_weight: 4.0
      GoalCritic:
        enabled: true
        cost_power: 1
        cost_weight: 5.0
        threshold_to_consider: 1.0
      GoalAngleCritic:
        enabled: true
        cost_power: 1
        cost_weight: 3.0
        threshold_to_consider: 0.4
      PreferForwardCritic:
        enabled: true
        cost_power: 1
        cost_weight: 5.0
        threshold_to_consider: 0.4
      ObstaclesCritic:
        enabled: true
        cost_power: 1
        repulsion_weight: 1.5
        critical_weight: 20.0
        consider_footprint: false
        collision_cost: 10000.0
        collision_margin_distance: 0.1
        near_goal_distance: 0.5
        inflation_radius: 0.55 # (only in Humble)
        cost_scaling_factor: 10.0 # (only in Humble)
      PathAlignCritic:
        enabled: true
        cost_power: 1
        cost_weight: 14.0
        max_path_occupancy_ratio: 0.05
        trajectory_point_step: 3
        threshold_to_consider: 0.40
        offset_from_furthest: 20
      PathFollowCritic:
        enabled: true
        cost_power: 1
        cost_weight: 5.0
        offset_from_furthest: 5
        threshold_to_consider: 0.6
      PathAngleCritic:
        enabled: true
        cost_power: 1
        cost_weight: 2.0
        offset_from_furthest: 4
        threshold_to_consider: 0.40
        max_angle_to_furthest: 1.0

controller_server_rclcpp_node:
  ros__parameters:
    use_sim_time: True

local_costmap:
  local_costmap:
    ros__parameters:
      update_frequency: 10.0
      publish_frequency: 10.0
      global_frame: odom
      robot_base_frame: base_link
      use_sim_time: True
      rolling_window: True
      width: 10
      height: 10
      resolution: 0.05
      robot_radius: 0.5
      plugins: [&quot;obstacle_layer&quot;, &quot;inflation_layer&quot;, &quot;js_layer&quot;]
      inflation_layer:
        plugin: &quot;nav2_costmap_2d::InflationLayer&quot;
        cost_scaling_factor: 0.4
        inflation_radius: 0.25
      obstacle_layer:
        plugin: &quot;nav2_costmap_2d::ObstacleLayer&quot;
        enabled: True
        observation_sources: scan
        scan:
          topic: /scan
          max_obstacle_height: 2.0
          clearing: True
          marking: True
          data_type: &quot;LaserScan&quot;
      js_layer:
        plugin: &quot;jackstand_objective_costmap_plugin::JSLayer&quot;
        enabled: true
        costmap_resolution: 0.05
        target_clear_radius: 0.5
      always_send_full_costmap: True
  local_costmap_client:
    ros__parameters:
      use_sim_time: True
  local_costmap_rclcpp_node:
    ros__parameters:
      use_sim_time: True

global_costmap:
  global_costmap:
    ros__parameters:
      update_frequency: 10.0
      publish_frequency: 10.0
      global_frame: map
      robot_base_frame: base_link
      use_sim_time: True
      rolling_window: True
      width: 200
      height: 200
      robot_radius: 0.5
      resolution: 0.05
      # origin_x: -100.0
      # origin_y: -100.0
      plugins: [&quot;static_layer&quot;, &quot;obstacle_layer&quot;, &quot;inflation_layer&quot;, &quot;js_layer&quot;]
      obstacle_layer:
        plugin: &quot;nav2_costmap_2d::ObstacleLayer&quot;
        enabled: True
        observation_sources: scan
        scan:
          topic: /scan
          max_obstacle_height: 2.0
          clearing: True
          marking: True
          data_type: &quot;LaserScan&quot;
          raytrace_max_range: 10.0
          raytrace_min_range: 0.0
          obstacle_max_range: 10.0
          obstacle_min_range: 0.0
      static_layer:
        plugin: &quot;nav2_costmap_2d::StaticLayer&quot;
        map_subscribe_transient_local: True
      inflation_layer:
        plugin: &quot;nav2_costmap_2d::InflationLayer&quot;
        cost_scaling_factor: 3.0
        inflation_radius: 0.55
      js_layer:
        plugin: &quot;jackstand_objective_costmap_plugin::JSLayer&quot;
        enabled: true
        costmap_resolution: 0.05
        target_clear_radius: 0.5
      always_send_full_costmap: True
  global_costmap_client:
    ros__parameters:
      use_sim_time: True
  global_costmap_rclcpp_node:
    ros__parameters:
      use_sim_time: True

map_server:
  ros__parameters:
    use_sim_time: True
    yaml_filename: &quot;test_map_occupancy_params.yaml&quot;

map_saver:
  ros__parameters:
    use_sim_time: True
    save_map_timeout: 5000
    free_thresh_default: 0.25
    occupied_thresh_default: 0.65
    map_subscribe_transient_local: True

planner_server:
  ros__parameters:
    expected_planner_frequency: 20.0
    use_sim_time: True
    planner_plugins: [&quot;GridBased&quot;]
    GridBased:
      plugin: &quot;nav2_theta_star_planner/ThetaStarPlanner&quot;
      how_many_corners: 8
      w_euc_cost: 1.0
      w_traversal_cost: 2.0
      w_heuristic_cost: 1.0

planner_server_rclcpp_node:
  ros__parameters:
    use_sim_time: True

smoother_server:
  ros__parameters:
    use_sim_time: True
    smoother_plugins: [&quot;simple_smoother&quot;]
    simple_smoother:
      plugin: &quot;nav2_smoother::SimpleSmoother&quot;
      tolerance: 1.0e-10
      max_its: 1000
      do_refinement: True

behavior_server:
  ros__parameters:
    costmap_topic: local_costmap/costmap_raw
    footprint_topic: local_costmap/published_footprint
    cycle_frequency: 5.0
    behavior_plugins: [&quot;spin&quot;, &quot;backup&quot;, &quot;drive_on_heading&quot;, &quot;wait&quot;]
    spin:
      plugin: &quot;nav2_behaviors/Spin&quot;
    backup:
      plugin: &quot;nav2_behaviors/BackUp&quot;
    drive_on_heading:
      plugin: &quot;nav2_behaviors/DriveOnHeading&quot;
    wait:
      plugin: &quot;nav2_behaviors/Wait&quot;
    global_frame: odom
    robot_base_frame: base_link
    transform_tolerance: 0.2
    use_sim_time: true
    simulate_ahead_time: 2.0
    max_rotational_vel: 1.0
    min_rotational_vel: 0.4
    rotational_acc_lim: 3.2

robot_state_publisher:
  ros__parameters:
    use_sim_time: True

waypoint_follower:
  ros__parameters:
    loop_rate: 20
    stop_on_failure: false
    waypoint_task_executor_plugin: &quot;wait_at_waypoint&quot;
    wait_at_waypoint:
      plugin: &quot;nav2_waypoint_follower::WaitAtWaypoint&quot;
      enabled: True
      waypoint_pause_duration: 200
</code></pre>
<p>I have messed with the settings for hours now and have seen no change in the behavior of the mobile robot. Setting the goal_tolerance lower than the default doesn't change anything with respect to how far the robot stops away from its goal.</p>
<p>I figured that setting the tolerances in the goal_checker would make it so that the robot would need to move closer to the navigation goal before being satisfied but that isn't the case.</p>
<p>Does anyone know what I'm missing? Or know how I can get the robot to move it's base_link closer to the navigation goal?</p>
<hr />
<p>Update:</p>
<p>So it looks like nav2 isn't even pulling any of the goal_checker_plugin parameters. I set the xy_goal_tolerance much higher than the default (1 meter) and it had no effect either. Any idea why it's not using these?</p>
",2023-06-14 20:51:00,,399,0,0,0,,21591458.0,,4/7/2023 19:17,8.0,,,,,,,
4700,77419699,B&R Automation Studio - Breakdown of cyclic network communication error,|automation|embedded|robotics|ansi-c|br-automation-studio|,"<p>I get this error on a B&amp;R controller computer that I use to control a 6-axis stewart platform, but we have not been able to figure out the exact cause with B&amp;R support. This problem comes on occasionally but goes away when I reboot the system and then comes back at random times. Can anyone help me if they have experienced this before?</p>
<p><img src=""https://i.stack.imgur.com/xKQ8f.jpg"" alt=""enter image description here"" /></p>
<p>Here it is controller link <a href=""https://www.br-automation.com/tr/ueruenler/enduestriyel-pcler/automation-pc-3100/system-units/5apc3100kbu0-000/"" rel=""nofollow noreferrer"">Controller</a></p>
<p>I thought it was related to the tasks I was using exceeding their time, so I monitored how much the controller was using the processor and it maxes out at 60% utilization. I also changed the connection cables thinking that there might be a bug with the powerlink communication but this did not solve the problem. This error does not happen after a certain process, so we cannot get the error by following certain steps in the system. Or we haven't discovered it yet, but for now it seems to us that it happens at random moments.</p>
",2023-11-03 21:04:00,,116,0,2,1,,18771823.0,,4/11/2022 12:27,12.0,,,,,,136515991.0,"The ethernet cable connected to the panel I use is connected to an ethernet switch port and different ethernet cables also enter this port, including the communication of the system connected to our stewart platform. I will check this, so I will try to arrange a cable that is more resistant to electromagnetic noise and protected."
4587,76247799,Path planning using an occupancy grid with specific constrains,|algorithm|graph-theory|robotics|,"<p>I have a binary occupancy grid representing an environment where zeros denote empty spaces and ones denote walls. I need to plan a path for a differential wheeled robot with a cylindrical shape. However, I have specific constraints that I am struggling to meet.</p>
<p>My objective is to generate a path that allows the robot to follow walls while maintaining a certain safe distance from the wall to avoid damaging it, additionally, I would like the robot to maintain a given maximum distance from the walls whenever possible. As an example in the distance trasform grid below I want a path that spans between ~15 and ~20, so when possible the path should go below the given maximum distance in order to cover certain areas of the map.</p>
<p>Could someone suggest an approach or algorithm that can help me achieve my desired path while considering these constraints? Any guidance or code examples would be greatly appreciated. Thank you!</p>
<p>Occupancy Grid:
<img src=""https://i.stack.imgur.com/bxKHP.png"" alt=""Occupancy Grid"" /></p>
<p>I have already attempted to generate a path by applying a distance transform to the grid and using it as a basis for planning. However, the resulting path is not perfect and exhibits some issues.</p>
<p>Distance Transform:
<img src=""https://i.stack.imgur.com/0kt3y.png"" alt=""Distance Transform"" /></p>
<p>Obtained Path:
<img src=""https://i.stack.imgur.com/RoKPr.png"" alt=""Obtained Path"" /></p>
<p>Desired path:
<img src=""https://i.stack.imgur.com/YlWYz.png"" alt=""Desired path"" /></p>
",2023-05-14 13:45:00,,222,1,0,2,,21896451.0,,5/14/2023 12:07,3.0,76248835.0,"<ul>
<li>Set R to the spacing you want between the robot and the wall.</li>
<li>Construct a rectangular grid of cells with spacing between the cells of R</li>
<li>Mark all cells the do not contain a wall and are less the 2 * R from a wall.</li>
<li>Create a graph with a vertex located at each marked cell and an edge between every adjacent vertex.</li>
<li>Run route planning code to generate a path that visits every vertex in graph.</li>
</ul>
<p>The last step is the tricky one!  For that, may I suggest my application called &quot;Obstacle&quot; which produces this sort of result:</p>
<p><a href=""https://i.stack.imgur.com/U5ZnW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/U5ZnW.png"" alt=""enter image description here"" /></a></p>
<p>Obstacle documentation and code:  <a href=""https://github.com/JamesBremner/PathFinder/wiki/Obstacles"" rel=""nofollow noreferrer"">https://github.com/JamesBremner/PathFinder/wiki/Obstacles</a></p>
",16582.0,0.0,0.0,,
4590,76258775,OpenAI-Gym Mojoco Walker2d-v4 model global cordinates error,|python|reinforcement-learning|robotics|openai-gym|mujoco|,"<p>I get the error</p>
<pre><code>ValueError: XML Error: global coordinates no longer supported. To convert existing models, load and save them in MuJoCo 2.3.3 or older
</code></pre>
<p>When i try to load the <code>walker2d-v4</code> model even with <code>**kwargs</code>.</p>
<pre><code>env = gym.make(
    'Walker2d-v4',
    # xml_file=&quot;walker2d.xml&quot;,
    forward_reward_weight=1.0,
    ctrl_cost_weight=1e-3,
    healthy_reward=1.0,
    terminate_when_unhealthy=True,
    healthy_z_range=(0.8, 2),
    healthy_angle_range=(-1, 1),
    reset_noise_scale=5e-3,
    exclude_current_positions_from_observation=True
)
</code></pre>
<p>Anyone face similar issues?</p>
<hr />
<p>Here's the solution to this, thanks to @yuval for pointing it out.</p>
<p>you need to load the <code>walker2d.xml</code> with this error into a model. For this you need to install mujoco 2.3.3 and then run the following code,</p>
<pre><code>model = mujoco.MjModel.from_xml_path('walker2d.xml')
mujoco.viewer.launch(model)
</code></pre>
<p>This should open up a mujoco viewer window that has an option to &quot;save XML&quot; - this should save the updated xml file as <code>mjmodel.xml</code> in the current dir. just copy paste that into <code>walker2d.xml</code> and your gym code should now be working.</p>
<h3>Walker2D XML that worked for me</h3>
<pre class=""lang-xml prettyprint-override""><code>&lt;mujoco model=&quot;walker2d&quot;&gt;
  &lt;compiler angle=&quot;radian&quot; autolimits=&quot;true&quot;/&gt;
  &lt;option integrator=&quot;RK4&quot;/&gt;
  &lt;default class=&quot;main&quot;&gt;
    &lt;joint limited=&quot;true&quot; armature=&quot;0.01&quot; damping=&quot;0.1&quot;/&gt;
    &lt;geom conaffinity=&quot;0&quot; friction=&quot;0.7 0.1 0.1&quot; rgba=&quot;0.8 0.6 0.4 1&quot;/&gt;
  &lt;/default&gt;
  &lt;asset&gt;
    &lt;texture type=&quot;skybox&quot; builtin=&quot;gradient&quot; rgb1=&quot;0.4 0.5 0.6&quot; rgb2=&quot;0 0 0&quot; width=&quot;100&quot; height=&quot;600&quot;/&gt;
    &lt;texture type=&quot;cube&quot; name=&quot;texgeom&quot; builtin=&quot;flat&quot; mark=&quot;cross&quot; rgb1=&quot;0.8 0.6 0.4&quot; rgb2=&quot;0.8 0.6 0.4&quot; markrgb=&quot;1 1 1&quot; width=&quot;127&quot; height=&quot;762&quot;/&gt;
    &lt;texture type=&quot;2d&quot; name=&quot;texplane&quot; builtin=&quot;checker&quot; rgb1=&quot;0 0 0&quot; rgb2=&quot;0.8 0.8 0.8&quot; width=&quot;100&quot; height=&quot;100&quot;/&gt;
    &lt;material name=&quot;MatPlane&quot; texture=&quot;texplane&quot; texrepeat=&quot;60 60&quot; specular=&quot;1&quot; shininess=&quot;1&quot; reflectance=&quot;0.5&quot;/&gt;
    &lt;material name=&quot;geom&quot; texture=&quot;texgeom&quot; texuniform=&quot;true&quot;/&gt;
  &lt;/asset&gt;
  &lt;worldbody&gt;
    &lt;geom name=&quot;floor&quot; size=&quot;40 40 40&quot; type=&quot;plane&quot; conaffinity=&quot;1&quot; material=&quot;MatPlane&quot; rgba=&quot;0.8 0.9 0.8 1&quot;/&gt;
    &lt;light pos=&quot;0 0 1.3&quot; dir=&quot;0 0 -1&quot; directional=&quot;true&quot; cutoff=&quot;100&quot; exponent=&quot;1&quot; diffuse=&quot;1 1 1&quot; specular=&quot;0.1 0.1 0.1&quot;/&gt;
    &lt;body name=&quot;torso&quot; pos=&quot;0 0 1.25&quot; gravcomp=&quot;0&quot;&gt;
      &lt;joint name=&quot;rootx&quot; pos=&quot;0 0 -1.25&quot; axis=&quot;1 0 0&quot; limited=&quot;false&quot; type=&quot;slide&quot; armature=&quot;0&quot; damping=&quot;0&quot;/&gt;
      &lt;joint name=&quot;rootz&quot; pos=&quot;0 0 -1.25&quot; axis=&quot;0 0 1&quot; limited=&quot;false&quot; type=&quot;slide&quot; ref=&quot;1.25&quot; armature=&quot;0&quot; damping=&quot;0&quot;/&gt;
      &lt;joint name=&quot;rooty&quot; pos=&quot;0 0 0&quot; axis=&quot;0 1 0&quot; limited=&quot;false&quot; armature=&quot;0&quot; damping=&quot;0&quot;/&gt;
      &lt;geom name=&quot;torso_geom&quot; size=&quot;0.05 0.2&quot; type=&quot;capsule&quot; friction=&quot;0.9 0.1 0.1&quot;/&gt;
      &lt;camera name=&quot;track&quot; pos=&quot;0 -3 -0.25&quot; quat=&quot;0.707107 0.707107 0 0&quot; mode=&quot;trackcom&quot;/&gt;
      &lt;body name=&quot;thigh&quot; pos=&quot;0 0 -0.2&quot; gravcomp=&quot;0&quot;&gt;
        &lt;joint name=&quot;thigh_joint&quot; pos=&quot;0 0 0&quot; axis=&quot;0 -1 0&quot; range=&quot;-2.61799 0&quot;/&gt;
        &lt;geom name=&quot;thigh_geom&quot; size=&quot;0.05 0.225&quot; pos=&quot;0 0 -0.225&quot; type=&quot;capsule&quot; friction=&quot;0.9 0.1 0.1&quot;/&gt;
        &lt;body name=&quot;leg&quot; pos=&quot;0 0 -0.7&quot; gravcomp=&quot;0&quot;&gt;
          &lt;joint name=&quot;leg_joint&quot; pos=&quot;0 0 0.25&quot; axis=&quot;0 -1 0&quot; range=&quot;-2.61799 0&quot;/&gt;
          &lt;geom name=&quot;leg_geom&quot; size=&quot;0.04 0.25&quot; type=&quot;capsule&quot; friction=&quot;0.9 0.1 0.1&quot;/&gt;
          &lt;body name=&quot;foot&quot; pos=&quot;0.2 0 -0.35&quot; gravcomp=&quot;0&quot;&gt;
            &lt;joint name=&quot;foot_joint&quot; pos=&quot;-0.2 0 0.1&quot; axis=&quot;0 -1 0&quot; range=&quot;-0.785398 0.785398&quot;/&gt;
            &lt;geom name=&quot;foot_geom&quot; size=&quot;0.06 0.1&quot; pos=&quot;-0.1 0 0.1&quot; quat=&quot;0.707107 0 -0.707107 0&quot; type=&quot;capsule&quot; friction=&quot;0.9 0.1 0.1&quot;/&gt;
          &lt;/body&gt;
        &lt;/body&gt;
      &lt;/body&gt;
      &lt;body name=&quot;thigh_left&quot; pos=&quot;0 0 -0.2&quot; gravcomp=&quot;0&quot;&gt;
        &lt;joint name=&quot;thigh_left_joint&quot; pos=&quot;0 0 0&quot; axis=&quot;0 -1 0&quot; range=&quot;-2.61799 0&quot;/&gt;
        &lt;geom name=&quot;thigh_left_geom&quot; size=&quot;0.05 0.225&quot; pos=&quot;0 0 -0.225&quot; type=&quot;capsule&quot; friction=&quot;0.9 0.1 0.1&quot; rgba=&quot;0.7 0.3 0.6 1&quot;/&gt;
        &lt;body name=&quot;leg_left&quot; pos=&quot;0 0 -0.7&quot; gravcomp=&quot;0&quot;&gt;
          &lt;joint name=&quot;leg_left_joint&quot; pos=&quot;0 0 0.25&quot; axis=&quot;0 -1 0&quot; range=&quot;-2.61799 0&quot;/&gt;
          &lt;geom name=&quot;leg_left_geom&quot; size=&quot;0.04 0.25&quot; type=&quot;capsule&quot; friction=&quot;0.9 0.1 0.1&quot; rgba=&quot;0.7 0.3 0.6 1&quot;/&gt;
          &lt;body name=&quot;foot_left&quot; pos=&quot;0.2 0 -0.35&quot; gravcomp=&quot;0&quot;&gt;
            &lt;joint name=&quot;foot_left_joint&quot; pos=&quot;-0.2 0 0.1&quot; axis=&quot;0 -1 0&quot; range=&quot;-0.785398 0.785398&quot;/&gt;
            &lt;geom name=&quot;foot_left_geom&quot; size=&quot;0.06 0.1&quot; pos=&quot;-0.1 0 0.1&quot; quat=&quot;0.707107 0 -0.707107 0&quot; type=&quot;capsule&quot; friction=&quot;1.9 0.1 0.1&quot; rgba=&quot;0.7 0.3 0.6 1&quot;/&gt;
          &lt;/body&gt;
        &lt;/body&gt;
      &lt;/body&gt;
    &lt;/body&gt;
  &lt;/worldbody&gt;
  &lt;actuator&gt;
    &lt;general joint=&quot;thigh_joint&quot; ctrlrange=&quot;-1 1&quot; gear=&quot;100 0 0 0 0 0&quot; actdim=&quot;0&quot;/&gt;
    &lt;general joint=&quot;leg_joint&quot; ctrlrange=&quot;-1 1&quot; gear=&quot;100 0 0 0 0 0&quot; actdim=&quot;0&quot;/&gt;
    &lt;general joint=&quot;foot_joint&quot; ctrlrange=&quot;-1 1&quot; gear=&quot;100 0 0 0 0 0&quot; actdim=&quot;0&quot;/&gt;
    &lt;general joint=&quot;thigh_left_joint&quot; ctrlrange=&quot;-1 1&quot; gear=&quot;100 0 0 0 0 0&quot; actdim=&quot;0&quot;/&gt;
    &lt;general joint=&quot;leg_left_joint&quot; ctrlrange=&quot;-1 1&quot; gear=&quot;100 0 0 0 0 0&quot; actdim=&quot;0&quot;/&gt;
    &lt;general joint=&quot;foot_left_joint&quot; ctrlrange=&quot;-1 1&quot; gear=&quot;100 0 0 0 0 0&quot; actdim=&quot;0&quot;/&gt;
  &lt;/actuator&gt;
&lt;/mujoco&gt;
</code></pre>
",2023-05-16 01:06:00,76262403.0,666,2,0,1,,11045673.0,,2/11/2019 14:11,3.0,76262403.0,"<p>See <a href=""https://github.com/deepmind/mujoco/issues/833"" rel=""nofollow noreferrer"">this issue</a> and links therein.</p>
",8593855.0,2.0,1.0,,
4564,75961498,Creating a Ros2 subscriber within a Nav2 Costmap Layer,|c++|robotics|ros2|,"<p>I am currently writing my own custom costmap plugin for use with the navigation2 stack. I'm trying to have the costmap layer take the navigation goal into account when updating costs near it. I've been trying to do this by adding a subscriber to the custom costmap layer that will pull the position of the navigation goal when it is set.</p>
<p>However, I haven't found a good way to do this.</p>
<p>I've tried to use the existing Lifecycle node within the nav2_costmap_2d::Layer class, but no matter what I do, I simply cannot get the data types to match. When I first tried to create the subscriber within the <code>onInitialize</code> method I wasn't able to build the plugin due to colcon complaining that it could not find the subscriber (error looked like this):</p>
<pre><code>error: no matching function for call to std::function&lt;void(const geometry_msgs::msg::Pose2D_&lt;std::allocator&lt;void&gt; &gt;&amp;)&gt;::function(std::_Bind&lt;void (jackstand_objective_costmap_plugin::JSLayer::*(std::shared_ptr&lt;rclcpp_lifecycle::LifecycleNode&gt;, std::_Placeholder&lt;1&gt;))(const geometry_msgs::msg::Pose2D_&lt;std::allocator&lt;void&gt; &gt;&amp;)&gt;&amp;)
  394 |       callback_variant_ = static_cast&lt;typename scbth::callback_type&gt;(callback);
</code></pre>
<p>I figured this was because the Lifecycle node needed to be in the process of being configured to create a subscriber. So the next thing I attempted to do was define an <code>on_configure</code> callback to the node within the Layer class. I attempted to do this via the <code>register_on_configure</code> method. I could not get my types to match though. When I made a <code>func</code> function the method always complained that it needed a type <code>std::function&lt;func&gt;</code>.</p>
<p>I tried making a separate regular node that I could set as a shared pointer within my costmap layer which also didn't work. This was due to the plugin not even recognizing the node I had made within my code.</p>
<p>Is there a way to add a subscriber to a custom costmap layer plugin? If so, would anyone please tell me how?</p>
",2023-04-07 19:43:00,,263,0,0,0,,21591458.0,,4/7/2023 19:17,8.0,,,,,,,
4509,75446976,How to measure the distance for robot center to obstacles?,|matlab|orientation|robotics|,"<p>I want to orient my robot based on the distance from the obstacle, but I do not know how to measure the distance from robot center to the obstacle surrounding.</p>
<p><a href=""https://i.stack.imgur.com/QGnPx.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QGnPx.jpg"" alt=""enter image description here"" /></a></p>
",2023-02-14 11:11:00,75532145.0,115,2,3,-1,,18055837.0,,1/28/2022 8:13,8.0,75448750.0,"<p>I think it might depend on how your robot moves. For instance, if your robot navigation algorithm guides the robot to move one direction at a time, (x then y, or y then x, along axes in XoY space), the <a href=""https://www.wikiwand.com/en/Taxicab_geometry"" rel=""nofollow noreferrer"">Manhattan distance</a> would be more appropriate, as shown in the picture below. <a href=""https://i.stack.imgur.com/vAaBh.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vAaBh.jpg"" alt=""Manhattan distance"" /></a> The <strong>Yellow</strong> and <strong>Blue</strong> distances are the same, while the <strong>Green</strong> distance is the Euclidean distance that will be described below.</p>
<p>However, most modern robots may move in any direction rather than one at a time. But from point A to point B the straight line is the obvious best solution to save a great effort. Then, the <a href=""https://www.wikiwand.com/en/Euclidean_distance"" rel=""nofollow noreferrer"">Euclidean distance</a> should be chosen, as shown below.<a href=""https://i.stack.imgur.com/FUX5a.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FUX5a.jpg"" alt=""enter image description here"" /></a></p>
",1300650.0,-1.0,0.0,133120731.0,"what do you mean ""best method"". Its a distance, its an euclidean norm. The distance between a point and e.g. a point or line is basic math, not a ""method"" among others."
4713,77522536,"I was learning ROS2NAV2, following the documentations, when I launched the file, The ODOM TF started moving itself in the space",|navigation|ros|robotics|slam|rviz|,"<p>I am learning ROS2 NAV2, created a URDF file, it runs perfectly in Gazebo...But after 20-25 seconds the ODOM Tf automatically starts moving upwards or sometimes goes down it the grid, For the code, I have literally copy pasted from the documentations twice, getting the same issue. Please do help if you have the solution.
<a href=""https://i.stack.imgur.com/odvix.png"" rel=""nofollow noreferrer"">ODOM TF moves away from the robot automatically </a>
<a href=""https://i.stack.imgur.com/zPamx.png"" rel=""nofollow noreferrer"">Rviz shows no error</a></p>
<p>I tried to go through the documentations and copy paste my code from it. But tbh I feel there is some issue in the code itself...</p>
",2023-11-21 11:43:00,,16,0,1,0,,21399590.0,,3/14/2023 21:44,1.0,,,,,,136687839.0,Please provide enough code so others can better understand or reproduce the problem.
4633,76765373,"If I run on Raspberry Pi some Python commands from the shell they are executed, but not from scripts",|python|shell|raspberry-pi|robot|gpiozero|,"<p>I'm trying to control a robot via a Raspberry Pi, using Python language.
If I run the commands from the shell they work fine. If I call them from a *.py script the robot doesn't move.
If in the script I put the command
print('hello') the word 'hello' is displayed but the motors still do not move.
How can I solve the problem?
Thank you</p>
<p><a href=""https://i.stack.imgur.com/sBL6A.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sBL6A.jpg"" alt=""enter image description here"" /></a></p>
<pre><code>from gpiozero import Motor

motor1 = Motor(4, 14)
motor2 = Motor(17, 27)

motor1.forward()
motor2.forward()
</code></pre>
<blockquote>
<p>UPDATE: I solved the problem with sleep(5) after the command for move motors. Thanks!!!</p>
</blockquote>
",2023-07-25 17:54:00,,65,1,7,1,,2203619.0,ITALY,3/24/2013 1:22,12.0,76766288.0,"<p>did you check this one, maybe the problem is related to the library <a href=""https://stackoverflow.com/a/72207592/11594679"">https://stackoverflow.com/a/72207592/11594679</a></p>
<p>alternatively, did you check, if the path of the library is a problem. Do you call the script with the same python version, that you execute as shell? Maybe the gpio library is not installed for the one you call the script with.</p>
",11594679.0,0.0,3.0,135334409.0,"If I run the ""python"" command and then manually run (example: >>> motor1.forward()) every single command works fine."
4670,77213531,"In Unity, make an object rotate between two angles that are greater than 180 degrees",|c#|unity-game-engine|quaternions|robotics|,"<p>I am trying to simulate a robotic arm in Unity. I am avoiding super expensive addons (500+ euro) that implement these features by default, so I got to somehow manage on my own or with a little bit of help. Currently I am using an addon called Hybrid IK, not that it matters too much, code is just code. So to the problem at hand:</p>
<p>I have a robot with some axes, each one rotating on either X, Y or Z. I am using quaternions to do the rotations, however quaternions have a problem they can only rotate from -180 to 180. The problem is my robot joints rotate more than that. For example my base rotates from -210 to 210. I tried several solutions but nothing worked.</p>
<p>I tried to introduce a method that reads which way the angle moves after passing from point 0 (initial position) and determining the way the angle rotates that way. So a movement going positive for a 210 angle, would mean, limit the angle from 0 to 180 and then once you flip, from -180 to -150. It didn't work.</p>
<p>Then I tried to just force Unity to go the long way, so a limit of 30, would mean that the angle would have to rotate the other way to reach that 30 (from 0 to -180 and then from 180 to 30) hence mimicking a 330 limit angle. That did not work either.</p>
<p>I am kind of stuck and I would really like to figure this out. I will share the code responsible for this function, but since this is a paid asset, I would rather not share too much.</p>
<pre><code>        public Quaternion LimitHinge(Quaternion rotation)
        {
            Quaternion offsetRot = Quaternion.Euler(limitsOffset);

            Vector3 offsetSwingAxis = (offsetRot * this.mainAxis);
            Vector3 offsetSecondaryAxis = (offsetRot * this.cross);
            Vector3.OrthoNormalize(ref offsetSwingAxis, ref offsetSecondaryAxis);
            Vector3 offsetCrossAxis = Vector3.Cross(offsetSwingAxis, offsetSecondaryAxis);

            Quaternion hingeOffset = Quaternion.AngleAxis(hingeAngleOffset, offsetSwingAxis);
            Quaternion minRotation = Quaternion.AngleAxis(-limitAngle, offsetSwingAxis) * hingeOffset;
            Quaternion maxRotation = Quaternion.AngleAxis(limitAngle, offsetSwingAxis) * hingeOffset;
            // Calculate the target quaternion for a free 1-degree-of-freedom rotation.
            Quaternion free1DOFTarget = Quaternion.FromToRotation(rotation * offsetSwingAxis, offsetSwingAxis) * rotation;

            // Check if the limit angle is greater than or equal to 180 degrees,
            // if so, no further processing is needed, return the target.
            if (limitAngle &gt;= 180)
                return free1DOFTarget;

            // Calculate the middle limit angle. If the limitAngle is greater than or equal to 90 degrees,
            // the middle limit is 180 degrees minus the limitAngle, otherwise, it's the same as limitAngle.

            // Check if the limitAngle is greater than or equal to 90 degrees.
            float midLimit;
            if (limitAngle &gt;= 90f)
            {
                // If it is, calculate midLimit as 180 degrees minus the limitAngle.
                midLimit = 180 - limitAngle;
            }
            else
            {
                // If not, midLimit is the same as limitAngle.
                midLimit = limitAngle;
            }

            // Calculate the quaternion for the middle limit rotation.
            Quaternion free1DOFMid = Quaternion.RotateTowards(minRotation, maxRotation, midLimit);

            // If the limitAngle is greater than or equal to 90 degrees, apply a 180-degree flip to the middle rotation.
            if (limitAngle &gt;= 90f)
            {
                Quaternion flip180 = Quaternion.AngleAxis(180, offsetSwingAxis);
                free1DOFMid *= flip180;
            }

            // Store the original target quaternion for comparison.
            Quaternion lastRotation = free1DOFTarget;

            // Calculate the angle between the target and middle limit rotations.
            float angle = Quaternion.Angle(free1DOFTarget, free1DOFMid);

            // Clamp the middle limit rotation towards the target by the specified limitAngle.
            Quaternion clampedFree1DOF = Quaternion.RotateTowards(free1DOFMid, free1DOFTarget, limitAngle);

            // Determine if the rotation has been clamped to the limit.
            isClampedToLimit = angle &gt;= limitAngle;

            // Return the clamped rotation.
            return clampedFree1DOF;
        }
</code></pre>
<p>P.S. I did try to contact the creator of the addon, but I couldn't reach him in anyway, seems like he is inactive since 2020 or 2021.</p>
",2023-10-02 06:16:00,,256,2,3,2,,17817710.0,"Athens, Greece",1/2/2022 21:21,7.0,77229417.0,"<p>You can try to put your angel from 0 to 1 at first and then multiply and add/subtract it in specific way to get your angles.
so for example you have from -180 to 180, then you need to add to your degrees 180, to get value from 0 to 180 * 2. the next step is to divide degrees, divide by 180 *2 then we got value from 0 to 1. after that you can multiply and add values in the sane way to get value in your custom degrees.</p>
",22553709.0,0.0,0.0,136121421.0,"Interpolate the angle instead, and use `Quaternion.AngleAxis` to construct the rotation from the angle. If you interpolate quaternions it will always rotate the ""short way""."
4557,75773727,How can I use HAL3/Camera2 API alongwith OpenCV to capture the video from two MIPI cameras?,|python|opencv|computer-vision|gstreamer|robotics|,"<p>I am using Qualcomm rb5 development kit along with two MIPI camera's OV9282. Somehow I am not able to use Gstreamer with OpenCV to access these stereo cameras. Does anyone knows how to use HAL3 +OpenCV? There aren't basic tutorials on that. I am stuck with this problem. Please help me.</p>
<p>I have tried using Gstreamer pipeline to access those cameras using below code.</p>
<pre class=""lang-py prettyprint-override""><code>import cv2
from threading import Thread
from time import sleep
import gi

gi.require_version(&quot;Gst&quot;, &quot;1.0&quot;)
from gi.repository import Gst, GLib

Gst.init(None)

main_loop = GLib.MainLoop()
thread = Thread(target=main_loop.run)
thread.start()

pipeline_str = &quot;&quot;&quot;
    qtiqmmfsrc camera=1 ! video/x-raw, format=NV12, width=1280, height=720, framerate=15/1 ! videoconvert ! waylandsink 
&quot;&quot;&quot;
pipeline = Gst.parse_launch(pipeline_str)
pipeline.set_state(Gst.State.PLAYING)

pipeline_str2 = &quot;&quot;&quot;
    qtiqmmfsrc camera=3 ! video/x-raw, format=NV12, width=1280, height=720, framerate=15/1 ! videoconvert ! waylandsink
&quot;&quot;&quot;
pipeline2 = Gst.parse_launch(pipeline_str2)
pipeline2.set_state(Gst.State.PLAYING)

cap = cv2.VideoCapture(pipeline_str, cv2.CAP_GSTREAMER)
cap2 = cv2.VideoCapture(pipeline_str2, cv2.CAP_GSTREAMER)

num = 0

while True:
    succes1, img = cap.read()
    succes2, img2 = cap2.read()

    if not succes1 or not succes2:
        break

    cv2.imshow('Img 1', img)
    cv2.imshow('Img 2', img2)

    k = cv2.waitKey(5)

    if k == 27:
        break
    elif k == ord('s'):
        cv2.imwrite('images/stereoLeft/imageL{}.png'.format(num), img)
        cv2.imwrite('images/stereoRight/imageR{}.png'.format(num), img2)
        print('images saved!')
        num += 1

cap.release()
cap2.release()
cv2.destroyAllWindows()

pipeline.set_state(Gst.State.NULL)
pipeline2.set_state(Gst.State.NULL)
main_loop.quit()
</code></pre>
<p>It is only displaying one camera and that camera stream is not getting fed into the rest of the code with VideoCapture function. I don't know what is wrong with it. Hence, I am trying to see if there is other ways to access camera's using HAL3/Camera2 APIs.</p>
",2023-03-18 03:34:00,,208,1,0,0,,21416083.0,,3/17/2023 2:09,11.0,75775196.0,"<p>There are 2 problems with your code:</p>
<ul>
<li>For using a gstreamer pipeline as capture from opencv, the pipeline would have to end with appsink (the opencv application).</li>
<li>Opencv videocapture will manage the pipeline creation and change states.</li>
</ul>
<p>First try:</p>
<pre><code>import cv2
from time import sleep

pipeline_str = &quot;&quot;&quot;
    videotestsrc ! video/x-raw, format=NV12, width=1280, height=720, framerate=15/1 ! videoconvert ! video/x-raw,format=BGR ! appsink drop=1
&quot;&quot;&quot;
cap = cv2.VideoCapture(pipeline_str, cv2.CAP_GSTREAMER)
if not cap.isOpened():
    print('Failed to open capture')
    exit(-1)
    
    
pipeline_str2 = &quot;&quot;&quot;
    videotestsrc pattern=ball ! video/x-raw, format=NV12, width=1280, height=720, framerate=15/1 ! videoconvert ! video/x-raw,format=BGR ! appsink drop=1
&quot;&quot;&quot; 
cap2 = cv2.VideoCapture(pipeline_str2, cv2.CAP_GSTREAMER)
if not cap2.isOpened():
    print('Failed to open capture2')
    exit(-2)

num = 0

while True:
    succes1, img = cap.read()
    succes2, img2 = cap2.read()

    if not succes1 or not succes2:
        break

    cv2.imshow('Img 1', img)
    cv2.imshow('Img 2', img2)

    k = cv2.waitKey(5)

    if k == 27:
        break
    elif k == ord('s'):
        cv2.imwrite('images/stereoLeft/imageL{}.png'.format(num), img)
        cv2.imwrite('images/stereoRight/imageR{}.png'.format(num), img2)
        print('images saved!')
        num += 1

cap.release()
cap2.release()
cv2.destroyAllWindows()
</code></pre>
<p>If it works, change the sources with your cameras.</p>
<p><strong>EDIT:</strong>
It is unclear what is your gstreamer stack with only camera source and waylandsink. If you don't have gstreamer appsink, you may not be able to use opencv VideoCapture that relies on that.</p>
<p>You may however try in such case to add a src pad probe to camera plugin. From the callback, you may be able to get buffer data and put it into an opencv mat or numpy array for further processing. You may push these into fifos or circular buffers to be picked up by the application, checking PTS for synchronization between cameras:</p>
<p><strong>Callback function:</strong></p>
<pre><code>def probe_callback(pad,info): 
    gst_buffer = info.get_buffer()
    print(' PTS:', gst_buffer.pts)
    ret, mapinfo = gst_buffer.map(Gst.MapFlags.READ)
    if ret:        
        #read the mapped buffer into np frame
        frameNV12 = np.frombuffer(mapinfo.data, dtype=np.uint8)
        frameNV12.shape = ((int)(720*3/2), 1280, 1)
        
        # Convert NV12 into BGR 
        frameBGR = cv2.cvtColor(frameNV12, cv2.COLOR_YUV2BGR_NV12)
                
        # Here you would push a copy of the frame into a queue or circular buffer for application
        # You may attach the pts for checking synchronization between various cameras 
        
        # Or write to disk for checking last image...Note that it may be slow depending on your hw
        #cv2.imwrite(&quot;test.png&quot;, frameBGR)
        
        del frameBGR
        del frameNV12
        gst_buffer.unmap(mapinfo)
    else:
        print('ERROR: Failed to map buffer')
        
    return Gst.PadProbeReturn.OK
</code></pre>
<p><strong>Before starting pipeline, add the probe to src pad of camera source plugin:</strong></p>
<pre><code>srcpad = source.get_static_pad('src')
probeID = srcpad.add_probe(Gst.PadProbeType.BUFFER, probe_callback)
</code></pre>
",5025035.0,1.0,6.0,,
4581,76164592,DJI RS 2: Calculate Attitude from Joint Angles,|python|numpy|robotics|,"<p>At the moment I'm working on the DJI Ronin RS 2 gimbal. The configuration is shown in the image. (<a href=""https://i.stack.imgur.com/iwrme.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/iwrme.png</a>) There is a camera mounted on the gimbal. The red arrows mark the axes of the gimbal. The joint angles alpha, beta and gamma are measured and therefore known. The green coordinate system is the fixed reference system.</p>
<p>So much for the setup. Now to the complicated part: How can I combine the angles alpha, beta and gamma to calculate one yaw angle (around green z-axis), one roll angle (around green x-axis) and one pitch angle (around green z-axis) in the reference system, to let me know where the camera is pointing to?</p>
<p>I know that I could also get the attitude-information directly from the gimbal by sending another &quot;obtain&quot;-command. However, I do need the joint angles and the attitude angles. Therefore I thought it would be easier to calculate the attitude angles from the joint angles.</p>
<p>Any help is highly appreciated! Thank you.</p>
<p>So far, I set up different rotation matrices (basic rotations around x-, y-, z-axis combined by multiplication) and tried to calculate the attitude with the following formulas:</p>
<pre><code>def Rx(a):
    # Transformation to radiant
    a = np.radians(a)
    Rx = [[1, 0, 0],
          [0, np.cos(a), np.sin(a)],
          [0, -np.sin(a), np.cos(a)]]
    Rx = np.round(Rx, decimals = 3)
    return Rx

def Ry(a):
    # Transformation to radiant
    a = np.radians(a)
    Ry = [[np.cos(a), 0, -np.sin(a)],
          [0, 1, 0],
          [np.sin(a), 0, np.cos(a)]]
    Ry = np.round(Ry, decimals = 3)
    return Ry

def Rz(a):
    # Transformation to radiant
    a = np.radians(a)
    Rz = [[np.cos(a), np.sin(a), 0],
          [-np.sin(a), np.cos(a), 0],
          [0, 0, 1]]
    Rz = np.round(Rz, decimals = 3)
    return Rz

# Rotation around z-axis
R10 = Rz(alpha)

# Rotation around y-axis
R21 = Ry(55)

# Rotation around x-axis
R32 = Rx(beta)

# Rotation around y-axis
R43 = Ry(gamma)

R20 = np.matmul(R21, R10)
R30 = np.matmul(R32, R20)
R = np.matmul (R43, R30)

pitch = -np.arcsin(R[2,0])
roll = np.arctan2(R[2,1]/np.cos(pitch), R[2,2]/np.cos(pitch))
yaw = np.arctan2(R[1,0]/np.cos(pitch), R[0,0]/np.cos(pitch))`
</code></pre>
<p>Unfortunately, this hasn't led to the right result yet. I guess that these formulas only apply to rotation around x-, y-, and z-axis in a certain order. This gimbal uses another rotation axis due to the joint in the middle.</p>
",2023-05-03 13:43:00,,35,0,1,1,,21805961.0,,5/3/2023 12:47,4.0,,,,,,134318799.0,"How do you define pitch, roll and yaw? I would expect pitch to be about the X axis, roll about the Y and yaw about the Z, but you don't seem to do this. So can you specify what you're using? Also, you mention alpha, beta and gamma but don't actually use them in your code. I assume you used them to calculate R. How?"
4703,77446977,Stop Python function running on a Raspberry Pi from a different Pi using TCP/IP connection,|python|tcp|raspberry-pi|robotics|,"<p>I am programming a robot that has a Raspberry Pi as the microcontroller using Python. The robot has multiple motors and sensors. The robot is controlled using a different Raspberry Pi that sends commands through TCP/IP connection using an ethernet cable (I will refer to this Raspberry Pi as the controller and the robot Raspberry Pi as the Robot). The controller is running a Tkinter UI that has buttons programed to send a message to the robot once they are pressed.</p>
<p>When the robot receives a message from the controller, it processes the message to see what is contained in it, and depending on the content of it, it runs a specific function on the robot program.</p>
<p>The comunications between the robot and controller work fine but the problem I am having now is that once the robot has received a message from the controller, and its running a function, there is no way of stopping the function running on the robot from the controller until it has finished or the power is turned off.</p>
<p>The code running on the robot that receives and processes the mesagges is the following:</p>
<pre><code>SERVER = '192.168.0.60'     # This is the server, CPU1 - this one!
PORT = 22001
ADDR = (SERVER, PORT)
    
# Create socket for the Control Client, Pi4(2) client and Server to use...
s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)  # Using TCP.
s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)

# Manage any binding errors...
try:
    s.bind(ADDR)
except socket.error:
    robbie.system_message = &quot;Socket bind failed.&quot;

s.listen(5)
    
robbie.system_message = &quot;Awaiting connection request from TT robot controller...&quot;

while True:
    (conn, addr) = s.accept() # Blocking call; this loop runs for each new client connection.
    connected = True
    robbie.system_message = f&quot;[NEW CONNECTION] {addr} connected. {threading.activeCount()-1}&quot;
        
     # This IP address is hard coded - very fixed indeed as any deviation will render the robot                         useless.

     if '192.168.0.70' in str(conn):
         robbie.cpu_connected = True
            
        filtered_msg = ''    
        
        msg_route_thread = threading.Thread(target = route_messages, args=(conn, addr))
        msg_route_thread.start()
         
        
def route_messages(conn, addr):
    global robbie, connected
    # This method provides message routing
    
    connected = True
    try:
        while connected:
            # Waiting for messages 
            raw_data = conn.recv(256) # custom command messages can be very long
            data = raw_data.decode()
            raw_data = &quot;&quot;
            x = len(raw_data)
            data = data.strip(' ,')
            
            msg_prefix = data[0:4]
            filtered_msg = data[5:]
            
           
        
            # Each of these procedure calls proceses one message. 
            if msg_prefix == &quot;CTRL&quot;:
                robbie.ctrl_latest_message = f&quot;{msg_prefix}~ {filtered_msg}&quot;
                process_ctrl_messages(conn, filtered_msg)
               

def process_ctrl_messages(conn, msg):
    global connected, robbie
    
    robbie.ctrl_latest_message = msg
    
    print(f&quot;CTRL~ ---&gt; {msg}&quot;)
    robbie.current_command = msg
    robbie.ctrl_connected = True
    
    # Few exapmles of messages that the robot receives from the controller
        if 'S-HOME' in msg:
            slide_home()
        elif 'P-HOME' in msg:
            pitch_home()
        elif 'R-HOME' in msg:
            roll_home()            
</code></pre>
<p>The code running on the controller that sends messages to the robot is:</p>
<pre><code># ------------------ Communications setup -------------------------------------------------------------------------------------
HOST = '192.168.0.60'
PORT = 22001

try:
    time.sleep(10)
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    s.connect((HOST, PORT))
    
except socket.error as msg:
    
    print('Check that the robot is powered, is running its own control program, has flashed its lights and is beeping.')
    print(msg)
    exit()
    
print('Controller connected to Tubetech rover.')

# ------------------ Set the various initial states ---------------------------------------------------------------------------
command = ''

# ------------------ Event functions ------------------------------------------------------------------------------------------ (Some examples of the functions that send the mesagges to the robot.)
def lightsToggle():
    command = 'CTRL~L-TOGGLE'
    s.send(command.encode())
    print(command)
    
def forward():
    # Request the rover to move forwards at the speed currently set.
    command = 'CTRL~FORWARD'
    s.send(command.encode())
    #reply = s.recv(32)
    print(command)
</code></pre>
<p>As I mentioned before, once the robot has received a message from the controller and it's running a function, the robot does not receive any message until the function has finished. When I get this problem, I have been able to verify that the messages are being sent from the controller but not received in the robot.</p>
<p>I have tried running a different function just for the stop messages on a thread and changing the route messages function but nothing has worked.</p>
<p>Could anyone please give me some guidance on any way of doing it or help me find if I have done something wrong? Even though the robot works fine, I think being able to stop it at anytime using the controller is something essential but I have been stuck on it for a few weeks.</p>
<p>If you feel like I haven't explained something properly or there is something missing please let me know.</p>
<p>Thank you :)</p>
",2023-11-08 15:37:00,,37,0,3,0,,22880720.0,,11/8/2023 14:20,0.0,,,,,,136544759.0,"I don't get a traceback on the program, it is just that the messages don't get processed on the robot when a function is being run until it has finished. My thought was that as I have the function to read the messages as a thread, I would be able to send messages from the controller to the robot constantly and process them. Also,  I have tried removing the index on the s.listen() but I still have the same problem."
4569,76039877,Reading sensors in real time NAO robot,|c++|robotics|nao-robot|,"<p>I'm working with a real time application with the robot NAO and i need to read the joints sensors fast, the function getData from ALMemory is too slow. I have seen the example with the modue almemoryfastaccess but i have an error when i run the .exe.</p>
<p>I have read about the almemoryfastaccess module but when i try to connect to the sensors with the function ConnectToVariables(getParentBroker(), fSensorKeys, false) i recieved the following error in the function getDataPtr: &quot;uncaught error pointer serialization not implemented&quot;. Does anyone have work with this?. Thanks.</p>
<p>I can't you getData from ALMemory because is too slow for my application.</p>
<p>The code is something like this. It compiles well but when I run the .exe the error appears:</p>
<p><a href=""https://i.stack.imgur.com/lbQ83.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lbQ83.png"" alt=""enter image description here"" /></a></p>
<pre class=""lang-cpp prettyprint-override""><code>#include &lt;almemoryfastaccess/almemoryfastaccess.h&gt;
#include &lt;boost/shared_ptr.hpp&gt;
#include &lt;string&gt;
#include &lt;vector&gt;
#include &lt;alcommon/albroker.h&gt;
#include &lt;qi/log.hpp&gt;

using namespace AL;
using namespace std;

boost::shared_ptr&lt;AL::ALMemoryFastAccess&gt; fMemoryFastAccess;
vector&lt;string&gt; fSensorKeys;
fSensorKeys.clear();
fSensorKeys.resize(3);
fSensorKeys[0] = &quot;Device/SubDeviceList/HeadPitch/Position/Sensor/Value&quot;;
fSensorKeys[1] = &quot;Device/SubDeviceList/HeadYaw/Position/Sensor/Value&quot;;
fSensorKeys[2] = &quot;Device/SubDeviceList/LAnklePitch/Position/Sensor/Value&quot;;
fMemoryFastAccess-&gt;ConnectToVariables(getParentBroker(), fSensorKeys, false); //in this line the error appears.
</code></pre>
",2023-04-17 23:00:00,76385774.0,94,1,5,-1,,14238185.0,,9/7/2020 23:46,7.0,76385774.0,"<p>This &quot;fast access&quot; API only works when your module runs in the same process as NAOqi.</p>
<p>When running your code from a remote client (your Windows PC here), it is not possible to achieve real-time data collection.</p>
<p>There are alternatives:</p>
<ul>
<li>cross-compile your module and have it loaded by NAOqi. You'll create a library instead of an executable (see <a href=""http://doc.aldebaran.com/2-5/dev/cpp/examples/core/helloworld/example.html#module-creation"" rel=""nofollow noreferrer"">the use of <code>qi_create_lib</code> here</a>). Sadly I do not know how to find the cross-toolchain. You might need to contact the support.</li>
<li>you can poll the data less frequently (say every 100ms) by using <a href=""http://doc.aldebaran.com/2-5/naoqi/core/almemory-api.html#ALMemoryProxy::getListData__AL::ALValueCR"" rel=""nofollow noreferrer""><code>ALMemory::getListData</code></a></li>
<li>you can poke for the module called <code>ALMemoryWatcher</code> by running <code>qicli info --show-doc ALMemoryWatcher</code> on the robot. It captures real-time data, but you are meant to collect the data by batches at a lower frequency.</li>
</ul>
",3836562.0,0.0,1.0,134115141.0,"Have a look at this answer, as it show how to get one sensor fast: https://stackoverflow.com/questions/72627941/nao-robot-imu-data-rates/72642451#72642451"
4681,77266977,Check if UR10 (Universal Robot) Finished job successfully,|c#|events|communication|robot|universal|,"<p>I am communicating with UR10 with C# under automation NuGet package.</p>
<p>How can I know if robot finished given job successfully?</p>
<p>we are giving UR10 jobs with scripts like this:</p>
<pre><code>def FullCommand():
    global curent_j_pose = get_actual_joint_positions ()
    global current_b_value = curent_j_pose[0]
     movep(p[.556075771156, -.493695531905, .571111521277, 1.426415630227, .726027077406, .605252480495], a = 1.5, v = 0.6, r = 0.05) ....
end
</code></pre>
<p>For now we are getting this information from OnKeyMessageReceived event from PrimaryInterface</p>
<pre><code>void OnKeyMessageReceived(int? DeviceID, KeyMessageEventArgs E)
{
    if (E.RobotMessageTitle.Contains(&quot;STOPPED&quot;))    
    {
                
    }
}       
</code></pre>
<p>Is this correct way to achieve this ?
Because I think sometimes this event is raised even when job was not finished successfully.</p>
<p>help.</p>
",2023-10-10 15:10:00,,22,0,0,0,,9404754.0,Georgia,2/24/2018 6:55,5.0,,,,,,,
4697,77416468,Detect whether a vehicle touched a white line drawn on ground in realtime,|python|image-processing|deep-learning|computer-vision|robotics|,"<p>I am currently working on a project where I take real time frames from an IP camera using RTSP protocol and do some processing on it.</p>
<p>The footage will be of vehicles captured from 3 different angles including its top view. I want to find whether the vehicle touches white lines drawn on the ground. I did it with OpenCV using background masking method. It worked for me. But then the shadow of the vehicle started causing problems as long shadows (during morning and evening hours) are detected as foreground crossing the white lines. So then I applied a shadow cancellation algorithm on it. but its too heavy and is causing too much delay so it wont work I real time.</p>
<p>I am planning to use deep learning to do the same now. I trained a resnet18 based model to do the task. but it is not generalizing and is getting overfit. So it is only able to detect those images I trained it. when I implement it, it started to show false results.</p>
<p>I would  like to have help in solving this project. What could the the best way I should have done this. Sorry if it is not the correct place to ask. Also do ask if any more information is needed.</p>
<p>sample image :</p>
<p><a href=""https://i.stack.imgur.com/RytU3.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RytU3.jpg"" alt=""enter image description here"" /></a></p>
<p>Regards.</p>
",2023-11-03 11:54:00,,38,0,3,0,,1493259.0,Trivandrum,6/30/2012 16:30,24.0,,,,,,136489037.0,@ChristophRackwitz I have  added a sample image. I need to fin whether a vehicle crossed the white line.
4498,75279981,cartographer: Why does the position of radar not change during real-time mapping,|lua|robotics|slam|,"<p><a href=""https://i.stack.imgur.com/Xa1LC.jpg"" rel=""nofollow noreferrer"">result Map</a></p>
<p>No matter how I move the radar, the position of the radar that rviz shows don't change.</p>
<p>Why is this happening? Do I need to add odom information to it?</p>
<p>I want the radar position on the map to change as the radar position changes</p>
<p><strong>.lua file:</strong></p>
<pre><code>options = {
  map_builder = MAP_BUILDER,
  trajectory_builder = TRAJECTORY_BUILDER,
  map_frame = &quot;map&quot;,
  tracking_frame = &quot;base_link&quot;,
  published_frame = &quot;base_link&quot;,
  odom_frame = &quot;odom&quot;,
  provide_odom_frame = true,
  publish_frame_projected_to_2d = false,
  use_pose_extrapolator = true,
  use_odometry = false,
  use_nav_sat = false,
  use_landmarks = false,
  num_laser_scans = 1,
  num_multi_echo_laser_scans = 0,
  num_subdivisions_per_laser_scan = 1,
  num_point_clouds = 0,
  lookup_transform_timeout_sec = 0.2,
  submap_publish_period_sec = 0.3,
  pose_publish_period_sec = 5e-3,
  trajectory_publish_period_sec = 30e-3,
  rangefinder_sampling_ratio = 1.,
  odometry_sampling_ratio = 1.,
  fixed_frame_pose_sampling_ratio = 1.,
  imu_sampling_ratio = 1.,
  landmarks_sampling_ratio = 1.,
}

MAP_BUILDER.use_trajectory_builder_2d = true
TRAJECTORY_BUILDER_2D.submaps.num_range_data = 30
TRAJECTORY_BUILDER_2D.use_imu_data = false
TRAJECTORY_BUILDER_2D.num_accumulated_range_data = 1
POSE_GRAPH.optimization_problem.huber_scale = 1e2
POSE_GRAPH.constraint_builder.min_score = 0.65
</code></pre>
<p><strong>.launch file:</strong></p>
<pre><code>&lt;launch&gt;
&lt;param name=&quot;robot_description&quot;
  textfile=&quot;$(find cartographer_ros)/urdf/car.urdf&quot; /&gt;
&lt;param name=&quot;/use_sim_time&quot; value=&quot;false&quot; /&gt;
&lt;node name=&quot;robot_state_publisher&quot; pkg=&quot;robot_state_publisher&quot;
  type=&quot;robot_state_publisher&quot; /&gt;
&lt;node name=&quot;cartographer_node&quot; pkg=&quot;cartographer_ros&quot;
  type=&quot;cartographer_node&quot; args=&quot;
  -configuration_directory $(find cartographer_ros)/configuration_files
  -configuration_basename myLds.lua&quot;
  output=&quot;screen&quot;&gt;
&lt;remap from=&quot;scan&quot; to=&quot;/scangkw&quot; /&gt;
&lt;/node&gt;
&lt;node name=&quot;cartographer_occupancy_grid_node&quot; pkg=&quot;cartographer_ros&quot;
  type=&quot;cartographer_occupancy_grid_node&quot; args=&quot;-resolution 0.05&quot; /&gt;
&lt;/launch&gt;
</code></pre>
",2023-01-30 03:19:00,75385722.0,105,1,2,-2,,17206746.0,,10/21/2021 1:44,5.0,75385722.0,"<p>Check the frame that your radar is publishing data on. Is the frame the same as base_link? If not, set the frame name that the radar is providing to be equal to the tracking_frame in the .lua configuration file.</p>
<p>If you do not want to do this and keep the original configuration, provide a static_transform_publisher which provides the transform between the radar frame and the base_link.</p>
<p>What may be happening here is that your radar is providing data on a different frame, while your cartographer is set up to accept data with position at the base_link frame.</p>
",16782709.0,0.0,2.0,132841853.0,"In addition, remove non-related flag such as C and python please. You should put the lua flag instead."
4689,77358482,How to create a bounding box for all the scene in CoppeliaSim?,|geometry|simulator|robotics|bounding-box|,"<p>I am trying to create a bounding box around all the elements in my scene.</p>
<p>I tried to start with a grouping, to get the bounds of all the objects and then calculate the bounding box. That is ok, but it still has issues.</p>
<p>First, the bounding box is not precise. It is not positioned properly. For example, for the object at the bottom the bounding box is not touching it, it is still under. For the object the top, the bounding box is cutting it because it is not up enough. The bounding box is lower than what it should. Sometimes it is half the object of error, or a little bit, or almost the whole object is out. This happens for the 3 axis.</p>
<p>I already positioned the bounding box at 0, 0, 0. Nothing changes, the bounding box was already at the center. But it is still not matching accurately with the objects of the scene.</p>
<p>My main problem, many problems with grouping. To restore everything, I ungroup. That ungroup is not working as expected. After ungrouping, the icons in the scene hierarchy change, I guess the properties of the objects have also changed. In addition, it is not only ungrouping my group, but everything, including robots. Finally, when the simulation ends, it destroys everything, including the floor.</p>
<p>I also thought about getting all the objects and their coordinates with a for loop. But I am reluctant to this solution because it is very time consuming. I am just looking for a reliable way to generate a bounding boy around all the scene, so you do not have to stick to the code I provided, I am open to other solutions. '</p>
<p>Thank you.</p>
<pre><code># python
from coppeliasim_zmqremoteapi_client import RemoteAPIClient
import time

'''
@dev: This function gets the x, y, and z lenghts of the bounding box of a given object
@param: the handle of the object to calculate its bounding box
@returns: 3 values. The x length, the y length and the z length
@author: Andres Masis
'''
def getObjectBoundingBoxSize(handle):
    # Gets the x lenght
    r, m = sim.getObjectFloatParameter(handle, sim.objfloatparam_objbbox_max_x)
    r, n = sim.getObjectFloatParameter(handle, sim.objfloatparam_objbbox_min_x)
    x = m - n

    # Gets the y lenght
    r, m = sim.getObjectFloatParameter(handle, sim.objfloatparam_objbbox_max_y)
    r, n = sim.getObjectFloatParameter(handle, sim.objfloatparam_objbbox_min_y)
    y = m - n

    # Gets the z lenght
    r, m = sim.getObjectFloatParameter(handle, sim.objfloatparam_objbbox_max_z)
    r, n = sim.getObjectFloatParameter(handle, sim.objfloatparam_objbbox_min_z)
    z = m - n

# Returns all the values
return x, y, z

# Access to the CoppeliaSim client
client = RemoteAPIClient()
sim = client.getObject('sim')

# Makes sure that the idle loop runs at full speed for this program:
defaultIdleFps = sim.getInt32Param(sim.intparam_idle_fps)
sim.setInt32Param(sim.intparam_idle_fps, 0)

# Run a simulation in stepping mode:
client.setStepping(True)
sim.startSimulation()

# Get all of the elements of the scene.
scene_objects = sim.getObjectsInTree(sim.handle_scene, sim.handle_all, 0)
client.step() # triggers next simulation step

# Create group of objects
groupHandle = sim.groupShapes(scene_objects, False)
client.step() # triggers next simulation step

x, y, z = getObjectBoundingBoxSize(groupHandle)

sim.ungroupShape(groupHandle)
client.step() # triggers next simulation step

boxHandle = sim.createPrimitiveShape(sim.primitiveshape_cuboid,[x, y, z], 0)
client.step() # triggers next simulation step

# Color
sim.setShapeColor(boxHandle, None, sim.colorcomponent_emission, [0, 255, 0])
client.step() # triggers next simulation step

# Transparency
sim.setShapeColor(boxHandle, None, sim.colorcomponent_transparency, [0.5])
client.step() # triggers next simulation step

# Position
sim.setObjectPosition(boxHandle, sim.handle_parent, [0.255, 0.225, 0.225])
client.step() # triggers next simulation step

time.sleep(10)

sim.stopSimulation()

# Restore the original idle loop frequency:
sim.setInt32Param(sim.intparam_idle_fps, defaultIdleFps)

print('Program ended')
</code></pre>
<p><a href=""https://i.stack.imgur.com/kCFnS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kCFnS.png"" alt=""My bounding box example"" /></a></p>
<p><strong>Tried:</strong></p>
<ul>
<li>Grouping solution</li>
<li>For loop solution to get all the coordinates</li>
<li>Ask in the CoppeliaSim forum</li>
</ul>
<p><strong>Expected:</strong></p>
<ul>
<li>Any reliable solution with an acceptable time performance to generate a bounding box</li>
</ul>
",2023-10-25 10:00:00,,35,0,0,0,,22694257.0,,10/6/2023 8:42,4.0,,,,,,,
4577,76085799,Pepper emulator not start because INSTALL_FAILED_INSUFFICIENT_STORAGE,|android-studio|kotlin|robotics|pepper|,"<p>I can't run my app by Pepper's emulator because from the log it tells me that there is not sufficient space in the emulator.</p>
<p><code>Installation did not succeed. The application could not be installed: INSTALL_FAILED_INSUFFICIENT_STORAGE  List of apks: \[0\] 'C:\\Users\\aless\\AndroidStudioProjects\\TestPepper\\app\\build\\intermediates\\apk\\debug\\app-debug.apk' The device needs more free storage to install the application (extra space is needed in addition to APK size). Retry Failed to launch an application on all devices</code></p>
<p>I have already reset the emulator to factory data but it keeps telling me there is no space.</p>
<p>How can i increase the space since in the Device Manager it doesn't show me the emulator?</p>
<p>Thank you very much</p>
<p>(<a href=""https://i.stack.imgur.com/XLuro.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/XLuro.png</a>)</p>
<p>(<a href=""https://i.stack.imgur.com/8MmCH.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/8MmCH.png</a>)</p>
<p>(<a href=""https://i.stack.imgur.com/r1k2V.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/r1k2V.png</a>)</p>
<p>(<a href=""https://i.stack.imgur.com/tifsB.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/tifsB.png</a>)</p>
",2023-04-23 15:43:00,,67,0,1,0,,21714826.0,,4/23/2023 15:04,2.0,,,,,,134184926.0,"Setup another device and see if it still does it.. I never use emulator, I always use a physical device. For the exact same reason as the issue you are having"
4666,77141683,Simulation tools to validate the maths models of an unmanned surface vehicle (USV) with a manipulator,|validation|model|simulation|robotics|manipulators|,"<p>I'm working on a custom unmanned surface vehicle (USV) with a waterproof manipulator attached to it.</p>
<p>I'm looking for a simulator where I can validate the maths models of the robot.</p>
<p>The ideal case would be where</p>
<ul>
<li>I do not need to put equations to calculate the hydrodynamics in the simulator,</li>
<li>but I can just put the geometry of my robot, force and torque inputs, and any other necessary parameters</li>
<li>and see that both the USV and the attached manipulator are affected by corresponding hydrodynamics (buoyancy, drag, lift, and added mass)</li>
<li>and the forces and torques (wrenches) propagated between the manipulator and the USV are also taken into account</li>
<li>and the combined system (the USV and the manipulator) moves/behaves accordingly and accurately.</li>
</ul>
<p>What would be the best simulation tool for this? Any other suggestions and advice would be appreciated as well.</p>
<p>Thank you very much.</p>
",2023-09-20 10:53:00,,16,0,0,0,,22599536.0,,9/20/2023 10:37,0.0,,,,,,,
4733,77678698,Unique Detections from a Camera video feed using Pytorch Faster-RCNN Model,|pytorch|conv-neural-network|robotics|faster-rcnn|,"<p>I have made a custom dataset then used that to fine tune Pytorch faster RCNN model. Now I have a conveyor, on top of which I have placed a camera. I am using the image from that camera and passing it into the model for inference.The model outputs a result dictionary which contains labels, bbox coordinates, score. Since I have a continuous camera feed and objects are coming on the conveyor, I am getting multiple entries for the same object. Say if I have a can on the conveyor then I am getting atleast 10-15 entries till the can goes out of cameras frame of view.
I want to save the unique detections in an array as I have to use that data further up in the pipeline.</p>
<p>I am at my wits end, I have tried using Bounding Box similarity as a factor to store unique detections, I have tried assigning unique ids to the detections, I have also tried using Bounding Box corner coordinates as a way to filter out unique detections. But all of that fails if I have say 2 cans simultaneously behind each other coming in on the conveyor, then the array only saves that as 1 detection and this messes up the further pipeline.
I would like to know if there is a better way to do this task or if I am doing something wrong. Any help would be much appreciated.</p>
",2023-12-18 11:14:00,,10,0,0,0,,23116043.0,,12/17/2023 10:34,1.0,,,,,,,
4490,75160864,How to project LiDAR points into camera according to literature,|matlab|computer-vision|robotics|matlab-cvst|extrinsic-parameters|,"<p>For my project I was trying 3D LiDAR to Camera projection. I was recommended MATLAB LiDAR-Camera modules for calibration and then use the results for the projection on a live stream of the data.</p>
<p>In MATLAB, I got the rotation matrix ( R ), the translation matrix ( T ) and the camera intrinsic matrix ( M ). When using the MATLAB tools I am getting the results as follows :
<a href=""https://i.stack.imgur.com/Y8npG.png"" rel=""nofollow noreferrer"">MATLAB generated projection for the same values of R, T and M</a></p>
<p>But using the literature in which the projection matrix is given as</p>
<p>( M 0 )x( [[ R T ], [ 0 1]] ) to get from a [x y z 1] to [u v w], I am getting the following result :</p>
<p><a href=""https://i.stack.imgur.com/Uvs5b.png"" rel=""nofollow noreferrer"">Projection of M, R and T as per the literature </a></p>
<p>M, R,T as calculated from MATLAB</p>
<pre><code>
M = array([[904.4679,   0.    , 596.9176],
       [  0.    , 814.7088, 349.8212],
       [  0.    ,   0.    ,   1.    ]])

R = array([[ 0.124 , -0.0038,  0.9923],
       [-0.9912,  0.0474,  0.124 ],
       [-0.0475, -0.9989,  0.0021]])

T = array([[-0.56  ,  0.241 , -0.4454]])

</code></pre>
<p>A code snippet looks like this</p>
<pre><code>rotation = np.array([[0.1240,-0.0038,0.9923],                                                       [-0.9912,0.0474,0.1240],[-0.0475,-0.9989,0.0021]])
traslation = np.array([[-0.5600,0.2410,-0.4454]])
traslation_1 = np.array([[-0.4454,0.2410,-0.5600]])
intrinsic = np.array([[904.4679,0,596.9176],[0,814.7088,349.8212],[0,0,1]])

a = np.concatenate((rotation,np.array([[0,0,0]])), axis =0)
b = np.concatenate((traslation_1.T, np.array([[1]])), axis =0)
c = np.concatenate((a,b), axis =1)

print('\n Extrinsic:\n \n',c)
d = np.concatenate((intrinsic, np.array([[0,0,0]]).T), axis =1)
print('\n Intrinsic:\n \n',d)
e = np.matmul(d,c)
print(&quot;\n Final:\n \n&quot;, e)
df = pd.read_csv('out_file.csv')

img = cv2.imread('images/0001.png')
v1_max = 0
v2_max = 0
uv = []
import matplotlib.pyplot as plt 
 
for i in range(df.shape[0]):
    point = np.array([[df['x'].iloc[i],df['y'].iloc[i],df['z'].iloc[i],1]]).T
    v = np.matmul(e, point)
    v = v/v[2]
    if v[0]&lt;=720 and v[0] &gt; 0  and v[1] &lt; 1280 and v[1] &gt; 0:
        img[int(np.floor(v[0])),int(np.floor(v[1]))] = [0,255,255] 
            
                      
        
cv2.imwrite('file.png', img) 

</code></pre>
<p>Hardware configuration :</p>
<p>Velodyne 64 Channel LiDAR @ 10 HZ</p>
<p>Camera : 1280*720 monocular camera</p>
<p>Checkerboard : 10*7 with 10 cm of pattern and a padding .</p>
<p>I need some help as to how I cam take the parameters from the MATLAB and do the same in the scripts and get the near results .</p>
<p>I am expecting as to where I am going wrong when projecting the LiDAR points onto the camera, as to when I am using MATLAB functions I am getting a correct projection. Same is not there with the literature.</p>
",2023-01-18 14:35:00,,174,0,3,1,,21035048.0,,1/18/2023 10:39,6.0,,,,,,132679767.0,"Thank you for your mention, this is my first question on the form. I have updated the code."
4580,76121679,Pybullet - Gripper is not able to Grasp Object,|python|robotics|pybullet|grasp|urdf|,"<p>I am using the Kinova Jaco robotic arm with the j2n6s300.urdf file and I simply want to grasp a mug from the side and live it up.
However, the gripper is not able to pick up the mug. It seems that there is an &quot;invisible&quot; barrier between gripper fingers and mugs. If I lift the gripper the mug stays where it was.</p>
<p>However, if I run the same code using the Franka Panda robot (slight adaptions with joints) I can lift the mug.
What is my problem? Is it related to the urdf file?</p>
<p>This is my code:</p>
<pre><code>import os 
import pybullet as pb
import pybullet_data
import math
import time
import pybullet_data as pd 
jaco_path = 'jaco_reza/j2n6s300.urdf'
pb.connect(pb.GUI)
path = pd.getDataPath()

jaco_orientation_euler = [0,math.pi/2,0]
jaco_orientation_quaternion = pb.getQuaternionFromEuler(jaco_orientation_euler)
jacoUid = pb.loadURDF(os.path.join(jaco_path,), useFixedBase=True, basePosition=[-0.63,0,0.46], baseOrientation=jaco_orientation_quaternion)

pos, ori = pb.getBasePositionAndOrientation(jacoUid)
tableUid = pb.loadURDF(os.path.join(pybullet_data.getDataPath(), &quot;table/table.urdf&quot;),basePosition=[0.5,0,-0.66])

mug_orientation_euler = [0,0,-math.pi/2]
mug_orientation_quaternion = pb.getQuaternionFromEuler(mug_orientation_euler)

mugUid = pb.loadURDF(os.path.join(pybullet_data.getDataPath(), &quot;objects/mug.urdf&quot;),basePosition=[0.148,0,-0.035], baseOrientation=mug_orientation_quaternion)

pb.setGravity(0,0,-10)

pb.resetDebugVisualizerCamera(
    cameraDistance=1.5, 
    cameraYaw=0, 
    cameraPitch=-20, 
    cameraTargetPosition=[-0.15,0,0.1])

# #init Jaco Position
rest_poses = [0,0,0,1.5,1.4,1.5,0,1.5,3,0,0.5,0,0.5,0,0.5]
for i in range(15):
     pb.resetJointState(jacoUid,i, rest_poses[i])


fingerAngle = 0.7 

for i in range(180):
    pb.setJointMotorControl2(jacoUid,9,pb.POSITION_CONTROL,targetPosition=fingerAngle,force=2000)
    pb.setJointMotorControl2(jacoUid,11,pb.POSITION_CONTROL,targetPosition=fingerAngle,force=1000)
    pb.setJointMotorControl2(jacoUid,13,pb.POSITION_CONTROL,targetPosition=fingerAngle,force=1000)
    pb.stepSimulation()
    fingerAngle += 0.1 / 100.
    if fingerAngle &gt; 2:
        fingerAngle = 2 #upper limit
    time.sleep(0.02)

for i in range(20):
    print(pb.getJointState(jacoUid,4)[0])
    pb.setJointMotorControl2(jacoUid, 4,
                                        pb.POSITION_CONTROL,
                                        targetPosition=1)
    pb.stepSimulation()

while True:
    pb.configureDebugVisualizer(pb.COV_ENABLE_SINGLE_STEP_RENDERING) 
    pb.stepSimulation()

</code></pre>
<p>I tried to change the .urdf file of the Kinova robotic but it was not working and I have the feeling that I am missing an important step.</p>
",2023-04-27 14:40:00,,116,0,0,1,,21755651.0,,4/27/2023 14:18,3.0,,,,,,,
4562,75920792,How to do robot lokalization while robot moves between walls and using Lidar and Vusal SLAM?,|localization|ros|robotics|,"<p>have a mobile robot equipped with 2DLidar, Stereo RGBD Camera , IMU Unit and wheels encoders. The robot is driving in a corridors area (means between walls-obstacles). So i Have ROS node that gives me distance from the robot to the obstacles-walls that are detected. ALso in the same node have the angle from the robot to those obstacles -walls. So also I have visual SLAM such as ORB_SLAM2 that gives me robot pose but with the time is not accurate. So I would like to use information from ROS LIDAR node that detect the walls and the distance and angle to the robot to get accurate pose (to say correct the VISUAL SLAM pose. ) I dont wonna do any ICP-scan matching using AMCL or Hector SLAM .</p>
<p>As said i like to use the LIDAR node with walls and distance to get accurate localization .I think the way is to use ROS transformation faction (tf) to correct the pose. But not sure is that right way. Any help?</p>
<p>So I have the following topics that can help me for the lokalization.</p>
<ul>
<li><p>/robot/data/vehicle_state : Current speed and yawrate calculated from
wheel speed.</p>
<p>/robot/data/twist :Twist data created from vehicle speed and IMU
data.</p>
<p>/robot/data/vslam_localization/pose :Output pose from ORB_SLAM</p>
<p>/camera/left/image_raw and /camera/right/image_raw : Images from
camera</p>
<p>/scan : YDLidar scan</p>
<p>/imu/sensor_msgs/Imu : IMU data</p>
</li>
</ul>
<p>And when use EKF then</p>
<ul>
<li><p>/robot/localization/ekf_base/set_pose External command to reset pose
of EKF for base_link</p>
<p>/robot/localization/ekf_odom/set_pose External command to reset pose
of EKF for odom</p>
</li>
</ul>
<p>And the following tf:</p>
<p>tf
map- map frame
odom- odom frame
base_link- vehicle pose (center of driving wheel axis)
slam_base -a frame where ORB_SLAM is working</p>
<p>Any help?</p>
<p>Thanks</p>
",2023-04-03 14:50:00,,46,0,1,0,,10319366.0,Singapore,9/5/2018 9:27,109.0,,,,,,133961129.0,Looks like this is the question I was hinting at in my [last comment here](https://stackoverflow.com/a/75886118/6411540). Looking at your question history it would really help if you could separate all the issues you are having into small concise questions with good formatting. And hint at the new questions instead of [moving the goalpost](https://stackoverflow.com/a/75886118). I'll later write an answer for this as well.
4715,77570751,L298n motor control with Raspberry pi,|python|python-3.x|raspberry-pi3|robotics|motordriver|,"<p>I was trying to implement a single L298n motor driver with a 4WD robot using 2 motors connected to a Raspberry Pi. Now, I can turn on and off the L298n driver (the speed of both the motors remains the same) but cannot set a variable PWM for each of the motors, even if I try to put different speed values they remain the same.</p>
<p>NOTE:
What I am trying to accomplish after getting proper motor control is that I want to control the driver from a virtual joystick.</p>
<p>My code follows:</p>
<pre class=""lang-py prettyprint-override""><code>import RPi.GPIO as GPIO
import time
import math
import smbus
import firebase_admin
from firebase_admin import credentials, db

# Initialize Firebase with the private key
cred = credentials.Certificate(&quot;/home/pi/Desktop/GCC/firebase_credentials.json&quot;)
firebase_admin.initialize_app(cred, {
    'databaseURL': '#################################'
})

# Reference to the Firebase Realtime Database for commands and sensor data
command_ref = db.reference('/command')
sensor_ref = db.reference('/Mobile')

#Board connect
GPIO.setmode(GPIO.BCM)

#forward
in1 = 20
in3 = 5
en1 = 12

#backward
in2 = 21
in4 = 19
en2 = 13

GPIO.setup(in1, GPIO.OUT)
GPIO.setup(in2, GPIO.OUT)
GPIO.setup(in3, GPIO.OUT)
GPIO.setup(in4, GPIO.OUT)
GPIO.setup(en1, GPIO.OUT)
GPIO.setup(en2, GPIO.OUT)

#duty cycles for l298n motor
p1 = GPIO.PWM(en1, 1000)
p2 = GPIO.PWM(en2, 1000)
p1.start(0)
p2.start(0)

motorSpeed1 = 0;
motorSpeed2 = 100;


def main():
    while True:
        data = command_ref.get()
        if data is not None:
            joystick_mode = data.get('joystick_mode', False)
            if joystick_mode:
                joystick_data = db.reference('/Joystick movement').get()
                x = joystick_data.get('xPercent')
                y = joystick_data.get('yPercent')
#                print(&quot;x {:.3f}, y {:.3f}&quot;.format(x,y))
                GPIO.output(in1, GPIO.LOW)
                GPIO.output(in2, GPIO.HIGH)
                GPIO.output(in3, GPIO.LOW)
                GPIO.output(in4, GPIO.HIGH)
                
                p1.ChangeDutyCycle(motorSpeed1)
                p2.ChangeDutyCycle(motorSpeed2)

                
                time.sleep(5)
                GPIO.output(in1, GPIO.LOW)
                GPIO.output(in2, GPIO.LOW)
                GPIO.output(in3, GPIO.LOW)
                GPIO.output(in4, GPIO.LOW)
                
                return

if __name__ == &quot;__main__&quot;:
    try:
        main()
    except KeyboardInterrupt:
        print(&quot;Program stopped&quot;)
    finally:
        GPIO.cleanup()
</code></pre>
<p>I have tried checking on the hardware connections, which seems correct; how do I set the correct PWM?</p>
",2023-11-29 11:28:00,,103,0,0,0,,23006805.0,,11/29/2023 11:02,1.0,,,,,,,
4515,75451788,What are the differences between exec and name in ros2 launch file?,|launch|robotics|ros2|,"<p>I'am a ros2 noob and I would like to understand differences between exec and name in launch file.</p>
<p>I'll give an example to be precise:</p>
<pre><code>&lt;launch&gt;
    &lt;node pkg=&quot;camera&quot; exec=&quot;&quot; name=&quot;&quot;&gt;
&lt;/launch&gt;
</code></pre>
<p>Thanks!</p>
",2023-02-14 18:19:00,75541108.0,233,1,0,0,,15496345.0,,3/27/2021 20:23,4.0,75541108.0,"<ul>
<li>exec: The filename of the executable</li>
<li>name: The name that <code>ros2 node list</code> shows</li>
</ul>
<p>For example, from the <a href=""https://docs.ros.org/en/foxy/How-To-Guides/Launch-file-different-formats.html"" rel=""nofollow noreferrer"">tutorial comparison in launch files</a>:</p>
<p>There is a line that sets both <code>name</code> and <code>exec</code></p>
<pre class=""lang-xml prettyprint-override""><code>&lt;node pkg=&quot;turtlesim&quot; exec=&quot;turtlesim_node&quot; name=&quot;sim&quot; namespace=&quot;turtlesim2&quot;&gt;
</code></pre>
<p>When I created a package to run that, here's what happens.</p>
<pre class=""lang-bash prettyprint-override""><code>$ ros2 node list
/listener
/mimic
/my/chatter/ns/listener
/my/chatter/ns/talker
/talker
/turtlesim1/sim
/turtlesim2/sim
</code></pre>
<p>You can see how the node name shows up, with an extra caveat - because namespace was also added, it appears as <code>&lt;namespace&gt;/&lt;name&gt;</code>.</p>
<p>Now, for the executable, you can just go see that in your ROS installation directory. For example, on my Linux computer running <code>humble</code>, there's the executable. I added <code>-l</code> to show it's executable.</p>
<pre><code>$ ls -l /opt/ros/humble/lib/turtlesim/turtlesim_node 
-rwxr-xr-x 1 root root 798384 Jan 17 18:00 /opt/ros/humble/lib/turtlesim/turtlesim_node
</code></pre>
<p>An easy way to find out the executables in your package is through tab complete with <code>ros2 pkg executables &lt;package_name&gt;</code></p>
<pre><code>$ ros2 pkg executables turtlesim
turtlesim draw_square
turtlesim mimic
turtlesim turtle_teleop_key
turtlesim turtlesim_node
</code></pre>
<p>The reason you have both is because you may re-use the same executable multiple times in a DDS domain, so the <code>name</code> , or <code>namespace</code>, can be used to differentiate them.</p>
",11032285.0,0.0,0.0,,
4690,77368485,How to put a label to an object in CoppeliaSim,|simulator|robotics|,"<p>I want to add a label to a shape in Coppelia.</p>
<p>I read about using a simSetObjectName(), but it does not appear in the RegularAPI documentation.
I read also about banners, but I do not find neither documentation about it.</p>
<p>Can you tell me please what to use in 2023.</p>
<p>For what I need, a Banner or something similar will work better than a renaming.</p>
<p>Thanks.</p>
<p>I already tried searching in the RegularAPI documentation. Please do not use simx functions because I am using a newer client. :)</p>
<p>I want to have like a little post-it or label attached to the object.</p>
",2023-10-26 15:41:00,,22,0,0,0,,22694257.0,,10/6/2023 8:42,4.0,,,,,,,
4668,77153709,Executable in a non-ROS workspace links to an executable in a ROS workspace,|ubuntu|cmake|linker|ros|robotics|,"<p>I'm working in ubuntu 20.04 and have ROS noetic. I have a catkin workspace (catkin_ws), and it has DBow2 in it at <code>/home/glenn/catkin_ws/devel/lib/libDBoW2.so</code>. I also have the library <a href=""https://github.com/UZ-SLAMLab/ORB_SLAM3"" rel=""nofollow noreferrer"">ORB-SLAM3</a> installed on my system. It uses DBoW2 and has it installed in its <code>Thirdparty</code> folder, so after building this package, its location is at <code>/home/glenn/ORB_SLAM3/Thirdparty/DBoW2/lib/libDBoW2.so</code>.</p>
<p>When I try to run any executable from ORB-SLAM3, for example, <code>./Monocular/mono_euroc ../Vocabulary/ORBvoc.txt ./Monocular/EuRoC.yaml ~/Documents/Trial_Data/euroc/V1_01 ./Monocular/EuRoC_TimeStamps/V101.txt dataset-V101_mono</code>, it gives the following error:</p>
<p><code>./Monocular/mono_euroc: symbol lookup error: /home/glenn/ORB_SLAM3/lib/libORB_SLAM3.so: undefined symbol: _ZN5DBoW24FORB1LE</code></p>
<p>When I typed <code>ldd -r /home/glenn/ORB_SLAM3/lib/libORB_SLAM3.so</code>, I get a large output, but the relevant one to my issue is the following line:</p>
<p><code> libDBoW2.so =&gt; /home/glenn/catkin_ws/devel/lib/libDBoW2.so (0x00007fd840758000)</code></p>
<p>which shows it's pointing to the DBoW library from the catkin workspace and not the ORB_SLAM3 workspace. To get around this, I simply comment out <code>source ~/catkin_ws/devel/setup.bash</code> line in my bashrc and source it again, the same line from the same command gets the right output:</p>
<p><code>libDBoW2.so =&gt; /home/glenn/ORB_SLAM3/Thirdparty/DBoW2/lib/libDBoW2.so (0x00007f9136949000)</code></p>
<p>and everything in ORB_SLAM3 works fine again.</p>
<p>I just wanted to know if there was a better way to manage a shared library looking in the wrong place at runtime without having to constantly comment out the line to source my catkin workspace? From ORB_SLAM3's CMakeLists.txt <a href=""https://github.com/UZ-SLAMLab/ORB_SLAM3/blob/master/CMakeLists.txt#L118-L126"" rel=""nofollow noreferrer"">lines 118-126</a>, it seems they do link to the appropriate DBoW library under the Thirdparty folder. I was thinking of possibly setting the RPATH by adding the following lines to the top-level CMakeLists.txt:</p>
<pre><code>set(CMAKE_SKIP_BUILD_RPATH FALSE)

set(CMAKE_BUILD_WITH_INSTALL_RPATH TRUE)
</code></pre>
<p>but that gave significantly more errors for libORB_SLAM3.so. Any advice would be appreciated. Thank you.</p>
",2023-09-21 21:20:00,,52,0,0,0,,16484468.0,,7/20/2021 1:34,7.0,,,,,,,
4538,75603072,How to activate the exact mesh of a link for contact simulation on Drake?,|python|simulation|robotics|drake|urdf|,"<p>We are trying to simulate the contact of a two-link brachiating robot with unactuated(hook-shaped) grippers on the support bar of a horizontal ladder. The following image(img4.png) is the .obj file of one of the links, opened in MeshLab. More details may be found at: <a href=""https://github.com/dfki-ric-underactuated-lab/acromonk"" rel=""nofollow noreferrer"">https://github.com/dfki-ric-underactuated-lab/acromonk</a></p>
<p>To simplify our task, we are first trying to simulate the hooking motion when the robot falls(due to gravity) over the support bar. The robot is given an initial configuration such that the gripper is exactly above the support bar. Theoretically, as the robot falls, the gripper clings onto the support bar and starts oscillating.</p>
<p>The problem is that gripper does not cling onto the support bar(as shown in the video, hi.gif, hell.gif, and images, img1.png, img2.png, &amp; img3.png). We believe that the simulator applies a collision model such that gripper hook is completely enveloped, and the cavity of the hook is disregarded(img5.png). This is happening inspite of including the .obj file shown above as the geometry mesh in the collision tag of the robot URDF. How do we correct this, and make the simulator consider the mesh file of the link as the collison model?</p>
<p>P.S. The other parts of the horizontal ladder don't have their collision model yet. Only the collision models of the robot and the support bar are active.</p>
<p><a href=""https://i.stack.imgur.com/mZeda.png"" rel=""nofollow noreferrer"">img4.png</a>
[img1.png[img2.png[img3.png<a href=""https://i.stack.imgur.com/7YF04.png"" rel=""nofollow noreferrer"">img5.png</a>](<a href=""https://i.stack.imgur.com/DNYTq.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/DNYTq.png</a>)](<a href=""https://i.stack.imgur.com/jmgnd.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/jmgnd.png</a>)](<a href=""https://i.stack.imgur.com/wU7WN.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/wU7WN.png</a>)</p>
<p><a href=""https://i.stack.imgur.com/VVQUt.gif"" rel=""nofollow noreferrer"">hi.gif</a></p>
<p>Thanks in advance,
Regards.</p>
",2023-03-01 11:27:00,75604333.0,136,2,0,1,,21310962.0,Bremen,3/1/2023 10:55,15.0,75604333.0,"<p>That's a great robot. Let's see if we can't get you set up properly.</p>
<p>tl;dr: Use <a href=""https://drake.mit.edu/doxygen_cxx/group__hydroelastic__user__guide.html"" rel=""nofollow noreferrer"">hydroelastic contact</a>; make the gripper rigid and make the cylindrical bars compliant.</p>
<p><strong>Details</strong></p>
<p>I'm assuming that you're using the default configuration for simulation. This means that you're using <em>point</em> contact to model contact between objects. That means that you're using <a href=""https://drake.mit.edu/doxygen_cxx/classdrake_1_1geometry_1_1_query_object.html#af3b16adde799eb804619dc86f3703f4f"" rel=""nofollow noreferrer"">this query</a>. This query has the property that arbitrary mesh objects are represented by their contact hulls. Exactly what you're seeing.</p>
<p>Drake has another, novel method for evaluating contact between bodies: <a href=""https://drake.mit.edu/doxygen_cxx/group__hydroelastic__user__guide.html"" rel=""nofollow noreferrer"">hydroelastic contact</a>. The linked guide should walk you through everything you need to do to tweak your simulation so that your hook is nicely interacting with the bars. The steps are along the lines of:</p>
<ol>
<li>Tweak your URDF to introduce the important Drake-specific hydroelastic properties. (<code>MultibodyPlant</code> is configured to make use of that information by default.)</li>
<li>Add flags to the hook to make it have a <em>rigid</em> hydroelastic representation (i.e., <code>&lt;drake:compliant_hydroelastic/&gt;</code>).</li>
<li>Replace the <em>mesh</em> cylinders with <code>&lt;cylinder&gt;</code> declarations that you can specifically declare to be <code>&lt;drake:compliant_hydroelastic/&gt;</code>.</li>
</ol>
<p>The rigid hook will interact with the compliant rods without any recourse to the convex hull.</p>
<p>I'd also recommend configuring the <code>MultibodyPlant</code> in <a href=""https://drake.mit.edu/doxygen_cxx/classdrake_1_1multibody_1_1_multibody_plant.html#a3aeb03d90d213da4702f2dc5e484fc86"" rel=""nofollow noreferrer""><em>discrete</em> mode</a> and also consider setting the <a href=""https://drake.mit.edu/doxygen_cxx/classdrake_1_1multibody_1_1_multibody_plant.html#a217d8aa9218c9d981b7bc2e8a983c4be"" rel=""nofollow noreferrer"">contact solver to SAP</a>.</p>
<hr />
<p>If hydroelastic contact proves problematic, you'll have to switch to a convex decomposition of your hook. Some Drake users have had success using <a href=""https://github.com/kmammou/v-hacd"" rel=""nofollow noreferrer"">v-hacd</a> to do the offline decomposition and using the resultant <em>family</em> of geometries in the URDF as the set of collision geometries.</p>
",7686256.0,1.0,2.0,,
4586,76231952,Inverse Rotation and Translation for Pinhole Projection,|python|camera|projection|robotics|perspectivecamera|,"<p>So, I feel really dumb to try to solve this problem for so long. I really need help.</p>
<p>I am trying to project the camera pixel coordinate to the world coordinate using Pinhole perspective projection. But somehow the rays I generate has weird rotation (direction). So the function is:</p>
<pre><code>def pixel_coords_to_world_coords_ray(pixel_coords, camera_matrix, distortion_coeffs, R_world_to_camera, t_world_to_camera, plane_normal_world, plane_distance_world):
    
    pixel_coords_undistorted, newcameramtx = undistort_contour_pixel(pixel_coords, camera_matrix, distortion_coeffs)
    pixel_coords_undistorted = np.squeeze(pixel_coords_undistorted, axis=1)
    pixel_coords_homogeneous = np.hstack((pixel_coords_undistorted, np.ones((len(pixel_coords_undistorted), 1))))
    
    # Invert the camera matrix to get pixel to ray conversion matrix
    camera_matrix_inv = np.linalg.inv(newcameramtx)
    
    # Compute the light ray direction in the camera frame.
    ray_directions_camera = (camera_matrix_inv @ pixel_coords_homogeneous.T)[:3, :]

    # Transform the light ray direction to the world frame
    R_camera_to_world = R_world_to_camera.T
    t_camera_to_world = -R_camera_to_world @ t_world_to_camera
    ray_directions_world = R_camera_to_world @ ray_directions_camera
    
    # Normalize the rays to have unit length
    ray_directions_world /= np.linalg.norm(ray_directions_world, axis=0)
    
    # Compute the intersection between the light rays and the plane
    ray_origin_world = t_camera_to_world.reshape(3)
    world_coords = []
    for ray_direction_world in ray_directions_world.T:
        intersection = ray_plane_intersection(ray_origin_world, ray_direction_world, plane_normal_world, plane_distance_world)
        if intersection is not None:
            world_coords.append(intersection)
    world_coords = np.array(world_coords)

    return world_coords
</code></pre>
<ul>
<li>So I've tried to debug it by setting the input pixel to be (w/2, h/2). It results to the expected value of ray_directions_camera = near [0,0,1]</li>
</ul>
<p>But somehow the rotation I get in the ray's direction in world frame is wrong.</p>
<ul>
<li>I also plot the result, the transformation and rotation of the camera. So the translation and rotation must be right.</li>
</ul>
<p><a href=""https://i.stack.imgur.com/s4hY6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s4hY6.png"" alt=""enter image description here"" /></a></p>
<p>the red line pointing downward at the middle is the resulting ray for the pixel (w/2, h/2)</p>
<p>Am I missing something here? It's been a week I am trying to solve this with many different method. I really need to finish this. Thank you all.</p>
",2023-05-11 22:18:00,,43,0,0,0,,7053355.0,braunschweog,10/21/2016 13:22,5.0,,,,,,,
4750,77923548,Multi layer abstraction design for physical liquid handlers,|architecture|robotics|,"<p>I am attempting to implement a solution for our robotic liquid handlers. The problem is that we have many different vendor liquid handlers in our company and even liquid handlers from the same company can differ. So I want to create a multi layer abstraction to simplify the programming. We cannot use open source projects hence why I need to do this internally. We have also not found any good open source options anyway.</p>
<p>As a high level overview:
You have a liquid handler that picks up tips and performs pipetting operations, this robot also has a plate arm that pickup and move plates around the pipetting deck. Nothing in the physical world is perfect so of course the liquid handler will occasionally run into errors. Also, even if the liquid handler is working perfectly user may incorrectly load something on the deck that could also cause an error. What I want to do is create a multi layer abstraction to help with programming but also to simplify error handling.</p>
<p>Per programming design I want to compose the layers in a way which is as decoupled as possible, and therein lies my problem:</p>
<ol>
<li>My driver layer is for raw command execution. Lots of low level errors can occur here.</li>
<li>My HAL layer is for simplified execution and the for the majority of error handling. Some errors will require user input (a UI dialogue or similar) and a retry (repeat the action or cleanup and then repeat).</li>
<li>My API layer will group HAL devices and commands together to create very complex behaviors.</li>
</ol>
<p>So my question is this: <strong>How can I design a system in such a way that I can handle the UI portion at such a low level without implementing UI components in the HAL or API layer?</strong> According to everything I have read the UI should be implemented in a UI layer or even dependency injection for even more flexibility.</p>
<p>I can add additional information as requested!</p>
<p>Thanks!</p>
",2024-02-01 21:42:00,,11,0,0,0,,6231022.0,,4/20/2016 14:58,19.0,,,,,,,
4797,78231595,Linear trinagulation yields poor results,|computer-vision|robotics|visual-odometry|,"<p>I am working ona  project regarding visual odometry and I have trouble extracting the 3D coordinates of matched features using the method of linear triangulation. I a using the KITTI dataset and specifically I am using the synced and rectified data. For simplification purposes i have chosen a feature that is detected only in the first and second camera frame. Since the rotation and translation of the camera poses are not given directly i have chosen to use the GPS/IMU rotation and translation matrices and use the relevant transformations to calculate the camera rotation and translation. acoording to equation (8) of the relevant papaer (<a href=""https://www.cvlibs.net/publications/Geiger2013IJRR.pdf"" rel=""nofollow noreferrer"">https://www.cvlibs.net/publications/Geiger2013IJRR.pdf</a>) a 3D point in GPS IMU coordinates (current coordinate system of the GPS/IMU) is projected to a 2D point in the cameras frame as follows :
y = P_rect<em>R_rect</em>Tvelo_cam*Timu_velo, where P_rect is the projection matrix of the camera after rectification. R_rect is the rectifying rotation matrix , Tvelo_cam is the transformation from the velodyne's coordinate frame to the cameras frame and Timu_velo is the transformation from the IMU's coordinate frame to the velodyne's coordinate frame. Note that I have chosen the first pose of the IMU as the origin of my world coordinate frame, meaning at point (0,0,0) the IMU has not rotated or moved.</p>
<p>Assuming that X is computed with respect to the real world coordinate frame, the results dont make sense : Specifically X,Y,Z = array([-10.55149986,   4.61478588,   2.40134485]). By taking into account the configuration of the worlds coordinate frame (x axis forward, y axis left and z axis up) this means that the point was detected behind the IMU (X&lt;0), which is impossible because the IMU is behind the camera. I would appreciate any kind of help, since I am stuck a long time in this.</p>
<pre class=""lang-py prettyprint-override""><code>gray1 = images[0,:,:]

gray2 = images[1,:,:]

kp1 , ds1 = sift.detectAndCompute(gray1, None)

kp2 , ds2 = sift.detectAndCompute(gray2, None)


bf = cv2.BFMatcher()
matches12 = bf.knnMatch(ds1, ds2, k=2)

       
        

ratio_thresh = 0.4
good_matches = []
for m, n in matches12:
    if m.distance &lt; ratio_thresh * n.distance:
        good_matches.append(m)

coordinates_kp1 = []
coordinates_kp2 = []

for match in good_matches:
    # Get the coordinates in the first image
    x1, y1 = kp1[match.queryIdx].pt
    coordinates_kp1.append((x1, y1))
    
    # Get the coordinates in the second image
    x2, y2 = kp2[match.trainIdx].pt
    coordinates_kp2.append((x2, y2))

    
XY1 = coordinates_kp1[440]
XY2 = coordinates_kp2[440]

#%% Slides

K2 = P0@R0@T_VelotoCam@T_IMUtoVelo
Rt1 = Rt_IMU[:2]

tracks = np.vstack((XY1,XY2)).T



A = np.zeros((2 * tracks.shape[1], 4))




P1 = K2@Rt1[0]
P2 = K2@Rt1[1]


A[0,:] = P1[2,:]*tracks[0,0] - P1[0,:]
A[1,:] = P1[2,:]*tracks[1,0] - P1[1,:]
A[2,:] = P2[2,:]*tracks[0,1] - P2[0,:]
A[3,:] = P2[2,:]*tracks[1,1] - P2[1,:]

U, S, Vt = np.linalg.svd(A)

V = np.transpose(Vt)

Xtilde = V[:, -1]

X = Xtilde[0:3] / Xtilde[3]
</code></pre>
",2024-03-27 12:13:00,,11,0,0,0,,23839991.0,,3/27/2024 11:31,2.0,,,,,,,
4773,78127251,trouble with Python package import command in Spyder5,|python|math|anaconda|robotics|,"<p>having the need to use a package not included in the Anaconda repository (<a href=""https://github.com/NxRLab/ModernRobotics"" rel=""nofollow noreferrer"">https://github.com/NxRLab/ModernRobotics</a>), I followed the instructions in the official Anaconda documentation and installed, running the Anaconda command terminal as administrator, pip and then followed the package of interest with the instruction <em>pip install modern-robotics</em>, receiving the positive result of the installation as feedback.
Afterwards I checked the presence of the package with <em>conda list</em> and then, for greater security, from Anaconda Navigator Environment, seeing it regularly installed.</p>
<p>Now, in a python script in Spider5, I entered <em>import modern-robotics as mr</em>, but I get the red error incorrect syntax.</p>
<blockquote>
<p>Invalid sintax pyfLakes E</p>
</blockquote>
<p>From the Code Analyzer report in Spyder IDE, I've</p>
<blockquote>
<p><em>************* Module robotics_test</em>
<em>E0001:syntax-error: 11,14: Parsing failed: 'invalid syntax (&lt;unknown&gt;, line 11)'</em></p>
</blockquote>
<p>But the package is there !
Having suspected that there might be inconsistencies in the updating of the packages as seen by Spider IDE, I closed the entire environment (including Anaconda Navigator) and restarted it, but the problem remains. Needless to say, I also trivially turned the PC off and on again.</p>
<p>I am not a guru of Python or other languages, I use them for scientific needs, but now I don't understand the origin of the error.</p>
",2024-03-08 11:06:00,,22,0,0,0,,23559725.0,,3/8/2024 10:41,2.0,,,,,,,
4741,77800615,Servo Controller Fail State,|stm32|robotics|encoder|servo|,"<blockquote>
<p>Hi, I am relatively new to programming and the robotics world, so I apologize if this is trivial. I am trying to control a DXW90 Servo using a KY-040 standard Rotary Encoder. The microcontroller I am using is NUCLEO-L476RG.</p>
<p>Below is the current code I have scrapped together from a few tutorials I have watched. While the code returns no errors my Putty won't read anything past &quot;Starting Up&quot; and the encoder dose not control the servo at all. I feel like I need to write a fail statement, but am unsure where to begin.</p>
<p>I appreciate any time or advice anyone can provide me on my code, or on how to get into robotics in general.</p>
</blockquote>
<pre><code>int counter=0;
uint16_t PWMVal =0;
uint8_t dir;
uint8_t MSG[50] = {'\0'};
/* USER CODE END 0 */

/**
* @brief  The application entry point.
* @retval int
*/
int main(void)
{
/* USER CODE BEGIN 1 */

/* USER CODE END 1 */

/* MCU Configuration--------------------------------------------------------*/

/* Reset of all peripherals, Initializes the Flash interface and the Systick. */
HAL_Init();

/* USER CODE BEGIN Init */

/* USER CODE END Init */

/* Configure the system clock */
SystemClock_Config();

/* USER CODE BEGIN SysInit */

/* USER CODE END SysInit */

/* Initialize all configured peripherals */
MX_GPIO_Init();
MX_USART1_UART_Init();
MX_TIM1_Init();
/* USER CODE BEGIN 2 */
sprintf(MSG, &quot;Starting Up&quot;);

HAL_UART_Transmit(&amp;huart1, MSG, sizeof(MSG), 100);
HAL_TIM_PWM_Start(&amp;htim1, TIM_CHANNEL_1);

/* USER CODE END 2 */

/* Infinite loop */
/* USER CODE BEGIN WHILE */
while (1)
{

/* USER CODE END WHILE */

/* USER CODE BEGIN 3 */
  if(HAL_GPIO_ReadPin(GPIOA, GPIO_PIN_1) == GPIO_PIN_RESET) //If the OUTA is RESET
  {
        if(HAL_GPIO_ReadPin(GPIOA, GPIO_PIN_2) == GPIO_PIN_RESET)// If the OUTB is also RESET
      {
while(HAL_GPIO_ReadPin(GPIOA, GPIO_PIN_2) == GPIO_PIN_RESET){}; // Wait for OUTB to go high
        counter++;
        dir = &quot;CW&quot;;
while(HAL_GPIO_ReadPin(GPIOA, GPIO_PIN_1) == GPIO_PIN_RESET){}; // Wait for OUTA to go high
        HAL_Delay(10); //Wait for some time
      }

      if(HAL_GPIO_ReadPin(GPIOA, GPIO_PIN_2) == GPIO_PIN_SET)// If the OUTB is also SET
      {
while(HAL_GPIO_ReadPin(GPIOA, GPIO_PIN_2) == GPIO_PIN_SET){}; // Wait for OUTB to go low
        counter--;
        dir = &quot;CCW&quot;;
while(HAL_GPIO_ReadPin(GPIOA, GPIO_PIN_1) == GPIO_PIN_RESET){}; // Wait for OUTA to go high
while(HAL_GPIO_ReadPin(GPIOA, GPIO_PIN_2) == GPIO_PIN_RESET){}; // Wait for OUTB to go high
        HAL_Delay(10); //Wait for some time
      }
 if (counter&lt;0) counter = 0;
 if (counter&gt;180) counter =180;
  }
  PWMVal = counter*55/10;
  htim1.Instance-&gt;CCR1 = 250 + PWMVal;
// NEED TO ADD A FAILED STATE SOMEWHERE EITHER AN ELSE STATEMENT OR SOMETHING OF THAT NATURE
 if(HAL_GPIO_ReadPin (GPIOA, GPIO_PIN_2))
 {
     sprintf(MSG, &quot;Counter= %d\n\r&quot;, counter);
     HAL_UART_Transmit(&amp;huart1, MSG, sizeof(MSG), 100);
 }
 HAL_Delay(100);
</code></pre>
<p>` }</p>
",2024-01-11 14:00:00,,14,0,1,0,,18486691.0,,3/16/2022 20:11,1.0,,,,,,137183558.0,"You have to start debugging your code more deeply.  ```HAL_UART_Transmit(&huart1, MSG, sizeof(MSG), 100);``` is a very good way of debugging. Use this to understand where your code stops working or crashes or know what is the state of your variables."
4760,78046244,Obtaining rotation and translation matrix from homogeneous transformation matrix,|c++|eigen|robotics|eigen3|,"<p>I have following piece of code:</p>
<pre><code>Eigen::Matrix4f transformation = myMethod(); // homogenous transformation matrix
std::cout &lt;&lt; &quot;transformation = &quot; &lt;&lt; std::endl &lt;&lt; transformation &lt;&lt;  std::endl &lt;&lt; std::endl;
Eigen::Isometry3f estimate = Eigen::Isometry3f(transformation);
r = estimate.rotation(); // rotation matrix
t = estimate.translation(); // translation matrix
std::cout&lt;&lt;&quot;r = &quot; &lt;&lt; std::endl &lt;&lt; r &lt;&lt; std::endl &lt;&lt; std::endl;
std::cout&lt;&lt;&quot;t = &quot; &lt;&lt; std::endl &lt;&lt; t &lt;&lt; std::endl &lt;&lt; std::endl;
</code></pre>
<p>It prints:</p>
<pre><code>transformation = 
     1.0000002384185791  4.9265406687482027e-07 -4.5500800638365035e-07  -2.384185791015625e-07
-5.2062489430682035e-07      1.0000003576278687  5.4357258250092855e-07 -3.5762786865234375e-07
 4.9866633844430908e-07 -4.6970637868071208e-07      1.0000002384185791  -2.384185791015625e-07
                      0                       0                       0                       1

r = 
     1.0000001192092896  7.2572328235764871e-07 -3.9811814644963306e-07
-6.7004600623477018e-07     0.99999994039535522  4.5586514829665248e-07
  5.422648428066168e-07 -4.4537293319990567e-07     0.99999994039535522

t = 
 -2.384185791015625e-07
-3.5762786865234375e-07
 -2.384185791015625e-07
</code></pre>
<p>Here, I am trying to obtain rotation and translation matrices from homogeneous transformation matrix. I guess translation matrix is the first three elements of the last column of homogeneous matrix and rotation matrix is top left 3x3 matrix. Translation matrix is returned correctly above. But rotation matrix is not exactly the top left 3x3 matrix of the homogeneous matrix.</p>
<p>I guess I am missing some basic concept here. What am doing wrong?</p>
",2024-02-23 09:04:00,,85,1,7,1,,6357916.0,,5/19/2016 19:14,480.0,78047046.0,"<p>The documentation of <a href=""https://eigen.tuxfamily.org/dox/classEigen_1_1Transform.html"" rel=""nofollow noreferrer""><code>Transform</code></a> describes the different storage modes:</p>
<ul>
<li><strong>Affine</strong>: the transformation is stored as a (Dim+1)^2 matrix, where the last row is assumed to be [0 ... 0 1].</li>
<li><strong>AffineCompact</strong>: the transformation is stored as a (Dim)x(Dim+1) matrix.</li>
<li><strong>Projective</strong>: the transformation is stored as a (Dim+1)^2 matrix without any assumption.</li>
<li><strong>Isometry</strong>: same as <strong>Affine</strong> with the additional assumption that the linear part represents a rotation. This assumption is exploited to speed up some functions such as <code>inverse()</code> and <code>rotation()</code>.</li>
</ul>
<p>And this actually <a href=""https://gitlab.com/libeigen/eigen/-/blob/a6dc930d16be86cffba1f2926dea196abab0fe71/Eigen/src/Geometry/Transform.h#L1015"" rel=""nofollow noreferrer"">happens</a>, <code>Isometry</code> simply returns what was stored in the constructor:</p>
<pre><code>template &lt;typename Scalar, int Dim, int Mode, int Options&gt;
EIGEN_DEVICE_FUNC typename Transform&lt;Scalar, Dim, Mode, Options&gt;::RotationReturnType
Transform&lt;Scalar, Dim, Mode, Options&gt;::rotation() const {
  return internal::transform_rotation_impl&lt;Mode&gt;::run(*this);
}

template &lt;int Mode&gt;
struct transform_rotation_impl {
  template &lt;typename TransformType&gt;
  EIGEN_DEVICE_FUNC static inline const typename TransformType::LinearMatrixType run(const TransformType&amp; t) {
    typedef typename TransformType::LinearMatrixType LinearMatrixType;
    LinearMatrixType result;
    t.computeRotationScaling(&amp;result, (LinearMatrixType*)0);
    return result;
  }
};
template &lt;&gt;
struct transform_rotation_impl&lt;Isometry&gt; {
  template &lt;typename TransformType&gt;
  EIGEN_DEVICE_FUNC static inline typename TransformType::ConstLinearPart run(const TransformType&amp; t) {
    return t.linear();                     &lt;===================== here
  }
};
</code></pre>
<p>But, this is version <strong>v3.4</strong>. As @chtz points out in the comment, this description and the optimization itself are not present in <strong>v3.3</strong>, <a href=""https://gitlab.com/libeigen/eigen/-/blob/3.3/Eigen/src/Geometry/Transform.h?ref_type=heads#L1049"" rel=""nofollow noreferrer""><code>rotation</code></a> uses no &quot;switchboard&quot;, but directly calls a fixed function:</p>
<pre><code>template&lt;typename Scalar, int Dim, int Mode, int Options&gt;
EIGEN_DEVICE_FUNC const typename Transform&lt;Scalar,Dim,Mode,Options&gt;::LinearMatrixType
Transform&lt;Scalar,Dim,Mode,Options&gt;::rotation() const
{
  LinearMatrixType result;
  computeRotationScaling(&amp;result, (LinearMatrixType*)0);
  return result;
}
</code></pre>
<p><code>computeRotationScaling</code> (the method directly below the linked one) then calculates SVD, and that's where numerical inaccuracies can get introduced:</p>
<pre><code>template&lt;typename Scalar, int Dim, int Mode, int Options&gt;
template&lt;typename RotationMatrixType, typename ScalingMatrixType&gt;
EIGEN_DEVICE_FUNC void Transform&lt;Scalar,Dim,Mode,Options&gt;::computeRotationScaling(RotationMatrixType *rotation, ScalingMatrixType *scaling) const
{
  JacobiSVD&lt;LinearMatrixType&gt; svd(linear(), ComputeFullU | ComputeFullV);

  Scalar x = (svd.matrixU() * svd.matrixV().adjoint()).determinant(); // so x has absolute value 1
  VectorType sv(svd.singularValues());
  sv.coeffRef(0) *= x;
  if(scaling) scaling-&gt;lazyAssign(svd.matrixV() * sv.asDiagonal() * svd.matrixV().adjoint());
  if(rotation)
  {
    LinearMatrixType m(svd.matrixU());
    m.col(0) /= x;
    rotation-&gt;lazyAssign(m * svd.matrixV().adjoint());
  }
}
</code></pre>
<p><code>Isometry</code> in v3.3 is practically an <code>Affine</code> with some parts switched off (like <code>scale</code> stops you with an assert, and there are some more).</p>
<blockquote>
<p>I guess I am missing some basic concept here. What am doing wrong?</p>
</blockquote>
<p>Seemingly you're using an older version of the library. Consider trying the code with v3.4.</p>
",7916438.0,1.0,0.0,137591630.0,"Sorry, I made mistake while renaming my context specific variables. `translation()` and `rotation()` are methods of variable `Eigen::Isometry3f estimate`. Now corrected that."
4757,78027853,"Robot that moves with putting in X,Y coordinates of a U shaped field",|python|robotics|dji-sdk|,"<p>So i just started a project for an employer. One of the things they want is a robot that runs on Python and can be controlled from anywhere with a internet connection and a laptop. They hired another person before me who convinced the company to buy a DJI Robomaster EP core<a href=""https://www.dji.com/nl/robomaster-ep"" rel=""nofollow noreferrer"">this</a>.
This robot is programmed in the robomasters app you can download from their website. Now one of the things i need to do is to make it possible to use pycharm to code the robot. So far i havent been able to do that.
Secondly i need to get the robot to move by giving it an X and Y coordinate (which must work from distance). I have been able to get a sort of code for the moving to X,Y and another function which returns it to the starting position(X=0,Y=0), but I haven't tested that yet as I need to figure something out to use pycharm to program the robot. The second problem with the code is that the area it needs to drive in is U shaped, and it cant hit the object in the middle as it is very expensive.</p>
<p>The code atm:</p>
<pre><code>import time
from dji_robomaster_sdk import RoboMaster
from dji_robomaster_sdk import robot
from dji_robomaster_sdk import armor
from dji_robomaster_sdk import chassis

# Initialize the robot
robo = RoboMaster()
robo.initialize()

# Define the starting position
starting_position = (0, 0)

# Define the U-shaped area
area_width = 100
area_height = 50

# Function to make the robot go back to the starting position
def go_to_starting_position():
    global starting_position
    chassis.move(0, 0, 0, 0).wait_for_completed()
    chassis.move(-starting_position[0], 0, 0, 0).wait_for_completed()
    chassis.move(0, 0, 0, 0).wait_for_completed()

# Function to check if the given coordinates are within the U-shaped area
def is_within_area(x, y):
    global area_width, area_height
    return -area_width / 2 &lt;= x &lt;= area_width / 2 and -area_height / 2 &lt;= y &lt;= area_height / 2

# Function to make the robot go to the given coordinates
def go_to_coordinates(x, y):
    global starting_position
    if not is_within_area(x, y):
        print(&quot;The given coordinates are outside the U-shaped area.&quot;)
        return
    if x &lt; 0:
        x = 0
    if y &lt; 0:
        y = 0
    if x &gt; area_width:
        x = area_width
    if y &gt; area_height:
        y = area_height
    chassis.move(x - starting_position[0], y - starting_position[1], 0, 0).wait_for_completed()
    starting_position = (x, y)

# Example usage
go_to_coordinates(50, 0)
time.sleep(2)
go_to_coordinates(0, 25)
time.sleep(2)
go_to_coordinates(-50, 0)
time.sleep(2)
go_to_coordinates(0, -25)
time.sleep(2)
go_to_starting_position()

# Shut down the robot
robo.close()
</code></pre>
<p>I'm not a programmer so i dont even know if the above code is correct.
All help is apreciated!</p>
",2024-02-20 13:47:00,,37,1,0,1,,23448729.0,,2/20/2024 13:25,2.0,78096812.0,"<p>This sounds like a nice robot path-planning problem. A few options:</p>
<ul>
<li>Discretize the space (break it up into a grid) and make some cells &quot;untraversable&quot; and use something like <a href=""https://en.wikipedia.org/wiki/A*_search_algorithm"" rel=""nofollow noreferrer"">A* (A Star)</a> to plan a shortest path through the grid cells. Then iteratively feed each grid cell location to your <code>go_to_coordinates</code> function.</li>
<li>Use a random sampling planner like RRT which generates a tree of randomly sampled points in a space to eventually reach a goal location without passing through obstacles (<a href=""https://github.com/AtsushiSakai/PythonRobotics/tree/master/PathPlanning/RRT"" rel=""nofollow noreferrer"">open source RRT code</a>).</li>
<li>Use a <a href=""https://github.com/mbpeterson70/tomma/blob/main/examples/dubins_trajectory_opt.ipynb"" rel=""nofollow noreferrer"">trajectory optimization approach</a> to create a sequence of timestamped trajectory points to reach the goal while avoiding the obstacle in the middle.</li>
</ul>
",14045755.0,0.0,0.0,,
4744,77819704,ESP32-CAM Arduino Upload Error: Invalid head of packet (0x61),|esp32|arduino-uno|robot|,"<p>i am trying to build a robot car with arduino and esp32-cam but when i'm trying to upload the code through the arduino i get this:</p>
<pre><code>A fatal error occurred: Failed to connect to ESP32: Invalid head of packet (0x61): Possible serial noise or corruption.
</code></pre>
<p>the compile goes smoothly and the problem happens at the upload.
i am connected to ESP32 WROVER MUDOLE, upload speed: 115200.
i checked connections, tried reset but nothing worked.</p>
<pre><code>#include &quot;esp_camera.h&quot;
#include &lt;Arduino.h&gt;
#include &lt;WiFi.h&gt;
#include &lt;AsyncTCP.h&gt;
#include &lt;ESPAsyncWebServer.h&gt;
#include &lt;iostream&gt;
#include &lt;sstream&gt;

struct MOTOR_PINS
{
  int pinEn;  
  int pinIN1;
  int pinIN2;    
};

std::vector&lt;MOTOR_PINS&gt; motorPins = 
{
  {12, 13, 15},  //RIGHT_MOTOR Pins (EnA, IN1, IN2)
  {12, 14, 2},  //LEFT_MOTOR  Pins (EnB, IN3, IN4)
};
#define LIGHT_PIN 4

#define UP 1
#define DOWN 2
#define LEFT 3
#define RIGHT 4
#define STOP 0

#define RIGHT_MOTOR 0
#define LEFT_MOTOR 1

#define FORWARD 1
#define BACKWARD -1

const int PWMFreq = 1000; /* 1 KHz */
const int PWMResolution = 8;
const int PWMSpeedChannel = 2;
const int PWMLightChannel = 3;

//Camera related constants
#define PWDN_GPIO_NUM     32
#define RESET_GPIO_NUM    -1
#define XCLK_GPIO_NUM      0
#define SIOD_GPIO_NUM     26
#define SIOC_GPIO_NUM     27
#define Y9_GPIO_NUM       35
#define Y8_GPIO_NUM       34
#define Y7_GPIO_NUM       39
#define Y6_GPIO_NUM       36
#define Y5_GPIO_NUM       21
#define Y4_GPIO_NUM       19
#define Y3_GPIO_NUM       18
#define Y2_GPIO_NUM        5
#define VSYNC_GPIO_NUM    25
#define HREF_GPIO_NUM     23
#define PCLK_GPIO_NUM     22

const char* ssid     = &quot;MyWiFiCar&quot;;
const char* password = &quot;12345678&quot;;

AsyncWebServer server(80);
AsyncWebSocket wsCamera(&quot;/Camera&quot;);
AsyncWebSocket wsCarInput(&quot;/CarInput&quot;);
uint32_t cameraClientId = 0;

const char* htmlHomePage PROGMEM = R&quot;HTMLHOMEPAGE(
&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
  &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no&quot;&gt;
    &lt;style&gt;
    .arrows {
      font-size:40px;
      color:red;
    }
    td.button {
      background-color:black;
      border-radius:25%;
      box-shadow: 5px 5px #888888;
    }
    td.button:active {
      transform: translate(5px,5px);
      box-shadow: none; 
    }

    .noselect {
      -webkit-touch-callout: none; /* iOS Safari */
        -webkit-user-select: none; /* Safari */
         -khtml-user-select: none; /* Konqueror HTML */
           -moz-user-select: none; /* Firefox */
            -ms-user-select: none; /* Internet Explorer/Edge */
                user-select: none; /* Non-prefixed version, currently
                                      supported by Chrome and Opera */
    }

    .slidecontainer {
      width: 100%;
    }

    .slider {
      -webkit-appearance: none;
      width: 100%;
      height: 15px;
      border-radius: 5px;
      background: #d3d3d3;
      outline: none;
      opacity: 0.7;
      -webkit-transition: .2s;
      transition: opacity .2s;
    }

    .slider:hover {
      opacity: 1;
    }
  
    .slider::-webkit-slider-thumb {
      -webkit-appearance: none;
      appearance: none;
      width: 25px;
      height: 25px;
      border-radius: 50%;
      background: red;
      cursor: pointer;
    }

    .slider::-moz-range-thumb {
      width: 25px;
      height: 25px;
      border-radius: 50%;
      background: red;
      cursor: pointer;
    }

    &lt;/style&gt;
  
  &lt;/head&gt;
  &lt;body class=&quot;noselect&quot; align=&quot;center&quot; style=&quot;background-color:white&quot;&gt;
     
    &lt;!--h2 style=&quot;color: teal;text-align:center;&quot;&gt;Wi-Fi Camera &amp;#128663; Control&lt;/h2--&gt;
    
    &lt;table id=&quot;mainTable&quot; style=&quot;width:400px;margin:auto;table-layout:fixed&quot; CELLSPACING=10&gt;
      &lt;tr&gt;
        &lt;img id=&quot;cameraImage&quot; src=&quot;&quot; style=&quot;width:400px;height:250px&quot;&gt;&lt;/td&gt;
      &lt;/tr&gt; 
      &lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td class=&quot;button&quot; ontouchstart='sendButtonInput(&quot;MoveCar&quot;,&quot;1&quot;)' ontouchend='sendButtonInput(&quot;MoveCar&quot;,&quot;0&quot;)'&gt;&lt;span class=&quot;arrows&quot; &gt;&amp;#8679;&lt;/span&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td class=&quot;button&quot; ontouchstart='sendButtonInput(&quot;MoveCar&quot;,&quot;3&quot;)' ontouchend='sendButtonInput(&quot;MoveCar&quot;,&quot;0&quot;)'&gt;&lt;span class=&quot;arrows&quot; &gt;&amp;#8678;&lt;/span&gt;&lt;/td&gt;
        &lt;td class=&quot;button&quot;&gt;&lt;/td&gt;    
        &lt;td class=&quot;button&quot; ontouchstart='sendButtonInput(&quot;MoveCar&quot;,&quot;4&quot;)' ontouchend='sendButtonInput(&quot;MoveCar&quot;,&quot;0&quot;)'&gt;&lt;span class=&quot;arrows&quot; &gt;&amp;#8680;&lt;/span&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td class=&quot;button&quot; ontouchstart='sendButtonInput(&quot;MoveCar&quot;,&quot;2&quot;)' ontouchend='sendButtonInput(&quot;MoveCar&quot;,&quot;0&quot;)'&gt;&lt;span class=&quot;arrows&quot; &gt;&amp;#8681;&lt;/span&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr/&gt;&lt;tr/&gt;
      &lt;tr&gt;
        &lt;td style=&quot;text-align:left&quot;&gt;&lt;b&gt;Speed:&lt;/b&gt;&lt;/td&gt;
        &lt;td colspan=2&gt;
         &lt;div class=&quot;slidecontainer&quot;&gt;
            &lt;input type=&quot;range&quot; min=&quot;0&quot; max=&quot;255&quot; value=&quot;150&quot; class=&quot;slider&quot; id=&quot;Speed&quot; oninput='sendButtonInput(&quot;Speed&quot;,value)'&gt;
          &lt;/div&gt;
        &lt;/td&gt;
      &lt;/tr&gt;        
      &lt;tr&gt;
        &lt;td style=&quot;text-align:left&quot;&gt;&lt;b&gt;Light:&lt;/b&gt;&lt;/td&gt;
        &lt;td colspan=2&gt;
          &lt;div class=&quot;slidecontainer&quot;&gt;
            &lt;input type=&quot;range&quot; min=&quot;0&quot; max=&quot;255&quot; value=&quot;0&quot; class=&quot;slider&quot; id=&quot;Light&quot; oninput='sendButtonInput(&quot;Light&quot;,value)'&gt;
          &lt;/div&gt;
        &lt;/td&gt;   
      &lt;/tr&gt;
    &lt;/table&gt;
  
    &lt;script&gt;
      var webSocketCameraUrl = &quot;ws:\/\/&quot; + window.location.hostname + &quot;/Camera&quot;;
      var webSocketCarInputUrl = &quot;ws:\/\/&quot; + window.location.hostname + &quot;/CarInput&quot;;      
      var websocketCamera;
      var websocketCarInput;
      
      function initCameraWebSocket() 
      {
        websocketCamera = new WebSocket(webSocketCameraUrl);
        websocketCamera.binaryType = 'blob';
        websocketCamera.onopen    = function(event){};
        websocketCamera.onclose   = function(event){setTimeout(initCameraWebSocket, 2000);};
        websocketCamera.onmessage = function(event)
        {
          var imageId = document.getElementById(&quot;cameraImage&quot;);
          imageId.src = URL.createObjectURL(event.data);
        };
      }
      
      function initCarInputWebSocket() 
      {
        websocketCarInput = new WebSocket(webSocketCarInputUrl);
        websocketCarInput.onopen    = function(event)
        {
          var speedButton = document.getElementById(&quot;Speed&quot;);
          sendButtonInput(&quot;Speed&quot;, speedButton.value);
          var lightButton = document.getElementById(&quot;Light&quot;);
          sendButtonInput(&quot;Light&quot;, lightButton.value);
        };
        websocketCarInput.onclose   = function(event){setTimeout(initCarInputWebSocket, 2000);};
        websocketCarInput.onmessage = function(event){};        
      }
      
      function initWebSocket() 
      {
        initCameraWebSocket ();
        initCarInputWebSocket();
      }

      function sendButtonInput(key, value) 
      {
        var data = key + &quot;,&quot; + value;
        websocketCarInput.send(data);
      }
    
      window.onload = initWebSocket;
      document.getElementById(&quot;mainTable&quot;).addEventListener(&quot;touchend&quot;, function(event){
        event.preventDefault()
      });      
    &lt;/script&gt;
  &lt;/body&gt;    
&lt;/html&gt;
)HTMLHOMEPAGE&quot;;


void rotateMotor(int motorNumber, int motorDirection)
{
  if (motorDirection == FORWARD)
  {
    digitalWrite(motorPins[motorNumber].pinIN1, HIGH);
    digitalWrite(motorPins[motorNumber].pinIN2, LOW);    
  }
  else if (motorDirection == BACKWARD)
  {
    digitalWrite(motorPins[motorNumber].pinIN1, LOW);
    digitalWrite(motorPins[motorNumber].pinIN2, HIGH);     
  }
  else
  {
    digitalWrite(motorPins[motorNumber].pinIN1, LOW);
    digitalWrite(motorPins[motorNumber].pinIN2, LOW);       
  }
}

void moveCar(int inputValue)
{
  Serial.printf(&quot;Got value as %d\n&quot;, inputValue);  
  switch(inputValue)
  {

    case UP:
      rotateMotor(RIGHT_MOTOR, FORWARD);
      rotateMotor(LEFT_MOTOR, FORWARD);                  
      break;
  
    case DOWN:
      rotateMotor(RIGHT_MOTOR, BACKWARD);
      rotateMotor(LEFT_MOTOR, BACKWARD);  
      break;
  
    case LEFT:
      rotateMotor(RIGHT_MOTOR, FORWARD);
      rotateMotor(LEFT_MOTOR, BACKWARD);  
      break;
  
    case RIGHT:
      rotateMotor(RIGHT_MOTOR, BACKWARD);
      rotateMotor(LEFT_MOTOR, FORWARD); 
      break;
 
    case STOP:
      rotateMotor(RIGHT_MOTOR, STOP);
      rotateMotor(LEFT_MOTOR, STOP);    
      break;
  
    default:
      rotateMotor(RIGHT_MOTOR, STOP);
      rotateMotor(LEFT_MOTOR, STOP);    
      break;
  }
}

void handleRoot(AsyncWebServerRequest *request) 
{
  request-&gt;send_P(200, &quot;text/html&quot;, htmlHomePage);
}

void handleNotFound(AsyncWebServerRequest *request) 
{
    request-&gt;send(404, &quot;text/plain&quot;, &quot;File Not Found&quot;);
}

void onCarInputWebSocketEvent(AsyncWebSocket *server, 
                      AsyncWebSocketClient *client, 
                      AwsEventType type,
                      void *arg, 
                      uint8_t *data, 
                      size_t len) 
{                      
  switch (type) 
  {
    case WS_EVT_CONNECT:
      Serial.printf(&quot;WebSocket client #%u connected from %s\n&quot;, client-&gt;id(), client-&gt;remoteIP().toString().c_str());
      break;
    case WS_EVT_DISCONNECT:
      Serial.printf(&quot;WebSocket client #%u disconnected\n&quot;, client-&gt;id());
      moveCar(0);
      ledcWrite(PWMLightChannel, 0);  
      break;
    case WS_EVT_DATA:
      AwsFrameInfo *info;
      info = (AwsFrameInfo*)arg;
      if (info-&gt;final &amp;&amp; info-&gt;index == 0 &amp;&amp; info-&gt;len == len &amp;&amp; info-&gt;opcode == WS_TEXT) 
      {
        std::string myData = &quot;&quot;;
        myData.assign((char *)data, len);
        std::istringstream ss(myData);
        std::string key, value;
        std::getline(ss, key, ',');
        std::getline(ss, value, ',');
        Serial.printf(&quot;Key [%s] Value[%s]\n&quot;, key.c_str(), value.c_str()); 
        int valueInt = atoi(value.c_str());     
        if (key == &quot;MoveCar&quot;)
        {
          moveCar(valueInt);        
        }
        else if (key == &quot;Speed&quot;)
        {
          ledcWrite(PWMSpeedChannel, valueInt);
        }
        else if (key == &quot;Light&quot;)
        {
          ledcWrite(PWMLightChannel, valueInt);         
        }     
      }
      break;
    case WS_EVT_PONG:
    case WS_EVT_ERROR:
      break;
    default:
      break;  
  }
}

void onCameraWebSocketEvent(AsyncWebSocket *server, 
                      AsyncWebSocketClient *client, 
                      AwsEventType type,
                      void *arg, 
                      uint8_t *data, 
                      size_t len) 
{                      
  switch (type) 
  {
    case WS_EVT_CONNECT:
      Serial.printf(&quot;WebSocket client #%u connected from %s\n&quot;, client-&gt;id(), client-&gt;remoteIP().toString().c_str());
      cameraClientId = client-&gt;id();
      break;
    case WS_EVT_DISCONNECT:
      Serial.printf(&quot;WebSocket client #%u disconnected\n&quot;, client-&gt;id());
      cameraClientId = 0;
      break;
    case WS_EVT_DATA:
      break;
    case WS_EVT_PONG:
    case WS_EVT_ERROR:
      break;
    default:
      break;  
  }
}

void setupCamera()
{
  camera_config_t config;
  config.ledc_channel = LEDC_CHANNEL_0;
  config.ledc_timer = LEDC_TIMER_0;
  config.pin_d0 = Y2_GPIO_NUM;
  config.pin_d1 = Y3_GPIO_NUM;
  config.pin_d2 = Y4_GPIO_NUM;
  config.pin_d3 = Y5_GPIO_NUM;
  config.pin_d4 = Y6_GPIO_NUM;
  config.pin_d5 = Y7_GPIO_NUM;
  config.pin_d6 = Y8_GPIO_NUM;
  config.pin_d7 = Y9_GPIO_NUM;
  config.pin_xclk = XCLK_GPIO_NUM;
  config.pin_pclk = PCLK_GPIO_NUM;
  config.pin_vsync = VSYNC_GPIO_NUM;
  config.pin_href = HREF_GPIO_NUM;
  config.pin_sscb_sda = SIOD_GPIO_NUM;
  config.pin_sscb_scl = SIOC_GPIO_NUM;
  config.pin_pwdn = PWDN_GPIO_NUM;
  config.pin_reset = RESET_GPIO_NUM;
  config.xclk_freq_hz = 20000000;
  config.pixel_format = PIXFORMAT_JPEG;
  
  config.frame_size = FRAMESIZE_VGA;
  config.jpeg_quality = 10;
  config.fb_count = 1;

  // camera init
  esp_err_t err = esp_camera_init(&amp;config);
  if (err != ESP_OK) 
  {
    Serial.printf(&quot;Camera init failed with error 0x%x&quot;, err);
    return;
  }  

  if (psramFound())
  {
    heap_caps_malloc_extmem_enable(20000);  
    Serial.printf(&quot;PSRAM initialized. malloc to take memory from psram above this size&quot;);    
  }  
}

void sendCameraPicture()
{
  if (cameraClientId == 0)
  {
    return;
  }
  unsigned long  startTime1 = millis();
  //capture a frame
  camera_fb_t * fb = esp_camera_fb_get();
  if (!fb) 
  {
      Serial.println(&quot;Frame buffer could not be acquired&quot;);
      return;
  }

  unsigned long  startTime2 = millis();
  wsCamera.binary(cameraClientId, fb-&gt;buf, fb-&gt;len);
  esp_camera_fb_return(fb);
    
  //Wait for message to be delivered
  while (true)
  {
    AsyncWebSocketClient * clientPointer = wsCamera.client(cameraClientId);
    if (!clientPointer || !(clientPointer-&gt;queueIsFull()))
    {
      break;
    }
    delay(1);
  }
  
  unsigned long  startTime3 = millis();  
  Serial.printf(&quot;Time taken Total: %d|%d|%d\n&quot;,startTime3 - startTime1, startTime2 - startTime1, startTime3-startTime2 );
}

void setUpPinModes()
{
  //Set up PWM
  ledcSetup(PWMSpeedChannel, PWMFreq, PWMResolution);
  ledcSetup(PWMLightChannel, PWMFreq, PWMResolution);
      
  for (int i = 0; i &lt; motorPins.size(); i++)
  {
    pinMode(motorPins[i].pinEn, OUTPUT);    
    pinMode(motorPins[i].pinIN1, OUTPUT);
    pinMode(motorPins[i].pinIN2, OUTPUT);  

    /* Attach the PWM Channel to the motor enb Pin */
    ledcAttachPin(motorPins[i].pinEn, PWMSpeedChannel);
  }
  moveCar(STOP);

  pinMode(LIGHT_PIN, OUTPUT);    
  ledcAttachPin(LIGHT_PIN, PWMLightChannel);
}


void setup(void) 
{
  setUpPinModes();
  Serial.begin(115200);

  WiFi.softAP(ssid, password);
  IPAddress IP = WiFi.softAPIP();
  Serial.print(&quot;AP IP address: &quot;);
  Serial.println(IP);

  server.on(&quot;/&quot;, HTTP_GET, handleRoot);
  server.onNotFound(handleNotFound);
      
  wsCamera.onEvent(onCameraWebSocketEvent);
  server.addHandler(&amp;wsCamera);

  wsCarInput.onEvent(onCarInputWebSocketEvent);
  server.addHandler(&amp;wsCarInput);

  server.begin();
  Serial.println(&quot;HTTP server started&quot;);

  setupCamera();
}


void loop() 
{
  wsCamera.cleanupClients(); 
  wsCarInput.cleanupClients(); 
  sendCameraPicture(); 
  Serial.printf(&quot;SPIRam Total heap %d, SPIRam Free Heap %d\n&quot;, ESP.getPsramSize(), ESP.getFreePsram());
}
</code></pre>
",2024-01-15 12:12:00,,36,0,1,0,,14888644.0,,12/25/2020 16:36,14.0,,,,,,137194280.0,is the esp32 in bootlotader mode? how is it connected to the computer? upload of Blink works?
