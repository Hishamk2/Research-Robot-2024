,questionId,questionTitle,Tags,questionBody,questionCreationDate,AcceptedAnswerId,questionViewCount,AnswerCount,CommentCount,questionScore,questionFavoriteCount,questionUserId,questionUserLocation,questionUserCreationDate,questionUserViews,answerId,answerBody,answerUserId,answerScore,answerCommentCount,commentId,commentText
2541,37311326,Command received by controller through serial interface but robot arm doesn't move,|c#|serial-port|embedded|robotics|,"<pre><code>using System;
using System.Collections.Generic;
using System.ComponentModel;
using System.Data;
using System.Drawing;
using System.IO.Ports;
using System.Linq;
using System.Text;
using System.Threading.Tasks;
using System.Windows.Forms;


namespace SerialPort
{
    public partial class Form1 : Form
    {

    public Form1()
    {
        InitializeComponent();
        cmdClose.Enabled = false;
        foreach (String s in System.IO.Ports.SerialPort.GetPortNames()) 
        {
            txtPort.Items.Add(s);
        }
    }

    public System.IO.Ports.SerialPort sport;

    public void serialport_connect(String port, int baudrate , Parity parity, int databits, StopBits stopbits) 
    {
        DateTime dt = DateTime.Now;
        String dtn = dt.ToShortTimeString();

        sport = new System.IO.Ports.SerialPort(
        port, baudrate, parity, databits, stopbits);
        try
        {
            sport.Open();
            cmdClose.Enabled = true;
            cmdConnect.Enabled = false;
            txtReceive.AppendText(""["" + dtn + ""] "" + ""Connected\n"");
            sport.DataReceived += new SerialDataReceivedEventHandler(sport_DataReceived);
        }
        catch (Exception ex) { MessageBox.Show(ex.ToString(), ""Error""); }
    }

    private void sport_DataReceived(object sender, SerialDataReceivedEventArgs e)  
    {
        this.BeginInvoke(new Action(() =&gt;
        {
            DateTime dt = DateTime.Now;
            String dtn = dt.ToShortTimeString();       
            txtReceive.AppendText(""["" + dtn + ""] "" + ""Received: "" + sport.ReadExisting() + ""\n"");
        }));
    }

    private void cmdConnect_Click(object sender, EventArgs e)
    {
        String port = txtPort.Text;
        int baudrate = Convert.ToInt32(cmbbaudrate.Text);
        Parity parity = (Parity)Enum.Parse(typeof(Parity), cmbparity.Text);
        int databits = Convert.ToInt32(cmbdatabits.Text);
        StopBits stopbits = (StopBits)Enum.Parse(typeof(StopBits), cmbstopbits.Text);

        serialport_connect(port, baudrate, parity, databits, stopbits);

    }

    private void button1_Click(object sender, EventArgs e)
    {
        DateTime dt = DateTime.Now;
        String dtn = dt.ToShortTimeString();
        String data = txtDatatoSend.Text;
        sport.Write(data);
        txtReceive.AppendText(""["" + dtn + ""] "" + ""Sent: "" + data + ""\n"");
    }

    private void cmdClose_Click_1(object sender, EventArgs e)
    {
        DateTime dt = DateTime.Now;
        String dtn = dt.ToShortTimeString();

        if (sport.IsOpen) 
        {
            sport.Close();
            cmdClose.Enabled = false;
            cmdConnect.Enabled = true;
            txtReceive.AppendText(""["" + dtn + ""] "" + ""Disconnected\n"");
        }
    }
}
</code></pre>

<p>}</p>

<p><a href=""https://i.stack.imgur.com/TZEJg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TZEJg.png"" alt=""enter image description here""></a></p>

<p>After I send command through the serial interface as shown in picture, my robot doesn't move at all. However, if I close visual studio and open software called Roboteq, my robot will move following the command I sent previously without even loading the port in Roboteq. Any idea why that is? I think the controller received my command for sure, but somehow it doesn't execute, and maybe opening Roboteq makes it execute. Thanks in advance.</p>
",5/18/2016 22:51,,251,2,0,0,,6342881.0,,5/16/2016 21:45,15.0,37311522.0,"<p>Does your device require the command be terminated with a newline?</p>

<pre><code>    sport.WriteLine(data);
</code></pre>

<p><a href=""https://msdn.microsoft.com/en-us/library/system.io.ports.serialport.writeline(v=vs.110).aspx"" rel=""nofollow"">https://msdn.microsoft.com/en-us/library/system.io.ports.serialport.writeline(v=vs.110).aspx</a></p>
",3390788.0,0.0,3.0,,
3287,54620356,How to correctly use ode45 function in MATLAB for differential drive robot?,|matlab|ode|robotics|,"<p>I am trying to determine the pose (x,y,theta) of a differential drive robot using ode45. The code I have below solves for the x position, but I am having an issue with the initial condition. I set it to 0 since at time 0 the robot is assumed to be at the origin, but I get an error. How do I set the initial condition for ode45 such that I get the expected output?</p>

<p>I tried to make the initial conditions vector of the same length as dxdt by setting initial conditions as a 41x1 zeros matrix, but I didn't understand the output and I don't believe I did it correctly.</p>

<pre><code>% Given conditions for a differential drive robot:
B = 20; % (cm) distance along axle between centers of two wheels
r = 10; % (cm) diameter of both wheels
w_l = 5*sin(3*t); % (rad/s) angular rate of left wheel
w_r = 5*sin(3*t); % (rad/s) angular rate of right wheel
v_l = r*w_l; % (cm/s) velocity of left wheel
v_r = r*w_r; % (cm/s) velocity of right wheel
v = (v_r+v_l)/B; % (cm/s) velocity of robot
theta = 90; % constant orientation of robot since trajectory is straight

% Solve differential equation for x:
dxdt = v*cos(theta); % diff equaition for x
tspan = [0 20]; % time period to integrate over
x0 = 0; % initial condition since robot begins at origin
[t,x] = ode45(@(t,y) dxdt, tspan, x0);
</code></pre>

<p>I want to solve the differential equation <code>dxdt</code> for <code>0</code> to <code>20</code> seconds with an initial condition of <code>0</code>. I expect the output to give me a vector of time from <code>0</code> to <code>20</code> and an array of for <code>x</code>. The problem I believe lies with the initial condition. MATLAB gives me an error in the live editor telling me, "" <code>@(t,y)dxdt returns a vector of length 69, but the length of initial conditions vector is 1. The vector returned by @(t,y)dxdt and the initial conditions vector must have the same number of elements.</code>""</p>
",2/10/2019 19:52,,801,1,0,0,,9832088.0,"Valley Stream, NY, USA",5/23/2018 3:59,9.0,54628271.0,"<p>The issue is not the initial condition. You need to define <code>dxdt</code> as a function i.e.</p>

<pre><code>% Define differential equation
function derivative = dxdt(t,y)

% Given conditions for a differential drive robot:
B = 20; % (cm) distance along axle between centers of two wheels
r = 10; % (cm) diameter of both wheels
w_l = 5*sin(3*t); % (rad/s) angular rate of left wheel
w_r = 5*sin(3*t); % (rad/s) angular rate of right wheel
v_l = r*w_l; % (cm/s) velocity of left wheel
v_r = r*w_r; % (cm/s) velocity of right wheel
v = (v_r+v_l)/B; % (cm/s) velocity of robot
theta = 90; % constant orientation of robot since trajectory is straight

derivative = v*cos(theta); % diff equation for x

end
</code></pre>

<p>Then when you use <code>ode45</code> you should tell it to pass the <code>t</code> and <code>y</code> variables as arguments to <code>dxdt</code> like</p>

<pre><code>[t,x] = ode45(@(t,y) dxdt(t,y), tspan, x0);
</code></pre>

<p>This should then work. In this case, as <code>dxdt</code> only takes the default arguments, you could also write</p>

<pre><code>[t,x] = ode45(@dxdt, tspan, x0);
</code></pre>

<p>The error you got indicates that at some point you made <code>dxdt</code> into a vector of length 69, whilst MATLAB was expecting to get back 1 value for <code>dxdt</code> when it passed one <code>t</code> and one <code>y</code> to your <code>dxdt</code> 'function'. Whenever you get errors like this, I'd recommend putting </p>

<p><strike><code>clear all</code></strike></p>

<pre><code>`clearvars` % better than clear all - see am304's comment below
</code></pre>

<p>at the top of your script to avoid polluting your workspace with previously defined variables.</p>
",6459621.0,1.0,1.0,,
1899,21178005,Find all polygons formed by intersections of lines,|geometry|computational-geometry|robotics|maze|,"<p>I have a maze described by walls as line segments (no given order). Given a point, I need to determine whether its inside the maze or no. Everything is in the Cartezian plane(no discretization).</p>

<hr>

<p>My idea is to transform the problem as follows:</p>

<p>Given some line segments in the plane, find all polygons with vertices in the endpoints of the given segments and with sides lying on the segments (you can see in the image below that you can't assume the sides will form a subset of segments).</p>

<p>And then just check: if a point is only inside one polygon, then its inside of the maze, otherwise no.</p>

<hr>

<p>The solution I have in mind would be: hash endpoints and line intersections, and then look for loops.</p>

<p>Any other suggestions?
Thanks!</p>

<p>(ignore the colors in the image)
<img src=""https://i.stack.imgur.com/vml8K.png"" alt=""enter image description here""></p>
",1/17/2014 4:36,,435,1,0,0,,1993650.0,"Cambridge, USA",1/19/2013 21:46,92.0,21181588.0,"<p>It is enough to find boundary (outer) polygon. That can be done by finding one point on the boundary and than traversing from that point by segments in one direction. If there are more possibilities to go than choose 'outer' one. Algorithm can be described:</p>

<pre><code>find boundary point
find first direction to go and go to that point
while current point is different than fist one
  find next direction to go
  go to next point
</code></pre>

<p>First point can be find as point with highest Y coordinate, if there are more like that than one with lowest X among them. We can call it upper-left point.</p>

<p>First direction to go: first point is connected to other points and that points have &lt;= Y coordinate, what means that connection segments are below first point. Choose right-most of them.</p>

<p>Next direction to go: current point is reached by some (incoming) segment, next segment to go is furthest away in positive direction from incoming one, what is same as first segment in clock-wise direction from incoming segment.</p>
",494076.0,0.0,2.0,,
1865,20167162,Which robotic simulation tool fits my need,|visualization|simulation|robotics|avatar|,"<p>Im trying to analyze which software products out there, that could fit into a product idea. </p>

<ul>
<li>I want a robotic simulation tool, that can show a model of a
selfdrawn robot. </li>
<li>It shall be possible to control the selfdrawn robot    through a
programming interface. C,java,c++.. or maybe multiple or a selfdescribed
programming interface.</li>
<li>In order to make this product easy to use, the programming interface
should be simple for the user and the execution of the code on the
drawn model aswell.</li>
<li>It has to run on windows.</li>
</ul>

<p>Eather im looking for a tool that can do all these things, or im looking for a tool that is easy to change/extend for the wanted look and feel. </p>

<p>Plaese help if you have some good tips within this subject.</p>
",11/23/2013 19:58,20863603.0,105,1,0,0,,3025756.0,,11/23/2013 19:37,6.0,20863603.0,"<p>It depends on what you want to do.  </p>

<p>""ROS"" is a good simulation environment and it can provide your robot with simulated sensor data from the simulation.  So for example you place a wall in the simulation and your robot can get maybe laser scanner or ultrasound distance from the wall.   Of cousre you'd have to write the model for the sensor but ROS can provide updates to the wall to sensor distance an as the robot drives around.</p>

<p>ROS is neat because it can run on multiple computers and passes data between nodes.  Some no=des can be simulations some can be inside real robot hardware.</p>

<p>If your ""robot"" is less sophisticated and really is just a remote control object your user directly controls then look at a game environment.  ""jmonkey"" is one and can animate game characters in a ""world"" you build.   If your robot can be represented by a video game character money would work.  It runs and every platform from Windows to Android.</p>

<p>Both are big software systems with learning curves.</p>

<p>BTW anything will run on Windows in the have VMware installed ;-)</p>
",3150208.0,1.0,0.0,,
2183,28951574,Implementing a stack to use backtracking for struct,|c|struct|backtracking|robot|,"<p>This is a program that enables a Robot to follow black lines through a ""maze"". The maze is made of 7x7 black lines with a entry line leading through it. There can be empty squares and lines leading nowhere. where the black lines meet there is a white square. </p>

<p>The idea is to save these squares in a struct and make the robot run through the maze and save the individual structs in a stack. </p>

<p>I want to do this so that the robot can find the way back through backtracking.</p>

<pre><code>void push(int value ){
if(top ==49){ //uncertain what to write here 
} else{
    top = top+1;
    a[top] = value;
}

int pop(){

if(top ==-1){
    return -234;
}else{

    int r = a[top];
    top = top-1;
    return r;
   }
 }
</code></pre>

<p>I'm uncertain how make this work together with this </p>

<pre><code>typedef struct proxy {
int edges[4];
int visited;
int exists;
int token;
} proxy;

int main() {

   /* This part is to make simulation work, idea is to start in the middle
      an 14x14, because that allows us to go in any direction. 
      the arrays are the values that the simulation gives back. 
      This seems to work even though it's not pretty. */

   static int x = 7; 
   static int y = 7;
   static int zaehler = 0;  //Zähler für Rundenbegrenzung
   int Ary[14][14];
   int hilfN, hilfO, hilfS, hilfW;
   int i,j;
   int N[8] = {16, 48, 80, 112, 144, 176, 208, 240};
   int O[8] = {128, 144, 160, 176, 192, 208, 224, 240};
   int S[8] = {32, 48, 96, 112, 160, 176, 224, 240};
   int W[8] = {64, 80, 96, 112, 192, 208, 224, 240};
   int Intersec = Robot_GetIntersections();
   int myRobot_Move(x,y){
   return Robot_Move(x-7,y-7);
}
for(i= 0; i &lt; 14; i++){

    for(j= 0; j &lt; 14; j++){
        Ary[i][j] = 0;
        }
}

myRobot_Move(x,y);

while (x &lt; 14 &amp;&amp; y &lt; 14) {
Intersec = Robot_GetIntersections();
Ary[x][y] = Intersec;

for (hilfN=0; hilfN&lt;8; hilfN++){
    if(N[hilfN] == Intersec){
        if(Ary[x][y+1] == 0){
            y++;
            myRobot_Move(x,y);
        }
    }
}
//Osten
for (hilfO=0; hilfO&lt;8; hilfO++){
        if(O[hilfO] == Intersec){
            if(Ary[x+1][y] == 0){
                x++;
                myRobot_Move(x,y);
            }
        }
    }
//Süden
for (i=0; i&lt;8; i++){
        if(S[i] == Intersec){
            if(Ary[x][y-1] == 0){
                y--;
                myRobot_Move(x,y);
            }
        }
    }
//Westen
for (i=0; i&lt;8; i++){
            if(W[i] == Intersec){
                if(Ary[x-1][y] == 0){
                x--;
                myRobot_Move(x,y);
            }
        }
    }

if (Ary[x][y+1] &amp;&amp; Ary[x+1][y] &amp;&amp; Ary[x][y-1] &amp;&amp; Ary[x-1][y] &gt; 0){
    break;
    printf(""End"");
   }
 }


return EXIT_SUCCESS;
}
</code></pre>
",3/9/2015 20:55,,76,1,0,0,,4651383.0,,3/9/2015 20:23,1.0,32453418.0,"<p>If you want your stack <code>a</code> to save <code>struct proxy</code>s instead of <code>int</code>s, change the presumed <code>int a[50];</code> to <code>proxy a[50];</code> and the functions <code>push()</code> and <code>pop()</code> as follows:</p>

<pre><code>void push(proxy value)
{
    if (top == 49) puts(""'a' stack overflow""), exit(1);
    else a[++top] = value;
}

proxy pop()
{
    if (top == -1) return (proxy){-234};
    else return a[top--];
}
</code></pre>
",2413201.0,0.0,0.0,,
4164,70197279,How to find robot TCP in image with CalibrateHandEye,|c++|opencv|camera-calibration|robotics|,"<p>I'm currently developing a programm for a robot with a camera attached to the end affector.
The goal is to calculate where the robots TCP appears in the camera frame. I'm using opencv in c++.
The robot is a UR5 from Universal Robots.</p>
<p>My plan:</p>
<ol>
<li>collect multiple (8) data-sets:</li>
</ol>
<ul>
<li>robot pose (XYZ in meters, directly from the robot controller)</li>
<li>robot rotation (rx ry rz in radians, directly from the robot controller)</li>
<li>take a picture of calibration pattern for each step</li>
</ul>
<ol start=""2"">
<li><p>run calibrateCamera over the set of pictures to get tvec and rvec for every step</p>
</li>
<li><p>run calibrateHandEye</p>
</li>
</ol>
<ul>
<li>for t_gripper2base i use the robot pose</li>
<li>for R_gripper2base i use the robot rotation</li>
<li>for t_target2cam i use tvec from calibrateCamera</li>
<li>for R_target2cam i use rvec from calibrateCamera</li>
</ul>
<p>I seem to get correct values (I measured the distance from cam to TCP and the t_cam2gripper seems to be correct.</p>
<pre><code>Translation vector target to cam: 
[-0.0001052803107026547;
 -0.0780872727019615;
 -0.1243323507069755]

Rotation matrix target to cam: 
[0.9999922523048892, 0.002655868335207422, -0.002905459271957709;
 -0.001229768871633805, 0.9119334002787367, 0.4103363999508009;
 0.003739384804660116, -0.4103296477461107, 0.9119296010009958]
</code></pre>
<p>The formula to transform the point from TCP coordinates to the image should look like this:</p>
<ul>
<li>(u,v,w) = C * T * (X,Y,Z,1)
But after the division by w my values are still way off (should be around (600,600)</li>
</ul>
<pre><code>Actual image position vector homogenized: 
[1778.542462313536;
 -1626.72483032188;
 1]
</code></pre>
<pre><code>#include &lt;QCoreApplication&gt;

#include &lt;opencv2/core.hpp&gt;
#include &lt;opencv2/highgui.hpp&gt;
#include &lt;opencv2/imgproc.hpp&gt;
#include &lt;opencv2/imgcodecs.hpp&gt;
#include &lt;opencv2/opencv.hpp&gt;

using namespace std;
using namespace cv;

int main(int argc, char *argv[])
{
    QCoreApplication a(argc, argv);

    Mat defaultCameraMatrix = (Mat_&lt;double&gt;(3, 3));
    Mat defaultDistortion = (Mat_&lt;double&gt;(1, 5));
    defaultCameraMatrix.at&lt;double&gt;(0, 0) = 1739.3749;   // default values from previous intrinsic camera calibration
    defaultCameraMatrix.at&lt;double&gt;(0, 1) = 0;
    defaultCameraMatrix.at&lt;double&gt;(0, 2) = 639.5;

    defaultCameraMatrix.at&lt;double&gt;(1, 0) = 0;
    defaultCameraMatrix.at&lt;double&gt;(1, 1) = 1739.3749;
    defaultCameraMatrix.at&lt;double&gt;(1, 2) = 479.5;

    defaultCameraMatrix.at&lt;double&gt;(2, 0) = 0;
    defaultCameraMatrix.at&lt;double&gt;(2, 1) = 0;
    defaultCameraMatrix.at&lt;double&gt;(2, 2) = 1;

    defaultDistortion.at&lt;double&gt;(0, 0) = -0.165909;
    defaultDistortion.at&lt;double&gt;(0, 1) = 0.303675;
    defaultDistortion.at&lt;double&gt;(0, 2) = 0.0;
    defaultDistortion.at&lt;double&gt;(0, 3) = 0.0;
    defaultDistortion.at&lt;double&gt;(0, 4) = 0.0;

    vector&lt;Mat&gt; R_gripper2base, t_gripper2base, R_target2cam, t_target2cam;
    Mat actualRobotPos1 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotPos2 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotPos3 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotPos4 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotPos5 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotPos6 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotPos7 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotPos8 = (Mat_&lt;double&gt;(3, 1));

    actualRobotPos1.at&lt;double&gt;(0,0) = -0.193139;
    actualRobotPos1.at&lt;double&gt;(1,0) = 0.463823;
    actualRobotPos1.at&lt;double&gt;(2,0) = -0.025;
    t_gripper2base.push_back(actualRobotPos1);

    actualRobotPos2.at&lt;double&gt;(0,0) = -0.193139;
    actualRobotPos2.at&lt;double&gt;(1,0) = 0.463823;
    actualRobotPos2.at&lt;double&gt;(2,0) = -0.025;
    t_gripper2base.push_back(actualRobotPos2);

    actualRobotPos3.at&lt;double&gt;(0,0) = -0.21273;
    actualRobotPos3.at&lt;double&gt;(1,0) = 0.4426;
    actualRobotPos3.at&lt;double&gt;(2,0) = -0.0288;
    t_gripper2base.push_back(actualRobotPos3);

    actualRobotPos4.at&lt;double&gt;(0,0) = -0.17213;
    actualRobotPos4.at&lt;double&gt;(1,0) = 0.4103;
    actualRobotPos4.at&lt;double&gt;(2,0) = 0.014;
    t_gripper2base.push_back(actualRobotPos4);

    actualRobotPos5.at&lt;double&gt;(0,0) = -0.13724;
    actualRobotPos5.at&lt;double&gt;(1,0) = 0.45;
    actualRobotPos5.at&lt;double&gt;(2,0) = 0.02978;
    t_gripper2base.push_back(actualRobotPos5);

    actualRobotPos6.at&lt;double&gt;(0,0) = -0.1655;
    actualRobotPos6.at&lt;double&gt;(1,0) = 0.478;
    actualRobotPos6.at&lt;double&gt;(2,0) = -0.0211;
    t_gripper2base.push_back(actualRobotPos6);

    actualRobotPos7.at&lt;double&gt;(0,0) = -0.17018;
    actualRobotPos7.at&lt;double&gt;(1,0) = 0.46458;
    actualRobotPos7.at&lt;double&gt;(2,0) = -0.03761;
    t_gripper2base.push_back(actualRobotPos7);

    actualRobotPos8.at&lt;double&gt;(0,0) = -0.193139;
    actualRobotPos8.at&lt;double&gt;(1,0) = 0.463823;
    actualRobotPos8.at&lt;double&gt;(2,0) = 0.025;
    t_gripper2base.push_back(actualRobotPos8);

    Mat actualRobotRotVec1 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotRotVec2 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotRotVec3 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotRotVec4 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotRotVec5 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotRotVec6 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotRotVec7 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotRotVec8 = (Mat_&lt;double&gt;(3, 1));

    actualRobotRotVec1.at&lt;double&gt;(0,0) = -3.14159;
    actualRobotRotVec1.at&lt;double&gt;(1,0) = 0.00;
    actualRobotRotVec1.at&lt;double&gt;(2,0) = 0.00719124;
    R_gripper2base.push_back(actualRobotRotVec1);

    actualRobotRotVec2.at&lt;double&gt;(0,0) = -2.06;
    actualRobotRotVec2.at&lt;double&gt;(1,0) = -2.36;
    actualRobotRotVec2.at&lt;double&gt;(2,0) = 0.03;
    R_gripper2base.push_back(actualRobotRotVec2);

    actualRobotRotVec3.at&lt;double&gt;(0,0) = 2.39;
    actualRobotRotVec3.at&lt;double&gt;(1,0) = 1.86;
    actualRobotRotVec3.at&lt;double&gt;(2,0) = 0.49;
    R_gripper2base.push_back(actualRobotRotVec3);

    actualRobotRotVec4.at&lt;double&gt;(0,0) = -2.66;
    actualRobotRotVec4.at&lt;double&gt;(1,0) = 0.08;
    actualRobotRotVec4.at&lt;double&gt;(2,0) = 0.09;
    R_gripper2base.push_back(actualRobotRotVec4);

    actualRobotRotVec5.at&lt;double&gt;(0,0) = -2.84;
    actualRobotRotVec5.at&lt;double&gt;(1,0) = 0.19;
    actualRobotRotVec5.at&lt;double&gt;(2,0) = 0.69;
    R_gripper2base.push_back(actualRobotRotVec5);

    actualRobotRotVec6.at&lt;double&gt;(0,0) = 2.1;
    actualRobotRotVec6.at&lt;double&gt;(1,0) = -2.34;
    actualRobotRotVec6.at&lt;double&gt;(2,0) = -0.02;
    R_gripper2base.push_back(actualRobotRotVec6);

    actualRobotRotVec7.at&lt;double&gt;(0,0) = 1.66;
    actualRobotRotVec7.at&lt;double&gt;(1,0) = -2.53;
    actualRobotRotVec7.at&lt;double&gt;(2,0) = -0.23;
    R_gripper2base.push_back(actualRobotRotVec7);

    actualRobotRotVec8.at&lt;double&gt;(0,0) = -3.14159;
    actualRobotRotVec8.at&lt;double&gt;(1,0) = 0.00;
    actualRobotRotVec8.at&lt;double&gt;(2,0) = 0.00719124;
    R_gripper2base.push_back(actualRobotRotVec8);

    //    for(int i = 0; i &lt; t_gripper2base.size(); i++)
    //    {
    //        cout &lt;&lt; t_gripper2base[i] &lt;&lt; endl &lt;&lt; endl;
    //    }

    //    for(int i = 0; i &lt; R_gripper2base.size(); i++)
    //    {
    //        cout &lt;&lt; R_gripper2base[i] &lt;&lt; endl &lt;&lt; endl;
    //    }

    vector&lt;String&gt; fileNames;
    glob(&quot;PATH*.png&quot;, fileNames, false); // directory of images
    vector&lt;vector&lt;Point2f&gt;&gt; corners(fileNames.size());
    Mat chessboardImg, chessboardImgGray;

    vector&lt;Point3f&gt; objp;
    vector&lt;vector&lt;Point3f&gt;&gt; worldCoordinates;
    int checkerBoard[2] = {9,6};
    double fieldSize = 0.008;

    Mat cameraMatrixHandEye, distortionHandEye;
    vector&lt;Mat&gt; rvecs, tvecs;

    for(int i = 1; i &lt; checkerBoard[1]; i++){
        for(int j = 1; j &lt; checkerBoard[0]; j++){
            objp.push_back(Point3f(j*fieldSize, i*fieldSize, 0));
        }
    }

    for(int i = 0; i &lt; 8; i++)
    {
        chessboardImg = imread(fileNames[i]);
        cvtColor(chessboardImg, chessboardImgGray, COLOR_BGR2GRAY);

        bool patternFound = findChessboardCorners(chessboardImgGray, Size(8,5), corners[i],  CALIB_CB_ADAPTIVE_THRESH + CALIB_CB_NORMALIZE_IMAGE);

        if(patternFound)
        {
            cornerSubPix(chessboardImgGray, corners[i],Size(11,11), Size(-1,-1), TermCriteria(TermCriteria::EPS + TermCriteria::MAX_ITER, 30, 0.1));
            drawChessboardCorners(chessboardImg, Size(8,5), corners[i], patternFound);
            worldCoordinates.push_back(objp);
        }
        //***** Check loaded images and detected chessboard *****
        //imshow(&quot;source&quot;, chessboardImgGray);
        //imshow(&quot;chess&quot;, chessboardImg);
        //waitKey(0);
        //*******************************************************
    }
    float reprojectionError = calibrateCamera(worldCoordinates, corners, Size(1280,960), cameraMatrixHandEye, distortionHandEye, rvecs, tvecs, CALIB_FIX_ASPECT_RATIO + CALIB_FIX_K3 +
                                              CALIB_ZERO_TANGENT_DIST + CALIB_FIX_PRINCIPAL_POINT);

    //***** Check camera calibration results *****
    //cout &lt;&lt; &quot;Reprojection Error CHE: &quot; &lt;&lt; endl &lt;&lt; reprojectionError &lt;&lt; endl &lt;&lt; endl;
    //cout &lt;&lt; &quot;Camera Matrix CHE: &quot; &lt;&lt; endl &lt;&lt; cameraMatrixHandEye &lt;&lt; endl &lt;&lt; endl;
    //cout &lt;&lt; &quot;Distortion CHE: &quot; &lt;&lt; endl &lt;&lt; distortionHandEye &lt;&lt; endl &lt;&lt; endl;
    //for(int i = 0; i &lt; numberOfPoses; i++)
    //{
    //    cout &lt;&lt; &quot;No. &quot; &lt;&lt; i+1 &lt;&lt; &quot; Target translation: &quot; &lt;&lt; endl &lt;&lt; tvecs[i] &lt;&lt; endl &lt;&lt; endl;
    //    cout &lt;&lt; &quot;No. &quot; &lt;&lt; i+1 &lt;&lt; &quot; Target rotation: &quot; &lt;&lt; endl &lt;&lt; rvecs[i] &lt;&lt; endl &lt;&lt; endl;
    //}
    //********************************************/

    for(int i = 0; i &lt; rvecs.size(); i++)
    {
        t_target2cam.emplace_back(tvecs[i]);
        R_target2cam.emplace_back(rvecs[i]);
    }

    //    for(int i = 0; i &lt; t_target2cam.size(); i++)
    //    {
    //        cout &lt;&lt; t_target2cam[i] &lt;&lt; endl &lt;&lt; endl;
    //    }

    //    for(int i = 0; i &lt; R_target2cam.size(); i++)
    //    {
    //        cout &lt;&lt; R_target2cam[i] &lt;&lt; endl &lt;&lt; endl;
    //    }

    Mat R_cam2gripper;
    Mat t_cam2gripper = (Mat_&lt;double&gt;(3, 1));
    calibrateHandEye(R_gripper2base, t_gripper2base, R_target2cam, t_target2cam, R_cam2gripper, t_cam2gripper);

    cout &lt;&lt; t_cam2gripper &lt;&lt; endl &lt;&lt; endl;
    cout &lt;&lt; R_cam2gripper &lt;&lt; endl &lt;&lt; endl;

    Mat transformationMat4x4 = (Mat_&lt;double&gt;(4, 4));
    Mat transformationMatInv4x4 = (Mat_&lt;double&gt;(4, 4));
    Mat R_cam2gripperInv = (Mat_&lt;double&gt;(3, 3));
    Mat t_cam2gripperInv = (Mat_&lt;double&gt;(3, 1));

    transformationMat4x4.at&lt;double&gt;(0, 0) = R_cam2gripper.at&lt;double&gt;(0, 0);
    transformationMat4x4.at&lt;double&gt;(0, 1) = R_cam2gripper.at&lt;double&gt;(0, 1);
    transformationMat4x4.at&lt;double&gt;(0, 2) = R_cam2gripper.at&lt;double&gt;(0, 2);
    transformationMat4x4.at&lt;double&gt;(0, 3) = t_cam2gripper.at&lt;double&gt;(0, 0);
    transformationMat4x4.at&lt;double&gt;(1, 0) = R_cam2gripper.at&lt;double&gt;(1, 0);
    transformationMat4x4.at&lt;double&gt;(1, 1) = R_cam2gripper.at&lt;double&gt;(1, 1);
    transformationMat4x4.at&lt;double&gt;(1, 2) = R_cam2gripper.at&lt;double&gt;(1, 2);
    transformationMat4x4.at&lt;double&gt;(1, 3) = t_cam2gripper.at&lt;double&gt;(1, 0);
    transformationMat4x4.at&lt;double&gt;(2, 0) = R_cam2gripper.at&lt;double&gt;(2, 0);
    transformationMat4x4.at&lt;double&gt;(2, 1) = R_cam2gripper.at&lt;double&gt;(2, 1);
    transformationMat4x4.at&lt;double&gt;(2, 2) = R_cam2gripper.at&lt;double&gt;(2, 2);
    transformationMat4x4.at&lt;double&gt;(2, 3) = t_cam2gripper.at&lt;double&gt;(2, 0);
    transformationMat4x4.at&lt;double&gt;(3, 0) = 0;
    transformationMat4x4.at&lt;double&gt;(3, 1) = 0;
    transformationMat4x4.at&lt;double&gt;(3, 2) = 0;
    transformationMat4x4.at&lt;double&gt;(3, 3) = 1;

    transformationMatInv4x4 = transformationMat4x4.inv();
    R_cam2gripperInv.at&lt;double&gt;(0,0) = transformationMatInv4x4.at&lt;double&gt;(0,0);
    R_cam2gripperInv.at&lt;double&gt;(0,1) = transformationMatInv4x4.at&lt;double&gt;(0,1);
    R_cam2gripperInv.at&lt;double&gt;(0,2) = transformationMatInv4x4.at&lt;double&gt;(0,2);

    R_cam2gripperInv.at&lt;double&gt;(1,0) = transformationMatInv4x4.at&lt;double&gt;(1,0);
    R_cam2gripperInv.at&lt;double&gt;(1,1) = transformationMatInv4x4.at&lt;double&gt;(1,1);
    R_cam2gripperInv.at&lt;double&gt;(1,2) = transformationMatInv4x4.at&lt;double&gt;(1,2);

    R_cam2gripperInv.at&lt;double&gt;(2,0) = transformationMatInv4x4.at&lt;double&gt;(2,0);
    R_cam2gripperInv.at&lt;double&gt;(2,1) = transformationMatInv4x4.at&lt;double&gt;(2,1);
    R_cam2gripperInv.at&lt;double&gt;(2,2) = transformationMatInv4x4.at&lt;double&gt;(2,2);

    t_cam2gripperInv.at&lt;double&gt;(0,0) = transformationMatInv4x4.at&lt;double&gt;(0,3);
    t_cam2gripperInv.at&lt;double&gt;(1,0) = transformationMatInv4x4.at&lt;double&gt;(1,3);
    t_cam2gripperInv.at&lt;double&gt;(2,0) = transformationMatInv4x4.at&lt;double&gt;(2,3);

    cout &lt;&lt; transformationMatInv4x4 &lt;&lt; endl &lt;&lt; endl;
    cout &lt;&lt; t_cam2gripperInv &lt;&lt; endl &lt;&lt; endl;


    Point3f objectPoints1, objectPoints2;
    vector&lt;Point3f&gt; objectPoints;

    objectPoints1.x = 0;    //TCP in TCP-Coordinates
    objectPoints1.y = 0;
    objectPoints1.z = 0;
    objectPoints.push_back(objectPoints1);

    vector&lt;Point2f&gt; imagePoints;
    projectPoints(objectPoints, R_cam2gripperInv, t_cam2gripperInv, defaultCameraMatrix, defaultDistortion, imagePoints);
    cout &lt;&lt; imagePoints[0] &lt;&lt; endl &lt;&lt; endl;

    return a.exec();
}
`
</code></pre>
",12/2/2021 9:52,,647,2,0,0,0.0,17087746.0,,10/6/2021 8:57,10.0,70201334.0,"<p>you need to use solvepnp to get rvec and tvec for each image separately and then you will have a list of rvecs and tvecs. list length equals no of images. To get a similar list of rvec and tvec for Gripper_to_base transformation, you need to derive the R and T matrices based on robot dynamics which take rotation angle as input. Then for each pose you need to input the rotation angle data to R,T matrices to get rvec and tvec for each pose and make list of same length. Then you input them to calibrateHandeye function.</p>
",12114977.0,0.0,9.0,,
588,3029978,"How to use CCR, DSS, VPL (aka Microsoft Robotics Development Studio) outside robotics?",|robotics|ccr|robotics-studio|,"<p>How to use CCR, DSS, VPL (aka Microsoft Robotics Development Studio) outside robotics?</p>

<p>I am looking for guidance in this field. I have tried all the examples and find the framework intriguing.</p>

<p>Can anyone post other uses and examples, outside robotics?</p>

<p>PS. I am looking for someone to explain some of the more complex stuff to me. I have questions regarding different implementations. If anyone is interested, i am willing to pay for a one to one talk (consulting) on the advanced topics. You can reach me via email, same name as here.</p>
",6/12/2010 20:24,3629649.0,1179,4,0,5,,293856.0,,3/15/2010 9:42,528.0,3073397.0,"<p>I've seen couple of channel9 videos where they demo using CCR outside robotics. I do not know the roots of CCR, but since the core product developers- George and Satnam Singh have backgrounds in XNA and related technologies, they  understand the problem which CCR addresses very well. Besides there are lots of research papers which I've seen outside the robotics world which people have used CCR for. I'm implementing some web services outside the robotics domain in MRDS's dsshost and CCR and will upload them shortly</p>
",143373.0,1.0,0.0,,
1935,22755882,Are interrupts the right thing to use for my robot?,|arduino|interrupt|robotics|motordriver|,"<p>So I have this old motorized wheelchair that I am trying to convert into a robot. I replaced the original motor driver with a sabertooth 2x12 and I’m using an Arduino micro to talk to it. The motors shaft goes all the way threw so I attached a magnet and a Hall Effect sensor to the back side to act as a rotary encoder. My current goal is to be able to tell the robot to move forward a certain amount of feet then stop. I wrote some code to do this linearly however this didn't work so well. Then I learned about interrupts and that sounded like exactly what I needed. So I tried my hand at that and things went wrong on several different levels.</p>

<p>LEVEL ONE: I have never seemed to be able to properly drive the motors it seems like any time I put the command to turn them on inside of a loop or if statement they decide to do what they want and move sporadically and unpredictably </p>

<p>LEVEL TWO: I feel like the interrupts are interrupting themselves and the thing I set in place to stop the wheels from moving forward because I can tell it to move 14 rotary encoder clicks forward and one wheel will continue moving way past 1000 clicks while the other stops</p>

<p>LEVEL THREE: couple times I guess I placed my interrupts wrong because when I uploaded the code windows would stop recognizing the Arduino and my driver would break until I uploaded the blink sketch after pressing the reset button which also reloaded and fixed my drivers. Then if I deleted one of my interrupts it would upload normally.</p>

<p>LEVEL FOUR: my Hall Effect sensors seem to not work right when the motors are on. They tend to jump from 1 to 200 clicks in a matter of seconds. This in turn floods my serial port and crashes the Arduino ide.</p>

<p>So as you can see there are several flaws somewhere in the system whether it’s hardware or software I don't know. Am I approaching this the right way or is there some Arduino secret I don’t know about that would make my life easier? If I am approaching this right could you take a look at my code below and see what I’m doing wrong.</p>

<pre><code> #include &lt;Servo.h&gt;//the motor driver uses this library

Servo LEFT, RIGHT;//left wheel right wheel

int RclickNum=0;//used for the rotory encoder
int LclickNum=0;//these are the number of ""clicks"" each wheel has moved

int D =115;//Drive
int R =70;//Reverse
int B =90;//Break

int Linterrupt = 1;//these are the interrupt numbers. 0 = pin 3 and 1 = pin 2
int Rinterrupt = 0;

int clickConvert = 7;// how many rotery encoder clicks equal a foot

void setup()
{
  Serial.begin(9600); //starting serial communication 
  LEFT.attach( 9, 1000, 2000);//attaching the motor controller that is acting like a servo
  RIGHT.attach(10, 1000, 2000);
  attachInterrupt(Linterrupt, LclickCounter, FALLING);//attaching the rotory encoders as interrupts that will 
  attachInterrupt(Rinterrupt, RclickCounter, FALLING);//trip when the encoder pins go from high to low


}
void loop()
{//This is for controling the robot using the standard wasd format
  int input= Serial.read();
  if(input == 'a')
    left(2);
  if(input == 'd')
    right(2);
  if(input == 'w')
    forward(2);
  if(input == 's')
    backward(2);
  if(input == 'e')
    STOP();
}

void forward(int feet)//this is called when w is sent threw the serial port and is where i am testing all of my code. 
{
  interrupts(); //turn on the interrupts
  while(RclickNum &lt; feet * clickConvert || LclickNum &lt; feet * clickConvert)// while either the left or right wheel hasnt made it to the desired distance
  {
    if(RclickNum &lt; feet * clickConvert)//check if the right wheel has gone the distance
      RIGHT.write(D); //make the right wheel move
    else
      RIGHT.write(B);//stop the right wheel

    if(LclickNum &lt; feet * clickConvert)
      LEFT.write(D);
    else
      LEFT.write(B);
  }
  noInterrupts();//stop the interrupts 
  resetCount();//set the click counters back to zero
}

//once i have the forward function working i will implament it through out the other functions
//----------------------------------------------------------------------

void backward(int feet)
{
  RIGHT.write(R);
  LEFT.write(R);
}

void left(int feet)
{
  RIGHT.write(D);
  LEFT.write(R);
}

void right(int feet)
{
  RIGHT.write(R);
  LEFT.write(D);
}

void STOP()
{
  resetCount();
  RIGHT.write(B);
  LEFT.write(B);
}

void LclickCounter()//this is called by the left encoder interrupt
{
  LclickNum++; 
  Serial.print(""L"");
  Serial.println(LclickNum); 
}

void RclickCounter()//this is called by the right encoder interrupt
{
  RclickNum++;
  M Serial.print(""R"");
  Serial.println(RclickNum);
}


void resetCount()
{
  RclickNum=0;
  LclickNum=0;
}
</code></pre>
",3/31/2014 7:33,22760663.0,929,1,1,0,,3106569.0,,12/16/2013 8:20,14.0,22760663.0,"<ol>
<li><p>don't use <code>interrupt()</code> and <code>nointerrupt()</code> (or <code>cli()</code> and <code>sei()</code>) as they will stop timer and serial interrupt, breaking a lot of things. Just set to 0 the counting variable OR use detachInterrupt and attachInterrupt.</p></li>
<li><p>variable used inside interrupt AND normal execution flow should be declared as <code>volatile</code>, or their value my be unsyncornized. So declare them like <code>volatile int RclickNum=0;</code></p></li>
<li><p>interrupt should be fast to execute, as by default other interrupt will NOT execute while inside an interrupt.</p></li>
<li><p>NEVER use Serial inside interrupt;  if Serial buffer is full, it will call Serial.flush(), that will wait for Serial interrupt of byte written, but because you are alreadi inside an interrupt will never happen...dead lock aka you code hangs forever!</p></li>
<li><p>because your ""moving"" function use quite a long time to execute, if multiple command arrive to the serial, thay will remain isnode the buffer until readed. So if in the terminal you write ""asd"" and then ""e"", you will see robot go left, backward, right, stop (yes, actually the stop function is not usefull as it does nothing because your ""moving"" function are ""blocking"", that mean they won't return until they ended, so the loop() code (and the read of ""e"") will not execute until the buffer of serial has been processed.</p></li>
</ol>
",1668605.0,1.0,0.0,34688720.0,"General advice: keep the interrupt handlers small and do not spawn other activities.  That means remove those Serial.println().  If you need something echo'd, do it in the loop().  Encoders always count.  Don't turn on and off the interrupts.  If I walk up and physically push the robot, the encoders should still count.  Similar, if it stops on a hill, you want it to hold its position.  As alternate to on/off: When you start a move, capture a snapshot of the current position and calculate the target.  When reach the target can stop.  Last: don't ride that thing without a helmet :)"
3735,62761806,How can I use an Arduino Ultrasonic Sensor connected to an Arduino UNO to measure distance using Pyfirmata or Python Generally?,|python|arduino|robotics|,"<p>Here is the Code I have in Arduino. Pyfirmata, as I am concerned doesn't have a PulseIn function, so how can I bypass this obstacle? I want to turn an LED on when the sensor senses an object that it's distance from it is 20cm or smaller!</p>
<pre><code>// defines pins numbers
const int trigPin = 12;
const int echoPin = 11;
int LED = 5;
// defines variables
long duration;
int distance;

void setup() {
    pinMode(trigPin, OUTPUT); // Sets the trigPin as an Output
    pinMode(echoPin, INPUT); // Sets the echoPin as an Input
    Serial.begin(9600); // Starts the serial communication
    pinMode (LED,OUTPUT);
}

void loop() {
    // Clears the trigPin
    digitalWrite(trigPin, LOW);
    delayMicroseconds(2);
    // Sets the trigPin on HIGH state for 10 micro seconds
    digitalWrite(trigPin, HIGH);
    delayMicroseconds(10);
    digitalWrite(trigPin, LOW);
    // Reads the echoPin, returns the sound wave travel time in microseconds
    duration = pulseIn(echoPin, HIGH);
    // Calculating the distance
    distance= duration*0.034/2;
    // Prints the distance on the Serial Monitor
    Serial.print(&quot;Distance: &quot;);
    Serial.println(distance);

    if (distance &lt;=20){
      digitalWrite(LED,HIGH);
    }
    else{
      digitalWrite (LED,LOW);
    }
}
</code></pre>
",7/6/2020 17:58,67685151.0,1137,1,1,1,,13349700.0,,4/18/2020 19:14,3.0,67685151.0,"<p>I came across the same problem and was trying to solve. I saw your question had no answers :D. It took me two days to figure out I went all out like a hungry wolf hunting.
Here's what u need to do.</p>
<p>First some back story.
PyFirmata is a library which acts like a bridge between Python and Arduino AVR. Which means in order for functions to work you need to define them in both the languages at eh backhand program (AVR and Python) so they can communicate on same terms.
Files in AVR are StandardFirmata.ino. Files in Python are pyFirmata.py. There are more as well u can see from here. These Files communicate between each other when program runs.</p>
<ol>
<li><a href=""https://github.com/tino/pyFirmata/pull/45/files"" rel=""nofollow noreferrer"">https://github.com/tino/pyFirmata/pull/45/files</a></li>
<li><a href=""https://github.com/jgautier/arduino-1/tree/pulseIn"" rel=""nofollow noreferrer"">https://github.com/jgautier/arduino-1/tree/pulseIn</a></li>
</ol>
<p>Some guys tried to add a PULSE_IN feature to calculate the distance from their Ultrasonic sensor and it worked fine. To make it work u have to update your standardfirmata file in arduino and pyFirmata file in Python manually to add the PULSE_IN codes from the link above. Here are the steps.</p>
<ol>
<li>Open your StandardFirmata sketch in Arduino IDE.</li>
<li>Copy paste this code to replace teh old code. this will add the PULSE_IN features i extracted from the links above and added to my standardfirmata.</li>
</ol>
<pre><code>/*
 * Firmata is a generic protocol for communicating with microcontrollers
 * from software on a host computer. It is intended to work with
 * any host computer software package.
 *
 * To download a host software package, please clink on the following link
 * to open the download page in your default browser.
 *
 * http://firmata.org/wiki/Download
 */
/*
  Copyright (C) 2006-2008 Hans-Christoph Steiner.  All rights reserved.
  Copyright (C) 2010-2011 Paul Stoffregen.  All rights reserved.
  Copyright (C) 2009 Shigeru Kobayashi.  All rights reserved.
  Copyright (C) 2009-2011 Jeff Hoefs.  All rights reserved.
  
  This library is free software; you can redistribute it and/or
  modify it under the terms of the GNU Lesser General Public
  License as published by the Free Software Foundation; either
  version 2.1 of the License, or (at your option) any later version.
 
  See file LICENSE.txt for further informations on licensing terms.
  formatted using the GNU C formatting and indenting
*/
/* 
 * TODO: use Program Control to load stored profiles from EEPROM
 */
#include &lt;Servo.h&gt;
#include &lt;Wire.h&gt;
#include &lt;Firmata.h&gt;
// move the following defines to Firmata.h?
#define I2C_WRITE B00000000
#define I2C_READ B00001000
#define I2C_READ_CONTINUOUSLY B00010000
#define I2C_STOP_READING B00011000
#define I2C_READ_WRITE_MODE_MASK B00011000
#define I2C_10BIT_ADDRESS_MODE_MASK B00100000
#define MAX_QUERIES               8
#define MINIMUM_SAMPLING_INTERVAL 10
#define REGISTER_NOT_SPECIFIED    -1
#define PULSE_IN                  0x74 // send a pulse in command

/*==============================================================================
 * GLOBAL VARIABLES
 *============================================================================*/
/* Ultrasonic Distance Measurement variables */
# include &quot;LiquidCrystal.h&quot;  //lcd libary                                       
LiquidCrystal lcd(22, 23, 24, 25, 26, 27);   //LCD object Parameters: (rs, enable, d4, d5, d6, d7)
const int trigPin = 35; //trig pin connection 
const int echoPin = 34;  //echopin connection 
long duration;
int distanceCm;
float liquid;
/* analog inputs */
int analogInputsToReport = 0; // bitwise array to store pin reporting
/* digital input ports */
byte reportPINs[TOTAL_PORTS];       // 1 = report this port, 0 = silence
byte previousPINs[TOTAL_PORTS];     // previous 8 bits sent
/* pins configuration */
byte pinConfig[TOTAL_PINS];         // configuration of every pin
byte portConfigInputs[TOTAL_PORTS]; // each bit: 1 = pin in INPUT, 0 = anything else
int pinState[TOTAL_PINS];           // any value that has been written
/* timer variables */
unsigned long currentMillis;        // store the current value from millis()
unsigned long previousMillis;       // for comparison with currentMillis
int samplingInterval = 19;          // how often to run the main loop (in ms)
/* i2c data */
struct i2c_device_info {
  byte addr;
  byte reg;
  byte bytes;
};
/* for i2c read continuous more */
i2c_device_info query[MAX_QUERIES];
byte i2cRxData[32];
boolean isI2CEnabled = false;
signed char queryIndex = -1;
unsigned int i2cReadDelayTime = 0;  // default delay time between i2c read request and Wire.requestFrom()
Servo servos[MAX_SERVOS];
/*==============================================================================
 * FUNCTIONS
 *============================================================================*/
void readAndReportData(byte address, int theRegister, byte numBytes) {
  // allow I2C requests that don't require a register read
  // for example, some devices using an interrupt pin to signify new data available
  // do not always require the register read so upon interrupt you call Wire.requestFrom()  
  if (theRegister != REGISTER_NOT_SPECIFIED) {
    Wire.beginTransmission(address);
    #if ARDUINO &gt;= 100
    Wire.write((byte)theRegister);
    #else
    Wire.send((byte)theRegister);
    #endif
    Wire.endTransmission();
    delayMicroseconds(i2cReadDelayTime);  // delay is necessary for some devices such as WiiNunchuck
  } else {
    theRegister = 0;  // fill the register with a dummy value
  }
  Wire.requestFrom(address, numBytes);  // all bytes are returned in requestFrom
  // check to be sure correct number of bytes were returned by slave
  if(numBytes == Wire.available()) {
    i2cRxData[0] = address;
    i2cRxData[1] = theRegister;
    for (int i = 0; i &lt; numBytes; i++) {
      #if ARDUINO &gt;= 100
      i2cRxData[2 + i] = Wire.read();
      #else
      i2cRxData[2 + i] = Wire.receive();
      #endif
    }
  }
  else {
    if(numBytes &gt; Wire.available()) {
      Firmata.sendString(&quot;I2C Read Error: Too many bytes received&quot;);
    } else {
      Firmata.sendString(&quot;I2C Read Error: Too few bytes received&quot;); 
    }
  }
  // send slave address, register and received bytes
  Firmata.sendSysex(SYSEX_I2C_REPLY, numBytes + 2, i2cRxData);
}
void outputPort(byte portNumber, byte portValue, byte forceSend)
{
  // pins not configured as INPUT are cleared to zeros
  portValue = portValue &amp; portConfigInputs[portNumber];
  // only send if the value is different than previously sent
  if(forceSend || previousPINs[portNumber] != portValue) {
    Firmata.sendDigitalPort(portNumber, portValue);
    previousPINs[portNumber] = portValue;
  }
}
/* -----------------------------------------------------------------------------
 * check all the active digital inputs for change of state, then add any events
 * to the Serial output queue using Serial.print() */
void checkDigitalInputs(void)
{
  /* Using non-looping code allows constants to be given to readPort().
   * The compiler will apply substantial optimizations if the inputs
   * to readPort() are compile-time constants. */
  if (TOTAL_PORTS &gt; 0 &amp;&amp; reportPINs[0]) outputPort(0, readPort(0, portConfigInputs[0]), false);
  if (TOTAL_PORTS &gt; 1 &amp;&amp; reportPINs[1]) outputPort(1, readPort(1, portConfigInputs[1]), false);
  if (TOTAL_PORTS &gt; 2 &amp;&amp; reportPINs[2]) outputPort(2, readPort(2, portConfigInputs[2]), false);
  if (TOTAL_PORTS &gt; 3 &amp;&amp; reportPINs[3]) outputPort(3, readPort(3, portConfigInputs[3]), false);
  if (TOTAL_PORTS &gt; 4 &amp;&amp; reportPINs[4]) outputPort(4, readPort(4, portConfigInputs[4]), false);
  if (TOTAL_PORTS &gt; 5 &amp;&amp; reportPINs[5]) outputPort(5, readPort(5, portConfigInputs[5]), false);
  if (TOTAL_PORTS &gt; 6 &amp;&amp; reportPINs[6]) outputPort(6, readPort(6, portConfigInputs[6]), false);
  if (TOTAL_PORTS &gt; 7 &amp;&amp; reportPINs[7]) outputPort(7, readPort(7, portConfigInputs[7]), false);
  if (TOTAL_PORTS &gt; 8 &amp;&amp; reportPINs[8]) outputPort(8, readPort(8, portConfigInputs[8]), false);
  if (TOTAL_PORTS &gt; 9 &amp;&amp; reportPINs[9]) outputPort(9, readPort(9, portConfigInputs[9]), false);
  if (TOTAL_PORTS &gt; 10 &amp;&amp; reportPINs[10]) outputPort(10, readPort(10, portConfigInputs[10]), false);
  if (TOTAL_PORTS &gt; 11 &amp;&amp; reportPINs[11]) outputPort(11, readPort(11, portConfigInputs[11]), false);
  if (TOTAL_PORTS &gt; 12 &amp;&amp; reportPINs[12]) outputPort(12, readPort(12, portConfigInputs[12]), false);
  if (TOTAL_PORTS &gt; 13 &amp;&amp; reportPINs[13]) outputPort(13, readPort(13, portConfigInputs[13]), false);
  if (TOTAL_PORTS &gt; 14 &amp;&amp; reportPINs[14]) outputPort(14, readPort(14, portConfigInputs[14]), false);
  if (TOTAL_PORTS &gt; 15 &amp;&amp; reportPINs[15]) outputPort(15, readPort(15, portConfigInputs[15]), false);
}
// -----------------------------------------------------------------------------
/* sets the pin mode to the correct state and sets the relevant bits in the
 * two bit-arrays that track Digital I/O and PWM status
 */
void setPinModeCallback(byte pin, int mode)
{
  if (pinConfig[pin] == I2C &amp;&amp; isI2CEnabled &amp;&amp; mode != I2C) {
    // disable i2c so pins can be used for other functions
    // the following if statements should reconfigure the pins properly
    disableI2CPins();
  }
  if (IS_PIN_SERVO(pin) &amp;&amp; mode != SERVO &amp;&amp; servos[PIN_TO_SERVO(pin)].attached()) {
    servos[PIN_TO_SERVO(pin)].detach();
  }
  if (IS_PIN_ANALOG(pin)) {
    reportAnalogCallback(PIN_TO_ANALOG(pin), mode == ANALOG ? 1 : 0); // turn on/off reporting
  }
  if (IS_PIN_DIGITAL(pin)) {
    if (mode == INPUT) {
      portConfigInputs[pin/8] |= (1 &lt;&lt; (pin &amp; 7));
    } else {
      portConfigInputs[pin/8] &amp;= ~(1 &lt;&lt; (pin &amp; 7));
    }
  }
  pinState[pin] = 0;
  switch(mode) {
  case ANALOG:
    if (IS_PIN_ANALOG(pin)) {
      if (IS_PIN_DIGITAL(pin)) {
        pinMode(PIN_TO_DIGITAL(pin), INPUT); // disable output driver
        digitalWrite(PIN_TO_DIGITAL(pin), LOW); // disable internal pull-ups
      }
      pinConfig[pin] = ANALOG;
    }
    break;
  case INPUT:
    if (IS_PIN_DIGITAL(pin)) {
      pinMode(PIN_TO_DIGITAL(pin), INPUT); // disable output driver
      digitalWrite(PIN_TO_DIGITAL(pin), LOW); // disable internal pull-ups
      pinConfig[pin] = INPUT;
    }
    break;
  case OUTPUT:
    if (IS_PIN_DIGITAL(pin)) {
      digitalWrite(PIN_TO_DIGITAL(pin), LOW); // disable PWM
      pinMode(PIN_TO_DIGITAL(pin), OUTPUT);
      pinConfig[pin] = OUTPUT;
    }
    break;
  case PWM:
    if (IS_PIN_PWM(pin)) {
      pinMode(PIN_TO_PWM(pin), OUTPUT);
      analogWrite(PIN_TO_PWM(pin), 0);
      pinConfig[pin] = PWM;
    }
    break;
  case SERVO:
    if (IS_PIN_SERVO(pin)) {
      pinConfig[pin] = SERVO;
      if (!servos[PIN_TO_SERVO(pin)].attached()) {
          servos[PIN_TO_SERVO(pin)].attach(PIN_TO_DIGITAL(pin));
      }
    }
    break;
  case I2C:
    if (IS_PIN_I2C(pin)) {
      // mark the pin as i2c
      // the user must call I2C_CONFIG to enable I2C for a device
      pinConfig[pin] = I2C;
    }
    break;
  default:
    Firmata.sendString(&quot;Unknown pin mode&quot;); // TODO: put error msgs in EEPROM
  }
  // TODO: save status to EEPROM here, if changed
}
void analogWriteCallback(byte pin, int value)
{
  if (pin &lt; TOTAL_PINS) {
    switch(pinConfig[pin]) {
    case SERVO:
      if (IS_PIN_SERVO(pin))
        servos[PIN_TO_SERVO(pin)].write(value);
        pinState[pin] = value;
      break;
    case PWM:
      if (IS_PIN_PWM(pin))
        analogWrite(PIN_TO_PWM(pin), value);
        pinState[pin] = value;
      break;
    }
  }
}
void digitalWriteCallback(byte port, int value)
{
  byte pin, lastPin, mask=1, pinWriteMask=0;
  if (port &lt; TOTAL_PORTS) {
    // create a mask of the pins on this port that are writable.
    lastPin = port*8+8;
    if (lastPin &gt; TOTAL_PINS) lastPin = TOTAL_PINS;
    for (pin=port*8; pin &lt; lastPin; pin++) {
      // do not disturb non-digital pins (eg, Rx &amp; Tx)
      if (IS_PIN_DIGITAL(pin)) {
        // only write to OUTPUT and INPUT (enables pullup)
        // do not touch pins in PWM, ANALOG, SERVO or other modes
        if (pinConfig[pin] == OUTPUT || pinConfig[pin] == INPUT) {
          pinWriteMask |= mask;
          pinState[pin] = ((byte)value &amp; mask) ? 1 : 0;
        }
      }
      mask = mask &lt;&lt; 1;
    }
    writePort(port, (byte)value, pinWriteMask);
  }
}
// -----------------------------------------------------------------------------
/* sets bits in a bit array (int) to toggle the reporting of the analogIns
 */
//void FirmataClass::setAnalogPinReporting(byte pin, byte state) {
//}
void reportAnalogCallback(byte analogPin, int value)
{
  if (analogPin &lt; TOTAL_ANALOG_PINS) {
    if(value == 0) {
      analogInputsToReport = analogInputsToReport &amp;~ (1 &lt;&lt; analogPin);
    } else {
      analogInputsToReport = analogInputsToReport | (1 &lt;&lt; analogPin);
    }
  }
  // TODO: save status to EEPROM here, if changed
}
void reportDigitalCallback(byte port, int value)
{
  if (port &lt; TOTAL_PORTS) {
    reportPINs[port] = (byte)value;
  }
  // do not disable analog reporting on these 8 pins, to allow some
  // pins used for digital, others analog.  Instead, allow both types
  // of reporting to be enabled, but check if the pin is configured
  // as analog when sampling the analog inputs.  Likewise, while
  // scanning digital pins, portConfigInputs will mask off values from any
  // pins configured as analog
}
/*==============================================================================
 * SYSEX-BASED commands
 *============================================================================*/
void sysexCallback(byte command, byte argc, byte *argv)
{
  byte mode;
  byte slaveAddress;
  byte slaveRegister;
  byte data;
  unsigned int delayTime; 
  
  switch(command) {
  case I2C_REQUEST:
    mode = argv[1] &amp; I2C_READ_WRITE_MODE_MASK;
    if (argv[1] &amp; I2C_10BIT_ADDRESS_MODE_MASK) {
      Firmata.sendString(&quot;10-bit addressing mode is not yet supported&quot;);
      return;
    }
    else {
      slaveAddress = argv[0];
    }
    switch(mode) {
    case I2C_WRITE:
      Wire.beginTransmission(slaveAddress);
      for (byte i = 2; i &lt; argc; i += 2) {
        data = argv[i] + (argv[i + 1] &lt;&lt; 7);
        #if ARDUINO &gt;= 100
        Wire.write(data);
        #else
        Wire.send(data);
        #endif
      }
      Wire.endTransmission();
      delayMicroseconds(70);
      break;
    case I2C_READ:
      if (argc == 6) {
        // a slave register is specified
        slaveRegister = argv[2] + (argv[3] &lt;&lt; 7);
        data = argv[4] + (argv[5] &lt;&lt; 7);  // bytes to read
        readAndReportData(slaveAddress, (int)slaveRegister, data);
      }
      else {
        // a slave register is NOT specified
        data = argv[2] + (argv[3] &lt;&lt; 7);  // bytes to read
        readAndReportData(slaveAddress, (int)REGISTER_NOT_SPECIFIED, data);
      }
      break;
    case I2C_READ_CONTINUOUSLY:
      if ((queryIndex + 1) &gt;= MAX_QUERIES) {
        // too many queries, just ignore
        Firmata.sendString(&quot;too many queries&quot;);
        break;
      }
      queryIndex++;
      query[queryIndex].addr = slaveAddress;
      query[queryIndex].reg = argv[2] + (argv[3] &lt;&lt; 7);
      query[queryIndex].bytes = argv[4] + (argv[5] &lt;&lt; 7);
      break;
    case I2C_STOP_READING:
      byte queryIndexToSkip;      
      // if read continuous mode is enabled for only 1 i2c device, disable
      // read continuous reporting for that device
      if (queryIndex &lt;= 0) {
        queryIndex = -1;        
      } else {
        // if read continuous mode is enabled for multiple devices,
        // determine which device to stop reading and remove it's data from
        // the array, shifiting other array data to fill the space
        for (byte i = 0; i &lt; queryIndex + 1; i++) {
          if (query[i].addr = slaveAddress) {
            queryIndexToSkip = i;
            break;
          }
        }
        
        for (byte i = queryIndexToSkip; i&lt;queryIndex + 1; i++) {
          if (i &lt; MAX_QUERIES) {
            query[i].addr = query[i+1].addr;
            query[i].reg = query[i+1].addr;
            query[i].bytes = query[i+1].bytes; 
          }
        }
        queryIndex--;
      }
      break;
    default:
      break;
    }
    break;
  case I2C_CONFIG:
    delayTime = (argv[0] + (argv[1] &lt;&lt; 7));
    if(delayTime &gt; 0) {
      i2cReadDelayTime = delayTime;
    }
    if (!isI2CEnabled) {
      enableI2CPins();
    }
    
    break;
  case SERVO_CONFIG:
    if(argc &gt; 4) {
      // these vars are here for clarity, they'll optimized away by the compiler
      byte pin = argv[0];
      int minPulse = argv[1] + (argv[2] &lt;&lt; 7);
      int maxPulse = argv[3] + (argv[4] &lt;&lt; 7);
      if (IS_PIN_SERVO(pin)) {
        if (servos[PIN_TO_SERVO(pin)].attached())
          servos[PIN_TO_SERVO(pin)].detach();
        servos[PIN_TO_SERVO(pin)].attach(PIN_TO_DIGITAL(pin), minPulse, maxPulse);
        setPinModeCallback(pin, SERVO);
      }
    }
    break;
  case SAMPLING_INTERVAL:
    if (argc &gt; 1) {
      samplingInterval = argv[0] + (argv[1] &lt;&lt; 7);
      if (samplingInterval &lt; MINIMUM_SAMPLING_INTERVAL) {
        samplingInterval = MINIMUM_SAMPLING_INTERVAL;
      }      
    } else {
      //Firmata.sendString(&quot;Not enough data&quot;);
    }
    break;
  case EXTENDED_ANALOG:
    if (argc &gt; 1) {
      int val = argv[1];
      if (argc &gt; 2) val |= (argv[2] &lt;&lt; 7);
      if (argc &gt; 3) val |= (argv[3] &lt;&lt; 14);
      analogWriteCallback(argv[0], val);
    }
    break;
  case CAPABILITY_QUERY:
    Serial.write(START_SYSEX);
    Serial.write(CAPABILITY_RESPONSE);
    for (byte pin=0; pin &lt; TOTAL_PINS; pin++) {
      if (IS_PIN_DIGITAL(pin)) {
        Serial.write((byte)INPUT);
        Serial.write(1);
        Serial.write((byte)OUTPUT);
        Serial.write(1);
      }
      if (IS_PIN_ANALOG(pin)) {
        Serial.write(ANALOG);
        Serial.write(10);
      }
      if (IS_PIN_PWM(pin)) {
        Serial.write(PWM);
        Serial.write(8);
      }
      if (IS_PIN_SERVO(pin)) {
        Serial.write(SERVO);
        Serial.write(14);
      }
      if (IS_PIN_I2C(pin)) {
        Serial.write(I2C);
        Serial.write(1);  // to do: determine appropriate value 
      }
      Serial.write(127);
    }
    Serial.write(END_SYSEX);
    break;
  case PIN_STATE_QUERY:
    if (argc &gt; 0) {
      byte pin=argv[0];
      Serial.write(START_SYSEX);
      Serial.write(PIN_STATE_RESPONSE);
      Serial.write(pin);
      if (pin &lt; TOTAL_PINS) {
        Serial.write((byte)pinConfig[pin]);
    Serial.write((byte)pinState[pin] &amp; 0x7F);
    if (pinState[pin] &amp; 0xFF80) Serial.write((byte)(pinState[pin] &gt;&gt; 7) &amp; 0x7F);
    if (pinState[pin] &amp; 0xC000) Serial.write((byte)(pinState[pin] &gt;&gt; 14) &amp; 0x7F);
      }
      Serial.write(END_SYSEX);
    }
    break;
  case ANALOG_MAPPING_QUERY:
    Serial.write(START_SYSEX);
    Serial.write(ANALOG_MAPPING_RESPONSE);
    for (byte pin=0; pin &lt; TOTAL_PINS; pin++) {
      Serial.write(IS_PIN_ANALOG(pin) ? PIN_TO_ANALOG(pin) : 127);
    }
    Serial.write(END_SYSEX);
    break;
  case PULSE_IN:
    unsigned long duration;
    byte responseArray[5];
    byte timeoutArray[4] = {
        (argv[2] &amp; 0x7F) | ((argv[3] &amp; 0x7F) &lt;&lt; 7)
       ,(argv[4] &amp; 0x7F) | ((argv[5] &amp; 0x7F) &lt;&lt; 7)
       ,(argv[6] &amp; 0x7F) | ((argv[7] &amp; 0x7F) &lt;&lt; 7)
       ,(argv[8] &amp; 0x7F) | ((argv[9] &amp; 0x7F) &lt;&lt; 7)
    };
    unsigned long timeout = ((unsigned long)timeoutArray[0] &lt;&lt; 24)
              | ((unsigned long)timeoutArray[1] &lt;&lt; 16)
              | ((unsigned long)timeoutArray[2] &lt;&lt; 8)
              | ((unsigned long)timeoutArray[3]);
    duration = pulseIn(argv[0],argv[1],timeout);
    responseArray[0] = argv[0];
    responseArray[1] = ((timeout &gt;&gt; 24) &amp; 0xFF) ;
    responseArray[2] = ((timeout &gt;&gt; 16) &amp; 0xFF) ;
    responseArray[3] = ((timeout &gt;&gt; 8) &amp; 0xFF);
    responseArray[4] = ((timeout &amp; 0xFF));
    Firmata.sendSysex(PULSE_IN,5,responseArray);
  }
}

void enableI2CPins()
{
  byte i;
  // is there a faster way to do this? would probaby require importing 
  // Arduino.h to get SCL and SDA pins
  for (i=0; i &lt; TOTAL_PINS; i++) {
    if(IS_PIN_I2C(i)) {
      // mark pins as i2c so they are ignore in non i2c data requests
      setPinModeCallback(i, I2C);
    } 
  }
   
  isI2CEnabled = true; 
  
  // is there enough time before the first I2C request to call this here?
  Wire.begin();
}
/* disable the i2c pins so they can be used for other functions */
void disableI2CPins() {
    isI2CEnabled = false;
    // disable read continuous mode for all devices
    queryIndex = -1;
    // uncomment the following if or when the end() method is added to Wire library
    // Wire.end();
}
/*==============================================================================
 * SETUP()
 *============================================================================*/
void systemResetCallback()
{
  // initialize a defalt state
  // TODO: option to load config from EEPROM instead of default
  if (isI2CEnabled) {
    disableI2CPins();
  }
  for (byte i=0; i &lt; TOTAL_PORTS; i++) {
    reportPINs[i] = false;      // by default, reporting off
    portConfigInputs[i] = 0;    // until activated
    previousPINs[i] = 0;
  }
  // pins with analog capability default to analog input
  // otherwise, pins default to digital output
  for (byte i=0; i &lt; TOTAL_PINS; i++) {
    if (IS_PIN_ANALOG(i)) {
      // turns off pullup, configures everything
      setPinModeCallback(i, ANALOG);
    } else {
      // sets the output to 0, configures portConfigInputs
      setPinModeCallback(i, OUTPUT);
    }
  }
  // by default, do not report any analog inputs
  analogInputsToReport = 0;
  /* send digital inputs to set the initial state on the host computer,
   * since once in the loop(), this firmware will only send on change */
  /*
  TODO: this can never execute, since no pins default to digital input
        but it will be needed when/if we support EEPROM stored config
  for (byte i=0; i &lt; TOTAL_PORTS; i++) {
    outputPort(i, readPort(i, portConfigInputs[i]), true);
  }
  */
}
void setup() 
{
  Firmata.setFirmwareVersion(FIRMATA_MAJOR_VERSION, FIRMATA_MINOR_VERSION);
  Firmata.attach(ANALOG_MESSAGE, analogWriteCallback);
  Firmata.attach(DIGITAL_MESSAGE, digitalWriteCallback);
  Firmata.attach(REPORT_ANALOG, reportAnalogCallback);
  Firmata.attach(REPORT_DIGITAL, reportDigitalCallback);
  Firmata.attach(SET_PIN_MODE, setPinModeCallback);
  Firmata.attach(START_SYSEX, sysexCallback);
  Firmata.attach(SYSTEM_RESET, systemResetCallback);
  Firmata.begin(57600);
  systemResetCallback();  // reset to default config

 /*Distance measurement with Ultrasonic Sensor */
  Serial.begin(9600);
  lcd.begin(16,2);                                                   
  pinMode(trigPin, OUTPUT);
  pinMode(echoPin, INPUT);
  lcd.setCursor(0,0);
  lcd.print(&quot;  Distance    &quot;);
  lcd.setCursor(0,1);
  lcd.print(&quot;  Measurement  &quot;);
  delay(2000);
  lcd.clear();
  lcd.setCursor(0,0);
  lcd.print(&quot;    Made By    &quot;);
  lcd.setCursor(0,1);
  lcd.print(&quot;    HASH    &quot;);
  delay(2000);
  lcd.clear();
}

/*==============================================================================
 * LOOP()
 *============================================================================*/
void loop() 
{
  byte pin, analogPin;
  /* DIGITALREAD - as fast as possible, check for changes and output them to the
   * FTDI buffer using Serial.print()  */
  checkDigitalInputs();  
  /* SERIALREAD - processing incoming messagse as soon as possible, while still
   * checking digital inputs.  */
  while(Firmata.available())
    Firmata.processInput();
  /* SEND FTDI WRITE BUFFER - make sure that the FTDI buffer doesn't go over
   * 60 bytes. use a timer to sending an event character every 4 ms to
   * trigger the buffer to dump. */
  currentMillis = millis();
  if (currentMillis - previousMillis &gt; samplingInterval) {
    previousMillis += samplingInterval;
    /* ANALOGREAD - do all analogReads() at the configured sampling interval */
    for(pin=0; pin&lt;TOTAL_PINS; pin++) {
      if (IS_PIN_ANALOG(pin) &amp;&amp; pinConfig[pin] == ANALOG) {
        analogPin = PIN_TO_ANALOG(pin);
        if (analogInputsToReport &amp; (1 &lt;&lt; analogPin)) {
          Firmata.sendAnalog(analogPin, analogRead(analogPin));
        }
      }
    }
    // report i2c data for all device with read continuous mode enabled
    if (queryIndex &gt; -1) {
      for (byte i = 0; i &lt; queryIndex + 1; i++) {
        readAndReportData(query[i].addr, query[i].reg, query[i].bytes);
      }
    }
  }
  /*Distance measurement with Ultrasonic Sensor */
  digitalWrite(trigPin, LOW);
  delayMicroseconds(2);
  digitalWrite(trigPin, HIGH);
  delayMicroseconds(10);
  digitalWrite(trigPin, LOW);
  duration = pulseIn(echoPin, HIGH);
  distanceCm= duration*0.034/2;                                                                                 
  lcd.setCursor(0,0);                                                 
  lcd.print(&quot;Distance Measur.&quot;);
  delay(10);
  lcd.setCursor(0,1);
  lcd.print(&quot;Distance:&quot;);
  lcd.print(distanceCm);
  lcd.print(&quot; Cm &quot;);
  Serial.print(distanceCm);
  delay(10);

}
</code></pre>
<ol start=""3"">
<li><p>You will see the Firmata.h and Firmata.cpp files in link#2 as well. Open notepad copy paste thier codes make two files on ur dekstop with the same name. Go where u have installed arduino. Find firmata folder ull see both files in there and replace the old files with these new files.
50% DONE.</p>
</li>
<li><p>Open LINK#1 and u'll see pyFirmata.py and util.py file which has the PULSE_IN feature added. The green areas are the additions  done in the code. Ull see three dots button on the left for more options select view file.</p>
</li>
<li><p>Open Python IDE open pyfirmata.py and util.py and replace it with the new code. Save.</p>
</li>
</ol>
<p>Now we have added the code for PULSE_IN in both the platforms Python and Arduino AVR. See the examples in Link#1 to make use of PULSE_IN function. ENJOY.</p>
<p>P.S. I didnt add the test.py file, couldnt find where to add it and what file to replace. If u have error try adding that as well. If u find anything do post.</p>
",14288649.0,4.0,0.0,110987711.0,FYI: [arduino.se]
2938,47102736,Forward Kinematics for Baxter,|python|robotics|kinematics|,"<p>I've put together this Forward Kinematics function for Baxter arm robot based on its <a href=""http://sdk.rethinkrobotics.com/wiki/Hardware_Specifications#Joint_Names"" rel=""noreferrer"">hardware specs</a> and the following joints axis:<a href=""https://i.stack.imgur.com/JRWHX.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/JRWHX.png"" alt=""baxter zero configuration""></a>
The joint positions for the following forward kinematics are not matching the corresponding Cartesian coordinates, what am I doing wrong here?</p>

<pre><code>def FK_function_2(joints):
    def yaw(theta): #(rotation around z)
        y = np.array([[np.cos(theta), -np.sin(theta), 0],
                      [np.sin(theta), np.cos(theta), 0],
                      [0, 0, 1] ])
        return y

    R01 = yaw(joints[0]).dot(np.array([[-1,     0,   0],
                                       [0,      0,   1],
                                       [0,      1,   0]]))
    R12 = yaw(joints[1]).dot(np.array([[0,      0,   -1],
                                       [-1,     0,   0],
                                       [0,      1,   0]]))
    R23 = yaw(joints[2]).dot(np.array([[-1,     0,   0],
                                       [0,      0,   1],
                                       [0,      1,   0]]))
    R34 = yaw(joints[3]).dot(np.array([[-1,     0,   0],
                                       [0,      0,   1],
                                       [0,      1,   0]]))
    R45 = yaw(joints[4]).dot(np.array([[-1,     0,   0],
                                       [0,      0,   1],
                                       [0,      1,   0]]))
    R56 = yaw(joints[5]).dot(np.array([[-1,     0,   0],
                                       [0,      0,   1],
                                       [0,      1,   0]]))
    R67 = yaw(joints[6]).dot(np.array([[1,      0,   0],
                                       [0,      1,   0],
                                       [0,      0,   1]]))

    d = np.array([0.27035, 0, 0.36435, 0, 0.37429, 0, 0.229525])
    a = np.array([0.069, 0, 0.069, 0, 0.010, 0, 0])

    l1 = np.array([a[0]*np.cos(joints[0]), a[0]*np.sin(joints[0]), d[0]]);
    l2 = np.array([a[1]*np.cos(joints[1]), a[1]*np.sin(joints[1]), d[1]]); 
    l3 = np.array([a[2]*np.cos(joints[2]), a[2]*np.sin(joints[2]), d[2]]); 
    l4 = np.array([a[3]*np.cos(joints[3]), a[3]*np.sin(joints[3]), d[3]]); 
    l5 = np.array([a[4]*np.cos(joints[4]), a[4]*np.sin(joints[4]), d[4]]);
    l6 = np.array([a[5]*np.cos(joints[5]), a[5]*np.sin(joints[5]), d[5]]);
    l7 = np.array([a[6]*np.cos(joints[6]), a[6]*np.sin(joints[6]), d[6]]);

    unit = np.array([0, 0, 0, 1])
    H0 = np.concatenate((np.concatenate((R01, l1.reshape(3, 1)), axis=1), unit.reshape(1,4)), axis=0)
    H1 = np.concatenate((np.concatenate((R12, l2.reshape(3, 1)), axis=1), unit.reshape(1,4)), axis=0)
    H2 = np.concatenate((np.concatenate((R23, l3.reshape(3, 1)), axis=1), unit.reshape(1,4)), axis=0)
    H3 = np.concatenate((np.concatenate((R34, l4.reshape(3, 1)), axis=1), unit.reshape(1,4)), axis=0)
    H4 = np.concatenate((np.concatenate((R45, l5.reshape(3, 1)), axis=1), unit.reshape(1,4)), axis=0)
    H5 = np.concatenate((np.concatenate((R56, l6.reshape(3, 1)), axis=1), unit.reshape(1,4)), axis=0)
    H6 = np.concatenate((np.concatenate((R67, l7.reshape(3, 1)), axis=1), unit.reshape(1,4)), axis=0)


    T = H0.dot(H1).dot(H2).dot(H3).dot(H4).dot(H5).dot(H6)

    return T[0:3, 3]
</code></pre>
",11/3/2017 18:57,,1780,1,2,7,,4588128.0,,2/20/2015 13:13,140.0,47298942.0,"<p>Ok, so I have been looking at this and checked your code. The code is good and works with your defined kinematic chain with transformations from the base to the end of the robotic arm. </p>

<p>(H0 * H1 * H2 * H3 * H4 * H5 * H6)  is the correct kinematic chain where each represents a transformation from one joint to the next in the chain starting at the base of the arm.</p>

<p>The problem is your transformations are wrong. Your representation of H0 through H6 is not right and it is the numbers in these matrices that cause your transformations to not match the real transformations that take place. You need to from up the correct transformations from the base all the way to the end of the arm. Other than that, your approach is correct. </p>

<p>It looks like you are using normal DH parameters for your transformation matrices. Your values for a and d (and alpha which isn't shown in your code) are off and causing the transformations to be expressed incorrectly. The DH parameters are seen in <a href=""https://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters</a>.</p>

<p>I found an exact guide for Baxter's forward kinematics to help after going through the modified DH table to set up the transformations. I would look at the modified DH parameters at the end of the wiki article above since the guide uses that. </p>

<p><strong>Baxter Forward Kinematic Guide:</strong> <a href=""https://www.ohio.edu/mechanical-faculty/williams/html/pdf/BaxterKinematics.pdf"" rel=""nofollow noreferrer"">https://www.ohio.edu/mechanical-faculty/williams/html/pdf/BaxterKinematics.pdf</a></p>

<p>In this paper, the author, Robert Williams, sets up the DH parameters for the Baxter robotic arm and gets values different than what you have (I know you are using the normal DH parameters, but I would look at using the modified ones). His table is:</p>

<p><a href=""https://i.stack.imgur.com/HfMA7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HfMA7.png"" alt=""See paper link above from Robert Williams""></a></p>

<p>With lengths of: </p>

<p><a href=""https://i.stack.imgur.com/l9J26.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l9J26.png"" alt=""See paper link above from Robert Williams""></a></p>

<p>And using the modified DH matrix:</p>

<p><a href=""https://i.stack.imgur.com/8wWqm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8wWqm.png"" alt=""See paper link above from Robert Williams""></a></p>

<p>Now you can calculate matrices H0 through H6 and if you want you can also add the end effector geometry if you have that for an additional H7. Once you multiple them all together you should get the proper forward kinematic transformation (see the paper for additional resource). Both the left and right arms have the same kinematics. </p>

<p>When you multiply it all together then you get the expressions for the coordinates of x7, y7, and z7 from the base of the arm that are functions of the rotations of the joints and the geometry of the robot arm. See the paper on page 17 for the expressions for x7,y7, and z7. Also see page 14 for the individual transformations. </p>

<p>Also don't forget to express the angles in radians since your code uses regular trig functions. </p>

<p><strong>One last update:</strong>
I just remembered that it is easier for me to think of the intermediate translation and rotational steps one-by-one (instead of jumping straight to the DH matrix). The two approaches will be equivalent, but I like to think of each individual step that it takes to get from one rotation frame to the next. </p>

<p>For this you can use these building blocks.</p>

<p><strong>Pure Translation:</strong></p>

<pre><code>[1   0   0   u;
0    1   0   v;
0    0   1   w;
0    0   0    1]
</code></pre>

<p>Where u is the distance from the previous frame to the new frame measured from the previous x frame axis.</p>

<p>Where v is the distance from the previous frame to the new frame measured from the previous y frame axis.</p>

<p>Where w is the distance from the previous frame to the new frame measured from the previous z frame axis.</p>

<p><strong>Rotation about the z-axis by arbitrary theta:</strong>
<strong>This represent the robot joint rotation to an arbitrary theta.</strong></p>

<pre><code>[cos(theta)    -sin(theta)        0 0;
sin(theta)     cos(theta)         0 0;
0                   0             1 0;
0                   0             0 1]
</code></pre>

<p><strong>Combination of rotations around intermediate frames to get to final frame position: (these angles will usually be in increments of pi/2 or pi to be able to get to the final orientation)</strong>
Can use a rotation about the intermediate x axis, y axis, or z axis shown below.</p>

<p><strong>(Rotation about x axis by alpha)</strong></p>

<pre><code>R_x(alpha) =         [1             0           0              0;
                      0         cos(alpha)  -sin(alpha)        0;
                      0         sin(alpha)  cos(alpha)         0;
                      0            0            0              1];
</code></pre>

<p><strong>(Rotation about y axis by beta)</strong></p>

<pre><code>R_y(beta) =   [  cos(beta)     0      sin(beta)    0;
                     0         1          0        0;
                 -sin(beta)    0      cos(beta)    0;
                     0         0          0        1];
</code></pre>

<p><strong>(Rotation about z axis by gamma):</strong></p>

<pre><code>[cos(gamma)  -sin(gamma)     0      0;
sin(gamma)    cos(gamma)     0      0;
       0          0          1      0;
       0          0          0      1]
</code></pre>

<p>So with these building blocks you can build the sequence of steps to go from one frame to another (essentially any H matrix can be decomposed into these steps). The chain would be something like:</p>

<p>[H](transformation from previous frame to the next frame) = [Pure Translation from previous joint to new joint expressed in the previous joint's frame] * [Rotation about previous frame's z-axis by theta (for the joint) (since the joint has many positions, theta is left as symbolic)] * [All other intermediate rotations to arrive at the new joint frame orientation expressed as rotations about intermediate axis frames]</p>

<p>That is essentially what the DH parameters helps you do, but I like to think of the individual steps to get from one frame to the next instead of jumping there with the DH parameters. </p>

<p>With the corrected H0 through H6 transformations, your approach is correct. Just change the definitions of H0 through H6 in your code. </p>
",7117084.0,1.0,0.0,81410343.0,"Maybe you could comment your code a little bit. It is not that straight forward to understand what you intend with each step. I figured e.g. out that RXX is the robots joints rotation, but then you do not give us your coordinates for joints..."
2246,30382559,serial interfacing of 89s52 with Hyperterminal... getting garbage values,|serial-port|embedded|microcontroller|robotics|8051|,"<p>I need to transfer data serially from AT89s52 to Hyperterminal of PC.
For that I made a sample program to print ""Hello"" on the hyperterminal of my PC by programming the below given code inside 89s52 microcontroller and connecting it to my PC via serial port. Now when I am opening hyperterminal for the respective port, I should be seeing ""Hello"" printed on the screen multiple times, but what actually I am seeing is some garbage data getting printed.
This is the code I have used.</p>

<pre><code>#include &lt; AT89X52.H&gt;

#include &lt; STDLIB.H&gt; 

#include &lt; STDIO.H&gt; 

unsigned int i; 

void main (void) 

{ 

TMOD = 0x20; 

SCON = 0x50; 

TH1 = 0xFD; 

TL1 = 0xFD; 

TR1 = 1; 

TI = 1; 

P1 = 0; 

while (1) 

{ 

puts(""Hello\r""); 

P1 ^= 0x01; /* Toggle P1.0 each time we print */ 

for(i=0;i&lt;25000;i++); 

} 

} 
</code></pre>

<p>In the Hyper terminal I am not getting correct output i.e. Hello. Instead I am seeing some Garbage characters..
Can anybody help on this please..?</p>
",5/21/2015 19:20,,396,1,6,1,,4924902.0,,5/21/2015 13:22,2.0,30447360.0,"<p>Can you See P1 is toggling? I would rather send a single character first and observe what is sending by using an oscilloscope. You should see a digital signal corresponding to the ASCII value of the character being split-out from the TX pin of the micro. Also you can check the baud rate (exact value) by using the scope. If you are convinced that the correct value is sent at the right baud rate it is most likely that you got a bad connection or perhaps the baud rate should slightly be changed. </p>
",4816842.0,0.0,0.0,48878942.0,What's the crystal frequency?
2750,43175819,Distance on robot,|python|robotics|,"<p>I created a <code>robot</code> that will run based on <code>python</code>. For its autonomous program I need it to run for a certain distance( say 10 feet). Currently I am using time to have it go the distance, but is there any way to implement distance in the code to make it more exact. Thank you.</p>

<p>This was code for an old robotics competition i did and i want to learn by improving it. I used these libraries:</p>

<pre><code>import sys
import wpilib
import logging
from time import time
</code></pre>

<p>This is the code:</p>

<pre><code>def autonomous_straight(self):
    '''Called when autonomous mode is enabled'''

    t0 = time()

    slow_forward = 0.25

    t_forward_time = 6.5
    t_spin_time = 11
    t_shoot_time = 11.5


    while self.isAutonomous() and self.isEnabled():
        t = time() - t0

        if t &lt; t_forward_time:

            self.motor_left.set(slow_forward)
            self.motor_right.set(-slow_forward)
            self.motor_gobbler.set(1.0)

        elif t &lt; t_spin_time:
            self.motor_left.set(2 * slow_forward)
            self.motor_right.set(2 * slow_forward)
            self.motor_shooter.set(-1.0)

        elif t &lt; t_shoot_time:
            self.motor_mooover.set(.5)

        else:
            self.full_stop()
            self.motor_gear.set(-1.0)

        wpilib.Timer.delay(0.01)

    self.full_stop()
</code></pre>
",4/3/2017 2:48,,508,1,5,0,,7748503.0,,3/21/2017 23:47,9.0,49038551.0,"<p>It looks like you're trying to drive a distance based on time.  While this may work over short distances at known speeds, it's generally much better to drive based on sensor feedback.  What you're going to want to take a look at are <code>encoders</code>, more specifically Rotary encoders.  Encoders are simply counters that keep track of 'ticks'.  Each 'tick' represents a percent rotation of the drive shaft.</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=d%20%3D%20cir%2Fres%20*%20num"" alt=""formula""></p>

<p>where <code>d</code> is distance traveled, <code>cir</code> is the wheel circumference <code>res</code> is the encoder 'resolution' (number of ticks per rotation), and <code>num</code> is the current tick count (read off the encoder)</p>

<pre><code># Example Implementation
import wpilib
import math

# Some dummy speed controllers
leftSpeed = wpilib.Spark(0)
rightSpeed = wpilib.Spark(1)

# Create a new encoder linked to DIO pins 0 &amp; 1
encoder = wpilib.Encoder(0, 1)

# Transform ticks into distance traveled
# Assuming 6"" wheels, 512 resolution
def travelDist (ticks):
  return ((math.pi * 6) / 512) * ticks

# Define auto, takes travel distance in inches
def drive_for(dist = 10):
  while travelDist(encoder.get()) &lt; dist:
    rightSpeed.set(1)
    leftSpeed.set(-1)  # negative because one controller must be inverted
</code></pre>

<p>This simple implementation will allow you to call <code>drive_for(dist)</code> to travel a desired distance with a fair degree of accuracy.  It does however, have quite a few problems.  Here we attempt to set a <em>linear</em> speed, notice there is no acceleration control.  This will cause error to build up over longer distances, the solution to this is PID control.  wpilib has constructs to simplify the math. Simply take the difference between your setpoint and current travel distance and plug it into your PID controller as an error.  The PID controller will spit out a new value to set your motor controllers to.  The PID controller (with some tuning) can account for acceleration, inertia, and overshoot.</p>

<p>docs on PID:
<a href=""http://robotpy.readthedocs.io/projects/wpilib/en/latest/_modules/wpilib/pidcontroller.html"" rel=""nofollow noreferrer"">http://robotpy.readthedocs.io/projects/wpilib/en/latest/_modules/wpilib/pidcontroller.html</a>?</p>
",9421876.0,1.0,0.0,73425414.0,Which parameters (wrt the robot's movement) do you have control over?
4455,74415334,KheperaIV Test File is more complicated than I expected,|c++|c|robotics|,"<p>I am working on an undergrad project involving the Khepera IV mobile robot, and as I'm reading the files that came with it, I came across this line that confuses me:</p>
<pre><code>for (i=0;i&lt;5;i++) {
    usvalues[i] = (short)(Buffer[i*2] | Buffer[i*2+1]&lt;&lt;8);
...
</code></pre>
<p>From the same file, usvalues[i] is initialized as usvalues[5] for each of the ultrasonic sensors on the robot, Buffer[] is initialized as Buffer[100] i assume for the sample rate of the ultrasonic sensors. But I've never seen a variable set like this. Can someone help me to understand this?</p>
",11/12/2022 18:03,,29,1,2,0,,20028653.0,,9/18/2022 21:06,4.0,74415684.0,"<p>Code reads the <code>Buffer[]</code> array (certainly it has 8-bit elements) 2 successive bytes per iteration in little endian order (lower addressed byte is the least significant byte).  It then forms a 16-bit value to save in <code>usvalues[]</code>.</p>
<pre><code>for (i=0;i&lt;5;i++) {
  usvalues[i] = (short)(Buffer[i*2] | Buffer[i*2+1]&lt;&lt;8);
</code></pre>
<hr />
<p>Code should use <code>uint8_t Buffer[100];</code> to prevent doing a <em>signed</em> left shift.</p>
<p><code>usvalues[]</code> better as some <em>unsigned</em> type like <code>uint16_t</code> or <code>unsigned</code> and use <em>unsigned</em> operations.</p>
<pre><code>uint8_t Buffer[100];
uint16_t /* or unsigned */ usvalues[5 /* or more */];

for (i = 0; i &lt; 5; i++) {
  usvalues[i] = Buffer[i*2] | (unsigned)Buffer[i*2+1] &lt;&lt; 8;
}
</code></pre>
",2410359.0,1.0,0.0,131368173.0,"Look at this list of operators : https://en.cppreference.com/w/cpp/language/operator_arithmetic. You will find `|` is bitwise or and `<<` is shift left. It is an efficient way to calculate 16 bit values from an 8bit buffer (little endian as Weather Vane said). The cast in C++ should be a static_cast<short> though not the ""C"" style cast (short) though."
2281,31547941,ROS - ROAR compile error on Ubuntu,|ubuntu|robotics|ros|,"<p>I am trying to install <code>ROAR</code> on Ubuntu 14.04 following this tutorial: <a href=""http://wiki.ros.org/roar"" rel=""nofollow"">http://wiki.ros.org/roar</a></p>

<p>However, I am stuck on <code>Compiling</code> step 3.2 <code>rosmake roar --rosdep-install</code><br/>
I get an error <code>error: sudo: rosmake: command not found</code></p>

<p>I downloaded <br/><code>svn co https://mediabox.grasp.upenn.edu/svn/penn-ros-pkgs/roar_stack/trunk ~/ros/roar_stack</code> into my home directory, not <code>/opt/ros</code><br/>
and set PATH to <code>export ROS_PACKAGE_PATH=~/ros:$/home/myfolder/ros/roar_stack</code></p>

<p>What am I doing wrong?</p>

<p><strong>update</strong> I installed a clean version of <code>Ubuntu 14.04</code> on VMware and installed <code>ROS Jade</code> and the same problem persists.</p>
",7/21/2015 19:28,,294,0,9,0,,1057045.0,,11/21/2011 2:22,531.0,,,,,,51057256.0,"I rebooted and now `rosmake -h` returns data, but i ran your steps then tried to run `rosmake` and now getting `command not found`"
4727,77649089,Scara Robot Inverse Kinematics,|c++|math|arduino|robotics|,"<p>I am working on a Scara Robot controlled by an Arduino and trying to figure out the inverse kinematics. I programmed a simple example sketch to test my function but it doesn't work because of a mathematical error. Can someone tell me what's wrong?</p>
<pre><code>const int L1 = 202.25;
const int L2 = 193.35;

int alpha = 0;
int beta = 0;

void calc_IK(int x, int y){
  beta = acos((x*x+y*y-L1*L1-L2*L2)/2*L1*L2); //I could also ise pow()
  alpha = atan((y/x)-atan(L2*sin(beta)/L1+L2*cos(beta)));
  Serial.println(alpha, beta);
}

void setup() {
  Serial.begin(9600);
}

void loop() {
  calc_IK(3, 5);
  delay(200);
}
</code></pre>
",12/12/2023 21:21,,79,1,4,-1,,17977602.0,,1/19/2022 19:23,6.0,77668814.0,"<p>I am not sure where the error was but I changed some variables to float and some to doubles. I also noticed that the result was in radians so I have to convert it to degrees. Here is my now functioning code:</p>
<pre><code>#include &lt;math.h&gt;

const double L1 = 202.25;
const double L2 = 193.35;

float alpha = 0; //radians
float beta = 0;

float beta_d; //degrees
float alpha_d;

void calc_IK(float x, float y){
  beta = acos((pow(x, 2)+pow(y, 2)-pow(L1, 2)-pow(L2, 2))/(2*pow(L1, 2)*pow(L2, 2)));
  alpha = atan((y/x)-atan((L2*sin(beta))/(L1+L2*cos(beta))));

  beta_d = beta * 180/M_PI;
  alpha_d = alpha * 180/M_PI;


  Serial.print(&quot;Beta: &quot;);
  Serial.print(beta_d);
  Serial.print(&quot; &quot;);
  Serial.print(&quot;Alpha: &quot;); 
  Serial.print(alpha_d);
  Serial.println(&quot; &quot;);

}

void setup() {
  Serial.begin(9600);
}

void loop() {
  calc_IK(3.0, 5.0);
}
</code></pre>
<p>It may not be the best but it works.</p>
",17977602.0,0.0,1.0,136894859.0,"the same goes for the angles `alfa,beta` and probably also coordiantes `x,y`... maybe `float` is enough as FPU is not a guarantee on arduino used MCUs ..."
2278,31478749,Controlling two dc motors (on arduino) through c++ source code,|c++|eclipse|opencv|arduino|robotics|,"<p>I'm working on a face detection robot project.I'm using opencv software to detect faces. When the face is detected i want to get the x-y coordinates and send them to an arduino board. </p>

<p>The arduino has two dc motors connected. The first dc motor will spin a base (the base of a robot's head) according to y-coordinates (y-axis). </p>

<p>The second dc motor will handle the x-coordinates on the x-axis (i want to make the robot's eyes go up and down). </p>

<p>I work with the code on eclipse (kepler), my os is ubuntu 12.04 and i have an arduino uno.  My source code is written in c++ and opencv. The arduino is connected with my pc through USB port.  </p>

<p>My question is how can i take the x-y coordinates from my opencv source code in order to transfer them to arduino?</p>

<p>And how can i receive and handle the coordinates in arduino?</p>
",7/17/2015 14:57,31479691.0,600,1,0,0,,2304504.0,Greece,4/21/2013 14:06,99.0,31479691.0,"<p>You need some way to interface with USB serial ports from your C++ code. A quick Google search leads me to this C++ serial library for Ubuntu: <a href=""https://apps.ubuntu.com/cat/applications/precise/libserial-dev/"" rel=""nofollow"">libserial-dev</a>. </p>

<p>On the Arduino side, you certainly want to look at the <a href=""https://www.arduino.cc/en/reference/serial"" rel=""nofollow"">Arduino Serial interface</a> to receive the data you're sending.</p>

<p>To follow that up, look for a tutorial on basic usage. Start with a simple ""Hello World"" and then try to echo back and forth between your C++ code and your Arduino. Then, it's up to you to design your data transfer protocol.</p>
",1576412.0,1.0,0.0,,
1332,9614729,Issue regarding practical approach on machine learning/computer vision fields,|c++|matlab|machine-learning|computer-vision|robotics|,"<p>I am really passionate about the machine learning,data mining and computer vision fields and I was thinking at taking things a little bit further.</p>

<p>I was thinking at buying a LEGO Mindstorms NXT 2.0 robot for trying to experiment machine learning/computer vision and robotics algorithms in order to try to understand better several existing concepts.</p>

<p>Would you encourage me into doing so? Do you recommend any other alternative for a practical approach in understanding these fields which is acceptably expensive like(nearly 200 - 250 pounds) ? Are there any mini robots which I can buy and experiment stuff with?</p>
",3/8/2012 8:28,9616077.0,466,2,1,4,0.0,442124.0,Romania,9/8/2010 6:42,607.0,9616077.0,"<p>If your interests are machine learning, data mining and computer vision then I'd say a Lego mindstorms is not the best option for you. Not unless you are also interested in robotics/electronics.</p>

<ul>
<li>Do do interesting machine learning you only need a computer and a problem to solve. Think <a href=""http://aichallenge.org/"">ai-contest</a> or <a href=""http://mlcomp.org/"">mlcomp</a> or similar.</li>
<li>Do do interesting data mining you need a computer, a lot of data and a question to answer. If you have an internet connection the amount of data you can get at is only limited by your bandwidth. Think <a href=""http://www.netflixprize.com/"">netflix prize</a>, try your hand at collecting and interpreting data from wherever. If you are learning, <a href=""http://www.autonlab.org/tutorials/"">this is a nice place to start</a>.</li>
<li>As for computer vision: All you need is a computer and images. Depending on the type of problem you find interesting you could do some processing of <a href=""http://www.opentopia.com/hiddencam.php"">random webcam images</a>, take all you holiday photo's and try to detect <a href=""https://en.wikipedia.org/wiki/Face_detection"">where all your travel companions are</a> in them. If you have a webcam your options are endless.</li>
</ul>

<p>Lego mindstorms allows you to combine machine learning and computer vision. I'm not sure where the datamining would come in, and you will spend (waste?) time on the robotics/electronics side of things, which you don't list as one of your passions. </p>
",7531.0,8.0,0.0,12199822.0,The question might be too subjective but I would definitely encourage you to do so. Those LEGO Mindstorms seem a bit expensive but I've seen many awesome applications for them.
3880,64070085,Question about relationship between wb_motor_set_position and wb_robot_step,|c|robotics|webots|,"<p>I'm really new to Webots and a novice at programming. I'm currently trying to drive a robot with rotational motors along a known path using a controller in C language. I'm mainly using the wb_motor_set_position function. However, the amount the robot actually travels in the simulation seems to depend on the position I set in this function, as well as the time step. I am currently running a while loop, setting wb_robot_step(TIME_STEP) and then using wb_motor_set_position. I've been reading the documentation on these, but it still doesn't seem to click to me. Does anyone know how these two functions are dependent/related to each other and how I could maybe determine the distance the robot would go with these (without using a position sensor first) - instead of my current just plug and chug method...? Thank you!</p>
",9/25/2020 19:07,,261,0,3,0,,14342141.0,,9/25/2020 18:59,4.0,,,,,,113298552.0,"Rather than only describe your code, post it here."
4294,71757025,C script for guiding a line follower,|c|state-machine|robotics|,"<p>So for class I am trying to write a C script for a line follower robot. The robot has 3 sensors (ABC) which give logic 1 on black and logic 0 on white, A is on the left side, B in the middle and C on the right when looking straight down on the robot. It also has 2 motors, one on each side.</p>
<p>The board I am using is an <a href=""https://eu.mouser.com/ProductDetail/Texas-Instruments/MSP-EXP430G2?qs=CLImetaeaXWH2pYG%252BA%252B4Vw%3D%3D"" rel=""nofollow noreferrer"">Texas Instruments MSP-EXP430G2</a> and I am using the ports P1.0 - P1.7.</p>
<p>Now, I have literally 0 experience writing C scripts so all pointers are very much appreciated.</p>
<p>Here is the code.</p>
<pre><code># include &quot;msp430G2553.h&quot;
# include &quot;stdio.h&quot;
# include &quot;math.h&quot;

#define Both_On 1 // States
#define Left_On 2
#define Right_On 3
int main (void)
{
char STATE;
P1DIR|=0x0F; // Port1 four lower bits as OUTput, others as INput(0000 1111)
WDTCTL = WDTPW + WDTHOLD;// stop watch dog
STATE = Both_On; // Start here
char MASK1=0x10; //Sensor mask 0001 0000 (P1.0 - P1.7 from left to right)
char L; //Direction switch
while(1)
{
L = P1IN &amp; MASK1; //Sensor value based on input-port
switch (STATE)
{
case Both_On:
    P1OUT = 0x03;//Both motors on
    puts(&quot;Both_On&quot;); //for testing
    if (L == 0x00 ){
        P1OUT=0x01;//left motor on
        STATE = Left_On;}
    else if (L==0x10){
        P1OUT=0x03;
        STATE = Both_On;}
    break;
case Left_On:
    P1OUT = 0x01;//left motor on
    puts(&quot;Left_on&quot;); //for testing
    if (L == 0x10 ){
        P1OUT=0x03;//both motors on
        STATE = Both_On;}
    else if (L==0x00){
        P1OUT= 0x02;
        STATE = Right_On;}
    break;
case Right_On:
    P1OUT = 0x02;//right motor on
    puts(&quot;Right_on&quot;); //for testing
    if (L == 0x10 ){
        P1OUT=0x03;//both motors on
        STATE = Both_On;}
    else if (L==0x00){
        P1OUT= 0x01;
        STATE = Left_On;}
    break;
break;
}//end of Switch
}// end of while
</code></pre>
<p>I think I've understood the method for switching from one active motor to another or both. The first 4 bits are reserved for the input from the sensors and the last 4 bits direct the motors. So P1OUT=0x03 basically means the output is 0000 0011 so the ports P1.6 and P1.7 are active and the motors wired to those ports would turn on.</p>
<p>I've been instructed to use an &quot;L&quot; variable as the direction switch. My problem is that I don't quite understand how this works.</p>
<p>I've been thinking that I would wire the B sensor to the port &quot;L&quot; takes it value from and then saying, if L==1 then both motors should be on and if L==0 then the state should change from &quot;Left_on&quot; to &quot;Right_on&quot; in a loop but this behavior doesn't feel very logical, I would want better detection.</p>
<p>Could I change the mask to be &quot;MASK1=0x70&quot; (0111) which would give me 3 ports to wire the sensors into, and guide the motors using for example:</p>
<pre><code>else if (L==0x40){ //0100 only the left side sensor A is on black
        P1OUT= 0x02; //0010 P1.6
        STATE = Right_On;} // turn the right motor on to keep following the line
    break;
</code></pre>
<p>As I said, all pointers and suggestions are highly appreciated.</p>
",4/5/2022 18:53,,250,0,2,0,,17988251.0,,1/20/2022 22:14,5.0,,,,,,126811174.0,"Then it would be good if you would highlight the exact problem you are having, and what is the actual question. The code you show at the end of the question, so what does it do if you run it on the actual robot?"
1914,21743600,How to measure distance at angle in image python,|python|mapping|robotics|particle-filter|,"<p>I'm working on a particle filter for an autonomous robot right now, and am having trouble producing expected distance measurements by which to filter the particles. I have an image that I'm using as a map. Each pixel represents a certain scaled area in the enviroment. Space the robot can occupy is white, walls are black, and areas that are exterior to the enviroment are grey.</p>

<p>If you are unfamiliar with what a particle filter is, my python code will create a predetermined number of random guesses as to where it might be (x,y,theta) in the white space. It will then measure the distance to the nearest wall with ultrasonic sensors at several angles. The script will compare these measurements with the measurements that would have been expected at each angle for each guessed location/orientation. Those that most closely match the actual measurements will survive while guesses that are less likely to be right will be eliminated.</p>

<p>My problem is finding the nearest wall AT a given angle. Say the sensor is measuring at 60°.  For each guess, I need to adjust the angle to account for the guessed robot orientation, and then measure the distance to the wall at that angle. It's easy enough find the nearest wall in the x direction:</p>

<pre><code>from PIL import Image
#from matplotlib._png import read_png
from matplotlib.pyplot import *
mapp = Image.open(""Map.png"")
pixels = mapp.load()
width = mapp.size[0]
height = mapp.size[1]
imshow(mapp)

pixelWidth = 5

for x in range(width):
    if mapp.getpixel((x, 100)) == (0,0,0,255): #Identify the first black pixel
        distance = x*pixelWidth self.x
</code></pre>

<p>The problem is that I can't tell the script to search one pixel at a time going at a 60°, or 23°, or whatever angle. Right now the best thing I can think of is to go in the x direction first, find a black pixel, and then use the tangent of the angle to determine how many pixels I need to move up or down, but there are obvious problems with this, mostly having to do with corners, and I can't imagine how many if statements it's going to take to work around it. Is there another solution?</p>
",2/13/2014 1:43,21746379.0,1597,1,0,2,,2202534.0,,3/23/2013 15:02,16.0,21746379.0,"<p>Okay, I think I found a good approximation of what I'm trying to do, though I'd still like to hear if anyone else has a better solution. By checking the tangent of the angle I've actually traveled so far between each pixel move, I can decide whether to move one pixel in the x-direction, or in the y-direction.</p>

<pre><code>for i in range(len(angles)):
    angle = self.orientation+angles[i]
       if angle &gt; 360:
           angle -= 360
        x = self.x
        y = self.y
        x1 = x
        y1 = y
        xtoy_ratio = tan(angle*math.pi/180)
        if angle &lt; 90:
            xadd = 1
            yadd = 1
        elif 90 &lt; angle &lt; 180:
            xadd = -1
            yadd = 1
        elif 180 &lt; angle &lt; 270:
            xadd = -1
            yadd = -1
        else:
            xadd = 1
            yadd = -1
        while mapp.getpixel(x,y) != (0,0,0,255):
            if (y-y1)/(x-x1) &lt; xtoy_ratio:
                y += yadd
            else:
                x += xadd
        distance = sqrt((y-y1)^2+(x-x1)^2)*pixel_width
</code></pre>

<p>The accuracy of this method of course depends a great deal on the actual length represented by each pixel. As long as pixel_width is small, accuracy will be pretty good, but if not, it will generally go pretty far before correcting itself.</p>

<p>As I said, I welcome other answers.</p>

<p>Thanks</p>
",2202534.0,0.0,0.0,,
3241,53493976,Loop arcsin possible answers,|python|python-3.x|python-2.7|robotics|,"<p>I am currently taking a robotic course in my study . There is a lab that we require to find inverse kinematics for our robots and in order to do that, there will be many arcsin and arccos needed to find the all the theta(s)</p>

<p>For example,</p>

<pre><code>theta1 = arcsin(2/3)    &lt;--- This will give two answers
theta2 = arcsin(theta1/2)     &lt;--- This will give 2 answers with 2 different theta1
theta3 = arcsin(theta1/theta2) &lt;--- This too
</code></pre>

<p>So my question is , how do I loop to find all the multiple solutions for those dependent equations ?</p>
",11/27/2018 6:30,,124,0,5,0,,5957146.0,"Ottawa, ON, Canada",2/20/2016 23:32,26.0,,,,,,93857654.0,"Since sin is a cyclic function, there is actually an infinite amount of solutions, x = x0 + 2k*pi for any integer k. arcsin just returns the solution with 0 <= x < 2*pi."
2349,32292996,interface with the computer in a motor vehicle using the .NET framework,|.net|interface|robotics|,"<p>Is there a way to communicate with the ECU or engine computer of a car to do things like read ODB/CAN codes similar to a handheld code scanner you would get at the local auto parts store? </p>

<p>Ideally a Microsoft .NET API or COM interface is what I am personally looking for, but I can see an API like that coming in other strange and legacy flavors.  </p>
",8/30/2015 3:53,,56,2,0,0,,589509.0,"Arkham, Essex County, Massachusetts.",1/25/2011 19:09,620.0,32293235.0,"<p>You can't just plug USB in to your car and expect it to work, there will be some kind of hardware between the two. That peice of hardware will come with a SDK to write software that uses it.</p>

<p>What you need to do is choose a piece of hardware to buy that comes with a SDK that works in .NET (in a fully managed class or, more likely, as a set of C interfaces you will need to P/Invoke to).</p>
",80274.0,0.0,0.0,,
4522,75532778,Controller manager not available in ROS2,|ros|robotics|ros2|gazebo-simu|urdf|,"<p>Hi I have added control plugin in robot arm urdf then created launch file but when I run launch file gazebo and robot arm model open but I get an error like below. How can I solve this problem?</p>
<pre><code>[robot_state_publisher-3] Parsing robot urdf xml string.
[robot_state_publisher-3] Error:   Error document empty.
[robot_state_publisher-3]          at line 71 in /tmp/binarydeb/ros-foxy-urdfdom-2.3.3/urdf_parser/src/model.cpp
[robot_state_publisher-3] terminate called after throwing an instance of 'std::runtime_error'
[robot_state_publisher-3]   what():  Unable to initialize urdf::model from robot description
[ros2_control_node-4] terminate called after throwing an instance of 'std::runtime_error'
[ros2_control_node-4]   what():  invalid URDF passed in to robot parser
[spawner.py-5] [INFO] [1677069069.554681723] [spawner_joint_state_broadcaster]: Waiting for /controller_manager services
[spawner.py-6] [INFO] [1677069069.580805205] [spawner_joint_trajectory_controller]: Waiting for /controller_manager services
[spawn_entity.py-2] [INFO] [1677069069.734785422] [spawn_entity]: Spawn Entity started
[spawn_entity.py-2] [INFO] [1677069069.735058596] [spawn_entity]: Loading entity XML from file /home/gursel/robotic_arm_ws/install/robotic_arm/share/robotic_arm/urdf/robotic_arm.urdf
[spawn_entity.py-2] [INFO] [1677069069.736418915] [spawn_entity]: Waiting for service /spawn_entity, timeout = 30
[spawn_entity.py-2] [INFO] [1677069069.736653475] [spawn_entity]: Waiting for service /spawn_entity
[ERROR] [gazebo-1]: process has died [pid 4545, exit code 255, cmd 'gazebo -s libgazebo_ros_factory.so'].
[ERROR] [robot_state_publisher-3]: process has died [pid 4549, exit code -6, cmd '/opt/ros/foxy/lib/robot_state_publisher/robot_state_publisher /home/gursel/robotic_arm_ws/install/robotic_arm/share/robotic_arm/urdf/robotic_arm.urdf --ros-args --params-file /tmp/launch_params_302rtmoc'].
[ERROR] [ros2_control_node-4]: process has died [pid 4551, exit code -6, cmd '/opt/ros/foxy/lib/controller_manager/ros2_control_node --ros-args --params-file /tmp/launch_params_wb0x7xnl --params-file /home/gursel/robotic_arm_ws/install/robotic_arm/share/robotic_arm/config/jtc.yaml'].
[spawner.py-5] [INFO] [1677069071.570302266] [spawner_joint_state_broadcaster]: Waiting for /controller_manager services
[spawner.py-6] [INFO] [1677069071.595951515] [spawner_joint_trajectory_controller]: Waiting for /controller_manager services
[spawner.py-5] [INFO] [1677069073.586642702] [spawner_joint_state_broadcaster]: Waiting for /controller_manager services
[spawner.py-6] [INFO] [1677069073.612373454] [spawner_joint_trajectory_controller]: Waiting for /controller_manager services
[spawner.py-5] [INFO] [1677069075.603461546] [spawner_joint_state_broadcaster]: Waiting for /controller_manager services
[spawner.py-6] [INFO] [1677069075.629372887] [spawner_joint_trajectory_controller]: Waiting for /controller_manager services
[spawner.py-5] [INFO] [1677069077.620421456] [spawner_joint_state_broadcaster]: Waiting for /controller_manager services
[spawner.py-6] [INFO] [1677069077.646128433] [spawner_joint_trajectory_controller]: Waiting for /controller_manager services
[spawner.py-5] [ERROR] [1677069079.637362419] [spawner_joint_state_broadcaster]: Controller manager not available
[spawner.py-6] [ERROR] [1677069079.662836259] [spawner_joint_trajectory_controller]: Controller manager not available
[ERROR] [spawner.py-5]: process has died [pid 4553, exit code 1, cmd '/opt/ros/foxy/lib/controller_manager/spawner.py joint_state_broadcaster --controller-manager /controller_manager --ros-args'].
[ERROR] [spawner.py-6]: process has died [pid 4555, exit code 1, cmd '/opt/ros/foxy/lib/controller_manager/spawner.py joint_trajectory_controller -c /controller_manager --ros-args'].
</code></pre>
<p><em><strong>SOLVED:</strong></em>
The problem was ROS2 version and changing in launch file. It works with <strong>Ubuntu 22.04 Humble</strong> and you should install all control package such as joint_trajectory_controller, joint_state_broadcaster, controller_manager, joint-state-publisher-gui then edit your launch file.</p>
",2/22/2023 12:37,,2393,0,1,1,,18021550.0,,1/24/2022 20:42,25.0,,,,,,134548261.0,"Had the same issue. sudo apt-get install ros-humble-gazebo-ros2-control
solved it for me."
1798,18162880,How to correctly compute direct kinematics for a delta robot?,|java|math|computational-geometry|robotics|kinematics|,"<p>I'm trying to put together a simple simulation for a delta robot and I'd like to use forward kinematics (direct kinematics) to compute the end effector's position in space by passing 3 angles.</p>

<p>I've started with the <a href=""http://forums.trossenrobotics.com/tutorials/introduction-129/delta-robot-kinematics-3276/"" rel=""noreferrer"">Trossen Robotics Forum Delta Robot Tutorial</a> and I can understand most of the math, but not all. I'm lost at the last part in forward kinematics, when trying to compute the point where the 3 sphere's intersect. I've looked at spherical coordinates in general but couldn't work out the two angles used to find to rotate towards (to E(x,y,z)). I see they're solving the equation of a sphere, but that's where I get lost.</p>

<p><img src=""https://i.stack.imgur.com/dhZ9o.png"" alt=""delta robot direct kinematics""></p>

<p><img src=""https://i.stack.imgur.com/VXSNG.png"" alt=""delta robot direct kinematics""></p>

<p><img src=""https://i.stack.imgur.com/P2L7Y.png"" alt=""delta robot direct kinematics""></p>

<p>A delta robot is a parallel robot (meaning the base and the end effector(head) always stay parallel). The base and end effector are equilateral triangles and the legs are (typically) placed at the middle of the triangle's sides. </p>

<p>The side of the base of the delta robot is marked <code>f</code>.
The side of the effector of the delta robot is marked <code>e</code>.
The upper part of the leg is marked <code>rf</code> and the lower side <code>re</code>.</p>

<p>The origin(O) is at the centre of the base triangle.
The servo motors are at the middle of the base triangle's sides (F1,F2,F3).
The joints are marked J1,J2,J3. The lower legs join the end effector at points E1,E2,E3
and E is the centre of the end effector triangle.</p>

<p>I can easily compute points F1,F2,F3 and J1,J2,J3.
It's E1,E2,E3 I'm having issues with. From the explanations,
I understand that point J1 gets translate inwards a bit (by half the end effector's median)
to J1' and it becomes the centre of a sphere with radius <code>re</code> (lower leg length).
Doing this for all joints will result in 3 spheres intersecting in the same place: E(x,y,z). By solving the sphere equation we find E(x,y,z).</p>

<p>There is also a formula explained:</p>

<p><img src=""https://i.stack.imgur.com/aceZj.png"" alt=""dk equation 1""></p>

<p><img src=""https://i.stack.imgur.com/rpIhH.png"" alt=""dk equation 2"">
but this is where I get lost. My math skills aren't great.
Could someone please explain those in a simpler manner, 
for the less math savvy of us ?</p>

<p>I've also used the sample code provided which (if you have a WebGL enabled 
browser) you can run <a href=""http://studio.sketchpad.cc/NvZk3mGPWx"" rel=""noreferrer"">here</a>. Click and drag to rotate the scene. To control the three angles use q/Q, w/W,e/E to decrease/increase angles.</p>

<p>Full code listing:</p>

<pre><code>//Rhino measurements in cm
final float e = 21;//end effector side
final float f = 60.33;//base side
final float rf = 67.5;//upper leg length - radius of upper sphere
final float re = 95;//lower leg length - redius of lower sphere (with offset will join in E(x,y,z))

final float sqrt3 = sqrt(3.0);
final float sin120 = sqrt3/2.0;   
final float cos120 = -0.5;        
final float tan60 = sqrt3;
final float sin30 = 0.5;
final float tan30 = 1/sqrt3;
final float a120 = TWO_PI/3;
final float a60 = TWO_PI/6;

//bounds
final float minX = -200;
final float maxX = 200;
final float minY = -200;
final float maxY = 200;
final float minZ = -200;
final float maxZ = -10;
final float maxT = 54;
final float minT = -21;

float xp = 0;
float yp = 0;
float zp =-45;
float t1 = 0;//theta
float t2 = 0;
float t3 = 0;

float prevX;
float prevY;
float prevZ;
float prevT1;
float prevT2;
float prevT3;

boolean validPosition;
//cheap arcball
PVector offset,cameraRotation = new PVector(),cameraTargetRotation = new PVector();

void setup() {
  size(900,600,P3D);
}

void draw() {
  background(192);
  pushMatrix();
  translate(width * .5,height * .5,300);
  //rotateY(map(mouseX,0,width,-PI,PI));

  if (mousePressed &amp;&amp; (mouseX &gt; 300)){
    cameraTargetRotation.x += -float(mouseY-pmouseY);
    cameraTargetRotation.y +=  float(mouseX-pmouseX);
  }
  rotateX(radians(cameraRotation.x -= (cameraRotation.x - cameraTargetRotation.x) * .35));
  rotateY(radians(cameraRotation.y -= (cameraRotation.y - cameraTargetRotation.y) * .35));

  stroke(0);
  et(f,color(255));
  drawPoint(new PVector(),2,color(255,0,255));
  float[] t = new float[]{t1,t2,t3};
  for(int i = 0 ; i &lt; 3; i++){
    float a = HALF_PI+(radians(120)*i);
    float r1 = f / 1.25 * tan(radians(30));
    float r2 = e / 1.25 * tan(radians(30));
    PVector F = new PVector(cos(a) * r1,sin(a) * r1,0);
    PVector E = new PVector(cos(a) * r2,sin(a) * r2,0);
    E.add(xp,yp,zp);
    //J = F * rxMat
    PMatrix3D m = new PMatrix3D();
    m.translate(F.x,F.y,F.z);
    m.rotateZ(a);
    m.rotateY(radians(t[i]));
    m.translate(rf,0,0);

    PVector J = new PVector();
    m.mult(new PVector(),J);
    line(F.x,F.y,F.z,J.x,J.y,J.z);
    line(E.x,E.y,E.z,J.x,J.y,J.z);
    drawPoint(F,2,color(255,0,0));
    drawPoint(J,2,color(255,255,0));
    drawPoint(E,2,color(0,255,0));
    //println(dist(F.x,F.y,F.z,J.x,J.y,J.z)+""\t""+rf);
    println(dist(E.x,E.y,E.z,J.x,J.y,J.z)+""\t""+re);//length should not change
  }
  pushMatrix();
    translate(xp,yp,zp);
    drawPoint(new PVector(),2,color(0,255,255));
    et(e,color(255));
    popMatrix();
  popMatrix(); 
}
void drawPoint(PVector p,float s,color c){
  pushMatrix();
    translate(p.x,p.y,p.z);
    fill(c);
    box(s);
  popMatrix();
}
void et(float r,color c){//draw equilateral triangle, r is radius ( median), c is colour
  pushMatrix();
  rotateZ(-HALF_PI);
  fill(c);
  beginShape();
  for(int i = 0 ; i &lt; 3; i++)
    vertex(cos(a120*i) * r,sin(a120*i) * r,0);
  endShape(CLOSE);
  popMatrix();
}
void keyPressed(){
  float amt = 3;
  if(key == 'q') t1 -= amt;
  if(key == 'Q') t1 += amt;
  if(key == 'w') t2 -= amt;
  if(key == 'W') t2 += amt;
  if(key == 'e') t3 -= amt;
  if(key == 'E') t3 += amt;
  t1 = constrain(t1,minT,maxT);
  t2 = constrain(t2,minT,maxT);
  t3 = constrain(t3,minT,maxT);
  dk();
}

void ik() {
  if (xp &lt; minX) { xp = minX; }
  if (xp &gt; maxX) { xp = maxX; }
  if (yp &lt; minX) { yp = minX; }
  if (yp &gt; maxX) { yp = maxX; }
  if (zp &lt; minZ) { zp = minZ; }
  if (zp &gt; maxZ) { zp = maxZ; }

  validPosition = true;
  //set the first angle
  float theta1 = rotateYZ(xp, yp, zp);
  if (theta1 != 999) {
    float theta2 = rotateYZ(xp*cos120 + yp*sin120, yp*cos120-xp*sin120, zp);  // rotate coords to +120 deg
    if (theta2 != 999) {
      float theta3 = rotateYZ(xp*cos120 - yp*sin120, yp*cos120+xp*sin120, zp);  // rotate coords to -120 deg
      if (theta3 != 999) {
        //we succeeded - point exists
        if (theta1 &lt;= maxT &amp;&amp; theta2 &lt;= maxT &amp;&amp; theta3 &lt;= maxT &amp;&amp; theta1 &gt;= minT &amp;&amp; theta2 &gt;= minT &amp;&amp; theta3 &gt;= minT ) { //bounds check
          t1 = theta1;
          t2 = theta2;
          t3 = theta3;
        } else {
          validPosition = false;
        }

      } else {
        validPosition = false;
      }
    } else {
      validPosition = false;
    }
  } else {
    validPosition = false;
  }

  //uh oh, we failed, revert to our last known good positions
  if ( !validPosition ) {
    xp = prevX;
    yp = prevY;
    zp = prevZ;
  }

}

void dk() {
  validPosition = true;

  float t = (f-e)*tan30/2;
  float dtr = PI/(float)180.0;

  float theta1 = dtr*t1;
  float theta2 = dtr*t2;
  float theta3 = dtr*t3;

  float y1 = -(t + rf*cos(theta1));
  float z1 = -rf*sin(theta1);

  float y2 = (t + rf*cos(theta2))*sin30;
  float x2 = y2*tan60;
  float z2 = -rf*sin(theta2);

  float y3 = (t + rf*cos(theta3))*sin30;
  float x3 = -y3*tan60;
  float z3 = -rf*sin(theta3);

  float dnm = (y2-y1)*x3-(y3-y1)*x2;

  float w1 = y1*y1 + z1*z1;
  float w2 = x2*x2 + y2*y2 + z2*z2;
  float w3 = x3*x3 + y3*y3 + z3*z3;

  // x = (a1*z + b1)/dnm
  float a1 = (z2-z1)*(y3-y1)-(z3-z1)*(y2-y1);
  float b1 = -((w2-w1)*(y3-y1)-(w3-w1)*(y2-y1))/2.0;

  // y = (a2*z + b2)/dnm;
  float a2 = -(z2-z1)*x3+(z3-z1)*x2;
  float b2 = ((w2-w1)*x3 - (w3-w1)*x2)/2.0;

  // a*z^2 + b*z + c = 0
  float a = a1*a1 + a2*a2 + dnm*dnm;
  float b = 2*(a1*b1 + a2*(b2-y1*dnm) - z1*dnm*dnm);
  float c = (b2-y1*dnm)*(b2-y1*dnm) + b1*b1 + dnm*dnm*(z1*z1 - re*re);

  // discriminant
  float d = b*b - (float)4.0*a*c;
  if (d &lt; 0) { validPosition = false; }

  zp = -(float)0.5*(b+sqrt(d))/a;
  xp = (a1*zp + b1)/dnm;
  yp = (a2*zp + b2)/dnm;

  if (xp &gt;= minX &amp;&amp; xp &lt;= maxX&amp;&amp; yp &gt;= minX &amp;&amp; yp &lt;= maxX &amp;&amp; zp &gt;= minZ &amp; zp &lt;= maxZ) {  //bounds check
  } else {
    validPosition = false;
  }

  if ( !validPosition ) {    
    xp = prevX;
    yp = prevY;
    zp = prevZ;
    t1 = prevT1;
    t2 = prevT2;
    t3 = prevT3;  
  }

}

void  storePrev() {
  prevX = xp;
  prevY = yp;
  prevZ = zp;
  prevT1 = t1;
  prevT2 = t2;
  prevT3 = t3;
}

float rotateYZ(float x0, float y0, float z0) {
  float y1 = -0.5 * 0.57735 * f; // f/2 * tg 30
  y0 -= 0.5 * 0.57735    * e;    // shift center to edge
  // z = a + b*y
  float a = (x0*x0 + y0*y0 + z0*z0 +rf*rf - re*re - y1*y1)/(2*z0);
  float b = (y1-y0)/z0;
  // discriminant
  float d = -(a+b*y1)*(a+b*y1)+rf*(b*b*rf+rf); 
  if (d &lt; 0) return 999; // non-existing point
  float yj = (y1 - a*b - sqrt(d))/(b*b + 1); // choosing outer point
  float zj = a + b*yj;
  return 180.0*atan(-zj/(y1 - yj))/PI + ((yj&gt;y1)?180.0:0.0);
} 
</code></pre>

<p>The problem is, when visualizing, the lower part changes length (as you can see in the printed message0 and it shouldn't, which further adds to my confusion.</p>

<p>I've used the supplied C code in Java/Processing, but the programming language is least important.</p>

<p><strong>[Edit by spektre]</strong></p>

<p>I just had to add this picture (for didactic reasons).</p>

<ul>
<li>the lined nonsense is not the best way for grasping the kinematics abilities</li>
<li>as I understand the base with the motors is on this image on the upper triangle plane</li>
<li>and the tool is on the bottom triangle plane</li>
</ul>

<p><img src=""https://i.stack.imgur.com/lDMXm.png"" alt=""delta robot""></p>
",8/10/2013 14:32,,16289,1,2,8,0.0,89766.0,"London, United Kingdom",4/11/2009 14:07,12545.0,21018861.0,"<p>I would do it as follows (algebraic representation of graphic solution): </p>

<ol>
<li>compute F1,F2,F3; </li>
<li><p>solve system</p>

<pre><code>// spheres from Ji to Ei ... parallelograms (use lower Z half sphere)
(x1-J1.x)^2 + (y1-J1.y)^2 +(z1-J1.z)^2 = re^2 
(x2-J2.x)^2 + (y2-J2.y)^2 +(z2-J2.z)^2 = re^2
(x3-J3.x)^2 + (y3-J3.y)^2 +(z3-J3.z)^2 = re^2
// Ei lies on the sphere
E1=(x1,y1,z1)
E2=(x2,y2,z2)
E3=(x3,y3,z3)
// Ei is parallel to Fi ... coordinate system must be adjusted 
// so base triangles are parallel with XY-plane
z1=z2
z1=z3
z2=z3
// distance between any Ei Ej must be always q
// else it is invalid position (kinematics get stuck or even damage)
|E1-E2|=q
|E1-E3|=q
|E2-E3|=q
// midpoint is just average of Ei
E=(E1+E2+E3)/3
</code></pre>

<ul>
<li>where q is the joint distance |Ei-E| which is constant</li>
</ul></li>
</ol>

<p><strong>[Notes]</strong></p>

<p>Do not solve it manually</p>

<ul>
<li>use derive or something to obtain algebraic solution</li>
<li>and use only valid solution</li>
<li>its quadratic system so there will be most likely more solutions so you have to check for the correct one</li>
</ul>

<p>Just a silly question why don't you solve inverse kinematics</p>

<ul>
<li>it is most likely what you need (if you just don't do a visualization only)</li>
<li>and also is a bit simpler in this case</li>
</ul>

<p>Also when you use just direct kinematics</p>

<ul>
<li>I am not entirely convinced that you should drive all 3 joints</li>
<li>most likely drive just 2 of them</li>
<li>and compute the 3.th so the kinematics stay in valid position</li>
</ul>

<p>[Edit1]</p>

<p>There is one simplification that just appear to me:</p>

<ol>
<li>Ti = translate Ji towards the Z axis by q (parallel to XY plane)</li>
<li><p>now if you just need to find intersection of 3 spheres from Ti</p>

<ul>
<li>this point is E</li>
</ul></li>
<li><p>so Ei is now simple translation of E (inverse from the Ji translation)</p></li>
</ol>

<p>PS. I hope you know how to compute angles when you have all the points ...</p>
",2521214.0,1.0,1.0,26632855.0,"After you plug (7) and (8) into (1), you get a quadratic equation, you simply need to solve it using `z=(-b+-sqrt(b^2-4*a*c))/(2*a)` where `a` is the coefficient of `z^2`, `b` of `z` and `c` is the free coefficient, then plug `z` into (7) and (8) to get `x` and `y`. I think the length changes because not any set of angles is viable i.e. in real life you cannot change one angle without changing the other two correspondingly."
2149,27716172,RobotC: Multithreading in TeleOp (controlling mode),|multithreading|robot|,"<p>I am making a program for a robot in a competition, and need to multithread.</p>

<p>When I make a second task (task two()) and try to start (startTask) it with a button press from a controller, it just executes the first statement of the task and only as long as the button is pressed, instead of the whole block. I've tried many things including putting a loop in the second task also, using a function instead of a task and sleeping for 200 milliseconds before, and after the startTask(two); function, but the same thing happens every time.</p>

<p>I can't post my program because I don't want other people to steal it, sorry.</p>

<p>What edits will make it run the whole block?
Any help would be appreciated.</p>
",12/31/2014 4:35,27750568.0,259,1,0,0,,3527863.0,,4/12/2014 22:27,31.0,27750568.0,"<p>Since this is Controller Mode, I'm assuming that you are setting the motors to stop when the corresponding button is not pressed. </p>

<pre><code>if([...])
[...]
else
{
   setMotorSpeed(motor10, 0);
}
</code></pre>

<p>This is the cause for the stopping of the motors when you release. All of the other methods that you tried had nothing to do with this, so they shouldn't have worked.</p>

<p>You need to put something like this:</p>

<pre><code>int  Motor10Speed;
[...]
if([...])
[...]
else
{
   setMotorSpeed(motor10, Motor10Speed);
}
</code></pre>

<p>This will control an individual motor. Repeat this for all other motors being used.</p>

<p>After that is done, make the function look something like this:</p>

<pre><code>task mini_function();

task main()
{
[...]
}

task mini_function()
{
Motor10Speed = 50;
sleep(1000);
Motor10Speed = 0;
}
</code></pre>

<p>Expand the above program so it matches your current function, while using the <code>MotorSpeed</code> variables as <code>setMotorSpeed</code> variables.</p>

<p>This should make you able to drive and run a function at the same time without them interrupting each other.</p>
",3527863.0,1.0,0.0,,
4006,67081917,How do I get my code to loop until I press a button (Lego EV3),|python|robotics|lego-mindstorms|lego-mindstorms-ev3|,"<p>The question pure and simple. I've tried using while loops, rearranging code, but all that happens is that the program starts when i press the button or does absolutely nothing at all. Can anyone see what I am doing wrong?</p>
<pre><code>#!/usr/bin/env python3
from ev3dev2.motor import MoveTank, OUTPUT_B, OUTPUT_C
from ev3dev2.button import Button
from ev3dev2.sound import Sound
from time import sleep
import time

sound = Sound()
btn = Button()
tank_pair = MoveTank(OUTPUT_B, OUTPUT_C)

def movement(left_speed, right_speed, rotations):
tank_pair.on_for_rotations(left_speed, right_speed, rotations, brake=True, block=True)


while not btn.any():
movement(20,20,10)
movement(-100,0,1.5)
movement(20,20,10)
sleep(0.01)

while btn.any():
sleep(0.01)
tank_pair.off()
sound.beep()
exit()
</code></pre>
",4/13/2021 20:25,,814,2,2,0,,15331530.0,Regina,3/4/2021 17:38,4.0,67082399.0,"<p>you can break your main loop using this</p>
<pre><code>pip install keyboard
</code></pre>
<p>then</p>
<pre><code>import keyboard

while True:
    # do something
    if keyboard.is_pressed(&quot;q&quot;):
        print(&quot;q pressed, ending loop&quot;)
        break
</code></pre>
",10413299.0,0.0,1.0,118574083.0,what do you mean?
2171,28616358,Python bus between processes,|python|robotics|,"<p>I am completely starting over with my robots brain. All is developed in Python.</p>
<p>I want to keep everything as modular as possible and allow the use of multiple CPU cores (Raspberry PI 2).</p>
<p>I thought of using multiple processes. One for serial communication, one for every sensor, one for every higher function.
All connected by a steering &quot;brain&quot; function.</p>
<p>I want to connect processes by message busses. E.g. Should each sensor dart it's own bus. Higher functions and the brain can then sign in to the bus. In optimal case I would like to send whole objects too.</p>
<ul>
<li>Is there a good framework to provide the busses?</li>
<li>is there maybe a better approach to the whole topic?</li>
</ul>
",2/19/2015 20:35,28616511.0,260,1,3,2,,3173818.0,,1/8/2014 15:04,21.0,28616511.0,"<p>Try <a href=""http://nanomsg.org/"" rel=""nofollow"">nanomsg</a> (follow-up project to ZeroMQ):</p>

<blockquote>
  <p>nanomsg is a socket library that provides several common communication
  patterns. It aims to make the networking layer fast, scalable, and
  easy to use. Implemented in C, it works on a wide range of operating
  systems with no further dependencies.</p>
  
  <p>The communication patterns, also called ""scalability protocols"", are
  basic blocks for building distributed systems. By combining them you
  can create a vast array of distributed applications. The following
  scalability protocols are currently available:</p>
  
  <ul>
  <li>PAIR - simple one-to-one communication</li>
  <li>BUS - simple many-to-many    communication</li>
  <li>REQREP - allows to build clusters of stateless services    to process user requests</li>
  <li>PUBSUB - distributes messages to large sets    of interested subscribers</li>
  <li>PIPELINE - aggregates messages from    multiple sources and load balances them among many destinations</li>
  <li>SURVEY - allows to query state of multiple applications in a single    go</li>
  </ul>
</blockquote>
",1099876.0,1.0,2.0,45535833.0,`multiprocessing` ... its a builtin library it has things called `Pipe`'s that do about what you want
1709,16250256,"Object distance to camera, using opencv",|c++|opencv|robotics|,"<p>I want to estimate the distance of an object to my camera. This must be using Opencv.
I read that I have to use 2 cameras in stead of one and I found some code with Matlab, but I don't have any experience in it.
Any help will be appreciated.</p>
",4/27/2013 9:04,16250458.0,7990,3,3,3,0.0,1807373.0,"Madinah, Saudi Arabia",11/7/2012 20:49,793.0,16250305.0,"<ol>
<li><p><a href=""https://stackoverflow.com/questions/4588485/is-it-possible-to-measure-distance-to-object-with-camera"">Is it possible to measure distance to object with camera?</a></p></li>
<li><p>see this <a href=""https://photo.stackexchange.com/a/12437"">answer:</a></p>

<pre><code>distance to object (mm) = focal length (mm) * real height of the object (mm) * image height (pixels)
                          ---------------------------------------------------------------------------
                          object height (pixels) * sensor height (mm)
</code></pre></li>
</ol>
",1031417.0,1.0,0.0,23250440.0,I know object dimension only. What do you mean by sensor? I have camera
4679,77264719,Python to Control CoppeliaSim correct library (RegularAPI or LegacyApi),|python|simulator|robotics|,"<p>I want to use CoppeliaSim with Python. I have tried the sim.py module from the legacyApi. Take as an example min 2:05 of: <a href=""https://www.youtube.com/watch?v=TGT7KbP7Dfs"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=TGT7KbP7Dfs</a>
However, I cannot not find documentation of its methods and since it is legacy code I would rather move to a more modern implementation. I want to switch to the Regular API: <a href=""https://www.coppeliarobotics.com/helpFiles/"" rel=""nofollow noreferrer"">https://www.coppeliarobotics.com/helpFiles/</a>
The methods are very well documented but I do not find how to import them. What import to I have to do? Do I have to do a pip install? Do I have to download any .py file?</p>
<ol>
<li>Plan A: Use the Regular API. Tell me how to import it. The methods are well documented but I do not know where are those. If possible I would be prefer this approach.</li>
<li>Plan B: Use the legacyApi. I already was able to find it and use it. But the methods are not documented.</li>
</ol>
",10/10/2023 9:56,,44,0,0,1,,22694257.0,,10/6/2023 8:42,4.0,,,,,,,
3681,61055315,Change speed of TestFX robots?,|java|javafx|robot|testfx|,"<p>Specifically the <code>WriteRobot</code> / <code>WriteRobotImpl</code>. It seems to write things rather slowly and I'd like to make it write faster.</p>

<p><strong>Edit</strong><br>
In response to M.S.'s comment I tried this (NB at this point I hadn't worked out that <code>WriteRobot</code> was involved, not <code>TypeRobot</code>):</p>

<pre><code>setup(){
...
    setFinalStatic( org.testfx.robot.impl.TypeRobotImpl.class.getDeclaredField(""SLEEP_AFTER_KEY_CODE_IN_MILLIS""), 5 );
}
...
static void setFinalStatic(Field field, Object newValue) throws Exception {
    field.setAccessible(true);
    Field modifiersField = Field.class.getDeclaredField(""modifiers"");
    modifiersField.setAccessible(true);
    modifiersField.setInt(field, field.getModifiers() &amp; ~Modifier.FINAL);
    field.set(null, newValue);
}
</code></pre>

<p>Unfortunately it appears to make no difference to the typing speed, even when set to 1 ms.</p>

<p><strong>Edit</strong><br>
I note the comment by Slaw.</p>

<p>I set the <code>System</code> property <code>testfx.robot.write_sleep</code> before running the test: this had no effect, despite one being able to see that it might have, from the source code at the top of WriteRobotImpl.java (see below). When I set this to 500 ms it also had no effect, making me conclude that the property was not being seen by the code there for some reason, so the default 25 ms was being set.</p>

<p>NB possible other causes: following the code there, it appears that <code>WriteRobot.write</code> always results in a call to <code>WriteRobot.typeCharacterInScene</code>, which in turn calls <code>BaseRobot.typeKeyboard</code> and <code>WaitForAsyncUtils.waitForFxEvents</code>. The latter might be a ""difficult customer"": if each pressed key then has to ""wait for events"" to bubble up, there may well be nothing to be done about things. </p>

<p>Still trying to work out why the following lines at the top of org.testfx.robot.impl.WriteRobotImpl.java would fail to see the <code>System</code> property:</p>

<pre><code>private static final int SLEEP_AFTER_CHARACTER_IN_MILLIS;

static {
    int writeSleep;
    try {
        writeSleep = Integer.getInteger(""testfx.robot.write_sleep"", 25);
    }
    catch (NumberFormatException e) {
        System.err.println(""\""testfx.robot.write_sleep\"" property must be a number but was: \"""" +
                System.getProperty(""testfx.robot.write_sleep"") + ""\"".\nUsing default of \""25\"" milliseconds."");
        e.printStackTrace();
        writeSleep = 25;
    }
    SLEEP_AFTER_CHARACTER_IN_MILLIS = writeSleep;
}
</code></pre>

<p>I also wondered whether maybe that <code>static{...}</code> code block happens so early than you need to set the <code>System</code> property before the tests are run. I tried setting this property in gradle.build. Still no success.</p>
",4/6/2020 7:58,,580,0,10,0,,595305.0,"London, United Kingdom",1/29/2011 21:24,2736.0,,,,,,108025333.0,[WriteRobotImpl](https://github.com/TestFX/TestFX/blob/ca90e066d9339282973e3872212d608402a8d9d6/subprojects/testfx-core/src/main/java/org/testfx/robot/impl/WriteRobotImpl.java)?
1234,7045073,Problematic Parallel Distributive Processing using Python Libraries or any language,|python|arduino|robotics|pdp|,"<p>I've been sitting on this idea of working on a ""networked intelligence"" to look into some interesting ideas on the nature of intelligence and computers. I've decided to go about doing this by designing small robotic agents that will utilize PDP across some medium, (i.e. wifi/IR or something, to be decided), to enable them to gather large quantities of data independently and then be able to process and find trends in data efficiently by utilizing them together as a ""supercomputer"" (I always think it's odd using that term, but it's apt, one is utilizing multiple independent processing units in unison). I'm aware that Python has some PDP libraries available, and I was hoping to program the robots onto little Arduinos, and I've got a strong idea of how to do every component of the system except for actually implementing the PDP architecture across the system. </p>

<hr>

<p><strong>TL;DR</strong>? I want to make a bunch of little robots that can essentially connect together to form a small supercomputer and share and amalgamate information across all the agents. Is it feasible to create a PDP program that will freely relinquish parts of its processing power and then add in new ones.</p>

<p>I'm a pretty strong programmer, so if it's a matter of complexity and time, I'm willing to apply myself, but if it's an issue of having to strip apart some BIOS software and writing in Assembly, then I'd rather not. I'm not as familiar with PDP ideas as I would like to, and if you have any recommended reading to get me started, much appreciated.</p>

<p><em>Another note</em>, the languages or platform is completely up for changes, I'd just like to see concrete evidence that one is better than the other.</p>
",8/12/2011 19:02,,200,1,0,2,0.0,892338.0,,8/12/2011 19:02,5.0,7046312.0,"<p>Interesting idea, remins me on <a href=""http://en.wikipedia.org/wiki/Wireless_sensor_network"" rel=""nofollow"">sensor networks</a>.</p>

<p>You may find that an ardunio is a little underpowered for what you want. Perhaps it would be more efficient and easier to send the data back to a PC for processing.</p>

<p>If you want to continue with the ardunio idea, you could implement <a href=""http://en.wikipedia.org/wiki/MapReduce"" rel=""nofollow"">MapReduce</a> which is a fairly simple construct that allows you to write distributed programs very easily.</p>

<p>I have a write up on the <a href=""http://stephenholiday.com/articles/2011/introduction-to-mapreduce/"" rel=""nofollow"">basics of MapReduce</a>.</p>

<p>There is the famous <a href=""http://hadoop.apache.org/"" rel=""nofollow"">Haddop implementation</a> as well as <a href=""http://discoproject.org/"" rel=""nofollow"">Disco</a> (python/erlang) and a very simple shell implementation called <a href=""https://github.com/erikfrey/bashreduce"" rel=""nofollow"">BashReduce</a> that Last.fm created.</p>
",429688.0,0.0,0.0,,
2456,35733974,Single Core processor: Undesirable behaviour with Pthreads and OpenCV,|multithreading|opencv|posix|robotics|,"<p>I'm developing a surveillance robot in which I'm using a PcDuino2 board (ARM Cortex A8 Single-Core) as the unique processor in the project. Ubuntu is installed on the board as my OS of choice.</p>

<p>My software was written in this way: I'm using Posix threads, I've a threaded TCP server which receives commands from a HTML interface (Websocket), thus, this commands are validated as soon as the server receives them and some functions are called in order to move the robot (up, down, left, right) or start/stop a OpenCV thread that looks for a colored ball (just an example). </p>

<p>The fact is when the OpenCV thread isn't running, the robot's movement works as expected, but when OpenCV thread is running I can't move the robot with my interface (even receiving the commands correctly and calling the correct functions). </p>

<p>It seems like the OpenCV thread consumes all the processor resources.</p>

<p>Well, my source files are hosted in my github:</p>

<p><a href=""https://github.com/victorsantosdev/thesis-arm/tree/master/thr_tennisBall"" rel=""nofollow"">https://github.com/victorsantosdev/thesis-arm/tree/master/thr_tennisBall</a></p>

<p>The main files to take a look are server.cpp and tasks.cpp, so you'll get a better understanding of what I've written.</p>

<p>It's possible, using a sigle-core processor, to keep motors running in parallel with OpenCV thread processing?</p>
",3/1/2016 21:33,,105,0,5,0,,6004484.0,,3/1/2016 20:08,2.0,,,,,,59378675.0,"not sure what the ""right"" way is to implement it. maybe som event-loop like system?"
4667,77147437,Smooth movement of Pick and drop robot with stepper motor,|c++|robotics|firmware|stepper|nucleo|,"<p>I am building a pick and drop robot which has 3 stepper motors. I am using NUCLEO-F746ZG microcontroller and C++ programming to control it.</p>
<p>Now the problem is the arm moves with jerks in start and end (inertia) and I want to smooth the movement. For that I want to accelerate while the motor starts and decelerate when the motor stops. I tried different methods but none was working, the motor was moving in constant speed only. Kindly help me with this problem to make stepper motor move smoothly.</p>
<pre class=""lang-c++ prettyprint-override""><code>#include &quot;stepperMotor.h&quot;
#include &quot;mbed.h&quot;

int motorSpeed; // stepper speed
const float acc = 100;
int mspeed;

sMotor::sMotor(PinName A0, PinName A1, PinName A2, PinName A3) : _A0(A0), _A1(A1), _A2(A2), _A3(A3)
{ // Defenition of motor pins
    _A0 = 0;
    _A1 = 0;
    _A2 = 0;
    _A3 = 0;
}

void sMotor::anticlockwise(int mspeed)
{ // rotate the motor 1 step anticlockwise 
     for (int i = 0; i &lt; 8; i++) {
        switch (i)
        { // activate the ports A0, A2, A3, A3 in a binary sequence for steps
            case 0:
            {
                _A0 = 0;
                _A1 = 0;
                _A2 = 1;
                _A3 = 0;
            }
            break;

            case 1:
            {
                _A0 = 0;
                _A1 = 0;
                _A2 = 0;
                _A3 = 0;
            }
            break;

            case 2:
            {
                _A0 = 0;
                _A1 = 0;
                _A2 = 0;
                _A3 = 1;
            }
            break;

            case 3:
            {
                _A0 = 0;
                _A1 = 1;
                _A2 = 0;
                _A3 = 1;
            }
            break;

            case 4:
            {
                _A0 = 0;
                _A1 = 1;
                _A2 = 1;
                _A3 = 1;
            }
            break;

            case 5:
            {
                _A0 = 1;
                _A1 = 1;
                _A2 = 1;
                _A3 = 1;
            }
            break;

            case 6:
            {
                _A0 = 1;
                _A1 = 0;
                _A2 = 1;
                _A3 = 1;
            }
            break;

            case 7:
            {
                _A0 = 1;
                _A1 = 0;
                _A2 = 1;
                _A3 = 0;
            }
            break;
        }

        wait_us(mspeed);
      // wait_us(motorSpeed); // wait time defines the speed 
        
    }
}

void sMotor::clockwise(int mspeed)
{ // rotate the motor 1 step clockwise 
    for (int i = 7; i &gt;= 0; i--) {
        switch (i)
        {
            case 0:
            {
                _A0 = 0;
                _A1 = 0;
                _A2 = 0;
                _A3 = 1;
            }
            break;

            case 1:
            {
                _A0 = 0;
                _A1 = 0;
                _A2 = 1;
                _A3 = 1;
            }
            break;

            case 2:
            {
                _A0 = 0;
                _A1 = 0;
                _A2 = 1;
                _A3 = 0;
            }
            break;

            case 3:
            {
                _A0 = 0;
                _A1 = 1;
                _A2 = 1;
                _A3 = 0;
            }
            break;

            case 4:
            {
                _A0 = 0;
                _A1 = 1;
                _A2 = 0;
                _A3 = 0;
            }
            break;

            case 5:
            {
                _A0 = 1;
                _A1 = 1;
                _A2 = 0;
                _A3 = 0;
            }
            break;

            case 6:
            {
                _A0 = 1;
                _A1 = 0;
                _A2 = 0;
                _A3 = 0;
            }
            break;

            case 7:
            {
                _A0 = 1;
                _A1 = 0;
                _A2 = 0;
                _A3 = 1;
            }
            break;
        }
        wait_us(mspeed);
    }
}

void sMotor::step(int num_steps, int direction, int speed)
{// steper function: number of steps, direction (0- right, 1- left), speed (default 1200)
    int count = 0; // initalize step count
    float stepTime = 1.0 / speed;  //aTry
    int w = num_steps;
    mspeed = 250;
    int speedMultiplier = 10;
    
    motorSpeed = speed; //set motor speed
    if (direction == 0) // turn clockwise
        do
        {
            clockwise(mspeed);
            count++;

            for (int i = 0; i &lt; (w/4); i++)
            {
                while (mspeed != speed)
                {
                    mspeed = mspeed - speedMultiplier;
                } 
            }
            for (int i = w/4; i &lt; 3 * (w / 4); i++)
            {
                mspeed = speed; 
            }            
            for (int i = 3 * (w / 4); i &lt; w; i++)
            {
                while (mspeed != 400)
                {
                    mspeed = mspeed + speedMultiplier;
                }
            }
        } while (count &lt; num_steps); // turn number of steps applied 
        
    else if (direction == 1) // turn anticlockwise
    { 
        count = 0;
        do
        {
            anticlockwise(mspeed);
            count++;      

            for (int i = 0; i &lt; (w / 4); i++)
            {
                while (mspeed != speed)
                {
                    mspeed = mspeed - speedMultiplier;
                } 
            }
            for (int i = w/4; i &lt; 3 * (w / 4); i++)
            {
                mspeed = speed; 
            }            
            for (int i = 3 * (w / 4); i &lt; w; i++)
            {
                while (mspeed != 400)
                {
                    mspeed = mspeed + speedMultiplier;
                }
            }
        } while (count &lt; num_steps);// turn number of steps applied 
    }
}
</code></pre>
",9/21/2023 5:40,,144,1,1,2,,20749673.0,,12/11/2022 15:52,4.0,77147682.0,"<p>You have a few possible improvements in your code: you have a field <code>mspeed</code> and you have a parameter <code>mspeed</code> in two methods - the parameter overrides the field. But as you implemented it now, the parameter wouldn't be necessary at all, because the methods <code>clockwise</code> and <code>anticlockwise</code> can access the field <code>mspeed</code>.</p>
<p>Also you assign the field <code>motorspeed</code> but you never use it.</p>
<p>The name of the field and parameters <code>mspeed</code> is misleading, usually a movement is faster with a higher speed, but you use it as waiting time, so <code>delay</code> would be a better name.</p>
<p>For the <code>clockwise</code> and <code>anticlockwise</code> methods: are you sure that you use the correct values and the correct order to activate the coils of the stepper motor? To my understanding it should be possible that you have a list of values for a full step and you can &quot;play&quot; it in one direction for clockwise and in the opposite direction for counter-clockwise motion. This way you would not have to use a <code>switch</code> statement and also only one method for both directions.</p>
<p>In the method <code>step</code> you should use a better name for <code>w</code>, maybe <code>position</code>?
Also in that method you have a huge code block for clockwise and anticlockwise movement that is completely equal, just the call to the <code>clockwise</code> and <code>anticlockwise</code> methods differ. To prevent code duplication errors, you should move that outer <code>if</code> further inside so that it just decides the actual call to one of the two methods. This is the most important improvement as it has impact on the fix if your problem!</p>
<p>To your actual problem:
In your <code>step</code> method you use three <code>for</code> loops to decrease the delay (speed) from a position of 0 to 1/4 of the target position, the keep it still from 1/4 to the 3/4 of the target position and finally increase the delay again from 3/4 to the complete target position.
Inside the acceleration and deceleration <code>for</code> loops you have a <code>while</code> loop.
And all of these nested loops are contained in one outer <code>do</code>-<code>while</code> loop for the stepping.</p>
<p><strong>BUT</strong>: you only need the outer <code>do</code>-<code>while</code> loop. The inner loops are run for each step, so the delay is accelerated, kept constant and decelerated for every step of the motor.
What you should have instead is a nested <code>if</code> inside that outermost loop that decides for each cycle:</p>
<ul>
<li>if <code>count</code> is in the first quarter of the target range (<code>num_steps</code>), do the acceleration</li>
<li>else if <code>count</code> is in the last quarter of the target range, do the deceleration</li>
<li>else use the delay that was given into the <code>step</code> method.</li>
</ul>
<p>Please note that for an even smoother movement, you have to process acceleration and deceleration also inside your method(s) to move the motor one full step, because actually you execute the same delay 8 times for a step, but you could vary it there as well!</p>
<p>Here some code for the <code>step</code> method - please note that my C++ knowledge was unused for several years and I also wasn't able to test this code, so I don't know if it even compiles. Feel free to fix it where necessary ;-)</p>
<pre class=""lang-c++ prettyprint-override""><code>void sMotor::step(int num_steps, int direction, int delay)
{// stepper function: number of steps, direction (0- right, 1- left), waiting time (default 1200)
    int speedMultiplier = 10; 

    // ensure that input delay param is used for the largest part.
    mspeed = delay + speedMultiplier * num_steps / 4;

    for (int step = 0; step &lt; num_steps; ++step)
    {
        if (step &lt; (num_steps / 4))
        {
            mspeed -= speedMultiplier;
        }
        else if (step &gt; (3 * (num_steps / 4)))
        {
            mspeed += speedMultiplier;
        }
        else
        {
            mspeed = delay;
        }

        if (direction == 0)
        {
            clockwise(mspeed);
        }
        else if (direction == 1)
        {
            anticlockwise(mspeed);
        }
    }
}
</code></pre>
<p>This code doesn't use acceleration and deceleration &quot;inbetween&quot; the steps, so that optimization is left up to you.</p>
",2846138.0,1.0,2.0,136037236.0,Just as a side note - the usual way to calculate a smooth trajectory is to use a power of 5 polynomial function. It's easy to find polynomial coefficients for such a function with constraints that velocity and acceleration are 0 at the end points of the trajectory and maximum in the middle point.
3045,49348325,Local minima in a Moore neighborhood,|algorithm|artificial-intelligence|robotics|motion-planning|,"<p>While trying to use a wavefront algorithm, I came across a scenario that I haven’t been able to easily solve.  I populate an array with a wavefront algorithm and using a Moore neighborhood, how do I decide which way to travel given that several of the neighbors are the exact same.</p>

<p><a href=""https://i.stack.imgur.com/ownV1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ownV1.png"" alt=""Moore Neighborhood""></a></p>

<p>My local neighborhood has the values of 4,4,5,5,5,4,5,6.  The local minima is 4, but I have 3 of them in this case.  How do I make a decision based if there are several local minima to move to.  Right now I am making a random decision if there are equal values, but I feel there’s a better way to decide.</p>

<p><a href=""https://i.stack.imgur.com/0ZbQj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0ZbQj.png"" alt=""Paths to destination""></a></p>

<p>All paths to the destination have the same number of steps (5 steps), but each path taken is different.  I am also assuming that the local minima is chosen for the pathway.</p>

<p>My question is, given an equal number of steps (in wavefront) is there a way other than randomization or order of the comparators are written in to make a decision on which way to move from one block to another?</p>

<p>Just a notation, 99 indicates a barrier.</p>

<p>The problem with just pure local minima or comparator order is a failure case pictured below.</p>

<p><a href=""https://i.stack.imgur.com/qzgx5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qzgx5.png"" alt=""Failure""></a></p>

<p>I’m not looking for an Astar solution to this question, but I feel in this case, I would be faced with the exact same decision problems as all paths are equal.</p>

<p>Thanks</p>

<p>EDIT:
I had incorrectly generated the numbers, embarrassing.</p>

<p>Here's a correct generation of the wavefront</p>

<p><a href=""https://i.stack.imgur.com/iZCuv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iZCuv.png"" alt=""corrected Wavefront""></a></p>

<p>When correctly generated, it shows there is only 1 path</p>

<p><a href=""https://i.stack.imgur.com/GpD6l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GpD6l.png"" alt=""Only 1 path showing in Moore""></a></p>

<p>Even when the barrier is a little more complex</p>

<p><a href=""https://i.stack.imgur.com/w2DAf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w2DAf.png"" alt=""Still Shortest pathway""></a></p>

<p>Note: in all the images (corrected) the pathway is generated using Van Neumann neighborhood, while the movement is via Moore neighborhood.</p>

<p>However, if using Van Neumann neighborhood generation and Van Neumann navigation, the problem pops up again.</p>

<p><a href=""https://i.stack.imgur.com/IjHt5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IjHt5.png"" alt=""enter image description here""></a> </p>

<p><a href=""https://i.stack.imgur.com/HY4Ea.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HY4Ea.png"" alt=""enter image description here""></a></p>
",3/18/2018 13:20,,161,0,2,0,,1413678.0,"Syracuse, NY",5/23/2012 21:48,36.0,,,,,,86349070.0,"You are absolutely correct on my pics, I calculated the wavefront by hand and placed on a spreadsheet to view the array. My calculation is off. I have since actually created a function to calculate automatically for me. After the numbers are correctly generated, the problem goes away.  The choice however resurfaces again using Van Neumann generation and navigation."
3655,60586230,Issues in spying a windows application utilizing BluePrism,|windows|robotics|blueprism|,"<p>I am facing the below issue while working on BluePrism, the issue relates to spying elements from a windows application called ""Cashier"", the application is written in VB, I provided the .exe file for it and the application launched properly, however, I can't properly spy the individual rows in the table shown in the attached picture. I can only spy the box as a whole, and no information is retrieved.
<a href=""https://i.stack.imgur.com/2Dm4S.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2Dm4S.jpg"" alt=""Windows Application Spying issue""></a></p>
",3/8/2020 9:43,,91,1,2,-1,,10403857.0,,9/23/2018 12:59,20.0,60705350.0,"<p>possible the application spawns the elements in a new process? perhaps check the task manager once and make sure another process hasn't spawned to show the results in?</p>
",4026969.0,-1.0,0.0,107302964.0,"yes, I did with no luck. @esqew"
425,1895242,failsafe for networked robot,|php|robotics|,"<p>I have a robot that I'm controlling via a browser. A page with buttons to go forward, reverse, etc is written in PHP hosted on an onboard computer. The PHP is just sending ASCII characters over a serial connection to a microcontroller. Anyway, I need to implement a failsafe so that when the person driving it gets disconnected, the robot will stop. The only thing I can think to do is to ping the person on the web page or something, but I'm sure there is a better way than that. The robot is connected either via an ad hoc network or a regular wireless network that is connected to the internet. Obviously if I go with the ping method then there will have to be a delay between the actual time disconnected and when it realizes it's been disconnected. I'd like this delay to be a small as possible, whatever the method used. I'd appreciate any ideas on how to do this.</p>
",12/13/2009 1:00,1895355.0,124,3,0,0,,90903.0,"Atlanta, GA",4/15/2009 2:09,184.0,1895258.0,"<p>Pinging a web client is somewhat unreliable, for you have to take into account, that the client ip might change.</p>

<p>On the other hand, you could emulate a ""dead-man-button"" via Ajax. Let the webpage send a defined command every now and then (maybe every 5 to 10 seconds). If the robot doesn't receive the message for some time, it can stop. The Ajax script could run in the background so the controlling user won't even notice anything.</p>

<p>This would of course mean, that your robot needs to have a counter which is incremented every second and reset when the message is received. The moment the timer variable is too high, FULL STOP</p>
",25253.0,2.0,0.0,,
2445,35374892,Quadricopter + arduino+mpu6050(gyro+oscillo) +PID,|arduino|arduino-uno|robotics|gyroscope|pid-controller|,"<p>so I'm trying to creat a quadricopter with an arduino and gyroscope mp6050 and this with PID algorithme (using arduino PID,mpu library) so I make everything work separately but when it comes to use PID  I don't know how to do it which one will be the input or setput ... I'm confused on how I can use gyroscope information and brushless information and other information to make my quadri fly ... thank you </p>
",2/13/2016 1:13,35464003.0,1612,1,0,-2,,4366398.0,,12/16/2014 12:48,33.0,35464003.0,"<p>It sounds like you need a better understanding of what PID does. Here is a great article about real world implmentation of PID  <a href=""http://eas.uccs.edu/~cwang/ECE4330F12/PID-without-a-PhD.pdf"" rel=""nofollow noreferrer"">http://eas.uccs.edu/~cwang/ECE4330F12/PID-without-a-PhD.pdf</a> </p>

<p>It follows this code from AVR's website exactly (they make the ATMega32p microcontroller  chip on the UNO boards) <a href=""http://www.atmel.com/Images/doc2558.pdf"" rel=""nofollow noreferrer"">PDF explanation</a> and <a href=""http://www.atmel.com/images/AVR221.zip"" rel=""nofollow noreferrer"">Atmel Code in C</a> </p>

<p><a href=""https://i.stack.imgur.com/TJTJC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TJTJC.png"" alt=""enter image description here""></a><br>
A PID controller is a feedback control loop. </p>

<p><strong>The input</strong> (""Command"" on the diagram) is the desired position, in your case this is what you want the gyroscope to read.</p>

<p><strong>The ""Output""</strong> is a control signal. It will tell your brushless motors (the ""Plant"") what to do to achieve the desired position.</p>

<p><strong>The Feedback (input/output)</strong> We then use our ""Sensors"" to read the actual position. In your case this is the gyroscope data</p>

<p>Now the PID controller takes the <code>error = desired position - actual position</code> and uses the error to create the next command. The overall goal is to drive the <code>error</code> down to 0, in other words <code>desired position = actual position</code> The exact details of your PID coefficients are based on your specific setup and usually they must be tuned manually in order to achieve desirable results (see the links provided for more details). </p>

<p>Loooking at the <a href=""http://playground.arduino.cc/Code/PIDLibrary"" rel=""nofollow noreferrer"">Arduino PID Library Documentation</a> shows that they have easy methods to set <code>KP, KD, KI, input, output</code> THey even have an autotune parameter that can automatically find your PID constants. Hope that helps, Good luck.</p>
",2705382.0,1.0,0.0,,
1312,9262830,What machine learning algorithm is appropriate for shooting basketballs?,|algorithm|machine-learning|computer-vision|robotics|,"<p>We are making a robot that shoots basketballs into hoops. </p>

<p>From an image and our knowledge of the camera's angle and the target's dimensions (the targets are coated with retroreflective tape), we know how far away we are, X and Y (distance being Z, more or less)</p>

<p>This is fed into the machine learning algorithm, which should spit out</p>

<ol>
<li>Speed to be sent to the canon</li>
<li>Horizontal tilt </li>
<li>Vertical tilt</li>
</ol>

<p>What kind of machine learning algorithm is this, and how would you train it?</p>
",2/13/2012 15:12,,344,2,4,1,,731881.0,"Boston, MA, USA",4/29/2011 23:10,118.0,9263009.0,"<p>I would recommend a reinforcement learning approach. It'll be slow ; so maybe you could initialize the solution with your own estimate (basic physics) and refine it with reinforcement learning.</p>
",71131.0,1.0,1.0,11673860.0,"If you know your current position, target position, why do you need machine learning at all? You can just calculate speed and tilt"
3654,60536927,TRPO - RL: I need to get a 8DOF robot arm to move to a specified point. I need to implement the TRPO RL code using OpenAI gym with Gazebo environment?,|reinforcement-learning|robotics|openai-gym|gazebo-simu|,"<p>TRPO - RL: I need to get a 8DOF robot arm to move a specified point. I need to implement the TRPO RL code using OpenAI gym. I already have the gazebo environement. But I am unsure of how to write the code for the reward functons and the algorithm for the joint space motion. Please help. </p>
",3/5/2020 1:18,,89,1,0,0,,1690356.0,,9/22/2012 5:05,16.0,60573165.0,"<h3>Reward</h3>

<p>Gazebo should be able to tell you the position of the end-effector link from which we can calculate the <strong>progress made towards a specified point after each step</strong> (i.e. positive if moving towards the goal, negative if away, and 0 otherwise).
This alone should encourage the end-effector towards the goal.</p>

<p>You may want to confirm that the system is able to learn with just this basic reward first before considering other criterions such as smoothness (avoid jerking motions), handedness (positioning the elbows on the left/right) etc.
These are significantly harder to specify and will have to be hand-designed according to your needs, possibly based on the joint states and/or some other derivatives that are available in your environment.</p>

<h3>Motion</h3>

<p>This will largely depend on your stack.
I am adding this part in just as a passing comment, but for instance, if you are using ROS as your middleware, then you can easily integrate <a href=""https://moveit.ros.org/documentation/faqs/"" rel=""nofollow noreferrer"">Move-It</a> to handle all the movement for you.</p>
",9147640.0,0.0,0.0,,
845,4173306,Can I implement potential field/depth first method for obstacle avoidance using boost graph?,|c++|boost|robotics|boost-graph|,"<p>I implemented an obstacle avoidance algorithm in Matlab which assigns every node in a graph a potential and tries to descend this potential (the goal of the pathplanning is in the global minimum). Now there might appear local minima, so the (global) planning needs a way to get out of these. I used the strategy to have a list of open nodes which are reachable from the already visited nodes. I visit the open node which has the smallest potential next. </p>

<p>I want to implement this in C++ and I am wondering if Boost Graph has such algorithms already. If not - is there any benefit from using this library if I have to write the algorithm myself and I will also have to create my own graph class because the graph is too big to be stored as adjacency list/edge list in memory.</p>

<p>Any advice appreciated!</p>
",11/13/2010 15:41,4200132.0,1226,2,0,4,,292233.0,Germany,3/12/2010 9:25,560.0,4179508.0,"<p>To my mind, <code>boost::graph</code> is really awesome for implementing new algorithms, because it provides various data holders, adaptors and commonly used stuff (which can obviously be used as parts of the newly constructed algorithms).</p>

<p><em>Last ones are also customizable due to usage of visitors and other smart patterns.</em></p>

<p>Actually, <code>boost::graph</code> may take some time to get used to, but in my opinion it's really worth it.</p>
",346332.0,1.0,0.0,,
3182,51877976,Troubles to move turtlebot_gazebo,|c++|geometry|ros|robotics|,"<p>I started a small project where I wanted to move the turtlebot in gazebo. I launched: roslaunch turtlebot_gazebo turtlebot_world.launch</p>

<p>I wrote a code to move it:</p>

<pre><code>#include ""ros/ros.h"" 
#include ""geometry_msgs/TwistWithCovariance.h"" 
#include ""nav_msgs/Odometry.h"" 
#include ""gazebo_msgs/LinkState.h"" 
#include ""geometry_msgs/Twist.h""

int main(int argc, char **argv){

ros::init(argc,argv,""move"");
ros::NodeHandle n;
ros::Publisher move_pub = n.advertise&lt;geometry_msgs::Twist&gt;(""moving"",1000);
ros::Rate loop_rate(10);

geometry_msgs::Twist msg;
while(ros::ok()){
    msg.linear.x = 0.0;
    msg.linear.y = 0.0;
    msg.linear.z = 20.0;
    move_pub.publish(msg);

    ros::spinOnce();

    loop_rate.sleep();

    }

}
</code></pre>

<p>This doesn't work. As you can see i included other msgs and tried it with them but I got the same result. I also tried to find the node turtlebot_teleop_keyboard to find out how it is done by inputs. But i couldn't find the path to it. So what do I have to do to move it? And how can I find the path to the node?</p>
",8/16/2018 13:14,,1129,2,0,0,,,,,,51894060.0,"<p>For Turtlebot, you have topics to publish linear velocities - x, y and z and also angular velocities - x, y and z. What you need to understand is what each of these physically mean.</p>

<p><strong>Linear velocity in x will make it move forward.</strong></p>

<p><strong>Angular velocity in z will make it turn about itself.</strong></p>

<p>Other parameters don't do anything.
So, in your code, using the above 2 parameters will move the bot. Rest of the parameters won't affect the bot in any way.</p>
",7615877.0,0.0,0.0,,
3035,49205761,How to run two turtle bots simultaneously in ROS Kinetic?,|ros|robotics|,"<p>I am trying to run two turtle bots from two different terminals on ROS. But as soon as I run the second command it closes the first turtlebot and runs the other with same name. I think I am looking for a way to change the turtlebot node names while running it. </p>
",3/10/2018 5:40,,412,1,1,0,,4928387.0,"Surat, Gujarat, India",5/22/2015 9:29,25.0,49215805.0,"<p>you can change  name of node in a launch file
for that create a launch file 
and in launch file</p>

<pre><code>&lt;launch&gt;
&lt;node pkg=""yourpackage"" type=""yourdefaultnodename""name=""yourcustomnodename""/&gt;
&lt;/launch&gt;
</code></pre>

<p>you can use remap in your launch file too</p>
",8772302.0,0.0,0.0,85419734.0,"Please share some more information what exactly you tried. Add some launch commands, your hardware setup, ... With the information give by you it's hard to help. Maybe this page helps: http://wiki.ros.org/Nodes"
1781,17724865,Hardware to Software incorporation/interaction,|robotics|hardware-interface|,"<p>I have taken interest in basic hardware interaction with software.</p>

<p>What's a good language to start learning to control hardware? Can Java do the job?</p>
",7/18/2013 13:30,17737491.0,188,2,2,0,,2128576.0,Philippines,3/3/2013 10:00,367.0,17733390.0,"<p>This depends on the platform. If you have a good java API for your device, it works well enough. In general though C or C++ are the languages of choice when it comes to hardware. The reason for that is that they are able to directly access arbitrary memory addresses through the pointer construct. This is in most cases the way to interact with hardware. This is not directly possible in java. </p>
",672634.0,2.0,1.0,33082173.0,"@darlinton I was pretty much talkinh about anything basic. Like, sending electricity to a port on command. So far Arduino was the best answer."
1901,21249131,Monte-Carlo localization for mobile-robot,|robotics|montecarlo|,"<p>I'm implementing Monte-Carlo localization for my robot that is given a map of the enviroment and its starting location and orientation. My approach is as follows:</p>

<ol>
<li>Uniformly create 500 particles around the given position</li>
<li>Then at each step:
<ul>
<li>motion update all the particles with odometry (my current approach is newX=oldX+ odometryX(1+standardGaussianRandom), etc.)</li>
<li>assign weight to each particle using sonar data (formula is for each sensor probability*=gaussianPDF(realReading) where gaussian has the mean predictedReading)</li>
<li>return the particle with biggest probability as the location at this step</li>
<li>then 9/10 of new particles are resampled from the old ones according to weights and 1/10 is uniformly sampled around the predicted position</li>
</ul></li>
</ol>

<p>Now, I wrote a simulator for the robot's enviroment and here is how this localization behaves: <a href=""http://www.youtube.com/watch?v=q7q3cqktwZI"" rel=""nofollow"">http://www.youtube.com/watch?v=q7q3cqktwZI</a></p>

<p>I'm very afraid that for a longer period of time the robot may get lost. If add particles to a wider area, the robot gets lost even easier.</p>

<p>I expect a better performance. Any advice?</p>
",1/21/2014 4:22,,859,1,2,0,,1993650.0,"Cambridge, USA",1/19/2013 21:46,92.0,21394245.0,"<p>The biggest mistake is that you assume the particle with the highest weight to be your posterior state. This disagrees with the main idea of the particle filter.</p>

<p>The set of particles you updated with the odometry readings is your proposal distribution. By just taking the particle with the highest weight into account you completely ignore this distribution. It would be the same if you just randomly spread particles in the whole state space and then take the one particle that explains the sonar data the best. You only rely on the sonar reading and as sonar data is very noisy your estimate is very bad. A better approach is to assign a weight to each particle, normalize the weights, multiply each particle state by its weight and sum them up to obtain your posterior state. </p>

<p>For your resample step i would recommend to remove the random samples around the predicted state as they corrupt your proposal distribution. It is legit to generate random samples in order to recover from failures, but those are supposed to be spread over the whole state space and explicitly not around your current prediction. </p>
",3127962.0,5.0,1.0,32053386.0,"Please don't ask the same question on [multiple stack exchange sites](http://robotics.stackexchange.com/q/2337/37). If you accidentally ask on the wrong site, it can be migrated to the correct one."
1343,9894559,having trouble with winavr gcc in eclipse c/c++,|c++|c|eclipse|robotics|winavr|,"<p>I am using eclipse c/c++ when i create a c project then it does not shows winavr gcc in the toolchain list but i have installed WinAVR-20100110 in c drive and my eclipse is also in the same directory.
it shows cygwin,solarize,linux,macosx,mingw gcc</p>
",3/27/2012 17:27,9895161.0,484,1,0,0,,,,,,9895161.0,"<p>Eclipse does not look for AVR toolchain by default, even if it is in path (you did add it there?). You need to create an cross GCC project, then tell it the prefix of your toolchain (avr- i guess). When you select ""New C project"" select Cross GCC in Toolchains listbox, then in next step enter the prefix, set path to toolchain (if not in path), etc. For this you need to install ""GCC Cross Compiler Support"". You do this from Help -> Install new software -> Mobile and Device development. You will probably also want at least GDB hardware debugging. </p>

<p>You can also try special <a href=""http://avr-eclipse.sourceforge.net/wiki/index.php/The_AVR_Eclipse_Plugin"" rel=""nofollow"">eclipse plug-in for AVR</a>.</p>
",395851.0,0.0,0.0,,
2361,32419077,General Advice to Creating a C++ Robot Template,|c++|oop|templates|robotics|,"<p>I'm working on a project that has evolved so many times, I've really started thinking about writing a generic class that controls other nested generic classes.</p>

<p>The issue is, I have so many ways I could go about it that I've become completely overwhelmed. I've reviewed vectors and template class examples, but I truly have no idea how to begin.</p>

<p>Let me explain,
I need to be able to use this template to create an object that I control directly.
The main, top level object, is the robot I'm working on documented here (Probably not helpful to read, included as reference):</p>

<pre><code>Github: MiddleMan5/Onyx-BeagleBone which pulls from: 
Github: MiddleMan5/Project-Onyx-Quadruped
(New member 2-link restriction)
</code></pre>

<p>Which has the C++ Assembly.h file I'm working on making generic:
<a href=""https://github.com/MiddleMan5/Onyx-BeagleBone/blob/master/include/Assembly.h"" rel=""nofollow"">""Assembly File""</a></p>

<p>Now I'm sorry if my syntax is a little rusty, I'm very new to C++ in a general sense. I have great difficulty with pointers, object vectors, etc.
I'm really just looking to be guided in a general direction of study, and less a ""Fix my code!"" answer.</p>

<p>Bear with me as I may mar C++ terms: 
I want to be able to create any type of robot configuration and manipulate EITHER it's individual assembly's End Effectors, or the entire chassis as a point particle physics object (if the robot is mobile). </p>

<p>This would allow me to use the same template to create a robot arm as well as my intended Quadruped. (Which is, in reality, a solid mobile mass that has a center and 4 ""Arms""</p>

<p>Each initialized Robot must contain only one Body that in-itself contains at least one Base, one Joint, and one End Effector.</p>

<p>The creation of Top-Level object should fail if any of these things should fail to be included before Body.assemble creates the now-active robot. (<code>Body.assemble</code> creates <code>Chassis Onyx</code>)</p>

<p>I can now move the entire structure by saying something like <code>Onyx.Leg(1).move(X,Y)</code> or <code>Onyx.rotate(X,45); //Move all legs to satisy final body rotation angle</code> </p>

<p>Documented in my Brainstorming file here: <a href=""https://github.com/MiddleMan5/Onyx-BeagleBone/blob/master/doc/Joints_Links_Bases.txt"" rel=""nofollow"">Please at least skim</a>.
This file contains diagrams and the ""definitions"" I'm using to prototype pseudo-code.</p>

<p>I only want one ""Robot"".h and ""Robot"".cpp to define actions, movements, sizes, shapes, etc. and one Main.cpp to provide entry into the code. (Maybe I'm asking a bit much) but logically to me, this seems do-able.</p>

<p>Includes:</p>

<pre><code>Main.cpp
        |_
        | Onyx.h
        |_Onyx.cpp //Body.move, body.rotate, body.raise
            |_Assembly.h To create the body as a combination of assemblies
</code></pre>

<p>Onyx as a quadruped looks like this:</p>

<pre><code>ONYX Complete Diagram Example:
        EE          EE
        ||    __    ||
         |   |  |   |                      
         0__0+__+0__0
             _||_
         __0+_  _+  &lt;-  0__
         0    \/          0  &lt;-\
         |                |  &lt;- - ""Leg"" and Body are two attached assemblies inside one complete containing Robot chassis          
        ||                || &lt;-/
        EE                EE
</code></pre>

<p>Should be controlled like:</p>

<pre><code>Onyx.EE1.moveTo(x,y);
</code></pre>

<p>or:</p>

<pre><code>Onyx.moveTo(x,y);
</code></pre>

<p>Where I could further define gaits and parameters that would allow the robot ro take steps to move it's entire chassis to location (X, Y)</p>

<p>and has definitions for joints, links, and bases as such:</p>

<pre><code>|Joint: Takes X and Y coordinates, rotates Link.            ----0----
|__ End Effector: A joint combined with an attached Link.   --------EE
|__ Base: A joint created that can be fixed statically through a link to another base joint
|   |__ Can be attached actively to a link and a joint 
|   |__ Can be used as a static reference to an assembly (Such as a leg) and attached to a joint on a different plane
|
|__ Primary Joint: the joint(s) that attach directly to the Body's base(s)

|Link: A length of physical mass between a joint.
|__Segment: two or more attached in-line links (multiple links combined at different angles can specify independent shapes or masses).
            Segments are useful for attaching spaced Bases. (Attach.Base(Segment1_2(End)); Segments can be attached either by their additive L1 or L2
</code></pre>

<p>I really am frustrated because the project has very quickly become way to complicated for me to even understand. This is why I was looking at creating a template. But it doesn't seem like accessing an object's object's method (Onyx.leg.move) is even possible in C++. Am I wrong?</p>

<p>If you actually read through to the end I want to give you a hand. Any advice you have on areas of study (Or advice to make my question clearer) would be GREATLY valued.</p>

<p>Thank You</p>

<p>Edit:
Okay, I went ahead and created a Class 'Assembly', which has methods to access a private subclass 'assemble' which attaches a ""Leg with links n"" to the assembly (god I need a different name) with subclass's Base, Link, and End_Effector. I guess I'm going to have to write individual methods and method handlers for each subclass of a subclass. This seems way to ugly and complicated with plenty of room to make errors.</p>

<p>The pseudo-flow as follows:</p>

<pre><code>Assembly Onyx //Creates Robot: Onyx
Onyx.assemble(4, 3, 2); //Adds 4 legs each with 3 joints which includes 2 axis
for(i; 0 ---&gt; 3)
Onyx.numleg_numL_length(i,1,114); //Defines L1 of Leg1 as 114mm
Onyx.numleg_numL_length(i,2,140); //Defines L2 of Leg1 as 114mm
</code></pre>

<p>and on and on and on and on.</p>

<p>Please tell me there's a simple trick that allows me to assign multiple variables to a named class instead of this infinite chain of methods functions and subclasses</p>
",9/6/2015 1:07,,222,0,3,0,,5304869.0,,9/6/2015 0:12,11.0,,,,,,52704546.0,"One quick reaction is that it’s possible to access an object’s members’ methods if they’re public, but you don’t want to do this. A basic approach is that the robot has certain parts, and the code to translate a command into low-level motions should be part of the `Robot`class methods.  The legs should be `private` data. So, the class itself should be implementing `Onyx.moveTo(x,y)` by turning it into a bunch of commands like `legs[0].move()`. If you want the controller to be able to move individual legs, too, there should be a public interface like `Onyx.moveLeg(whichLeg, how)`."
2466,36340961,Building robotlocomotion drake-distro,|java|installation|cygwin|robotics|,"<p>I have 64 bit win 8 and installed cygwin 64(and jdk 8).
when I use make command this pops up:
error message </p>

<p><a href=""https://i.stack.imgur.com/ysnuN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ysnuN.png"" alt=""enter image description here""></a>
the distro' link is <a href=""https://github.com/RobotLocomotion/drake"" rel=""nofollow noreferrer"">drake distro</a>
Thanks in advance</p>
",3/31/2016 18:26,,62,0,3,0,,5556374.0,,11/12/2015 20:33,67.0,,,,,,60310194.0,"It might help to be more specific about what this project is, and/or where it's from. Is it from here: https://github.com/lcm-proj/lcm ? If so, specific build issues with building from source might get quicker responses if you create an issue ticket on the project?"
2184,28994556,Can somebody help explain RobotC Syntax?,|c|robotics|,"<p>I have been programming for some time in Python, and JavaScript.  I have also programmed with the arduino language which is a mix between C and C++.  I was just introduced to RobotC.  The syntax used for RobotC is not like any language I have learned.  Can someone help explain these syntax differences so I can better understand it?</p>

<p><strong>Problem 1:</strong></p>

<p>When making a motor turn, you can use the following syntax:</p>

<pre><code>motor[motorA] = 50
</code></pre>

<p>What did that line just do?  In any other programming language that is how you would change a value in an array, but in RobotC it acts like a function call.  Is 'motor' an array, or an object?  And why do I need a function when controlling servos?</p>

<p><strong>Problem 2:</strong></p>

<p>When in the history of programming is this allowed?</p>

<pre><code>motor[leftMotor] = motor[rightMotor] = speed = 127;
</code></pre>

<p>And what which of the following would this code do?</p>

<pre><code>speed = 127;
motor[rightMotor] = speed;
motor[leftMotor] = motor[rightMotor];
</code></pre>

<p>or</p>

<pre><code>speed = 127;
motor[rightMotor] = 127;
motor[leftMotor] = 127;
</code></pre>
",3/11/2015 18:26,28994604.0,423,2,7,-1,,4206450.0,,11/2/2014 0:44,23.0,28994604.0,"<p>1) You are setting the value of the item in array <code>motor</code> at index <code>motorA</code> to be equal to <code>50</code>.</p>

<p>2) Multiple in-line assignments are evaluated from right to left, so this is the same as</p>

<pre><code>speed = 127;
motor[rightMotor] = speed;
motor[leftMotor] = motor[rightMotor];
</code></pre>
",3133680.0,5.0,0.0,46237974.0,"To understand Problem 1, you need to find the definition of `motor`. If that doesn't clear things up for you, then you'll need to add the definition of `motor` to the question. All three snippets in #2 do the same thing, and the first snippet has been allowed since C was originally developed, which would be around 1970."
4372,73703077,ROS AttributeError: module 'moveit_commander' has no attribute 'MoveGroupCommander',|python|ros|robotics|,"<p><a href=""https://i.stack.imgur.com/Dlgex.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Dlgex.png"" alt=""enter image description here"" /></a>I am using ROS and python to write a robot program. This program can run normally on ROS melodic, but it will raise <em><strong>AttributeError: module 'moveit_commander' has no attribute 'MoveGroupCommander'</strong></em> on noetic, may I ask this is ros Is there a reason for the version? Is there a way to end it?<a href=""https://i.stack.imgur.com/tI2hc.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>this is my code</p>
<pre><code>def pick1(self, forward, side, object, euler, nanko):
        target_x = forward+0.07
        target_y = side
        if object == 1:
            object_placeX = 0.55
            object_placeY = 0.2
        elif object == 2:
            object_placeX = 0.4
            object_placeY = 0.25
        elif object == 3:
            object_placeX = 0.3
            object_placeY = 0.3
        else:
            sys.exit()

        if nanko &gt; 0:
            object_placeX -= 0.05

        # 左手
        left_arm = moveit_commander.MoveGroupCommander(&quot;l_arm_waist_group&quot;)
        left_arm.set_max_velocity_scaling_factor(0.2)
        left_gripper = actionlib.SimpleActionClient(&quot;/sciurus17/controller2/left_hand_controller/gripper_cmd&quot;, GripperCommandAction)
        left_gripper.wait_for_server()

        gripper_goal = GripperCommandGoal()
        gripper_goal.command.max_effort = 2.0

        rospy.sleep(0.3)

        gripper_goal.command.position = -0.9
        left_gripper.send_goal(gripper_goal)
        left_gripper.wait_for_result(rospy.Duration(1.0))

        # SRDFに定義されている&quot;home&quot;の姿勢にする
        left_arm.set_named_target(&quot;l_arm_waist_init_pose&quot;)
        left_arm.go()
        gripper_goal.command.position = 0.0
        left_gripper.send_goal(gripper_goal)
        left_gripper.wait_for_result(rospy.Duration(1.0))

        # 掴む準備をする
        target_pose = geometry_msgs.msg.Pose()
        target_pose.position.x = target_x
        target_pose.position.y = target_y
        target_pose.position.z = 0.3
        q = quaternion_from_euler(-3.14 / 2.0, 0.0, euler)  # 上方から掴みに行く場合
        target_pose.orientation.x = q[0]
        target_pose.orientation.y = q[1]
        target_pose.orientation.z = q[2]
        target_pose.orientation.w = q[3]
        left_arm.set_pose_target(target_pose)  # 目標ポーズ設定
        left_arm.go()  # 実行

        # ハンドを開く
        gripper_goal.command.position = -0.7
        left_gripper.send_goal(gripper_goal)
        left_gripper.wait_for_result(rospy.Duration(1.0))

        # 掴みに行く
        target_pose = geometry_msgs.msg.Pose()
        target_pose.position.x = target_x
        target_pose.position.y = target_y
        target_pose.position.z = 0.11
        q = quaternion_from_euler(-3.14 / 2.0, 0.0, euler)  # 上方から掴みに行く場合
        target_pose.orientation.x = q[0]
        target_pose.orientation.y = q[1]
        target_pose.orientation.z = q[2]
        target_pose.orientation.w = q[3]
        left_arm.set_pose_target(target_pose)  # 目標ポーズ設定
        left_arm.go()  # 実行

        # ハンドを閉じる
        gripper_goal.command.position = -0.3
        left_gripper.send_goal(gripper_goal)
        left_gripper.wait_for_result(rospy.Duration(1.0))

        # 持ち上げる
        target_pose = geometry_msgs.msg.Pose()
        target_pose.position.x = target_x
        target_pose.position.y = target_y
        target_pose.position.z = 0.3
        q = quaternion_from_euler(-3.14 / 2.0, 0.0, euler)  # 上方から掴みに行く場合
        target_pose.orientation.x = q[0]
        target_pose.orientation.y = q[1]
        target_pose.orientation.z = q[2]
        target_pose.orientation.w = q[3]
        left_arm.set_pose_target(target_pose)  # 目標ポーズ設定
        left_arm.go()  # 実行

        # 移動する
        target_pose = geometry_msgs.msg.Pose()
        target_pose.position.x = object_placeX
        target_pose.position.y = object_placeY
        target_pose.position.z = 0.3
        q = quaternion_from_euler(-3.14 / 2.0, 0.0, 0.0)  # 上方から掴みに行く場合
        target_pose.orientation.x = q[0]
        target_pose.orientation.y = q[1]
        target_pose.orientation.z = q[2]
        target_pose.orientation.w = q[3]
        left_arm.set_pose_target(target_pose)  # 目標ポーズ設定
        left_arm.go()  # 実行

        # 下ろす
        target_pose = geometry_msgs.msg.Pose()
        target_pose.position.x = object_placeX
        target_pose.position.y = object_placeY
        target_pose.position.z = 0.14
        q = quaternion_from_euler(-3.14 / 2.0, 0.0, 0.0)  # 上方から掴みに行く場合
        target_pose.orientation.x = q[0]
        target_pose.orientation.y = q[1]
        target_pose.orientation.z = q[2]
        target_pose.orientation.w = q[3]
        left_arm.set_pose_target(target_pose)  # 目標ポーズ設定
        left_arm.go()  # 実行

        # ハンドを開く
        gripper_goal.command.position = -0.7
        left_gripper.send_goal(gripper_goal)
        left_gripper.wait_for_result(rospy.Duration(1.0))

        # 少しだけハンドを持ち上げる
        target_pose = geometry_msgs.msg.Pose()
        target_pose.position.x = object_placeX
        target_pose.position.y = object_placeY
        target_pose.position.z = 0.2
        q = quaternion_from_euler(-3.14 / 2.0, 0.0, 0.0)  # 上方から掴みに行く場合
        target_pose.orientation.x = q[0]
        target_pose.orientation.y = q[1]
        target_pose.orientation.z = q[2]
        target_pose.orientation.w = q[3]
        left_arm.set_pose_target(target_pose)  # 目標ポーズ設定
        left_arm.go()  # 実行

        # SRDFに定義されている&quot;home&quot;の姿勢にする
        left_arm.set_named_target(&quot;l_arm_waist_init_pose&quot;)
        left_arm.go()

        print(&quot;done&quot;)
</code></pre>
",9/13/2022 12:30,,796,1,0,2,,19985720.0,,9/13/2022 12:20,1.0,73724246.0,"<p>The reason it works different between versions is because Noetic uses Python3 and Melodic uses Python2.7. A key difference here is how they handle imports, and thus why you're having a problem. Make sure at the top of your script you have:</p>
<pre><code>import moveit_commander
</code></pre>
<p>If that doesn't work it means you need to install the Noetic version of moveit via: <code>sudo apt install ros-noetic-moveit</code></p>
",11245187.0,0.0,2.0,,
4632,76756325,ModuleNotFoundError: No module named 'cv_bridge.boost.cv_bridge_boost',|python|ros|robotics|ros2|rosbag|,"<p>I have already installed <code>ros noetic</code> distribution and the <code>sudo apt-get install ros-noetic-cv-bridge</code> and <code>sudo apt-get install ros-noetic-cv-bridge</code> successfully. However, when I am trying to run the following code in pycharm, I am getting the following error:</p>
<pre><code>import os
import argparse
import pdb
import cv2
import rosbag
from sensor_msgs.msg import Image
from cv_bridge import CvBridge

bag_file = './bag_files/20230707_152832.bag'
output_dir= './frames/rgb_bag_output'


image_topic = '/device_0/sensor_1/Color_0/image/data'
# image_topic ='sensor_msgs/Image'

bag = rosbag.Bag(bag_file, &quot;r&quot;)
bridge = CvBridge()

#gg =bag.read_messages(topics = '/device_0/sensor_1/Color_0/image/data')
# bag.get_message_count()

count = 0
for topic, msg, t in bag.read_messages(topics=image_topic):
    cv_img = bridge.imgmsg_to_cv2(msg, desired_encoding= &quot;rgb8&quot;)#&quot;passthrough&quot;)
    cv2.imwrite(os.path.join(output_dir, &quot;frame%06i.png&quot; % count), cv_img)
    print(&quot;Wrote image %i&quot; % count)
    count += 1

bag.close() 
</code></pre>
<p>the error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/es/PycharmProjects/2-Process-RGBD/extract_bag_frame.py&quot;, line 32, in &lt;module&gt;
    cv_img = bridge.imgmsg_to_cv2(msg, desired_encoding= &quot;rgb8&quot;)#&quot;passthrough&quot;)
  File &quot;/home/es/anaconda3/envs/hsi-env/lib/python3.8/site-packages/cv_bridge/core.py&quot;, line 163, in imgmsg_to_cv2
    dtype, n_channels = self.encoding_to_dtype_with_channels(img_msg.encoding)
  File &quot;/home/es/anaconda3/envs/hsi-env/lib/python3.8/site-packages/cv_bridge/core.py&quot;, line 99, in encoding_to_dtype_with_channels
    return self.cvtype2_to_dtype_with_channels(self.encoding_to_cvtype2(encoding))
  File &quot;/home/es/anaconda3/envs/hsi-env/lib/python3.8/site-packages/cv_bridge/core.py&quot;, line 91, in encoding_to_cvtype2
    from cv_bridge.boost.cv_bridge_boost import getCvType
ModuleNotFoundError: No module named 'cv_bridge.boost.cv_bridge_boost'
</code></pre>
<p>How can I resolve this issue?</p>
",7/24/2023 16:17,,587,0,0,1,,6494707.0,,6/21/2016 14:33,543.0,,,,,,,
1414,11725274,Computing a matrix which transforms a quadrangle to another quadrangle in 2D,|matlab|computer-vision|robotics|numerical-methods|projective-geometry|,"<p>In the figure below the goal is to compute the homography matrix H which transforms the points a1 a2 a3 a4 to their counterparts b1 b2 b3 b4. 
That is:</p>

<pre><code>[b1 b2 b3 b4] = H * [a1 a2 a3 a4]
</code></pre>

<p>What way would you suggest to be the best way to calculate H(3x3). a1...b4  are points in 2D which are represented in homogeneous coordinate systems ( that is [a1_x a1_y 1]', ...).
<strong>EDIT</strong>:
For these types of problems we use SVD, So i would like to see how this can be simply done in Matlab.</p>

<p><strong>EDIT</strong>:</p>

<p>Here is how I <strong>initially</strong> tried to solve it using svd (H=Q/P) in Maltlab. Cosider the following code for the given example</p>

<pre><code>px=[0 1 1 0];  % a square
py=[1 1 0 0];

qx=[18 18 80 80];    % a random quadrangle
qy=[-20 20 60 -60];
if (DEBUG)
  fill(px,py,'r');
  fill(qx,qy,'r');
end

Q=[qx;qy;ones(size(qx))];
P=[px;py;ones(size(px))];
H=Q/P;
H*P-Q
answer:
   -0.0000         0         0         0         0
  -20.0000   20.0000  -20.0000   20.0000    0.0000
   -0.0000         0         0         0   -0.0000
</code></pre>

<p>I am expecting the answer to be a <strong>null matrix</strong> but it is not!... and that's why I asked this question in StackOverflow.
Now, we all know it is a <strong>projective transformation</strong> not obviously Euclidean. However, it is good to know if in general care calculating such matrix using only 4 points is possible.</p>

<p><img src=""https://i.stack.imgur.com/ZvaZK.png"" alt=""matrix computation""></p>
",7/30/2012 16:07,11731236.0,5275,5,6,6,0.0,699559.0,,4/9/2011 1:47,978.0,11726686.0,"<p>You can use the <a href=""http://en.wikipedia.org/wiki/Direct_linear_transformation"" rel=""nofollow"">DLT algorithm</a> for this purpose.  There are MATLAB routines available for doing that in <a href=""http://www.csse.uwa.edu.au/~pk/research/matlabfns/"" rel=""nofollow"">Peter Kovesi's homepage</a>.</p>
",128966.0,1.0,0.0,15558765.0,"in Matlab simply H=B/A where B=[b1 b2 b3 b4], A=[a1 a2 a3 a4]"
1292,8897938,I am designing a Java Robot?,|java|robotics|,"<p>I am using a robot programmed with Java with a distance and touch sensor (but no gps or compass) to navigate a 1.0 by 2.5 metre obstacle course. The robot only knows its position by dead reckoning (like the number if turns of its wheels). When it turns it can measure the number of degrees from its last path travelled. After it finds the obstacles it needs to produce a map of where they are most likely to be. I want to extend a JPanel Class and override its paintComponent() method and then use the methods of the Graphics class to draw on the JPanel. I know that there are many drawxxxx methods for drawing. But I was wondering how I could actually achieve this, like the actual code that is necessary to produce this?!</p>
",1/17/2012 16:13,,219,1,2,2,,1154353.0,,1/17/2012 16:10,67.0,8897979.0,"<p>There's a great lesson on that on the official Java Swing tutorial page:</p>

<p><a href=""http://docs.oracle.com/javase/tutorial/uiswing/painting/step2.html"" rel=""nofollow"">http://docs.oracle.com/javase/tutorial/uiswing/painting/step2.html</a></p>
",939023.0,0.0,0.0,11127026.0,[What have you tried?](http://mattgemmell.com/2008/12/08/what-have-you-tried/)
4707,77461417,Switch between different ROS (Robot Operating System) versions,|ubuntu|simulation|ros|robotics|,"<p>As a ROS (Robot Operating System) developer, how is it possible to switch between ROS versions within my projects? Different Ubuntu OS needed to be installed?</p>
<p>I have tried virtual machines and dual booting but I did not find that a good solution.</p>
",11/10/2023 16:14,,86,1,5,0,,22894370.0,,11/10/2023 15:56,1.0,77469197.0,"<p>One option is using the Ubuntu version that is compatible with both ROS1 and ROS2. Then, you can just switch ROS1&lt;-&gt;ROS2 when you are sourcing it.</p>
<p>Another option is using docker, which is more recommended.</p>
",18268514.0,0.0,5.0,136594760.0,"Actually I do not get it, currently I am using Ubuntu 18 LTS with ROS1 Melodic, while there is Robot in the laboratory which is programmed on ROS1 Noetic. I do not know what to do? It seems, I need to upgrade my Ubuntu system to ver 20.04 LTS and change the programs coded in Melodic. Is n't it?
(I am not familiar with docker to handle these problem.)"
1363,10460809,What is * in AT command?,|protocols|robotics|at-command|,"<p>I came accros this kind of line in a proprietary use of the AT commands:</p>

<blockquote>
  <p>AT*REF=1,290717696&lt;LF&gt;</p>
</blockquote>

<p>It is a proprietary command as it is used in a protocol to control a robot.</p>

<p>According to what I read on <a href=""http://en.wikipedia.org/wiki/Hayes_command_set#Description"" rel=""nofollow"">Wikipedia</a> and other sources, AT command extensions should use ""\"" or ""%"". There is no mention of ""*"".</p>

<p>So what does <code>*</code> define?</p>
",5/5/2012 10:06,,141,1,2,1,,757293.0,France,5/17/2011 11:52,283.0,19373686.0,"<p>There are more than two different characters in use by various manufacturers when implementing the first character of a proprietary AT command.  Some that I've seen:</p>

<p>! @ # $ % ^ * _</p>

<p>The manufacturer of your device may have chosen '*' for commands that have similar functionality, or they may have chosen to implement ALL of their proprietary AT commands with '*' as the first character.</p>

<p>There are many AT command references available online in PDF format from many different manufacturers.  Perhaps the manufacturer of your device makes this information available as well.</p>
",2592733.0,0.0,0.0,13514626.0,"My question is not about the command, but about the choice of * in the command."
4717,77601467,Position control in Mujoco,|simulation|robotics|mujoco|,"<p>Being new to Mujoco, I just created a dummy scene in mujoco for which I now try to control some of the objects in it. The scene contains a fixed table on which a movable book is places, which can be moved by controlling the actuated spatula frame:</p>
<pre><code>xml = &quot;&quot;&quot;
&lt;mujoco&gt;
  &lt;worldbody&gt;
    &lt;light name=&quot;top&quot; pos=&quot;0 0 1&quot;/&gt;
    
    &lt;body name=&quot;table&quot; pos=&quot;0 0 0.025&quot;&gt;
      &lt;geom name=&quot;plate&quot; type=&quot;box&quot; size=&quot;0.25 0.2 0.025&quot; rgba=&quot;.8 .8 .8 1&quot;/&gt;
      &lt;geom name=&quot;bound0&quot; type=&quot;box&quot; size=&quot;.25 .01 .05&quot; pos=&quot;0 -.19 .075&quot;/&gt;
      &lt;geom name=&quot;bound1&quot; type=&quot;box&quot; size=&quot;.01 .19 .05&quot; pos=&quot;-.24 .01 .075&quot;/&gt;
    &lt;/body&gt;

    &lt;body name=&quot;booklink&quot; pos=&quot;0 0 0.065&quot;&gt;
      &lt;freejoint/&gt;
      &lt;geom name=&quot;book&quot; type=&quot;box&quot; size=&quot;.1 .05 .0125&quot; rgba=&quot;0 1 0 1&quot; mass=&quot;.1&quot;/&gt;
    &lt;/body&gt;  

    &lt;body name=&quot;spatulalink&quot; pos=&quot;0 0 .2&quot;&gt;
        &lt;joint name=&quot;transX&quot; type=&quot;slide&quot; axis=&quot;1 0 0&quot; limited=&quot;true&quot; range=&quot;-.6 .6&quot;/&gt;
        &lt;joint name=&quot;transY&quot; type=&quot;slide&quot; axis=&quot;0 1 0&quot; limited=&quot;true&quot; range=&quot;-.6 .6&quot;/&gt;
        &lt;joint name=&quot;transZ&quot; type=&quot;slide&quot; axis=&quot;0 0 1&quot; limited=&quot;true&quot; range=&quot;-.6 .6&quot;/&gt;
        &lt;joint name=&quot;hingeX&quot; type=&quot;hinge&quot; axis=&quot;1 0 0&quot; limited=&quot;true&quot; range=&quot;-3.2 3.2&quot;/&gt;
        &lt;joint name=&quot;hingeY&quot; type=&quot;hinge&quot; axis=&quot;0 1 0&quot; limited=&quot;true&quot; range=&quot;-3.2 3.2&quot;/&gt;
        &lt;joint name=&quot;hingeZ&quot; type=&quot;hinge&quot; axis=&quot;0 0 1&quot; limited=&quot;true&quot; range=&quot;-3.2 3.2&quot;/&gt;
        &lt;geom name=&quot;spatula&quot; type=&quot;box&quot; size=&quot;.05 .05 5e-4&quot; rgba=&quot;1 1 0 1&quot; friction=&quot;.1 .1 .1&quot; mass=&quot;.1&quot;/&gt;
    &lt;/body&gt;
  &lt;/worldbody&gt;
  &lt;actuator&gt;
    &lt;position ctrllimited=&quot;true&quot; ctrlrange=&quot;-.6 .6&quot; joint=&quot;transX&quot;/&gt;
    &lt;position ctrllimited=&quot;true&quot; ctrlrange=&quot;-.6 .6&quot; joint=&quot;transY&quot;/&gt;
    &lt;position ctrllimited=&quot;true&quot; ctrlrange=&quot;-.6 .6&quot; joint=&quot;transZ&quot;/&gt;
    &lt;position ctrllimited=&quot;true&quot; ctrlrange=&quot;-3.2 3.2&quot; joint=&quot;hingeX&quot;/&gt;
    &lt;position ctrllimited=&quot;true&quot; ctrlrange=&quot;-3.2 3.2&quot; joint=&quot;hingeY&quot;/&gt;
    &lt;position ctrllimited=&quot;true&quot; ctrlrange=&quot;-3.2 3.2&quot; joint=&quot;hingeZ&quot;/&gt;
  &lt;/actuator&gt;
&lt;/mujoco&gt;
&quot;&quot;&quot;
model = mujoco.MjModel.from_xml_string(xml)
data = mujoco.MjData(model)
</code></pre>
<p>Now my question is how I can properly run a trajectory in this environment during which I control the robot. Naively, I tried to put some controls by directly writing into <code>data.qpos</code> and <code>data.qvel</code>. This creates the desired motions, but the objects tunnel through each other, i.e., there are collisions of the geometries, which I of course do not want. You can see a picture of this here (you see the yellow spatula object going through the boundary in collision):</p>
<p><a href=""https://i.stack.imgur.com/oD1YR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oD1YR.png"" alt=""sim error"" /></a></p>
<p>Now, after some reading, I now learned that the correct way of controlling actuators / joints in Mujoco is instead to set the controls directly, which in my case would be <code>data.crtl = some_position_vector</code>. I do this as follows:</p>
<pre><code># Construct a BSpline to interpolate the via-points
spline_ref = BSpline_reference(data.qpos[7:], data.qvel[6:], data.time)
spline_ref.append(path, times, data.time)

with mujoco.viewer.launch_passive(model, data) as viewer:
  # Close the viewer automatically after 30 wall-seconds.
  start = time.time()
  sim_steps = int(n_steps // tau)
  i = 0
  while viewer.is_running(): #and i &lt; sim_steps:
    # Get a target position from the reference spline using current sim state
    qreal = data.qpos[7:]
    qDot_real = data.qvel[6:]
    qref, qDotref = spline_ref.getReference(qreal, qDot_real, data.time)
    data.ctrl = q_ref
    mujoco.mj_step(model, data)

    # Pick up changes to the physics state, apply perturbations, update options from GUI.
    viewer.sync()

    # Slow down sim. for visualization
    time.sleep(1e-2)
    i += 1
</code></pre>
<p>Somehow, the movement I get from this is completely different than when I set <code>data.qpos</code> directly. Can someone explain to me how the controls can be set properly?</p>
",12/4/2023 17:35,77691739.0,613,2,0,0,,19249619.0,"Berlin, Deutschland",6/1/2022 15:06,3.0,77691739.0,"<p>What you have in your model are position actuators, which apply a torque or force of <code>kp * (q_desired - q) - kv * qdot</code>. So the position is not set magically as it would when you set qpos directly.</p>
<p>The <a href=""https://mujoco.readthedocs.io/en/stable/XMLreference.html#actuator-position"" rel=""nofollow noreferrer"">defaults</a> are <code>kp=&quot;1&quot;</code> and <code>kv=&quot;0&quot;</code>. <code>kp</code> might be too small for your system.</p>
<p>Similarly to how you would on a real robot, you should tune the gains on your position actuators so you get the responsiveness you want without introducing unwanted oscillations.</p>
<p>Note that the kv parameter will essentially create a reference velocity of zero. If you want to track a reference velocity, as well as position, you could add extra velocity actuators to your model, and set the ctrl entries for those to your qDotref.</p>
",155171.0,0.0,2.0,,
2412,34290860,Tracking position and velocity using a kalman filter,|opencv|localization|robotics|kalman-filter|,"<p>I am using a kalman filter (constant velocity model) to track postion and velocity of an object. I measure x,y of the object and track x,y,vx,vy . Which works but if a add gausian noise of +- 20 mm to the sensor readings x,y,vx,vy fluctuates even though the point is not moving just noise. For location that is good enough for my needs but velocity changes when the point is stationary and that is causing problems with my object speed calculations. Is there a way around this problem? also if switching to constant acceleration model improve on this? I am tracking a robot via a camera.</p>

<p>I am using opencv implementation and my kalman model is same as [1]</p>

<p>[1] <a href=""http://www.morethantechnical.com/2011/06/17/simple-kalman-filter-for-tracking-using-opencv-2-2-w-code/"" rel=""noreferrer"">http://www.morethantechnical.com/2011/06/17/simple-kalman-filter-for-tracking-using-opencv-2-2-w-code/</a></p>
",12/15/2015 13:45,,3764,1,0,6,0.0,89904.0,,4/12/2009 4:27,3143.0,34324218.0,"<p>The most important thing about designing a Kalman filter is not the data, it's the error estimates.  The matrices in that example seem to be chosen arbitrarily, but you should pick them using specific knowledge of your system.  In particular:</p>

<ul>
<li>Error covariance has units.  It's standard deviation squared.  So your position error is in ""length squared"" and velocity in ""length per time squared"".  The values in those matrices will be different depending on whether you work in m or mm.</li>
<li>You are implementing a ""constant velocity"" model, but the ""processNoiseCov"" from the example sets the same values for both position and velocity process noise.  This means that you might be wrong about your position due to being wrong about your velocity, <em>and you might be wrong because the object teleported around in a way that's independent of velocity</em>.  In a CV model you would expect that the position process noise would be very low (basically nonzero only for numerical reasons and to cover modelling error) and the true unknown motion of the system would be attributed to unknown velocity.  This problem also interferes with the KF's ability to infer velocity from position input, because the ""teleportation error"" of position is not attributed to velocity change.</li>
<li>If you're putting in +/-20mm of error (you should really put in Gaussian noise if you want to simulate ideal behavior) you have an approximate standard deviation of 11.5mm or a variance of (11.5mm)^2.  Not knowing what your units are (mm or m) I can't say what the numerical value of ""measurementNoiseCov"" should be, but it's not 0.1 (as in the example).</li>
</ul>

<p>And finally, even with all of that correct, keep in mind that the KF is ultimately a linear filter.  Whatever noise you put in will show up in the output, just scaled by some factor (the Kalman gain).</p>
",479989.0,3.0,0.0,,
2470,36482498,How to register Point clouds using HandEye calibration,|robot|point-clouds|,"<p>I have built a fringe projection based 3D-Scanner with a projector and camera each. I intend to mount this system on an industrial robot and use it for automatic scanning of components. I know that HandEye calibration needs to be performed to find transformation between Camera co-ord system to Robot base co-ord system. Now, I want to use this information to register point clouds. </p>

<p>To make it more clear:</p>

<p>At position A, I capture PCD1, then I move to position B and capture PCD2. A and B are in Robot co-ord system but I can convert them into Sensor Co-ordinate system using the HandEye calibration data. So, the Sensor was at A’ and B’. Using this information, I want to roughly register PCD1 and PCD2. </p>

<p>Can any one suggest ways to achieve this or refer to any relevant publication?</p>
",4/7/2016 16:49,,248,1,0,0,,4362116.0,,12/15/2014 11:22,19.0,36483418.0,"<p>You should check this thesis:</p>

<p><a href=""http://mediatum.ub.tum.de/doc/800632/800632.pdf"" rel=""nofollow"">http://mediatum.ub.tum.de/doc/800632/800632.pdf</a></p>

<p>And this library:</p>

<p><a href=""http://pointclouds.org/"" rel=""nofollow"">http://pointclouds.org/</a></p>
",6076911.0,0.0,1.0,,
1915,21744642,Is there any Arduino Virtual Simulator for Desktop PC?,|serial-port|arduino|virtual|robotics|,"<p>If I dont have any <a href=""http://arduino.cc/en/Main/ArduinoBoardUno"" rel=""nofollow"">arduino</a> device, is it possible to simulate code virtually in my Desktop PC?
That means I want test my code.</p>
",2/13/2014 3:32,21752931.0,4761,1,0,0,,797196.0,Bangladesh,6/14/2011 7:31,114.0,21752931.0,"<p>there is <a href=""http://web.simuino.com/"" rel=""nofollow"">simuino</a> this is free
and <a href=""http://www.virtualbreadboard.com/"" rel=""nofollow"">Virtual Breadbord</a> paid</p>
",616100.0,0.0,0.0,,
3390,56067475,"Doing things (driving servos, turning on led) using joystick input",|python|robotics|joystick|evdev|,"<p>I'm building a basic DVa Mech robot (hobby).  This is non-bipedal.  Wheeled chassis. All code in python. </p>

<p>How do I continuously perform activities while holding down a button, pushing a joystick?  And can I even do two (or more) at once: move wheels forward, turn torso, fire guns?</p>

<p>I'm reading joystick input fine. Servos work too.
I can't seem to figure out the logic loops of 'while button pushed - do something - and keep scanning for more input'</p>

<p>Tried various things...they didn't work so they are out of the code below.</p>

<p>Running 6 continuous servos (4 for chassis, two for mini-guns)
Logitech F710 joystick</p>

<pre><code>from evdev import InputDevice, categorize, ecodes, KeyEvent
from adafruit_servokit import ServoKit
import time
kit = ServoKit(channels = 16)
gamepad = InputDevice('/dev/input/event7')
print (gamepad)
for event in gamepad.read_loop():
    if event.type == ecodes.EV_KEY:
            keyevent = categorize(event)
            if keyevent.keystate == KeyEvent.key_down:
                    print(keyevent)
                    ....
                    elif keyevent.keycode == 'BTN_TL':
                            print (""Guns"")
    elif event.type == ecodes.EV_ABS:
            absevent = categorize(event)
            print(absevent.event.code)
            if ecodes.bytype[absevent.event.type][absevent.event.code] == 'ABS_HAT0X':
                    if absevent.event.value == -1:
                            print('left')
                    elif absevent.event.value == 1:
                            print('right')
            if ecodes.bytype[absevent.event.type][absevent.event.code] == 'ABS_HAT0Y':
                    if absevent.event.value == -1:
                            print('forward')
                    elif absevent.event.value == 1:
                            print('back')
</code></pre>

<p>Fairly basic...when BTN_TL is pressed, servos 5 and 6 should spin until the button is released</p>

<p>Likewise with HAT0X and 0Y the servos should move forward/back/left/right while pressed.</p>

<p>I've tried while loops and whatnot...but there's a logic/timing sequence in joystick input I'm not putting in the right place</p>
",5/9/2019 20:58,,445,1,0,0,,11477889.0,,5/9/2019 20:34,0.0,56068204.0,"<p>For the <strong>servo</strong> part and based on <a href=""https://github.com/adafruit/Adafruit_CircuitPython_ServoKit"" rel=""nofollow noreferrer"">Servokit documentation</a> there is two ways to control servos:</p>

<ol>
<li>Set desired shaft angle:</li>
</ol>

<pre><code>    kit.servo[servonum].angle = 180
</code></pre>

<ol start=""2"">
<li>Indicate rotation direction (<code>1</code>: forward, <code>-1</code>: backward, <code>0</code>: stop) like:</li>
</ol>

<pre><code>    kit.continuous_servo[servonum].throttle = 1 
    kit.continuous_servo[servonum].throttle = -1
    kit.continuous_servo[servonum].throttle = 0
</code></pre>

<p>I'd rather use angles, even if I'd have to increment/decrement their values (according to time) in the loop. It would give you a hand on speed (or speed curve) and position.</p>

<p>For the <strong>Joystick</strong> part, the <a href=""https://ericgoebelbecker.com/2015/07/raspberry-pi-and-gamepad-programming-part-3-adding-a-gun-turret/"" rel=""nofollow noreferrer"">Eric Goebelbecker</a>'s ""tutorial"" worth a reading.</p>

<p>EDIT: Solution with <code>continuous_servo</code> (for future reading)</p>

<p><code>ABS_HAT0{X,Y}</code> notifies DOWN event with -1 or +1 on the active axis. And UP with 0 value.</p>

<pre><code>axis_servo = {
    'ABS_HAT0X': 5,
    'ABS_HAT0Y': 6,
}

[...]
        axis = ecodes.bytype[absevent.event.type][absevent.event.code]
        if axis == 'ABS_HAT0X':
            servonum = axis_servo[axis]
            kit.continuous_servo[servonum].throttle = absevent.event.value
        if axis == 'ABS_HAT0Y':
            servonum = axis_servo[axis]
            # Reverse Y axis (1 -&gt; up direction)
            kit.continuous_servo[servonum].throttle = -absevent.event.value
</code></pre>

<p>Without <code>continuous_servo</code>, one should consider the use of <code>select()</code> with <strong>timeout</strong> (<code>select([gamepad], [], [], timeout)</code>) as described in <a href=""https://python-evdev.readthedocs.io/en/latest/tutorial.html#reading-events-from-multiple-devices-using-select"" rel=""nofollow noreferrer"">readthedoc: python-evdev</a>.</p>

<p>The timeout will allow angle computation.</p>
",425540.0,0.0,1.0,,
2883,45944714,LED stripe should follow a person,|python|ros|led|robot|,"<p>I got a LED stripe connected with a RasPi3. The stripe should be installed at an automated guided vehicle as the human maschine interface. I would like to program the stripe so that there are ""eyes"" on it (e.g. <em>3 LED pixel on -- 5 LED pixel off -- 3 LED pixel on</em>), which follow automatically a person who is standing in front of it.</p>

<p>Actually i have the methods:</p>

<p><em>""set_eye_position(msg)""</em> which is able to set the LED pixel on an interval from -99 (completely left) to +99 (completely right) as input parameter (msg) and </p>

<p><em>""set_eyes_to_coord(msg)""</em> which get two input parameters: The x and y coordinates of the person who is standing next to the vehicle. My approach is to set a coordinate system in the middle of the robot (see <a href=""https://i.stack.imgur.com/5KdZ8.jpg"" rel=""nofollow noreferrer"">Picture</a>)</p>

<p><strong>The reason for my question is, if there is an opportunity to calculate the exact position of the LED pixel at given input parameters (x,y)?</strong></p>

<p>I'm writing with Python and I'm quite a newbee in programming, so I would really appreciate if I get some ideas how to realize my issue.</p>

<p>Thanks in advance</p>

<hr>

<p>EDIT:</p>

<p>Assuming the approach from bendl <a href=""https://i.stack.imgur.com/7Ye4H.jpg"" rel=""nofollow noreferrer"">THIS</a> is the new setup, right? I do not really know what to do with the variables boe_left, boe_right and boe_dist. But maybe I'm just too dumb to understand it.</p>
",8/29/2017 17:09,,83,1,4,-1,,8533129.0,Germany,8/29/2017 14:19,15.0,45945573.0,"<p>Here's something to get you started off:</p>

<pre><code>def pixels_on(x, y):
    assert y &gt; 0                                # person must be in front of robot
    boe_left, boe_right = -10, 10               # x location of back of eye left, back of eye right
    boe_dist = - 3                              # distance to back of eye

    m_left =  (x - boe_left) / (y - boe_dist)   # slope from back of left eye to person
    m_right =  (x - boe_right) / (y - boe_dist) # slope from back of right eye to person

    c_left = boe_left - m_left * boe_dist       # center of front of left eye
    c_right = boe_right - m_right * boe_dist    # center of front of right eye

    return list(map(int, [c_left - 1, c_left, c_left + 1, c_right - 1, c_right, c_right + 1]))
</code></pre>

<p>This works by drawing a line between an fixed point you can think of as the imaginary back of the robot's eye and the person. The point where this line intersects the LED strip is the point that the eye should be centered. This solution makes the (naive) assumption that there is one LED per unit distance, so you'll have to make some changes, but we can't do EVERYTHING for you ;)</p>
",5090081.0,0.0,2.0,79080390.0,"Being 'dumb' and 'new to programming' are two very different things. Don't start thinking you're dumb because someone with years of experience is better at something than yourself. I would also struggle with the code I gave you back when I was a newbie. That being said, have you solved your problem, or do you need more guidance?"
3156,51187676,What's the difference between ROS2 and DDS?,|robotics|data-distribution-service|ros2|,"<p>ROS2 is a distributed architecture using publisher/subscriber messaging between nodes.</p>

<p>ROS2 has taken a different approach in its messaging layer and now employs the industry standard called Data Distributed Services (DDS).</p>

<p>But, DDS is a middleware for communication, also support publisher/subscriber.</p>

<p>So, we can use DDS directly, why use ROS2?</p>
",7/5/2018 9:20,55204587.0,9577,4,0,14,0.0,9920709.0,"Beijing, 北京市中国",6/10/2018 9:45,90.0,51188588.0,"<p>Indeed, ROS2 is based on DDS for the communication. (<a href=""https://github.com/ros2/ros2/wiki/DDS-and-ROS-middleware-implementations"" rel=""noreferrer"">https://github.com/ros2/ros2/wiki/DDS-and-ROS-middleware-implementations</a>)</p>

<p>ROS2 is used because it adds an abstraction making DDS easier to use. DDS needs lot of setup and configuration (partitions, topic name, discovery mode, message creation,...) which is done in the RMW package of ROS2. This package is also responsible to handle error when a message is published/received (taken).</p>

<p>You can use DDS directly (if you configure properly your publisher and subscriber you can also communicate with ROS2 publisher and subscriber) however you will have to create the message (.idl), call the generator to get the corresponding structure and sources files, create a domain, assign a topic, configure the datawriters/datareader,.. (have a look at some examples <a href=""https://github.com/rticommunity/rticonnextdds-examples/tree/master/examples/listeners/c"" rel=""noreferrer"">https://github.com/rticommunity/rticonnextdds-examples/tree/master/examples/listeners/c</a>)</p>

<p>So ROS2 is making your life easier. Plus, there are lots of packages that can be used above the messages.</p>
",6949178.0,6.0,0.0,,
3071,49668825,Align Point Clouds with PCL on only one rotation axis,|computer-vision|point-cloud-library|robotics|point-clouds|,"<p>I'm trying to align two point clouds acquired from a robot with PCL. I compute correspondences and then align them with <code>estimateRigidTransformation</code>:</p>

<pre><code>pcl::registration::TransformationEstimationSVD&lt;PointT, PointT&gt; transformation;
transformation.estimateRigidTransformation(*source_keypoints, *target_keypoints, *correspondences, correspondence_transformation);
</code></pre>

<p>Then I do an ICP refinement:</p>

<pre><code>Eigen::Matrix4f transform; 
pcl::IterativeClosestPoint&lt;PointT, PointT&gt; icp; 
icp.setMaximumIterations(max_iterations);

TransformationEstimationLM::Ptr transformEst(new TransformationEstimationLM); 
icp.setTransformationEstimation(transformEst); 
icp.setInputSource(source); 
icp.setInputTarget(target); 
icp.align(*result);
</code></pre>

<p>This works well if the scene is not too cluttered but also often fails and then gives a completely wrong transformation (randomly rotated). Since the point clouds are already aligned with the ground plane and the robot only moves on the ground plane the clouds actually only differ in translation and the y (up) rotation. Is is possible to set this in PCL?</p>
",4/5/2018 9:28,,1734,1,0,1,0.0,4848533.0,,4/29/2015 23:17,2.0,50121742.0,"<p><a href=""http://docs.pointclouds.org/trunk/classpcl_1_1registration_1_1_warp_point_rigid3_d.html"" rel=""nofollow noreferrer"">WarpPointRigid3D</a> will allow you to do a 1D rotation and a 2D translation.  Here's a <a href=""http://www.pcl-users.org/Restricting-Transform-Dimensions-in-Cloud-Registration-ICP-td4020375.html"" rel=""nofollow noreferrer"">relevant mailing list</a> post.</p>
",730138.0,0.0,0.0,,
1219,6645215,ROS on Android Phone,|android|robotics|,"<p>Currently I'm trying to run ROS node on Android Phone to remotely control a robotic. I found that ROS node on Android Phone could publish message to a Topic, but could not subscribe to any topic. Is anyone know the reason?</p>

<p>Thanks,
Liu.</p>
",7/11/2011 2:27,,845,1,1,2,,838134.0,,7/11/2011 2:27,7.0,6655346.0,"<p>You should search <a href=""http://answers.ros.org/questions/"" rel=""nofollow"">ROS Answers</a> for your question or others similar that might help you solve your problem. If you don't find a solution, feel free to post the question there. Since ROS Answers is a ROS specific forum, you'll likely get much better support there than here on StackOverflow.</p>

<p>If you do post there, please include as much detail as you can to help with debugging the problem. Any error messages, how you have your network configured, etc would be helpful (and if possible, include links to your source code).</p>
",130251.0,4.0,0.0,7852557.0,Are you getting any errors or is it silently failing?
2687,42371935,How to make a stable feedback controller which can only generate intermittent impulses?,|signal-processing|robotics|robot|control-theory|feedback-loop|,"<p>I have a kuka iiwa (7 joint robot arm).  Attached to it is a circular aluminum platter with a steel ball on it.  The goal of the project (for giggles/ challenge) was to use the internal torque sensors of the robot in order to balance the ball in the middle of the platter.  Due to the fact that I was unable/ not allowed to use FRI (fast robot interface) whereby I can control the robot from C at about 3ms feedback loop, I can only update the robot position at about 4Hz... My quick and dirty solution consisted of the following:</p>

<p>Measure the torque on the final two axes of the arm and apply a mapping to generate the ball's position (filtration and hysteresis were well implemented to improve the data quality).  If the ball velocity was sufficiently stable, generate a motion that would cancel out that velocity (with an impulse ""go to angle and return to neutral position"" motion).  Overlaid on that was also a small proportional gain which would tend the ball towards the center of the platter.</p>

<p>My question:  What is the professional/ correct solution to this situation (where your controller can only hit the system with impulses rather than continuous feedback)?  </p>

<p>Here is a picture of the setup:
<a href=""https://i.stack.imgur.com/0j6Th.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0j6Th.jpg"" alt=""enter image description here""></a></p>
",2/21/2017 15:50,,75,1,0,1,0.0,7298298.0,"Milwaukee, WI, USA",12/14/2016 18:31,405.0,46008757.0,"<p>A slightly dampened negative feedback loop.
Where bal.posX dictates strength y.rot.arm ?</p>

<p><a href=""https://en.wikipedia.org/wiki/Control_system"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Control_system</a></p>

<p>I once coded something alike it, my colleague who's into PLC's called it that.
I did fuzzy logic optimization then..</p>
",613326.0,1.0,0.0,,
2761,43414357,Why is my nested while loop not working?,|while-loop|nested-loops|robotics|robotc|,"<p>I'm currently programming in RobotC, for a Vex 2.0 Cortex. I'm using encoders to make my robot go straight.</p>

<p>This is my code:</p>

<pre><code>void goforwards(int time)
{
    int Tcount = 0;
    int speed1 = 40;
    int speed2 = 40;
    int difference = 10;


    motor[LM] = speed1;
    motor[RM] = speed2;
    while (Tcount &lt; time)
    {
        nMotorEncoder[RM] = 0;
        nMotorEncoder[LM] = 0;

        while(nMotorEncoder[RM]&lt;1000)
        {
            int REncoder = -nMotorEncoder[RM];
            int LEncoder = -nMotorEncoder[LM];

            if (LEncoder &gt; REncoder)
            {
                motor[LM] = speed1 - difference;
                motor[RM] = speed2 + difference;
                if (motor[RM]&lt;= 0)
                {
                    motor[RM] = 40;
                    motor[LM] = 40;
                }
            }
            if (LEncoder &lt; REncoder)
            {
                motor[LM] = speed1 + difference;
                motor[RM] = speed2 - difference;
                if (motor[RM]&lt;= 0)
                {
                    motor[RM] = 40;
                    motor[LM] = 40;
                }
                Tcount ++;
            }
        }
    }
}


task main()
{

    goforwards(10);
}
</code></pre>

<p>For reference, these are my Pragma settings:</p>

<pre><code>#pragma config(I2C_Usage, I2C1, i2cSensors)
#pragma config(Sensor, dgtl2,  ,               sensorDigitalIn)
#pragma config(Sensor, dgtl7,  ,               sensorDigitalOut)
#pragma config(Sensor, I2C_1,  ,               sensorQuadEncoderOnI2CPort,    , AutoAssign )
#pragma config(Sensor, I2C_2,  ,               sensorQuadEncoderOnI2CPort,    , AutoAssign )
#pragma config(Motor,  port1,           RM,            tmotorVex393_HBridge, openLoop, reversed, encoderPort, I2C_2)
#pragma config(Motor,  port10,          LM,            tmotorVex393_HBridge, openLoop, encoderPort, I2C_1)
</code></pre>

<p>When I excecute the code, the Robot's encoder values are very close, but the robot stops moving when they reach 1000. I thought the code I wrote should return the values of the encoders back to 0 after they reach 1 thousand, and thereby the code should re-iterate in the shell loop 10 times (in this case). What have I done wrong?</p>
",4/14/2017 15:13,43414410.0,206,2,1,0,0.0,6131978.0,,3/29/2016 23:37,5.0,43414410.0,"<p>You are updating <code>Tcount</code> at the wrong place. Do it Just at the end of the outer loop.</p>

<p>What you have written now makes <code>Tcount</code> increase everytime it moves ahead. By the time it reaches 1000 steps, <code>Tcount</code> is already 1000.</p>

<p>Your <code>times</code> is 10. So `Tcount >= time and it wont enter the outer while loop again.</p>
",2858773.0,1.0,0.0,73887612.0,RobotC is **not C**!
3692,61319036,How to control a physical robot using a web interface,|ubuntu-18.04|robot|ros2|,"<p>I would like to know how I could control the movement of a physical robot using a web interface. For example, I have created a web interface with four movement buttons (front, back, left, right) but do not know how to connect that interface to the physical robot and control its movements. I have experience in controlling a simulated Turtlebot (in Gazebo) with the interface locally on my laptop using ROSBRIDGE and SimpleHTTPServer. Would I have to use these as well to control a physical robot?</p>

<p>I'm running ROS2 Crystal, Ubuntu 18.04. Thank you!</p>
",4/20/2020 9:22,,512,2,0,1,,13151812.0,,3/30/2020 9:29,8.0,61363449.0,"<p>Yes,  The interface to control a physical robot would be the same as simulation.<br>
You will need to to publish control command to <code>/cmd_vel</code> topic and then you can subscribe to the topic to convert those velocity commands to actual motor commands.   </p>

<p>You can also look into using <a href=""http://robotwebtools.org/"" rel=""nofollow noreferrer"">Robot Web Tools</a> for the web interface.   </p>

<p>Additionally if you could provide more detiails about your setup I could give more information.</p>
",2661982.0,2.0,0.0,,
207,742369,Pathfinding Algorithm for Robot,|c|algorithm|artificial-intelligence|robotics|,"<p>I have a robot that uses an optical mouse as a position track. Basically, as the robot moves it is able to track change in X and Y directions using the mouse. The mouse also tracks which direction you are moving - ie negative X or positive X. These values are summed into separate X and Y registers. <br></p>

<p>Now, the robot rotates in place and moves forward only. So the movement of the robot is ideally in straight lines (although the mouse tracking can pickup deviations if you veer off) at particular angles. A particular set of movements of the robot would be like:
<br>
A: Rotate 45 degrees, move 3 inches <br>
B: Rotate 90 degrees, move 10 inches <br>
C: Rotate -110 degrees, move 5 inches <br>
D: Rotate 10 degrees, move 1 inch <br>
But each time the mouse X and mouse Y registers give the real distances you moved in each direction.
<br><br>
Now, if I want to repeat the movement set going from A to D only, how can I do this using the information I have already gathered. I know I can basically sum all the angles and distances I feed into it already, but this would prove to be inaccurate if there were large errors in each movement orders. How can I use the raw information from my mouse? A friend provided an idea that I could continuously sine and cosine the mouse values and calculate the final vector but I'm not really sure how this would work.</p>

<p>The problem is that the mouse only gives relative readings so rotating or moving backwards, you are potentially erasing information. So yeah, what I am wondering is how you can implement the algorithm so it can continually track changes to give you a shortest path if you moved in zigzags to get there originally.</p>
",4/12/2009 20:29,742476.0,2722,3,5,2,0.0,90046.0,"Montreal, Canada",4/12/2009 20:22,132.0,742458.0,"<p>Not an answer to your question, but perhaps a cautionary tale...
I did exactly this kind of robot as a school project a year back. It was an utter failure, though I learnt quite a bit while doing it.
As for using the mouse for tracking how far you have driven: It did not work well for us at all, or any of the other groups. Probably because the camera in the mouse was out of focus due to the fact that we needed to have the mouse a few mm above the floor. The following year no group doing the same project used this methid. They instead put markings on the weels and used a simple ir-sensor to calculate how many revolutions the wheels made.</p>
",13565.0,0.0,3.0,554134.0,"The problem is that the mouse only gives relative readings so rotating or moving backwards, you are potentially erasing information. So yeah, what I am wondering is how you can implement the algorithm so it can continually track changes to give you a shortest path if you moved in zigzags on the way."
1135,5765591,General Advice - Robotics / AI,|artificial-intelligence|robotics|,"<p>This is not for asking any doubts or queries. I know this is a technical forum and hence would be the best platform for me to get advice on.</p>

<p>I am a Master's student in computer engineering and hold interest in Robotics. I am confused as to where should I start if from. I have 2 courses one is on controlling of robots and other is based on introduction to AI. I don't want to take both the courses together. I am confused as to do I need to go for controlling of robots first or AI first?</p>

<p>Also, if you know any good forums/blogs on AI then please share... Would be a lot helpful.</p>

<p>Thanks. </p>
",4/23/2011 16:58,5802255.0,422,2,2,0,,667851.0,,3/20/2011 0:59,170.0,5765709.0,"<p>Well, I would take the AI class first, because I would want to know more about the logic before going to the control part.<br>
As far as forums go, you could check out the <a href=""http://www.ai-forum.org/"" rel=""nofollow"">AI Forum</a>, and the <a href=""http://ai-depot.com/"" rel=""nofollow"">AI Depot</a> (the AI Depot is not exactly a forum, but it has some good resources and articles).  </p>

<p>You can also check out these Area51 StackExchange site proposals:<br>
<a href=""http://area51.stackexchange.com/proposals/2149/cognitive-science"">Cognitive Science</a><br>
<a href=""http://area51.stackexchange.com/proposals/29987/robotics-research"">Robotics Research</a><br>
<a href=""http://area51.stackexchange.com/proposals/26434/machine-learning"">Machine Learning</a>  </p>
",545616.0,2.0,1.0,6604513.0,"Probably I'm not the best person to give this kind of advice (that's why I leave just a comment), but I'd go for the control class before. You should know what robots can/can't do and how do they work before trying to control them"
1995,24158126,Factors to find weight painting formula for Auto-Rigging,|algorithm|opengl|animation|artificial-intelligence|robotics|,"<p>I'm trying to re-implement Auto-rigging for human skeletons. (similar to Blender and Mixamo's)</p>

<p>For each vertex in the human skin, I've to find the joints that affect this vertex. (I could do this.)</p>

<p>Now I've to find how much each joint should affect this vertex. (assigning weights for each vertex)</p>

<p>The human skin can be represented by an array of traingles each containing 3 vertices and the joints can be represented by array of vertices.</p>

<p>Note that each vertex can be affected by n number of joints(n>=1) which means no vertex should remain un-weighted.</p>

<p>I can manage to construct a connected graph of the skin. I don't know how to assign weights for each vertex from this graph. Help/Suggestions?</p>
",6/11/2014 8:29,,397,1,4,3,0.0,,,,,24352306.0,"<p>This may explain your case. The algorithm is too big. For me it explains the things well. Have a look </p>

<p><a href=""http://people.csail.mit.edu/ibaran/papers/2007-SIGGRAPH-Pinocchio.pdf"" rel=""nofollow"">http://people.csail.mit.edu/ibaran/papers/2007-SIGGRAPH-Pinocchio.pdf</a></p>
",3219193.0,0.0,1.0,37283598.0,"I still think it's too big. Just my opinion, though, perhaps someone else will find the time and will to answer you."
1015,5287575,"Modelling a robotic arm motion in 3D, ideas?",|python|modeling|robotics|inverse-kinematics|,"<p>I hope you don't mind if I ask for a bit of advice regarding modelling robotic systems. I've recently become rather interested in using inverse kinematics (IK) to control a 5 dof robotic manipulator. I have a solid foundation in IK but what I'm having trouble with is a way to visualize how the manipulator moves with respect to joint angles.</p>
<p>I've looked into using 3D toolkits (such as Blender, Panda3D, vPython) to create a 3d model of the arm, but I'm not sure if I should be looking something with physics support. I'm also not sure how well I can model motion with these packages. Anyone have any suggestions? What I'm NOT looking for is a full blown robotic simulator like Microsoft's Robotic Studio, I'd like to start with the basics and learn how everything works first, ie code the IK in Python, then visualize the motion in 3D. I'm very familiar with Python, so something that interfaces with Python would be preferable.</p>
",3/13/2011 5:06,5287605.0,3335,3,0,2,0.0,,,,,5287605.0,"<p>Well the great thing about Blender is that its API is actually in python!</p>
<p>In addition, it supports inverse kinematics (IK) quite well in addition to many other modeling tools.</p>
<p><a href=""http://www.blendercookie.com/"" rel=""nofollow noreferrer"">Blender Cookie</a> is a great resource.</p>
<p><a href=""http://www.blendercookie.com/2011/02/07/creating_fk_ik_rig/"" rel=""nofollow noreferrer"">Here is a tutorial on making IK rigs in Blender.</a></p>
<p>Blenders python api is documented quite extensively, and it even has an interactive python shell built right in so that you can see the effects of your script as you go along.</p>
<p>The physics engine that blender uses is the popular bullet physics engine, which has been used in many commercial games as well as a few feature films (2012 among them).</p>
",272231.0,2.0,1.0,,
4751,77928392,How to Visualize the Safety_Lidar/Depth camera data on RViz from Docker Container,|ros|docker-swarm|robotics|docker-container|,"<p>I am currently collaborating with a mobile robot, as depicted in Image 1.</p>
<p><a href=""https://i.stack.imgur.com/YlRKW.jpg"" rel=""nofollow noreferrer"">Image 1</a></p>
<p>The robot is equipped with two sick_safety scanners, positioned at the front and rear. The data nodes/topics are publishing from a Docker container. To access the robot's PC, I utilize SSH by entering 'ssh username@ip.' Once inside the container, I can see the topic, and echoing the topic displays the raw text data.</p>
<p>I attempted two different approaches to visualize the data on rviz:</p>
<p>Approach 1:
I opened rviz from within the container, enabling me to view the rostopic list. However, upon attempting to open rviz, an error message appeared: 'qt.qpa.xcb: could not connect to display :0' (refer to Image 2).</p>
<p><a href=""https://i.stack.imgur.com/8oKpd.jpg"" rel=""nofollow noreferrer"">Image 2</a></p>
<p>Approach 2:
I configured the IP addresses of both the robot PC and my PC. The robot was connected to a wifi-router via LAN, while my PC was connected via Wi-Fi on the same network (refer to Image 3).
<a href=""https://i.stack.imgur.com/jMgss.jpg"" rel=""nofollow noreferrer"">Image 3</a></p>
<p>Although I could see all the topics on my PC, running rviz did not display any data; there was no TF data, and the Fixed Frame option was not available (refer to Image 4).
<a href=""https://i.stack.imgur.com/Vo4Wi.jpg"" rel=""nofollow noreferrer"">Image 4</a></p>
<p>Looking ahead, if I plan to integrate a RealSense camera with the robot, how can I visualize the camera data on rviz, whether on the robot's pc (GUI Tool) or my PC's?</p>
<p>Thank you.</p>
",2/2/2024 16:10,,28,0,0,0,,23337932.0,,2/2/2024 15:54,0.0,,,,,,,
1011,5207515,Robot camera + motion detection,|camera|detection|robotics|motion|,"<p>I have a project in which we (me and my student) will develop a system for robot.
In this robot we have a camera that capture.</p>
<p>My question is how to detect motions and movements?
Is there a solution? Which technics and tools to use?
Which language to use (possible for Java for example)?</p>
",3/5/2011 23:30,5207561.0,1163,4,1,0,0.0,604156.0,"Paris, France",2/5/2011 6:51,1331.0,5207561.0,"<p>Consider using OpenCV:</p>

<p><a href=""http://opencv.org"" rel=""nofollow"">http://opencv.org</a></p>

<p>It has a lot of useful vision algorithms built in, and supports, C, C++ and Python, as well as GPU functionality.</p>
",213489.0,3.0,0.0,19870840.0,"[Moving camera_motion_detection][1] thread may be useful as it deals with the similar problem.


  [1]: http://stackoverflow.com/questions/12986265/detecting-motion-on-opencv-c-moving-camera"
3586,58808291,Printing serial data from IRobot Create 2,|c++|byte|robotics|,"<p>I'm working on an interface for the irobot create 2 and am having trouble reading a single incoming data packet. Where can I find more detailed information about doing this via the use of termios, read(), write(), and printf()? I'm fairly new to this kind of programming, aside from some robotics projects during college, and am probably missing some key points. Please spare my ignorance.</p>

<p>So far I've successfully confirmed sending of commands which initialize the robot, start it in various modes, start/stop the IO, and turn the robot off. To read the data, which comes in as a single byte, I've sent the commands to locate the designated sensor packet via the write function and this is where I get confused. I've allocated a vector for a single byte and am trying to read() the returned value and then printf() it. I'm not sure which data types to use when printing the value or if I'm even getting what I want. </p>

<pre><code>*yungBot.cpp*
#include ""yungBot.h""
#include &lt;iostream&gt;


/*initialize robot*/
int yungBot::init(const char *yungin){

    fd = open(yungin, O_RDWR | O_NOCTTY | O_NDELAY);
    if(fd == -1){ 
        perror(""Error, failed to connect"");
    }
    else{
        fcntl(fd,F_SETFL,0);
        tcflush (fd, TCIFLUSH);
    }
    struct termios parameters;

    int get = tcgetattr(fd, &amp;parameters);
    if(get == -1){ 
        perror(""Error getting attributes"");
    }
    else{ 
        printf(""%s\n"", ""Get attributes: success"");
    }
    cfmakeraw (&amp;parameters); 
    //sets input and output baud rate
    cfsetispeed(&amp;parameters,B115200);
    cfsetospeed(&amp;parameters,B115200);

    // or forces values to 1; and forces all values to 0 (off); 
    parameters.c_iflag &amp;= ~(IXON | IXOFF); //flow control off; 
    parameters.c_cflag |=(CLOCAL | CREAD);
    parameters.c_cflag &amp;= ~(PARENB | CSTOPB);//no parity 
    parameters.c_cflag &amp;= ~CSIZE; //mask the character bits
    parameters.c_cflag |= (CS8); //8 bit character size 
    parameters.c_oflag = 0;
    parameters.c_lflag = 0;//ICANON=canonical mode
    parameters.c_cc[VMIN] = 0; // 1 input byte is enough to return from read()
    parameters.c_cc[VTIME] = 1;// Timer off 

    //set attribute immediately
    int set = tcsetattr(fd, TCSANOW, &amp;parameters); 
    if(set == -1){
        perror(""Error setting attributes \n"");
    }
    if (fd == -1){
        perror(yungin);
        return -1;
    }   
    usleep(200000);         
    return fd;
}


/*stream desired data packets of 1 unsigned byte*/
int yungBot::stream(int packet){
    unsigned char command[]={142,packet};
    if(write(fd, command, sizeof(command))==-1){
        perror(""failed to retrieve data packet"");
        return -1;
    }
    unsigned char response[1];
    if(read(fd,response,sizeof(response))!=1){
        perror(""failed to write data packet"");
        return -1;
    }
    //shift through the byte for individual bits
    /*unsigned char bit = response[1]; 
    for(int i = 0; i &lt; CHAR_BIT; i++){
    printf(""%d"", (bit&gt;&gt;i)&amp;1);
    }
    */
    printf(""%i"",response[0]);
return response[0];
}
</code></pre>

<p>I'm honestly not sure what to expect. If I read something such as the charge state it seems to work returning a value of 0 when not charged and currently 3 when charged bc the battery is full. When returning something such as the bump and wheel drops sensors (which are sent as a single byte and have a range of 0-15, i'm not sure what to do. I get different values within that range when pressing different parts of the bumper etc..but 4 bits of the byte correspond to 4 different values and the other 4 are ""reserved"". How do I go about reading something like this? I've noticed for the wheel drop/bump sensors that i get certain values depending where I press/lift the robot, for example:</p>

<pre><code>right bump=1
left bump=2
middle bump=3

right drop=4
left drop =8
both dropped = 12
</code></pre>

<p>Is this all i need?</p>
",11/11/2019 20:30,58808429.0,78,1,0,0,0.0,12293457.0,,10/29/2019 19:31,23.0,58808429.0,"<p>Oh, this is pretty simple.  You can read those bits with something like...</p>

<pre><code>#define LEFT_BUMPER 1
#define RIGHT_BUMPER 2
#define REAR_BUMPER 4
#define FRONT_BUMPER 8

if(bumper_state &amp; LEFT_BUMPER){ printf(""left bumper actuated\n"") }
if(bumper_state &amp; RIGHT_BUMPER){ printf(""right bumper actuated\n"") }
</code></pre>

<p>The best data type to use us usually an unsigned type of the same size as the thing you're reading, in this case a uint8_t would be good.</p>

<p>Also, you don't need an array, you could read into a single byte if you wanted.  It'll probably help avoid confusion later.</p>
",405312.0,0.0,6.0,,
3319,55038180,How to use the uArm Pro Python library,|python-2.7|robotics|,"<p>I am just getting started with uArm Pro and need to write a test controller in Python. I came across the python library for uArm on Github. How can I use it to my advantage?</p>
",3/7/2019 7:24,,985,1,0,-1,,10133412.0,"Haldia, West Bengal, India",7/25/2018 12:25,12.0,55038414.0,"<p>uArm provides baisc Movement on Python. The library only supports uArm Swift/SwiftPro. For Metal, please use pyuarm or pyuf instead.</p>

<p>pyuarm can be installed from PyPI, either manually downloading the files and installing as described below or using:</p>

<pre><code>pip install pyuarm
</code></pre>

<p>Download the archive from <a href=""https://github.com/uArm-Developer/uArm-Python-SDK"" rel=""nofollow noreferrer"">https://github.com/uArm-Developer/uArm-Python-SDK</a> . Unpack the archive, enter the uArm-Python-SDK directory and run:</p>

<pre><code>python setup.py install
</code></pre>

<p>Try examples using this link <a href=""https://github.com/uArm-Developer/uArm-Python-SDK/tree/2.0/examples/api"" rel=""nofollow noreferrer"">https://github.com/uArm-Developer/uArm-Python-SDK/tree/2.0/examples/api</a></p>
",7484853.0,1.0,0.0,,
4630,76714440,An error in building code in stm32 cube ide (make: error),|robotics|stm32f4discovery|stm32f4|stm32cubeide|,"<p>I'm trying to build code in stm32f401ccu6 blackpill but I keep getting this error :</p>
<blockquote>
<p>make: ***[Drivers/STM32F4xx_HAL_Driver/Src/subdir.mk:61: Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_cortex.o] Error 1</p>
</blockquote>
<p>This is the log from console tab.</p>
<blockquote>
<p>15:51:00 **** Incremental Build of configuration Debug for project WHY ****
make -j4 all
arm-none-eabi-gcc &quot;../Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal.c&quot; -mcpu=cortex-m4 -std=gnu11 -g3 -DDEBUG -DUSE_HAL_DRIVER -DSTM32F401xC -c -I../Core/Inc -I../Drivers/STM32F4xx_HAL_Driver/Inc -I../Drivers/STM32F4xx_HAL_Driver/Src -I../Drivers/STM32F4xx_HAL_Driver/Inc/Legacy -I../Drivers/CMSIS/Device/ST/STM32F4xx/Include -I../Drivers/CMSIS/Include -O0 -ffunction-sections -fdata-sections -Wall -fstack-usage -MMD -MP -MF&quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal.d&quot; -MT&quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal.o&quot; --specs=nano.specs -mfpu=fpv4-sp-d16 -mfloat-abi=hard -mthumb -o &quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal.o&quot;
arm-none-eabi-gcc &quot;../Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_cortex.c&quot; -mcpu=cortex-m4 -std=gnu11 -g3 -DDEBUG -DUSE_HAL_DRIVER -DSTM32F401xC -c -I../Core/Inc -I../Drivers/STM32F4xx_HAL_Driver/Inc -I../Drivers/STM32F4xx_HAL_Driver/Src -I../Drivers/STM32F4xx_HAL_Driver/Inc/Legacy -I../Drivers/CMSIS/Device/ST/STM32F4xx/Include -I../Drivers/CMSIS/Include -O0 -ffunction-sections -fdata-sections -Wall -fstack-usage -MMD -MP -MF&quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_cortex.d&quot; -MT&quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_cortex.o&quot; --specs=nano.specs -mfpu=fpv4-sp-d16 -mfloat-abi=hard -mthumb -o &quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_cortex.o&quot;
arm-none-eabi-gcc &quot;../Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_dma.c&quot; -mcpu=cortex-m4 -std=gnu11 -g3 -DDEBUG -DUSE_HAL_DRIVER -DSTM32F401xC -c -I../Core/Inc -I../Drivers/STM32F4xx_HAL_Driver/Inc -I../Drivers/STM32F4xx_HAL_Driver/Src -I../Drivers/STM32F4xx_HAL_Driver/Inc/Legacy -I../Drivers/CMSIS/Device/ST/STM32F4xx/Include -I../Drivers/CMSIS/Include -O0 -ffunction-sections -fdata-sections -Wall -fstack-usage -MMD -MP -MF&quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_dma.d&quot; -MT&quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_dma.o&quot; --specs=nano.specs -mfpu=fpv4-sp-d16 -mfloat-abi=hard -mthumb -o &quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_dma.o&quot;
arm-none-eabi-gcc &quot;../Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_dma_ex.c&quot; -mcpu=cortex-m4 -std=gnu11 -g3 -DDEBUG -DUSE_HAL_DRIVER -DSTM32F401xC -c -I../Core/Inc -I../Drivers/STM32F4xx_HAL_Driver/Inc -I../Drivers/STM32F4xx_HAL_Driver/Src -I../Drivers/STM32F4xx_HAL_Driver/Inc/Legacy -I../Drivers/CMSIS/Device/ST/STM32F4xx/Include -I../Drivers/CMSIS/Include -O0 -ffunction-sections -fdata-sections -Wall -fstack-usage -MMD -MP -MF&quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_dma_ex.d&quot; -MT&quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_dma_ex.o&quot; --specs=nano.specs -mfpu=fpv4-sp-d16 -mfloat-abi=hard -mthumb -o &quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_dma_ex.o&quot;
arm-none-eabi-gcc: fatal error: cannot execute 'd:/robocon/stm32cubeide_1.10.1/stm32cubeide/plugins/com.st.stm32cube.ide.mcu.externaltools.gnu-tools-for-stm32.10.3-2021.10.win32_1.0.0.202111181127/tools/bin/../lib/gcc/arm-none-eabi/10.3.1/../../../../arm-none-eabi/bin/as.exe': CreateProcess: No such file or directory
compilation terminated.
arm-none-eabi-gcc: fatal error: cannot execute 'd:/robocon/stm32cubeide_1.10.1/stm32cubeide/plugins/com.st.stm32cube.ide.mcu.externaltools.gnu-tools-for-stm32.10.3-2021.10.win32_1.0.0.202111181127/tools/bin/../lib/gcc/arm-none-eabi/10.3.1/../../../../arm-none-eabi/bin/as.exe': CreateProcess: No such file or directory
compilation terminated.
arm-none-eabi-gcc: fatal error: cannot execute 'd:/robocon/stm32cubeide_1.10.1/stm32cubeide/plugins/com.st.stm32cube.ide.mcu.externaltools.gnu-tools-for-stm32.10.3-2021.10.win32_1.0.0.202111181127/tools/bin/../lib/gcc/arm-none-eabi/10.3.1/../../../../arm-none-eabi/bin/as.exe': CreateProcess: No such file or directory
compilation terminated.
arm-none-eabi-gcc: fatal error: cannot execute 'd:/robocon/stm32cubeide_1.10.1/stm32cubeide/plugins/com.st.stm32cube.ide.mcu.externaltools.gnu-tools-for-stm32.10.3-2021.10.win32_1.0.0.202111181127/tools/bin/../lib/gcc/arm-none-eabi/10.3.1/../../../../arm-none-eabi/bin/as.exe': CreateProcess: No such file or directory
compilation terminated.
make: *** [Drivers/STM32F4xx_HAL_Driver/Src/subdir.mk:61: Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal.o] Error 1
make: *** Waiting for unfinished jobs....
make: *** [Drivers/STM32F4xx_HAL_Driver/Src/subdir.mk:61: Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_cortex.o] Error 1
make: *** [Drivers/STM32F4xx_HAL_Driver/Src/subdir.mk:61: Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_dma_ex.o] Error 1
make: *** [Drivers/STM32F4xx_HAL_Driver/Src/subdir.mk:61: Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_dma.o] Error 1
&quot;make -j4 all&quot; terminated with exit code 2. Build might be incomplete.
15:51:00 Build Failed. 5 errors, 0 warnings. (took 718ms)</p>
</blockquote>
<p>What does that error code mean and is there any applicable solution that I can use?</p>
<p>I expected my code to built, but it turns out the error keeps coming and I don't know what to do.</p>
",7/18/2023 15:34,,440,0,0,0,,20469939.0,,11/10/2022 15:19,4.0,,,,,,,
2901,46455274,RobotC Sonar Sensor Array,|arrays|while-loop|rotation|robot|robotc|,"<p>I am using a sonar sensor to create an obstacle avoiding robot. The robot needs to be able to check 180 degrees in front of it, so I have made a ""head"" the sensor is mounted on that is attached to a motor with an axle that runs through a potentiometer. I have found the potentiometer values for the 5 sets of 45 degree intervals with the total 180 degrees required and documented. The sonar sensor must be able to scan a distance and then move 45 degrees, repeating the process until it reaches 180 degrees (to the right) only once it reaches that point of rotation, the scan distances are put into an array to be used by an avoidance task to be developed at a later time.
However, the sonar sensor stores values for certain angles before the head has actually reached that specified angle. 
<a href=""https://i.stack.imgur.com/CjFeQ.jpg"" rel=""nofollow noreferrer"">Obstacle Avoidance Robot (Head rotation system in the middle)</a></p>

<pre><code>  #pragma config(Sensor, in1,    headrot,        sensorPotentiometer)
#pragma config(Sensor, dgtl1,  fsonar,         sensorSONAR_inch)
#pragma config(Sensor, dgtl3,  bsonar,         sensorSONAR_inch)
#pragma config(Sensor, dgtl5,  steerrot,       sensorQuadEncoder)
#pragma config(Sensor, dgtl7,  wheelrot,       sensorQuadEncoder)
#pragma config(Motor,  port2,           head,          tmotorVex393_MC29, openLoop)
#pragma config(Motor,  port4,           drivem,        tmotorVex393_MC29, openLoop)
#pragma config(Motor,  port5,           steer,         tmotorVex393_MC29, openLoop)

int headrotationspeed = 25;
int frontscandistance[5]; //Array that hold each of the 5 angles at 45 degree intervals
int headangle =0;
task avoidance()
{
}
task frontscanner()
{
    //0 Degrees = 3500
    //45 Degrees = 2800
    //90 Degrees = 1900
    //135 Degrees = 1200
    //180 Degrees = 530
    while(true)
    {
        switch(headangle)
        {
        case 0:
            while(SensorValue[headrot]&lt;3500 &amp;&amp; SensorValue[headrot]&gt;3400)
            {
                motor[head]=-headrotationspeed;
            }
                motor[head]=0;
            frontscandistance[0] = SensorValue[fsonar];
            headangle+=45;
            break;

        case 45:
            while(SensorValue[headrot]&lt;2800 &amp;&amp; SensorValue[headrot]&lt;2700)
            {
                motor[head]=headrotationspeed;
            }
                motor[head]=0;
            frontscandistance[1] = SensorValue[fsonar];
            headangle+=45;
            break;

        case 90:
            while(SensorValue[headrot]&lt;1900 &amp;&amp; SensorValue[headrot]&lt;1800)
            {
                motor[head]=headrotationspeed;
            }
                motor[head]=0;
            frontscandistance[2] = SensorValue[fsonar];
            headangle+=45;
            break;

            case 135:
            while(SensorValue[headrot]&lt;1200 &amp;&amp; SensorValue[headrot]&lt;1100)
            {
                motor[head]=headrotationspeed;
            }
                motor[head]=0;
            frontscandistance[3] = SensorValue[fsonar];
            headangle+=45;
            break;
            case 180:
            while(SensorValue[headrot]&lt;550 &amp;&amp; SensorValue[headrot]&lt;440)
            {
                motor[head]=headrotationspeed;
            }
            motor[head]=0;
            frontscandistance[4] = SensorValue[fsonar];
            headangle=0;
            break;
        }
    }
}

    task main()
    {

        startTask(frontscanner);
        }
</code></pre>

<p>The sonar does not scan once the head has hit each 45 degree interval respectively even though the programming seems correct. What is causing the array to store values before the while loops finish positioning the head to the proper angle?</p>
",9/27/2017 18:54,50846432.0,335,1,0,0,,7099908.0,,11/1/2016 13:44,5.0,50846432.0,"<p>To start with, your indentation is off. That isn't good practice, as it can lead to placing blocks of code inside loops unintentionally.</p>

<p>To answer your question, after the first case is executed, all the other while loops are checking if the value is smaller than both your upper AND lower limits. The first case doesn't do this. </p>

<p>The following is your code, but with the changes mentioned above implemented.</p>

<pre><code>#pragma config(Sensor, in1,    headrot,        sensorPotentiometer)
#pragma config(Sensor, dgtl1,  fsonar,         sensorSONAR_inch)
#pragma config(Sensor, dgtl3,  bsonar,         sensorSONAR_inch)
#pragma config(Sensor, dgtl5,  steerrot,       sensorQuadEncoder)
#pragma config(Sensor, dgtl7,  wheelrot,       sensorQuadEncoder)
#pragma config(Motor,  port2,           head,          tmotorVex393_MC29, openLoop)
#pragma config(Motor,  port4,           drivem,        tmotorVex393_MC29, openLoop)
#pragma config(Motor,  port5,           steer,         tmotorVex393_MC29, openLoop)

int headrotationspeed = 25;
int frontscandistance[5]; //Array that hold each of the 5 angles at 45 degree intervals
int headangle =0;
task avoidance()
{
}

task frontscanner()
{
    //0 Degrees = 3500
    //45 Degrees = 2800
    //90 Degrees = 1900
    //135 Degrees = 1200
    //180 Degrees = 530
    while(true)
    {
        switch(headangle)
        {
        case 0:
            while(SensorValue[headrot]&gt;3500 || SensorValue[headrot]&lt;3400)
            {
                motor[head]=-headrotationspeed;
            }
            motor[head]=0;
            frontscandistance[0] = SensorValue[fsonar];
            headangle+=45;
            break;

        case 45:
            while(SensorValue[headrot]&gt;2800 || SensorValue[headrot]&lt;2700)
            {
                motor[head]=headrotationspeed;
            }
            motor[head]=0;
        frontscandistance[1] = SensorValue[fsonar];
        headangle+=45;
        break;

        case 90:
            while(SensorValue[headrot]&gt;1900 || SensorValue[headrot]&lt;1800)
            {
                motor[head]=headrotationspeed;
            }
            motor[head]=0;
            frontscandistance[2] = SensorValue[fsonar];
            headangle+=45;
            break;

        case 135:
            while(SensorValue[headrot]&gt;1200 || SensorValue[headrot]&lt;1100)
            {
                motor[head]=headrotationspeed;
            }
            motor[head]=0;
            frontscandistance[3] = SensorValue[fsonar];
            headangle+=45;
            break;

        case 180:
            while(SensorValue[headrot]&gt;550 || SensorValue[headrot]&lt;440)
            {
                motor[head]=headrotationspeed;
            }
            motor[head]=0;
            frontscandistance[4] = SensorValue[fsonar];
            headangle=0;
            break;
        }
    }
}

task main()
{
    startTask(frontscanner);
}
</code></pre>

<p>Also, I would like to point out that your code is written to turn the robot's head only when the measurement is IN the desired range. If any of the cases execute while the head is outside the desired range, the head won't move. If I understand correctly, you want the opposite. </p>

<p>The code below contains the modification necessary to not only turn the head while it is OUT of the desired range, but also automatically turns the head in the desired direction. </p>

<pre><code>#pragma config(Sensor, in1,    headrot,        sensorPotentiometer)
#pragma config(Sensor, dgtl1,  fsonar,         sensorSONAR_inch)
#pragma config(Sensor, dgtl3,  bsonar,         sensorSONAR_inch)
#pragma config(Sensor, dgtl5,  steerrot,       sensorQuadEncoder)
#pragma config(Sensor, dgtl7,  wheelrot,       sensorQuadEncoder)
#pragma config(Motor,  port2,           head,          tmotorVex393_MC29, openLoop)
#pragma config(Motor,  port4,           drivem,        tmotorVex393_MC29, openLoop)
#pragma config(Motor,  port5,           steer,         tmotorVex393_MC29, openLoop)

int headrotationspeed = 25;
int frontscandistance[5]; //Array that hold each of the 5 angles at 45 degree intervals
int headangle =0;
int reverse = 1 //Set this to -1 if the motor is turning in the wrong direction.

task avoidance()
{
}

task frontscanner()
{
    //0 Degrees = 3500
    //45 Degrees = 2800
    //90 Degrees = 1900
    //135 Degrees = 1200
    //180 Degrees = 530
    while(true)
    {
        switch(headangle)
        {
        case 0:
            while(SensorValue[headrot]&lt;3500 &amp;&amp; SensorValue[headrot]&gt;3400)
            {
                motor[head]=headrotationspeed*reverse*(3500-SensorValue[headrot]/abs(3500-SensorValue[headrot]));
            }
            motor[head]=0;
            frontscandistance[0] = SensorValue[fsonar];
            headangle+=45;
            break;

        case 45:
            while(SensorValue[headrot]&lt;2800 &amp;&amp; SensorValue[headrot]&gt;2700)
            {
                motor[head]=headrotationspeed*reverse*(2800-SensorValue[headrot]/abs(3500-SensorValue[headrot]));
            }
            motor[head]=0;
        frontscandistance[1] = SensorValue[fsonar];
        headangle+=45;
        break;

        case 90:
            while(SensorValue[headrot]&lt;1900 &amp;&amp; SensorValue[headrot]&gt;1800)
            {
                motor[head]=headrotationspeed*reverse*(1900-SensorValue[headrot]/abs(3500-SensorValue[headrot]));
            }
            motor[head]=0;
            frontscandistance[2] = SensorValue[fsonar];
            headangle+=45;
            break;

        case 135:
            while(SensorValue[headrot]&lt;1200 &amp;&amp; SensorValue[headrot]&gt;1100)
            {
                motor[head]=headrotationspeed*reverse*(1200-SensorValue[headrot]/abs(3500-SensorValue[headrot]));
            }
            motor[head]=0;
            frontscandistance[3] = SensorValue[fsonar];
            headangle+=45;
            break;

        case 180:
            while(SensorValue[headrot]&lt;550 &amp;&amp; SensorValue[headrot]&gt;440)
            {
                motor[head]=headrotationspeed*reverse*(550-SensorValue[headrot]/abs(3500-SensorValue[headrot]));
            }
            motor[head]=0;
            frontscandistance[4] = SensorValue[fsonar];
            headangle=0;
            break;
        }
    }
}

task main()
{
    startTask(frontscanner);
}
</code></pre>

<p>Also, I would highly recommend the VEX forums (found here: <a href=""https://www.vexforum.com/"" rel=""nofollow noreferrer"">https://www.vexforum.com/</a>) for future questions involving vex. It is more specialized towards your needs. My username is Tark1492. Feel free to message me directly if you ever get stuck on a specific question about RobotC.</p>
",9937692.0,0.0,0.0,,
266,1050911,Is it possible to use anonymous functions in C++ .NET?,|.net|c++|function|anonymous|robotics|,"<p><a href=""http://en.wikipedia.org/wiki/Anonymous_function"" rel=""nofollow noreferrer"">Wikipedia</a> seems to say that C++0x will support anonymous functions. Boost also seem to support it. However I'm using .NET so if I could stick with it it would be awesome.</p>

<p>Basically I just want to write some quick code for objects. I have a robot which can have about 85 - 90 states. Most of the states are just ""integer values passed to the robot microcontroller"". So I tell the robot to go to state 35 for example.</p>

<p>However some states require additionnal manipulations such as user input so I'd like to keep it simple and write just a few lines of code for the differences. I've considered using derived classes but it involves a lot of code just to modify a few lines.</p>
",6/26/2009 19:48,,959,3,0,1,0.0,6367.0,"Montreal, Canada",9/14/2008 21:53,1610.0,1065052.0,"<p>Anonymous functions, alternatively called <a href=""http://msdn.microsoft.com/en-us/library/bb397687.aspx"" rel=""nofollow noreferrer"">Lambda Expressions</a> or Delegates, are a language feature of C# and not part of the .NET framework. I don't think Microsoft has added anonymous functions to managed-C++, and I've found some <a href=""http://www.codeproject.com/KB/interviews/whidbey_cpp.aspx?fid=28262&amp;df=90&amp;mpp=25&amp;noise=3&amp;sort=Position&amp;view=Quick&amp;select=1215662#xx1215662xx"" rel=""nofollow noreferrer"">comments</a> which seem to agree with me.</p>

<p>Not to worry, though. As you mentioned, <a href=""http://www.boost.org/doc/libs/1_39_0/doc/html/lambda.html"" rel=""nofollow noreferrer"">Boost.Lambda</a> is a nifty library that you can use. What is nice is that it is implemented as templates completely in headers. So, all you have to do is include the headers. Any standards-conforming C++ compiler should support it. I understand your desire to stick with what you already have, but the effort it takes to download and use these headers should really be minimal.</p>

<p>If you really don't want to use Boost, then you can try using C#, but I recommend that you just try the Boost Lambda library. It is probably easier than you think.</p>
",35881.0,-1.0,2.0,,
2554,38647114,ORB_SLAM installation on Ubuntu Xenial 16.04,|opencv|ubuntu|ros|robotics|slam|,"<p>Is it possible to install ORB_SLAM/ORB_SLAM2 on last version of Ubuntu (Xenial 16.04) without black magic? I know that the recommendation is to use Ubuntu 14.04 according to <a href=""https://github.com/raulmur/ORB_SLAM2"" rel=""nofollow"">https://github.com/raulmur/ORB_SLAM2</a>, but I currently have last version and I don't really want to change it or install 14 together with 16. I use OpenCV 2.4.8 and ROS/catkin build system and get the next error:</p>

<pre><code>/home/roman/ORB_SLAM2/src/Optimizer.cc:1244:1:   required from here
/usr/include/eigen3/Eigen/src/Core/util/StaticAssert.h:32:40: error: static assertion failed: YOU_MIXED_DIFFERENT_NUMERIC_TYPES__YOU_NEED_TO_USE_THE_CAST_METHOD_OF_MATRIXBASE_TO_CAST_NUMERIC_TYPES_EXPLICITLY
</code></pre>

<p>What's wrong with it? Thanks.</p>
",7/28/2016 21:34,,3011,2,0,2,,6117436.0,,3/26/2016 12:03,20.0,39684187.0,"<p>I had this same issue, this is what worked for me.</p>

<p>Install <strong><code>eigen</code></strong> form here <a href=""https://launchpad.net/ubuntu/trusty/amd64/libeigen3-dev/3.2.0-8"" rel=""nofollow"">https://launchpad.net/ubuntu/trusty/amd64/libeigen3-dev/3.2.0-8</a></p>

<p>Download the <code>.deb</code> file and install using </p>

<pre><code>sudo dpkg -i libeigen3-dev_3.2.0-8_all.deb
</code></pre>
",6876969.0,2.0,0.0,,
2405,33680363,Map representation for localization,|java|dictionary|localization|robotics|data-representation|,"<p>I would like to write in Java localization system for a robot. However I am stuck at the very beginning. I don't know how to represent the map. The map is not complicated and will never be bigger than few meters by few meters. It doesn't change when robot is moving. </p>

<p>The readings that I will have from sensors are angle (provided by compass) and pairs of integers (angle and distance). </p>
",11/12/2015 20:05,33689342.0,169,1,3,0,,5522101.0,,11/3/2015 21:42,2.0,33689342.0,"<p><a href=""http://probabilistic-robotics.org/"" rel=""nofollow"">Probabilistic Robotics</a> by Thrun, Burgard and Fox covers a number of different techniques for modelling maps suitable for robotics applications. These include:</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Occupancy_grid_mapping"" rel=""nofollow"">Occupancy Grids</a>. Occupancy grids are conceptually similar to an image - black pixels are obstacles, white pixels are passable. </li>
<li>Feature Based Maps. Feature based maps estimate position of each obstacle in a list.</li>
</ul>

<p>The suitability of each approach depends on how sparse / cluttered the environment is and what types of sensors are available for updating the map.</p>
",2246.0,1.0,0.0,55133802.0,What is in the map? Is it discrete or vector based? And why do you want to represent a map? Isn't what you have a list of readings instead of a map?
1594,14219623,frisbee trajectory,|c++|robotics|,"<p>This is my first post.  I'm the lead programmer on a FIRST robotics team, and this year's competition is about throwing Frisbees.  I was wondering if there was some sort of ""grand unified equation"" for the trajectory that takes into account the air resistance, initial velocity, initial height, initial angle, etc.  Basically, I would like to acquire data from an ultrasonic rangefinder, the encoders that determine the speed of our motors, the angle of our launcher, the rotational force (should be pretty constant.  We'll determine this on our own) and the gravitational constant, and plug it into an equation in real time as we're lining up shots to verify/guesstimate whether or not we'll be close.  If anyone has ever heard of such a thing, or knows where to find it, I would really appreciate it!  (FYI, I have already done some research, and all I can find are a bunch of small equations for each aspect, such as rotation and whatnot.  It'll ultimately be programmed in C++). Thanks!</p>
",1/8/2013 16:30,14219916.0,607,1,5,1,0.0,1958743.0,,1/8/2013 16:17,11.0,14219916.0,"<p>I'm a mechanical engineer who writes software for a living. Before moving to tech startups, I worked for Lockheed Martin writing simulation software for rockets. I've got some chops in this area.</p>

<p>My professional instinct is that there is no such thing as a ""grand unified equation"". In fact, this is a hard enough problem that there might not be very good theoretical models for this even if they are correct: for instance, one of your equations will have to be the lift generated from the frisbee, which will depend on its cross-section, speed, angle of attack, and assumptions about the properties of the air. Unless you're going to put your frisbee in a wind tunnel, this equation will be, at best, an approximation.</p>

<p>It gets worse in the real world: will you be launching the frisbee where there is wind? Then you can kiss your models goodbye, because as casual frisbee players know, the wind is a huge disturbance. Your models can be fine, but the real world can be brutal to them.</p>

<p>The way this complexity is handled in the real world is that almost all systems have feedback: a pilot can correct for the wind, or a rocket's computer removes disturbances from differences in air density. Unless you put a microcontroller with control surfaces on the frisbee, you're just not going to get far with your open loop prediction - which I'm sure is the trap they're setting for you by making it a frisbee competition.</p>

<p>There is a solid engineering way to approach the problem. Give Newton a boot and do they physics equations yourselves.</p>

<p>This is the empirical modeling process: launch a frisbee across a matrix of pitch and roll angles, launch speeds, frisbee spin speeds, etc... and backfit a model to your results. This can be as easy as linear interpolation of the results of your table, so that any combination of input variables can generate a prediction.</p>

<p>It's not guess and check, because you populate your tables ahead of time, so can make some sort of prediction about the results. You will get much better information faster than trying the idealized models, though you will have to keep going to fetch your frisbee :)</p>
",53139.0,6.0,0.0,19853022.0,"We'll be competing in an enclosed stadium, so unless the cheering fans somehow knock our Frisbee off path, almost everything is constant.  I guess we'll have to make data tables  :/  The only actual variable we will be manipulating is the angle of the launcher.  Because of this, the Frisbee should only be angled in one axis.  Our launcher is bizarrely consistent, so I guess making data tables won't be terrible, just really time consuming.  Hopefully we'll be able to generate a parabolic equation that can get us to within a few feet of our target.  Thanks!"
3910,64563152,Issue with Google's edge TPU compiler,|artificial-intelligence|robotics|tpu|google-coral|,"<p>I tried to install edge TPU compiler on my raspberrypi 3b+ but apparently it is no longer supported on 32bit. Can I install compiler on a 64bit machine, compile my code, turn it into a code readable by the edge tpu and bring this code back and run it on my raspberry pi? Or will there be conflict of some sort?</p>
",10/27/2020 21:28,,115,1,2,1,,14531758.0,,10/27/2020 21:23,2.0,64579829.0,"<ul>
<li><p>As far as application goes:
You can write your application code on the rpi. The code that you compiles on an x86-64 machine won't be executable on the rpi due to architecture differences. Unless you are using a cross compiler.</p>
</li>
<li><p>Now on the edgetpu compiler:
The edgetpu compiler can only be installed on the <code>x86_64</code> machine, although since the format of the model is the same for all platform (flat buffer), you can use the same model compiled on the <code>x86_64</code> platform on your rpi just fine!</p>
</li>
</ul>
",6262757.0,0.0,0.0,114167270.0,"Assuming that you have connected a TPU device(Like USB Accelerator) with your RPI. You can run the compiled model on that RPI. Please see the requirements at : https://coral.ai/docs/accelerator/get-started/#requirements.
It's just that the compiler cannot be installed on a 32-bit machine but the output (.tflite) format can be executed on TPU connected to a ARMv7 32-bit machine."
1977,24095107,Working of CCD algorithm for Inverse Kinematics,|algorithm|3d|vector-graphics|robotics|inverse-kinematics|,"<p>Lets say I've a robotic arm with joints at points A,B,C,D in a 3D space. Let D be the end effector(bottommost child) and A be the topmost parent. Let T be the target point anywhere in the space. The aim is to make the end effector reach the target with minimum rotations in top levels(parents).</p>

<p>What I initially thought:</p>

<p>1) Rotate the arm C by angle TCD.
2) Then rotate the arm B by new angle TBD.
3) Then rotate the arm A by new angle TAD.</p>

<p>But the end effector seems to point away from the target after step 2. What am I doing wrong and how can I fix it?</p>
",6/7/2014 7:57,,2239,2,9,1,0.0,1137624.0,"Chennai, India",1/8/2012 21:32,2514.0,24135139.0,"<p>Before I started use some more advanced approaches I did it like this (using simple <strong>CCD cyclic coordinate descent</strong>):</p>
<pre class=""lang-cpp prettyprint-override""><code>pe=desired_new_position;

for (i=0;i&lt;number_of_actuators;i++)
 {
 // choose better direction
                   p=direct_kinematics(); d =|p-pe|; // actual step
 actuator(i)--;  p=direct_kinematics(); d0=|p-pe|; // previous step
 actuator(i)+=2; p=direct_kinematics(); d1=|p-pe|; // next step
 actuator(i)--;  dir=0; d0=d;
      if ((d0&lt;d)&amp;&amp;(d0&lt;d1)) dir=-1;
 else if ((d1&lt;d)&amp;&amp;(d1&lt;d0)) dir=+1;
 else continue;

 for (;;)
  {
  actuator(i)+=dir; p=direct_kinematics(); d =|p-pe|;
  if (d&gt;d0) { actuator(i)-=dir; break; }
  if (actuator(i) on the edge limit) break;
  }

 }
</code></pre>
<p>[notes]</p>
<ol>
<li><p>you can modify it to inc/dec actuator position by some step instead of 1</p>
<p>stop if difference crossed zero then start again with smaller step until <code>step == 1</code> This will improve performance but for most application is <code>step=1</code> enough because new position is usually near the last one.</p>
</li>
<li><p>beware that this can get stuck in local min/max</p>
<p>if the output get stuck (effector position is unchanged) then randomize the actuators and try again. Occurrence of this depend on the kinematics complexity and on the kind of path you want to use</p>
</li>
<li><p>if the arms are driven more on the top  then on the bottom</p>
<p>then try reverse the i-for loop</p>
</li>
<li><p>if you have to control the effector normal</p>
<p>then you have to exclude its rotation axises from CCD and set it before CCD</p>
</li>
</ol>
",2521214.0,1.0,7.0,37164184.0,Do you calculate the actual angle? Or the angle in the arm's movement plane? The latter would be the correct approach.
4308,71928676,"Cannot find module ""qfi"" for running JdeRobot drone_cat_mouse exercise from source",|python|ros|robotics|,"<p>I want to run JdeRobot drone_cat_mouse on my Ubuntu 20.04. I'm using ROS Noetic and has faithfully followed <a href=""https://github.com/JdeRobot/drones/blob/noetic-devel/installation20.md"" rel=""nofollow noreferrer"">these installation instructions</a>. Everything it told me to test was working properly.</p>
<p>When I first ran roslaunch drone_cat_mouse.launch, there was an import error for teleopWidget and sensorsWidget which I fixed by using relative imports. Then I had an error <code>No module named qfi</code>.</p>
<p><a href=""https://i.stack.imgur.com/RsOcr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RsOcr.png"" alt=""No module named qfi"" /></a></p>
<p>Unlike teleopWidget and sensorsWidget, I couldn't find the qfi module in the <a href=""https://github.com/JdeRobot/drones"" rel=""nofollow noreferrer"">JdeRobot/drones source code.</a> So I googled it, and the only relevant result that popped up was <a href=""https://gsyc.urjc.es/pipermail/jde-developers/2017-April/004421.html"" rel=""nofollow noreferrer"">this</a>, which led to <a href=""https://github.com/JdeRobot/base/issues/738"" rel=""nofollow noreferrer"">this link</a>. They said to:</p>
<p><code>sudo touch /usr/lib/python2.7/dist-packages/qfi/__init__.py</code></p>
<p>But I ran that command and this happened!</p>
<p><a href=""https://i.stack.imgur.com/ivoTP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ivoTP.png"" alt=""qfi module no exist"" /></a></p>
<p>Not even pip has a &quot;qfi&quot; module!</p>
<p><a href=""https://i.stack.imgur.com/qDZuy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qDZuy.png"" alt=""y u no qfi?!"" /></a></p>
<p>So I thought to check <a href=""https://github.com/search?q=org%3AJdeRobot%20qfi&amp;type=code"" rel=""nofollow noreferrer"">JdeRobot's entire repository.</a> Turns out it was in JdeRobot/base, and that repo is not maintained anymore!</p>
<p>After further digging, there was <a href=""https://github.com/JdeRobot/RoboticsAcademy/issues/847"" rel=""nofollow noreferrer"">this issue</a> which basically tells us forget about it and move to the web release! But I can't, circumstances forced me to use the source code option (deliverables are drone_cat_mouse.world and my_solution.py, it's impossible for me to get the former in the docker web version and the latter's format is different between the source code version and the web version).</p>
<p>In a nutshell, how do I fix this <code>qfi module</code> problem so that I can run the exercises from source <a href=""https://www.youtube.com/watch?v=rZCDixH1RV8"" rel=""nofollow noreferrer"">like</a> <a href=""https://www.youtube.com/watch?v=IwAGndG8nx0"" rel=""nofollow noreferrer"">these</a> <a href=""https://www.youtube.com/watch?v=eadm9I4ZDso"" rel=""nofollow noreferrer"">people</a>?</p>
",4/19/2022 17:03,,73,2,0,0,,10618936.0,,11/7/2018 14:45,28.0,71928978.0,"<p>I'm just stupid, as usual. all I need to do was clone <a href=""https://github.com/JdeRobot/ThirdParty"" rel=""nofollow noreferrer"">https://github.com/JdeRobot/ThirdParty</a>, get the qfi module, copy it to
~/catkin_ws/src/drones/rqt_drone_teleop/src/rqt_vel_teleop/ and replace all qfi imports with its relative import version. All common sense</p>
<p>No errors in terminal, gazebo runs, but somehow the rqt widget for drone vision never appears.</p>
<p><a href=""https://i.stack.imgur.com/zZlSf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zZlSf.png"" alt=""why"" /></a></p>
<p>Forget it, I'm giving up on this dumpster fire of a program.</p>
<p>Edit: I did another fresh install, followed the steps, noticed troubleshooting for qfi which required qmake, but same end result</p>
",10618936.0,0.0,1.0,,
4202,70695409,how to go from pixel coordinates to angle off the optical axis (Object detection alignment),|raspberry-pi|camera|computer-vision|robotics|,"<p>I am making a robot that detects a ball and goes to it.</p>
<p>Because I am doing the detection on a raspberry pi I thought that it will be better to work with images and not with real time detection.</p>
<p>The robot rotates 45 degrees and take a photo. If the ball isn’t detected, it moves another 45 degrees till it detects the ball.</p>
<p>Here it is the problem: after the detection the ball could be anywhere on the image, so I need to make an algorithm that says to the robot how many degrees it should turn to be centered aligned to the ball.</p>
<p>Here is how the robot detects the ball:</p>
<ul>
<li>Google cloud vision API, and if the ball isn't detected...</li>
<li>a TF-lite model detection will run. If the ball isn't detected with this...</li>
<li>it rotates.</li>
</ul>
<p>The camera used for the project: Raspberry pi Noir V2 (res HD)</p>
<p>Language: Python, but mostly I need ideas.</p>
<p>P.S.: I am a newbie to robotics, so any help will be appreciated.
Sorry for missing out some info, it is the first time asking on stackoverflow.</p>
",1/13/2022 10:57,70701562.0,814,1,2,-1,,17923574.0,,1/13/2022 10:51,8.0,70701562.0,"<p>You're asking how to handle a camera matrix, how to work with the &quot;intrinsics&quot; of a camera, and a little linear algebra.</p>
<p><a href=""https://elinux.org/Rpi_Camera_Module#Technical_Parameters_.28v.2_board.29"" rel=""nofollow noreferrer"">specs for the &quot;Raspberry pi Noir V2 (res HD)&quot;</a>:</p>
<ul>
<li>1.12 µm pixel size (design)</li>
<li>3.04 mm focal length (design)</li>
<li>full resolution: 3280 x 2464 (design)</li>
<li>full resolution FoV: H 62.28°, V 48.83°, D 74.16° (calculated)</li>
</ul>
<p>The camera matrix <code>K</code> generally looks like</p>
<p><a href=""https://docs.opencv.org/3.4/dc/dbb/tutorial_py_calibration.html"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3MaAot.png"" alt=""camera matrix"" /></a></p>
<p>The full resolution focal length (in pixels) is <code>f = (3.04 mm) / (1.12 µm/pixel) = 2714.3 [pixels]</code></p>
<p>The &quot;1920x1080&quot; video mode does no binning, only cropping. That means <code>f = 2714</code> goes for it too. That means the video mode's field of view is actually 38.96° horizontally, 22.5° vertically, 44.2° diagonally.</p>
<p>NB: for binned modes, if the binning is 2x2, then <code>f</code> halves, i.e. use 1357.</p>
<p>I have no information on how still pictures are produced. The full sensor has 4:3 aspect ratio. Assuming a 1920x1080 frame is fully fitted in there (touching sides, cropping top and bottom), the scale factor is 0.585 and f = 1589, with FoV being 62.28° by 37.54°, 69.46° diagonally.</p>
<p>Focal length can also be calculated from resolution and field of view, but pixel pitch and lens focal length are the nominal design parameters and the field of view derives from that (and imperfections like lens distortion).</p>
<p>Then we have <code>cx = (width-1) / 2 = 959.5</code> and <code>cy = (height-1) / 2 = 539.5</code>.</p>
<p>So now you have the values for the matrix.</p>
<p>A 3D point <code>p</code> is projected onto the image by calculating <code>K p</code>, which is a matrix multiplication. The opposite can be done. You can reproject a point on the picture back into the world. It's now a vector, a ray, a direction.</p>
<p>If you have <code>(x,y)</code> as picture coordinates, calculate:</p>
<pre><code>v_x = (x - cx) / f
# v_y = (y - cy) / f
</code></pre>
<p>and finally the horizontal angle:</p>
<pre><code>alpha = atan(v_x) # radians, or
alpha = atan(v_x) * 180/pi # degrees
</code></pre>
<p>(Diagonal angle away from the optical axis would be <code>atan(hypot(v_x, v_y))</code>)</p>
<p>For a point on the left edge (x=0), that would mean turning 19.5 degrees left.</p>
<p>All of this assumes that the picture was not distorted by the lens. For small angles, this doesn't matter. Some cameras have special lenses that hardly distort at all. Some cameras, especially action cameras, intentionally have almost-fisheye lenses (meaning strong distortion).</p>
<p>If you need to deal with lens distortion, that's another topic. There are common models for lens distortion that work with as few as 4 parameters. Both MATLAB and OpenCV come with calibration methods.</p>
",2602877.0,1.0,12.0,124984300.0,"I need to make an algorithm that calculate the degrees the robot should turn to be aligned,Angle of View: 62.2 x 48.8 degrees,the resolution of the pic(1920x1080),and i know the x1,x2,y1,y2 of the label box.But i want the formula mainly."
2983,48189649,ROS Python Script is not executable when catkin build is done,|python|ros|robotics|catkin|,"<p>I'm new to ROS.</p>

<p>I have developed a ROS python project. : <a href=""https://github.com/ildoonet/ros-video-recorder"" rel=""nofollow noreferrer"">https://github.com/ildoonet/ros-video-recorder</a></p>

<p>After cloning the repo into my ros workspace, It is not executed since scripts don't have a permission to run.</p>

<p>It is working fine if I add a permission for execution to the script files.</p>

<p>So.. I have to run 'chmod +x src/{repo_name}/scripts/{script_name}' on every scripts to run this script.</p>

<p>As I have experienced, there are ROS projects that is python based and also is able to be executed right after I download the git. (no need to add a permission)</p>

<p>How can I make my repo to do that? Do I have to add some commands in CMakelists or package.xml?</p>
",1/10/2018 14:30,,1291,1,0,1,,5107689.0,,7/12/2015 9:59,17.0,48257864.0,"<p>Turns out that I can change permissions on script files and commit them on github. Their permission will hold in other machines. </p>
",5107689.0,1.0,0.0,,
3059,49614877,How to move a delta ASDA-B2 motor to position x and y?,|java|robotics|servo|,"<p>I want to write a method which gets two number, <code>x</code> and <code>y</code> and move the servo to the correct position. <code>x</code> and <code>y</code> are the position of an object in an image which I extracted before. I have no idea how to move the servo in java. I skimmed the <a href=""http://www.acontrol.com.pl/uploads/pdf/instrukcje/ASDA-B2-user-guide.pdf"" rel=""nofollow noreferrer"">manual</a> and it seems there is nothing related to my issue. The servo should point to the position of the object. Mapping the position <code>x</code> and <code>y</code> to the suitable number for servo movement can be done after finding the way to move the servo.In addition, the servo is placed in the position N and M/2 of an NxM image and should point to the position of the object.
<strong>I use a PC or Laptop to communicate with servo (maybe USB or COM port)</strong></p>

<p>I use Java 9 and maven.</p>

<p>here is the simplified version of my problem:</p>



<pre><code>public void servoInitialization(){
     //initializeation
}
public void servoMove (int x, int y){

    //computation to map the `x` and `y` to correct number for moving the  servo
    //move the servo to the position
}
</code></pre>

<p><a href=""https://i.stack.imgur.com/zIbvH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zIbvH.png"" alt=""enter image description here""></a></p>
",4/2/2018 16:23,,156,0,6,1,0.0,7030791.0,USA,10/17/2016 11:37,503.0,,,,,,94966876.0,"Not sure this will help you though. Please check the protocol being used for your ASDA-B2 is this one. If it is, you can start playing around with it by sending bits to RS232 port(by using the protocol specification). https://www.manualslib.com/manual/1286287/Delta-Asd-B2-0121-B.html?page=253#manual For Cartesian to polar coordinate conversion, [![enter image description here](https://i.stack.imgur.com/u7dYz.jpg)](https://i.stack.imgur.com/u7dYz.jpg)"
3347,55313613,How rotate source vector to target (to nearest vector) by using only 2 Euler angles,|matrix|vector|geometry|coordinates|robotics|,"<p>I need to rotate source 3D vector to target vector by rotation around only two axes YZ.</p>

<p>I have mechanism with 2 motors rotating around Y and then around Z. I have random source vector attached to this mechanism.</p>

<p>I think that source vector can't be rotated to random target 3D vector by using only 2 angles and I need to rotate it to the nearest to target vector.</p>

<p>I need to align only orientation, not coordinates.</p>

<p>What is the best way to do it with rotation matrices, quaternions, etc?</p>

<p>I can calculate shortestArcQuat from source vector to target. Then multiply target vector by inverse of this quat. And then get YZ angles from rotation matrix with the result of prev operation as Z vector but I think it's wrong.</p>

<p>Solution should be analytic.</p>
",3/23/2019 12:14,55315048.0,456,1,0,0,,11246956.0,,3/23/2019 11:49,2.0,55315048.0,"<p>For clarity in the notation let me define the unit vectors </p>

<pre><code>E1 = (1,0,0)
E2 = (0,1,0)
E3 = (0,0,1)
</code></pre>

<p>Given a rotation matrix R the goal is deconpose it in two rotation matrices rotating an angle ""a"" around axis E2 and an angle ""b"" around axis E3:</p>

<pre><code>R = exp(a E2) exp(b E3)
</code></pre>

<p>Multiplying both sides by E3</p>

<pre><code>R E3 = exp(a E2) exp(b E3) E3
</code></pre>

<p>We get:</p>

<pre><code>W = exp(a E2) E3
</code></pre>

<p>Where W is the vector E3 rotated by R: W = R E3</p>

<pre><code>a = atan2( W • E1, W • E3)
</code></pre>

<p>Where (•) is the dot product.</p>

<p>Now taking the transpose of R we get:</p>

<pre><code>R^T = exp(b E3)^T exp(a E2)^T
</code></pre>

<p>Multiplying both sides by E2:</p>

<pre><code>R^T E2 = exp(b E3)^T exp(a E2)^T E2

R^T E2 = exp(-b E3) exp(-a E2) E2

S = exp(-b E3) E2
</code></pre>

<p>Where S is the vector E2 rotated by R^T: S = R^T E2</p>

<pre><code>b = - atan2( S • E1, S • E2)
</code></pre>

<p>I have just derived those equations so this is untested and there might be some mistake. Take it as is.</p>
",9147444.0,0.0,0.0,,
2127,26935863,Robotics - Recursive function for fractal.,|python|recursion|robotics|myro|calico-project|,"<p>I'm currently working with Myro/Calico with Robotics.  I'm trying to run a recursive function of a fractal. I'm using Python.</p>

<p>I've been following pseudocode here.
<a href=""http://wiki.scratch.mit.edu/wiki/Recursion_and_Fractals"" rel=""nofollow"">Fractal</a></p>

<p>So far I've tried to implement the first step without recursion.  And it runs well</p>

<pre><code># 1 foot per 2 seconds.  x * 2 = feet desired.  
def fractal(x):
    waitTime = x*2
    turnRight(1, 0.825) #90 degree turn
    forward(1, x/3) #move length/3 steps
    turnLeft(1, 0.55)#60 degree turn
    forward(1, x/3) #move length/3 steps
    turnRight(1, 1.1) #120 degree turn
    forward(1, x/3) #move length/3 steps
    turnLeft(1, 0.55) #60 degree turn
    forward(1, x/3) #move length/3 steps
</code></pre>

<p>While this works, my goal is to go through this recursively, but make a smaller curve at each iteration.  I attempted to do it, but my robot isn't moving as desired. </p>

<p>Here's my attempt at recursion</p>

<pre><code>def fractal(x):
    waitTime = x*2
    if (x == 1):
        forward(x/3)
    else:
        (x-1)/3
    turnLeft(1,0.55) #60 degrees
    if (x == 1):
        forward(x/3)
    else:
        (x-1)/3
    turnRight(1, 1.1) #120 degree turn
    if (x == 1):
        forward(x/3)
    else:
        (x-1)/3
    turnLeft(1, 0.55)#60 degree turn
    if (x == 1):
        forward(x/3)
    else:
        (x-1)/3
</code></pre>

<p>My robot is only turning left and right, but it's not making the full shape.  The one without the recursion started the fractal.  I just need recursion to go through the whole fractal.  </p>
",11/14/2014 17:44,26936172.0,267,1,7,0,,3577397.0,,4/27/2014 2:15,141.0,26936172.0,"<p>I'm thinking this is what you want to do</p>

<pre><code>x = number of interations
l = lenth(wait time)
def fractal(x, l):
    if (x == 1):
        forward(l/3)
    else:
        fractal((x-1), l/3)
    turnLeft(1,0.55) #60 degrees
    if (x == 1):
        forward(l/3)
    else:
        fractal((x-1), l/3)
    turnRight(1, 1.1) #120 degree turn
    if (x == 1):
        forward(l/3)
    else:
        fractal((x-1), l/3)
    turnLeft(1, 0.55)#60 degree turn
    if (x == 1):
        forward(l/3)
    else:
        fractal((x-1), l/3)
</code></pre>
",2097159.0,1.0,1.0,42416511.0,"I'm not sure, but I think you need to have two variables to `fractal` instead of just one. You can't easily represent the fractal depth _and_ the width of the curve with just one variable. What if I want a five iteration Koch curve that's 100 pixels wide? What would the call to `fractal` look like, and how would it differ if I wanted a seven iteration curve that's 100 pixels wide?"
4682,77274713,Change color of CoppeliaSim shapes with Python,|python|colors|simulator|robotics|,"<p>I am using the CoppeliaSim Regular API with Python.</p>
<p>To set a color to my primitive shapes I am using the method setShapeColor().
To set a shape green I have the following code.</p>
<h1>code to set a shape green</h1>
<p>sim.setShapeColor(capsuleHandle, None, sim.colorcomponent_emission, [0, 255, 0])</p>
<p>However I am not convinced with the result. Although it is green, it looks quite pale and very shiny/reflective.</p>
<p>But from the setShapeColor() there are 2 parameters that I do not understand well.
<a href=""https://www.coppeliarobotics.com/helpFiles/"" rel=""nofollow noreferrer"">https://www.coppeliarobotics.com/helpFiles/</a>
What should the string colorName contain? Are there already predefined color names? Or it is that a tag just for me? Or what is the use of that string?</p>
<p>Also I do not understand the color components.
<a href=""https://www.coppeliarobotics.com/helpFiles/en/apiConstants.htm#colorComponents"" rel=""nofollow noreferrer"">https://www.coppeliarobotics.com/helpFiles/en/apiConstants.htm#colorComponents</a>
The documentation only gives the names but they do not explain what is the purpose of each component or when to use each.</p>
<p>So can someone please:</p>
<ol>
<li>Explain me how to use the colorName string and the color components</li>
<li>How can I not only change the color of the shape, but also control the intensity of the color and if it shiny or dull.</li>
<li>I also need to control the transparency of the shape. How to do that?</li>
</ol>
<p>Thank you :)</p>
<ul>
<li>Read the Regular API documentation</li>
<li>Trying all the color components but the result was always the same</li>
<li>For the transparency I tried: Using the transparency component, in the RGB instead of sending a list send a single value for the transparency, send a list with just 1 value, send a list with 4 values (RGB + transparency).</li>
</ul>
",10/11/2023 16:09,,73,0,1,0,,22694257.0,,10/6/2023 8:42,4.0,,,,,,136234841.0,It is a proprietary software. Why not ask the vendor? See [the official forum](https://forum.coppeliarobotics.com/).
2254,30577463,Get turn angle to position on coordinate map?,|python|math|geometry|robotics|,"<p>I'm trying to get a robot to turn to face another robot, based on their respective coordinates on a global coordinate map.</p>

<p>This is the code I wrote, but it doesn't seem to work at all:</p>

<pre><code>def calcAngleToCoords(self, curAngle, curPosition, targPosition):
    retVal = False

    if type(curPosition) is list and type(targPosition) is list:
        x_1, y_1 = curPosition
        x_2, y_2 = targPosition
        # Sets origin coordinate to zero
        x_2 = x_2 - x_1
        y_2 = y_2 - y_1

        radius = math.sqrt(y_2 ** 2 + x_2 ** 2) # Pythagorean Thereom, a^2 + b^2 = c^2 | Radius = c, y_2 = a, x_2 = b
        angle = curAngle * (math.pi / 180)

        x_1 = radius * math.cos(angle)
        y_1 = radius * math.sin(angle)

        turnArc = math.atan( (y_1 - y_2) / (x_2 - x_1) ) * (180 / math.pi)

        retVal = turnArc
        # TODO: Check to see if the angle is always clockwise.
    else:
        raise TypeError(""Invalid parameter types. Requires two lists."")

    return(retVal)
</code></pre>

<p>Can any one tell me a better way to do this or what I'm doing wrong? It's for a project I'm working on and the deadline is coming up really soon so any help would be appreciated!</p>
",6/1/2015 15:32,,600,1,1,1,,4962011.0,,6/1/2015 15:27,11.0,30579215.0,"<p>There is no need to calculate radius</p>

<pre><code>    x_2 = x_2 - x_1
    y_2 = y_2 - y_1
    angle = curAngle * (math.pi / 180)
    dx = math.cos(angle)
    dy = math.sin(angle)
    turnArc = math.atan2(x_2 * dy - y_2 * dx,  x_2 * dx + y_2 * dy ) * (180 / math.pi)
</code></pre>

<p>Note using of atan2 function, that returns angle between -pi and pi. It correctly determines rotation direction and finds the shortest turn. If you need to rotate in clockwise direction only, add 180 if turnArc is negative</p>
",844416.0,0.0,1.0,49226183.0,How do you know it doesn't work?
4233,71397376,Why is the default value of WHOAMI register of MPU9250 is 0x71?,|microcontroller|robotics|mpu6050|,"<p>I'm reading the datasheet of MPU9250 and I find out the address of MPU6050 inside MPU9250 is b110100x. But when I see the description of WHOAMI register, it says the default value is 0x71. And I cannot figure out any relationship between b110100x and 0x71.</p>
<p>Can anyone tell me what is going on? Thank you～</p>
<p>BTW why does everyone put 0 in front of the address and say that the address of MPU9250 is 0x68? It seems more sense to put 0 on the LSB since that LSB represent read or write.</p>
",3/8/2022 15:19,,607,0,5,0,,15707867.0,,4/20/2021 15:56,17.0,,,,,,126226029.0,"@ThomasJager Yeah `0xD0` is what I mean! I just think it is really confusing to shift the address right to become `0x68`...
Thank you for you explaining~~ I never thought that address and number will be indepent!"
3663,60682233,Problem with robot orientation due to euler angles,|python|ros|robotics|euler-angles|gazebo-simu|,"<p>I am trying to implement a basic path planning algorithm using python with a 4 wheels robot created for ROS and Gazebo, RViz.</p>

<p>The only thing required for my algorithm is to make my robot oriented towards a given (using mouse) point in the x,y plane.</p>

<p>The problem I am facing is that I am converting from quaternion to Euler angles and I always (no mater how I manipulate my code) come to a situation like the following:</p>

<pre><code>Theta to target:257.106918658
Theta_deg:-179.85
dTheta:-436.96
Theta to target:257.106918658
Theta_deg:-179.85
dTheta:-436.96
Theta to target:257.118158708
Theta_deg:179.99
dTheta:-77.13
Theta to target:257.118158708
Theta_deg:179.99
dTheta:-77.13
Theta to target:257.118158708
Theta_deg:179.99
dTheta:-77.13
Theta to target:257.118158708
Theta_deg:179.99
</code></pre>

<p>So:</p>

<p>Theta to target: Euler [-180, 180] Angle in [deg] of the line connecting the center of mass of robot to the given point.</p>

<p>Theta_deg: Euler [-180, 180] Angle in [deg] of the orientation (vertical vector on the front face of the robot) to X axis.</p>

<p>As you can see from the data above, <code>Theta_deg</code> experiences a non-continuous step from:</p>

<pre><code>Theta_deg:-179.99
</code></pre>

<p>to</p>

<pre><code>Theta_deg:179.99
</code></pre>

<p>I know that this is an issue with using Euler angles. 
How can I overcome this issue?</p>

<p>Relative code:</p>

<pre><code>while not rospy.is_shutdown():
    theta_deg = (round(180*(theta/math.pi),2))
    old_distance = round(distance_to_target,2)

    theta_to_target =180*(math.pi/2+math.atan2((y-goal.y),(x-goal.x)))/math.pi        
    dTheta =(round(((theta_deg)-(theta_to_target)),2))

    dtt = round((math.sqrt((goal.x - x)**2 + (goal.y - y)**2)),2)

    if dtt &lt; 0.1:
        print ""&lt;&lt;&lt; Destination Reached &gt;&gt;&gt;""
    else:
        theta_to_target =180*(math.pi/2+math.atan2((y-goal.y),(x-goal.x)))/math.pi
        dTheta =(round(((theta_deg)-(theta_to_target)),2))
        print ""Theta to target:"" + str(theta_to_target)
        print ""Theta_deg:"" + str(theta_deg)
        print ""dTheta:"" + str(dTheta)
        # if dTheta&lt;-360: dTheta=dTheta+360
        # if dTheta&gt;360: dTheta=dTheta-360

        if abs(dTheta)&gt;10:
            if abs(theta_to_target - theta_deg)&gt;180:
                speed.angular.z = -0.2
                speed.linear.x = 0.0
                pub.publish(speed)
            else:
                speed.angular.z = +0.2
                speed.linear.x = 0.0
                pub.publish(speed)
        else:
            speed.linear.x = 0.6


    pub.publish(speed)
    r.sleep()
</code></pre>
",3/14/2020 11:29,,464,0,2,0,,12969878.0,,2/26/2020 23:42,10.0,,,,,,107371483.0,"Instead of fixing the maths aspect, can you make it so that your robot always goes in the shortest rotation direction? For example from `-179.99` to `+179.99` degrees would not be a full turn but `-0.02` degrees. I think `dθ = min(dθ%360, dθ%360 + 360, key=abs)` would work."
3177,51480521,ValueError: Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph,|python|tensorflow|deep-learning|ros|robot|,"<p>I want to use tensorflow model in ROS.and this code can work,however,it is very slow because of everytime it restore the model</p>

<pre><code>class image_converter:
def __init__(self):
    self.demo_pub = rospy.Publisher(
        ""/demo/pic"", Image, queue_size=1)
    # todo pub distance and angle data
    self.bridge = CvBridge()
    self.image_sub = rospy.Subscriber(
        ""/camera/live_view"", Image, self.callback)
def callback(self, data):
    self.callback_once(data)
def callback_once(self, data):
    # imgmsg_to_cv2
    try:
        cv_image = self.bridge.imgmsg_to_cv2(data, ""bgr8"")
    except CvBridgeError as e:
        print(e)
    img1 = cv_image
    data,_,img=read_img(img1)
        #print(data)
    with tf.Session() as sess:
        saver = tf.train.import_meta_graph('./model4/model.ckpt.meta')
        saver.restore(sess,tf.train.latest_checkpoint('./model4/'))
        graph = tf.get_default_graph()
        x = graph.get_tensor_by_name(""x:0"")
        feed_dict = {x:data}
        logits = graph.get_tensor_by_name('logits_eval:0')
        classification_result = sess.run(logits,feed_dict)
        index=(tf.argmax(classification_result,1).eval())
</code></pre>

<p>so i change this code like this :</p>

<pre><code>class image_converter:
def __init__(self):
    self.sess=tf.Session()
    saver = tf.train.import_meta_graph('./model4/model.ckpt.meta')
    saver.restore(self.sess,tf.train.latest_checkpoint('./model4/'))
    self.graph = tf.get_default_graph()
    self.x = self.graph.get_tensor_by_name(""x:0"")
    self.demo_pub = rospy.Publisher(
        ""/demo/pic"", Image, queue_size=1)
    # todo pub distance and angle data
    self.bridge = CvBridge()
    self.image_sub = rospy.Subscriber(
        ""/camera/live_view"", Image, self.callback)
def callback(self, data):
    try:
        cv_image = self.bridge.imgmsg_to_cv2(data, ""bgr8"")
    except CvBridgeError as e:
        print(e)
    data,_,img=read_img(cv_image)
    feed_dict = {self.x:data}
    logits = self.graph.get_tensor_by_name('logits_eval:0')
    classification_result = self.sess.run(logits,feed_dict)
    print(classification_result)
    index=(tf.argmax(classification_result,1).eval(session=self.sess))
</code></pre>

<p>there is an error occoured :in the last line 
<strong>ValueError: Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph.</strong></p>

<p>so,what should i do to solve this error.thank you very much.</p>
",7/23/2018 13:53,,774,0,0,2,0.0,8211166.0,,6/25/2017 6:24,117.0,,,,,,,
611,3077380,AI Behavior Decision making,|artificial-intelligence|machine-learning|simulation|physics|robotics|,"<p>I am running a physics simulation and applying a set of movement instructions to a simulated skeleton. I have a multiple sets of instructions for the skeleton consisting of force application to legs, arms, torso etc. and duration of force applied to their respective bone. Each set of instructions (behavior) is developed by testing its effectiveness performing the desired behavior, and then modifying the behavior with a genetic algorithm with other similar behaviors, and testing it again. The skeleton will have an array behaviors in its set list.</p>

<p>I have fitness functions which test for stability, speed, minimization of entropy and force on joints. The problem is that any given behavior will work for a specific context. One behavior works on flat ground, another works if there is a bump in front of the right foot, another if it's in front of the left, and so on. So the fitness of each behavior varies based on the context. Picking a behavior simply on its previous fitness level won't work because that fitness score doesn't apply to this context.</p>

<p>My question is, how do I program to have the skeleton pick the best behavior for the context? Such as picking the best walking behavior for a randomized bumpy terrain.</p>
",6/19/2010 21:55,,577,5,4,4,0.0,364282.0,"Houston, TX",6/11/2010 7:01,137.0,3077466.0,"<p>You're using a genetic algorithm to modify the behavior, so that must mean you have devised a fitness function for each combination of factors.  Is that your question?</p>

<p>If yes, the answer depends on what metrics you use to define best walking behavior:</p>

<ol>
<li>Maximize stability</li>
<li>Maximize speed</li>
<li>Minimize forces on joints</li>
<li>Minimize energy or entropy production </li>
</ol>

<p>Or do you just try a bunch of parameters, record the values, and then let the genetic algorithm drive you to the best solution?</p>

<p>If each behavior works well in one context and not another, I'd try quantifying how to sense and interpolate between contexts and blend the strategies to see if that would help.</p>
",37213.0,0.0,1.0,3154152.0,"My skeleton is omniscient of what is happening inside the simulation within a range (a 20m sphere). Anything that is in that sphere, it knows about. I need that to translate into picking the correct parts of it's body to move so that it can walk without falling over."
1819,18718014,Arduino: void loop() gives unexpected result,|c++|arduino|robotics|,"<p>I am developing a simple robot (with 4 wheeled base) which moves forward and detects an object  through distance sensor, picks it with an arm. Object is a box with some sort of handle from upward direction, hence arm is placed below the, so called, handle of object and lifts it upward. The task I want to achieve that the robot should move forward and place the object in a container (it is also a physical object) placed at some specified location. </p>

<p>The robot moves with 4 DC motors present in wheels of its base and arm is controlled by a separate DC motor.</p>

<p>What I want is that: </p>

<p>The robot should move forward until it detects the object. <br>
When object is detected then it should stop the the wheels and start the arm to lift the object upward. <br>
Then it should move forward until it detects the second object i.e. the container. <br>
When second object (container) is detected then the wheels should be stopped and arm should be activated to put the object in the container.</p>

<p>For this I have written the following code</p>

<pre><code>#define mp1 3
#define mp2 4
#define m2p1 5
#define m2p2 6

#define echoPin 7 // Echo Pin
#define trigPin 8 // Trigger Pin
#define LEDPin 13 // Onboard LED

#define armPin1 9  // Pin 1 of arm
#define armPin2 10  // Pin 2 of arm

int maximumRange = 200; // Maximum range needed
int minimumRange = 18; // Minimum range needed
long duration, distance; // Duration used to calculate distance

int first = 0;

void setup()
{
  Serial.begin(9600);

 //Setting the pins of motors
 pinMode(mp1, OUTPUT);
 pinMode(mp2, OUTPUT);
 pinMode(m2p1, OUTPUT);
 pinMode(m2p2, OUTPUT);

 //Setting the pins of distance sensor
 pinMode(trigPin, OUTPUT);
 pinMode(echoPin, INPUT);

 pinMode(LEDPin, OUTPUT); // Use LED indicator (if required)

 pinMode(armPin1, OUTPUT);
 pinMode(armPin2, OUTPUT);
}// end setup method

long calculateDistance(){
  //Code of distance sensor
  digitalWrite(trigPin, LOW); 
  delayMicroseconds(2); 

  digitalWrite(trigPin, HIGH);
  delayMicroseconds(20); 

  digitalWrite(trigPin, LOW);
  duration = pulseIn(echoPin, HIGH);

 //Calculate the distance (in cm) based on the speed of sound.
 return duration/58.2;
}

void loop()
{
  distance = calculateDistance();

  while(distance &gt; minimumRange) {
    forward();
    distance = calculateDistance();
  }

  while(distance &lt;= minimumRange &amp;&amp; first == 0){
    stopMotors();

    pickObject();

    distance = calculateDistance();
  }

  while(distance &gt; minimumRange) {
    forward();
    distance = calculateDistance();
  }

  while(distance &lt;= minimumRange &amp;&amp; first == 1){
    stopMotors();

    putObject();

    distance = calculateDistance();
  }// end second while copied 

  first = 1;

}// end loop function

void pickObject(){
  digitalWrite(armPin1, LOW);
  digitalWrite(armPin2, HIGH);
}

void putObject(){
  digitalWrite(armPin1, HIGH);
  digitalWrite(armPin2, LOW);
}

void stopMotors(){
  digitalWrite(mp1, LOW);
  digitalWrite(mp2, LOW);
  digitalWrite(m2p1, LOW);
  digitalWrite(m2p2, LOW);
}

void forward(){
  digitalWrite(mp1, LOW);
  digitalWrite(mp2, HIGH);
  digitalWrite(m2p1, HIGH);
  digitalWrite(m2p2, LOW);
}
</code></pre>

<p>But the code does not work as I want it to do. It actually moves the arm in downward direction. It may be because I have not understood the flow of <code>loop()</code> function. </p>

<p>Can anyone tell what is the problem in my code and what should be my code to get the desired result?</p>
",9/10/2013 11:40,18719619.0,1028,4,2,0,,534984.0,,12/8/2010 12:43,180.0,18719274.0,"<p>Here is your problem:</p>

<p>The loop function runs constantly, every time it ends it starts right back up.  You set first to 1, but you never re set it to 0, so every loop it goes through first is 1.</p>

<pre><code>while(distance &lt;= minimumRange &amp;&amp; first == 1){
    stopMotors();

    putObject();

    distance = calculateDistance();
  }// end second while copied 

  first = 1;
</code></pre>
",642230.0,1.0,0.0,27582414.0,"@TheTerribleSwiftTomato : It does stop when it senses any object, but then moves the arm in downward direction every time."
469,2490067,"Microsoft Robotics Studio, simple simulation",|robotics|robotics-studio|,"<p>I am soon to start with Microsoft Robotics Studio. </p>

<p>My question is to all the gurus of MSRS, <strong>Can simple simulation (as obstacle avoidance and wall following) be done without any hardware ?</strong> </p>

<p>Does MSRS have 3-dimensional as well as 2-dimensional rendering? As of now I do not have any hardware and I am only interested in simulation, when I have the robot hardware I may try to interface it! </p>

<p>Sorry for a silly question, I am a MSRS noob, but have previous robotics h/w and s/w experience.</p>

<p>Other than MSRS and Player Project (Player/Stage/Gazebo) is there any other Software to simulate robots, effectively ?</p>
",3/22/2010 4:46,2503995.0,1508,2,0,2,0.0,280945.0,"New Delhi, India",2/25/2010 5:52,524.0,2503995.0,"<p>MSRS tackles several key areas. One of them is simulation. The 3D engine is based on the AGeia Physics engine and can simulate not only your robot and its sensors, but a somewhat complex environment.</p>

<p>The demo I saw had a Pioneer with a SICK lidar running around a cluttered appartment living room, with tables, chairs and etc.</p>

<p>The idea is that your code doesn't even need to know if it's running on the simulator or the real robot.</p>

<p><b>Edit:</b><br/>
A few links as requested:<br />
Start here: <a href=""http://msdn.microsoft.com/en-us/library/dd939184.aspx"" rel=""nofollow noreferrer"">http://msdn.microsoft.com/en-us/library/dd939184.aspx</a>
<a href=""http://i.msdn.microsoft.com/Dd939184.image001(en-us,MSDN.10).jpg"" rel=""nofollow noreferrer"">alt text http://i.msdn.microsoft.com/Dd939184.image001(en-us,MSDN.10).jpg</a></p>

<p>Then go here: <a href=""http://msdn.microsoft.com/en-us/library/dd939190.aspx"" rel=""nofollow noreferrer"">http://msdn.microsoft.com/en-us/library/dd939190.aspx</a>
<a href=""http://i.msdn.microsoft.com/Dd939190.image008(en-us,MSDN.10).jpg"" rel=""nofollow noreferrer"">alt text http://i.msdn.microsoft.com/Dd939190.image008(en-us,MSDN.10).jpg</a></p>

<p>Then take a look at some more samples: <a href=""http://msdn.microsoft.com/en-us/library/cc998497.aspx"" rel=""nofollow noreferrer"">http://msdn.microsoft.com/en-us/library/cc998497.aspx</a>
<a href=""http://i.msdn.microsoft.com/Cc998496.Sumo1(en-us,MSDN.10).jpg"" rel=""nofollow noreferrer"">alt text http://i.msdn.microsoft.com/Cc998496.Sumo1(en-us,MSDN.10).jpg</a></p>
",229081.0,3.0,7.0,,
2219,29399433,Path planning algorithm for connected multi-robot system,|algorithm|robotics|motion-planning|,"<p>Say I have three robots connected as a triangle, how do I manage to avoid obstacles in working space?</p>

<p>I'm thinking about using A*, but I got problem that it works for one robots but when comes to 3 robots it will cause collision. </p>

<p>I'm really new to robotics, could anyone at least have some idea that which algorithm will be a better fit?</p>

<p>I don't need the actual code, just a feasible idea is fine.</p>

<p>Thanks in advance!!!</p>
",4/1/2015 19:43,,116,0,2,1,,2730755.0,,8/29/2013 20:26,143.0,,,,,,46975006.0,"What exactly do you mean by 'connected as a triangle'? Are they attached to inflexible rods and must move simultanously, or are they 'connected' in terms of communication?"
422,1775840,coachable players for RoboCup Soccer Simulator 2d v14,|simulation|robotics|robocup|,"<p>I am doing a work similar to <a href=""http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=4133817"" rel=""nofollow noreferrer"">this one</a> but the coachable players i found <a href=""http://rcsscoachable.sourceforge.net/"" rel=""nofollow noreferrer"">online</a> are 3 years old and don't work with the latest version of the <a href=""http://sserver.sourceforge.net/"" rel=""nofollow noreferrer"">soccer server</a>.</p>

<p>does anyone know any alternatives? or have any sugestions?<br>
thanks</p>
",11/21/2009 15:51,7661969.0,416,1,0,2,,86845.0,"Porto, Portugal",4/3/2009 18:19,768.0,7661969.0,"<p>What you want is a framework for implementing agents that implement the CLang (not the LLVM stuff) language, used by the coach to communicate with the agents using the virtual environment (like it was talking to the players in a soccer field).</p>

<p>The most complete frameworks that I know are Dainamite and agent2d. Take a look at then and see if they implement all the features that you need.</p>

<p><a href=""http://www.dainamite.de/"" rel=""nofollow"">http://www.dainamite.de/</a></p>

<p><a href=""http://pt.sourceforge.jp/projects/rctools/releases/"" rel=""nofollow"">http://pt.sourceforge.jp/projects/rctools/releases/</a></p>

<p>Good luck!</p>
",2238005.0,1.0,0.0,,
2473,36625422,how robot tracking ball with fixed distance?,|c|tracking|robotics|,"<p>I am tracking ball with camera in my android phone and send x,y position,radius of ball (x,y position is a pixel in screen android phone ) to my stm32f board via bluetooth. I assemble my phone and stm32f1 kit in a mobile robot. Now i would like my robot move to ball with a fixed distance. </p>

<p>Ex: I set distance 10cm. When i move ball forward, my robot forward to ball and always keep 10cm from robot to ball</p>
",4/14/2016 13:59,36625617.0,122,1,0,-1,,5962385.0,,2/22/2016 10:25,133.0,36625617.0,"<p>Here is some pseudo code to get you started:</p>

<pre><code>while (TRUE) do
    get x, y position of ball
    get x, y position of self
    calculate distance between self and ball (hint: use Pythagoras)
    if (distance &lt; 10 cm)
        move away from ball
    else if (distance &lt; 10 cm)
        move towards ball
end
</code></pre>

<p>Now all you have to do is code this up in C.</p>
",253056.0,1.0,9.0,,
4582,76194249,How to display a grasps on a pointcloud using open3d (python),|python|point-clouds|robotics|open3d|grasp|,"<p>The main goal is to display the grasp received from a ur10 robot in a pointcoud using open3d. The open3d is used to depict an object as pointclouds and we want to show a grasp skeleton (the idea of grasp skeleton was taken from the s4g repository) around it.
Currently, open3d shows us the pointcloud of the object but the grasp shown is at the wrong orientation.</p>
<p>The ur10 gives us the following output regarding its pose:</p>
<p>position= [0.015238827715672282, 0.026318999049784485, 0.231598307617698]<br />
quats =  [0.7125063395828215, -0.678258641135711, -0.17649390437631773, 0.03390919487441209]</p>
<p>As of now, we are tried to convert 'quats' into oritentation by doing the following: orientation = quat2mat(quats)&gt; We then load the pointcloud and the gripper skeleton, but the gripper always seems to be in a wrong place. there seems to be something wrong in the rotation of the frames.&gt; The rest of the code is as follows&gt;</p>
<pre><code>type here
pc = o3d.io.read_point_cloud(&quot;/home/max_j/Pictures/full_pcd/obj1.pcd&quot;)
vis = o3d.visualization.VisualizerWithEditing()
vis.create_window()
vis.add_geometry(pc)
vis.run()
vis.destroy_window()
vis_list = [pc]

# Gripper Configuration
BACK_COLLISION_MARGIN = 0.0  # points that collide with back hand within this range will not be detected
HALF_BOTTOM_WIDTH = 0.10
BOTTOM_LENGTH = 0.0025
FINGER_WIDTH = 0.0003
HALF_HAND_THICKNESS = 0.0003
FINGER_LENGTH = 0.20
HAND_LENGTH = BOTTOM_LENGTH + FINGER_LENGTH
HALF_BOTTOM_SPACE = HALF_BOTTOM_WIDTH - FINGER_WIDTH
create_box = o3d.geometry.TriangleMesh.create_box
back_hand = create_box(height=2 * HALF_BOTTOM_WIDTH,
                        depth=HALF_HAND_THICKNESS * 2,
                        width=BOTTOM_LENGTH - BACK_COLLISION_MARGIN)

temp_trans = np.eye(4)
temp_trans[0, 3] = -BOTTOM_LENGTH
temp_trans[1, 3] = -HALF_BOTTOM_WIDTH
temp_trans[2, 3] = -HALF_HAND_THICKNESS
back_hand.transform(temp_trans)

finger = create_box((FINGER_LENGTH + BACK_COLLISION_MARGIN),
                    FINGER_WIDTH,
                    HALF_HAND_THICKNESS * 2)
color=(0.1, 0.6, 0.3)
finger.paint_uniform_color(color)
back_hand.paint_uniform_color(color)
left_finger = copy.deepcopy(finger)

temp_trans = np.eye(4)
temp_trans[1, 3] = HALF_BOTTOM_SPACE
temp_trans[2, 3] = -HALF_HAND_THICKNESS
temp_trans[0, 3] = -BACK_COLLISION_MARGIN
left_finger.transform(temp_trans)
temp_trans[1, 3] = -HALF_BOTTOM_WIDTH
finger.transform(temp_trans)

hmt = np.eye(4)  #if we use R to make hmt
hmt[:3,:3] = orientation
hmt[:3,3] = position
T_global_to_local = hmt
print(&quot;the  global to local before correction is: \n&quot;, T_global_to_local)

#T_local_to_global = np.linalg.inv(T_global_to_local)
coord_frame_hand = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1)
coord_frame_hand = coord_frame_hand.transform(T_global_to_local)

# o3d.visualization.draw_geometries([coord_frame_hand_, coord_frame_hand_])

back_hand.transform(T_global_to_local)
finger.transform(T_global_to_local)
left_finger.transform(T_global_to_local)

vis_list_ = [back_hand, left_finger, finger,coord_frame_hand]


hand = vis_list_
o3d.visualization.draw_geometries(hand)



vis_list = vis_list + hand 
o3d.visualization.draw_geometries(vis_list)

</code></pre>
",5/7/2023 13:34,,74,0,0,0,,13295077.0,,4/12/2020 16:16,29.0,,,,,,,
1524,12994888,Path Finding - Autonomous Rover,|infinite-loop|dijkstra|robot|,"<p>I am working on a navigation task for an autonomous rover. Right now, the rover can calculate the shortest path between the current position and a final destination given certain obstacles. I am using dijkstra's algorithm to find the shortest path and it's working well. </p>

<p>The rover has a fixed range with which it can identify that there is an obstacle infront of it or not. The problem I am facing is that the rover gets stuck in an infinite loop of same path (from point A to B, then point B to A) when an the final destination happens to be on a region that cannot be reached or seen by the vision of the rover. </p>

<p>My question is how should I detect that I am stuck in this loop and I can't reach the destination now that I should get a new final destination or just quit. </p>
",10/21/2012 4:06,,180,1,1,0,,1473690.0,,6/22/2012 1:47,33.0,13815930.0,"<p>It depends on your application, if the destination cannot be reached you can detect this checking that all the reachable places in your map are visited and the destination is not one of them. Otherwise if you just missed some possible path, once you detect that you are stuck you can try to change your rover behavior. For example trying to go close to walls with sensors (if you have one) pointing to it.</p>
",1058082.0,0.0,0.0,17668059.0,The problem with the infinite loop is not clear (to me). Would you mind explaining it a bit more (preferable with an example).
3842,63516972,how to convert coordinates from azimuth/altitude to tilt-tilt,|robotics|astronomy|kinematics|,"<p>I'm trying to build a lightweight antenna tracker with two servos. For mechanical reasons, I'm first mounting servo1 on a base so that it tilts forward/backwards, then mount servo2 on it twisted 90˚so it can tilt left/right.</p>
<p>I can basically use the first servo to select a from the great circles that go through azimuth=0˚ and alt=0˚ and az=180˚ and alt=0˚, and use hte 2nd servo to move on the chosen great circle. This way, I should be able to point at the entire upper hemisphere even though I might need to reposition the antenna when crossing the midline (the servos only have 180 degrees of movement.)</p>
<p>I'm trying to find the function that maps az/alt to the tilt/tilt servo angles. I suspect that should be similar to how equatorial telescope mounts work, but I couldn't find a good reference on how to do it - neither do I trust my own math.</p>
<p>I found this astronomy lecture notes vaguely helpful: <a href=""http://ircamera.as.arizona.edu/Astr_518/ametry.pdf"" rel=""nofollow noreferrer"">http://ircamera.as.arizona.edu/Astr_518/ametry.pdf</a> especially page 22/23 on the ecliptic coordinate system, but I think the problem solved here is slightly different.</p>
<p>This seems a standard kinematics problem, it bothers me I can't figure it out or even find online resources. I'd be super thankful for any pointers. Happy to give more details on the servo setup.</p>
",8/21/2020 5:25,,690,1,0,-1,,12835990.0,,2/4/2020 0:56,54.0,63571545.0,"<p>I think I figured this out on math.stackexchange.com:</p>
<p><a href=""https://math.stackexchange.com/questions/3799191/direct-conversion-from-az-el-to-ecliptic-coordinates"">https://math.stackexchange.com/questions/3799191/direct-conversion-from-az-el-to-ecliptic-coordinates</a></p>
<p>Short answer (for python, consider parameter order for atan2 in your language):</p>
<p>$\epsilon=\atan2(\sin\delta, \cos\delta\cdot\sin\alpha)$</p>
<p>$\lambda = \arccos(\cos\alpha\cdot\cos\delta)$</p>
<p>where $\alpha$ is azimuth, $\delta$ is elevation or altitude, $epsilon$ is the angle for the first and $lambda$ the angle for the 2nd servo. This seems to work for all values of $\alpha$ and for values in $[0,\pi/2]$ for $\delta$.</p>
",12835990.0,0.0,0.0,,
4619,76481567,How could I expose rqt_graph to tcp port?,|python|c++|ros|robotics|ros2|,"<p>I wrote a project to control a vehicle with ros2. I access this vehicle through ssh and write code directly from there.</p>
<p>Clearly, there is no GUI and I would like to expose the result of rqt_graph on a tcp port so I can see it. Is there any way?</p>
<p>I've tried to use <a href=""https://github.com/dheera/rosboard/"" rel=""nofollow noreferrer"">Rosboard</a> but I didn't find anything to view graph like rqt_graph.</p>
",6/15/2023 10:26,,37,0,0,0,,15496345.0,,3/27/2021 20:23,4.0,,,,,,,
607,3069144,Finding distance travelled by robot using Optical Flow,|localization|opencv|computer-vision|robotics|opticalflow|,"<p>I'm working on a project right now in which we are developing an autonomous robot. I have to basically find out the distance travelled by the robot between any 2 intervals. I'm using OpenCV, and using the <em>Optical Flow functions</em> of OpenCV, I'm able to find out the velocity/distance of each pixel in 2 different images. Using this information, I want to be able to find out the distance travelled by the robot in the interval between those 2 images.</p>

<p>I thought of a way in which we could develop an input output mapping between the distance travelled by pixels and the distance travelled by the bot (using some tests). In this way, using neural networks, we would be able to find the relationship. However, the optical flow would depend on the distance of the camera from the pixel, which would cause problems.</p>

<p>Is there any way to solve this problem?</p>
",6/18/2010 11:19,,3513,4,0,1,0.0,280454.0,India,2/24/2010 15:38,573.0,3070004.0,"<p>I hope you do end up accepting answers you received in the past. Anyway, I had posted the solution to this problem on SO (in OpenCV) a while back, so here it is:</p>

<p><a href=""https://stackoverflow.com/questions/2135116/how-can-i-determine-distance-from-an-object-in-a-video/2152097#2152097"">How can I determine distance from an object in a video?</a></p>
",71131.0,3.0,2.0,,
2950,47165908,Running python script in the background - Raspbian,|python|background-process|raspberry-pi3|robotics|,"<p>I have recently constructed a small robot car using my raspberry pi 3 with raspbian stretch. I am currently at the point where I can drive the car around using an xbox360 controller plugged into the pi USB port. My issue is that I do not like having to ssh into the pi and run the script I have written in order to use the robot (robot.py).</p>

<p>What I would like to do is have a script running in the background which works something like this:</p>

<pre><code>#while true
    #if joystick is detected
        #robotoffflag=true
        #if startup button is pressed on the joystick &amp;&amp; robotoffflag
            #robotoffflag=false
            #run robot.py to control the robot
            #robot.py already has controls for shutting down the robot
            #which disables the motors and sets robotoffflag to true
</code></pre>

<p>This way, if my pi is powered up, I can at any point just press a button on my controller and start using the robot. Then when I am done, press another button on the controller to disable the robot.</p>

<p>My question is, am I going about this in the right way? Having a script running in the background which is basically an infinite loop with conditionals inside seems silly to me. Would it be better to have something more event driven?</p>
",11/7/2017 19:22,,555,1,0,0,,8088724.0,,5/30/2017 21:21,13.0,47166379.0,"<p>Here is something I might do, thought a bit of a hack.  Put the python code in to a loop, adding a very short sleep at the end of each loop to limit cpu usage.  Then run the python script inside a session of tmux, detached of course.  The python script will then run until the tmux session is killed, even if your ssh session exits.  tmux is just an example utility, by the way.  There are others.
Of course you could get fancy and implement the loop within a signal-and-wait mechanism, which would wait for a signal from your controller, then launch that from an operating system daemon.  Depends how deep you want to go ...</p>
",2363348.0,0.0,0.0,,
4566,75963064,How to calculate the most likely mobile robot position based on the distance to nearby objects (walls) which have a known position?,|orientation|robotics|least-squares|,"<p>I got the distance and angle to the robot from the  detected obstacles (walls) from 2D LiDAR while robot is moving. So now I would like to calculate the most likely robot  pose based on the distance and angle to nearby objects (walls) which have a known position. So if I use the least squares estimator I can get the position of the robot but then how to get the orientation?</p>
<p>For example this gives me the position <a href=""https://www.th-luebeck.de/fileadmin/media_cosa/Dateien/Veroeffentlichungen/Sammlung/TR-2-2015-least-sqaures-with-ToA.pdf"" rel=""nofollow noreferrer"">robot position estimator using least square method</a> but then how to get the orientation? Any help?</p>
",4/8/2023 2:40,,47,0,0,0,,10319366.0,Singapore,9/5/2018 9:27,109.0,,,,,,,
3846,63705598,unity3d cant open port,|c#|unity-game-engine|robotics|,"<p>I'm trying to control my Dynamixel AX-12+ servo using unity3d, using the dynamixel SDK (C#). the servo is connected to my windows 10 pc using a u2d2 (not an Arduino!) and it works fine using visual studio. I imported the dll to unity and wrote a script for controlling the servomotor and there are no errors in the script but when I try to run it, it fails to open the port and after that, the visual studio code also fails to open the port (until i disconnect and reconnect the USB).</p>
<p>the part of the code trying to open the port:</p>
<pre><code>  // Open port (COM9)
  if (dynamixel.openPort(port_num))
  {
    Debug.Log(&quot;Succeeded to open the port!&quot;);
  }
  else
  {
    Debug.Log(&quot;Failed to open the port!&quot;);
    
  }
</code></pre>
<p>i used the example SDK codes to see if the motor works correctly with visual studio <a href=""https://emanual.robotis.com/docs/en/software/dynamixel/dynamixel_sdk/sample_code/csharp_read_write_protocol_1_0/#csharp-read-write-protocol-10"" rel=""nofollow noreferrer"">(like this one)</a></p>
<p>a complete unity script :</p>
<pre><code>    using System;
using System.Runtime.InteropServices;
using System.Collections;
using System.Collections.Generic;
using UnityEngine;

namespace dynamixelunity {
    
    //further below you can find the &quot;DynamixelObject&quot; class.
    public class dynamixel : MonoBehaviour
      {
        const string dll_path = &quot;dxl_x64_c&quot;;

        #region PortHandler
        [DllImport(dll_path)]
        public static extern int    portHandler         (string port_name);

        [DllImport(dll_path)]
        public static extern bool   openPort            (int port_num);
        [DllImport(dll_path)]
        public static extern void   closePort           (int port_num);
        [DllImport(dll_path)]
        public static extern void   clearPort           (int port_num);

        [DllImport(dll_path)]
        public static extern void   setPortName         (int port_num, string port_name);
        [DllImport(dll_path)]
        public static extern string getPortName         (int port_num);

        [DllImport(dll_path)]
        public static extern bool   setBaudRate         (int port_num, int baudrate);
        [DllImport(dll_path)]
        public static extern int    getBaudRate         (int port_num);

        [DllImport(dll_path)]
        public static extern int    readPort            (int port_num, byte[] packet, int length);
        [DllImport(dll_path)]
        public static extern int    writePort           (int port_num, byte[] packet, int length);

        [DllImport(dll_path)]
        public static extern void   setPacketTimeout    (int port_num, UInt16 packet_length);
        [DllImport(dll_path)]
        public static extern void   setPacketTimeoutMSec(int port_num, double msec);
        [DllImport(dll_path)]
        public static extern bool   isPacketTimeout     (int port_num);
        #endregion

        #region PacketHandler
        [DllImport(dll_path)]
        public static extern void   packetHandler       ();

        [DllImport(dll_path)]
        public static extern IntPtr getTxRxResult       (int protocol_version, int result);
        [DllImport(dll_path)]
        public static extern IntPtr getRxPacketError    (int protocol_version, byte error);

        [DllImport(dll_path)]
        public static extern int    getLastTxRxResult   (int port_num, int protocol_version);
        [DllImport(dll_path)]
        public static extern byte   getLastRxPacketError(int port_num, int protocol_version);

        [DllImport(dll_path)]
        public static extern void   setDataWrite        (int port_num, int protocol_version, UInt16 data_length, UInt16 data_pos, UInt32 data);
        [DllImport(dll_path)]
        public static extern UInt32 getDataRead         (int port_num, int protocol_version, UInt16 data_length, UInt16 data_pos);

        [DllImport(dll_path)]
        public static extern void   txPacket            (int port_num, int protocol_version);

        [DllImport(dll_path)]
        public static extern void   rxPacket            (int port_num, int protocol_version);

        [DllImport(dll_path)]
        public static extern void   txRxPacket          (int port_num, int protocol_version);

        [DllImport(dll_path)]
        public static extern void   ping                (int port_num, int protocol_version, byte id);

        [DllImport(dll_path)]
        public static extern UInt16 pingGetModelNum     (int port_num, int protocol_version, byte id);

        [DllImport(dll_path)]
        public static extern void   broadcastPing       (int port_num, int protocol_version);
        [DllImport(dll_path)]
        public static extern bool   getBroadcastPingResult(int port_num, int protocol_version, int id);

        [DllImport(dll_path)]
        public static extern void   reboot              (int port_num, int protocol_version, byte id);

        [DllImport(dll_path)]
        public static extern void   factoryReset        (int port_num, int protocol_version, byte id, byte option);

        [DllImport(dll_path)]
        public static extern void   readTx              (int port_num, int protocol_version, byte id, UInt16 address, UInt16 length);
        [DllImport(dll_path)]
        public static extern void   readRx              (int port_num, int protocol_version, UInt16 length);
        [DllImport(dll_path)]
        public static extern void   readTxRx            (int port_num, int protocol_version, byte id, UInt16 address, UInt16 length);

        [DllImport(dll_path)]
        public static extern void   read1ByteTx         (int port_num, int protocol_version, byte id, UInt16 address);
        [DllImport(dll_path)]
        public static extern byte   read1ByteRx         (int port_num, int protocol_version);
        [DllImport(dll_path)]
        public static extern byte   read1ByteTxRx       (int port_num, int protocol_version, byte id, UInt16 address);

        [DllImport(dll_path)]
        public static extern void   read2ByteTx         (int port_num, int protocol_version, byte id, UInt16 address);
        [DllImport(dll_path)]
        public static extern UInt16 read2ByteRx         (int port_num, int protocol_version);
        [DllImport(dll_path)]
        public static extern UInt16 read2ByteTxRx       (int port_num, int protocol_version, byte id, UInt16 address);

        [DllImport(dll_path)]
        public static extern void   read4ByteTx         (int port_num, int protocol_version, byte id, UInt16 address);
        [DllImport(dll_path)]
        public static extern UInt32 read4ByteRx         (int port_num, int protocol_version);
        [DllImport(dll_path)]
        public static extern UInt32 read4ByteTxRx       (int port_num, int protocol_version, byte id, UInt16 address);

        [DllImport(dll_path)]
        public static extern void   writeTxOnly         (int port_num, int protocol_version, byte id, UInt16 address, UInt16 length);
        [DllImport(dll_path)]
        public static extern void   writeTxRx           (int port_num, int protocol_version, byte id, UInt16 address, UInt16 length);

        [DllImport(dll_path)]
        public static extern void   write1ByteTxOnly    (int port_num, int protocol_version, byte id, UInt16 address, byte data);
        [DllImport(dll_path)]
        public static extern void   write1ByteTxRx      (int port_num, int protocol_version, byte id, UInt16 address, byte data);

        [DllImport(dll_path)]
        public static extern void   write2ByteTxOnly    (int port_num, int protocol_version, byte id, UInt16 address, UInt16 data);
        [DllImport(dll_path)]
        public static extern void   write2ByteTxRx      (int port_num, int protocol_version, byte id, UInt16 address, UInt16 data);

        [DllImport(dll_path)]
        public static extern void   write4ByteTxOnly    (int port_num, int protocol_version, byte id, UInt16 address, UInt32 data);
        [DllImport(dll_path)]
        public static extern void   write4ByteTxRx      (int port_num, int protocol_version, byte id, UInt16 address, UInt32 data);

        [DllImport(dll_path)]
        public static extern void   regWriteTxOnly      (int port_num, int protocol_version, byte id, UInt16 address, UInt16 length);
        [DllImport(dll_path)]
        public static extern void   regWriteTxRx        (int port_num, int protocol_version, byte id, UInt16 address, UInt16 length);

        [DllImport(dll_path)]
        public static extern void   syncReadTx          (int port_num, int protocol_version, UInt16 start_address, UInt16 data_length, UInt16 param_length);
        // syncReadRx   -&gt; GroupSyncRead
        // syncReadTxRx -&gt; GroupSyncRead

        [DllImport(dll_path)]
        public static extern void   syncWriteTxOnly     (int port_num, int protocol_version, UInt16 start_address, UInt16 data_length, UInt16 param_length);

        [DllImport(dll_path)]
        public static extern void   bulkReadTx          (int port_num, int protocol_version, UInt16 param_length);
        // bulkReadRx   -&gt; GroupBulkRead
        // bulkReadTxRx -&gt; GroupBulkRead

        [DllImport(dll_path)]
        public static extern void   bulkWriteTxOnly     (int port_num, int protocol_version, UInt16 param_length);
        #endregion

        #region GroupBulkRead
        [DllImport(dll_path)]
        public static extern int    groupBulkRead       (int port_num, int protocol_version);

        [DllImport(dll_path)]
        public static extern bool   groupBulkReadAddParam   (int group_num, byte id, UInt16 start_address, UInt16 data_length);
        [DllImport(dll_path)]
        public static extern void   groupBulkReadRemoveParam(int group_num, byte id);
        [DllImport(dll_path)]
        public static extern void   groupBulkReadClearParam (int group_num);

        [DllImport(dll_path)]
        public static extern void   groupBulkReadTxPacket   (int group_num);
        [DllImport(dll_path)]
        public static extern void   groupBulkReadRxPacket   (int group_num);
        [DllImport(dll_path)]
        public static extern void   groupBulkReadTxRxPacket (int group_num);

        [DllImport(dll_path)]
        public static extern bool   groupBulkReadIsAvailable(int group_num, byte id, UInt16 address, UInt16 data_length);
        [DllImport(dll_path)]
        public static extern UInt32 groupBulkReadGetData    (int group_num, byte id, UInt16 address, UInt16 data_length);
        #endregion

        #region GroupBulkWrite
        [DllImport(dll_path)]
        public static extern int    groupBulkWrite            (int port_num, int protocol_version);

        [DllImport(dll_path)]
        public static extern bool   groupBulkWriteAddParam    (int group_num, byte id, UInt16 start_address, UInt16 data_length, UInt32 data, UInt16 input_length);
        [DllImport(dll_path)]
        public static extern void   groupBulkWriteRemoveParam (int group_num, byte id);
        [DllImport(dll_path)]
        public static extern bool   groupBulkWriteChangeParam (int group_num, byte id, UInt16 start_address, UInt16 data_length, UInt32 data, UInt16 input_length, UInt16 data_pos);
        [DllImport(dll_path)]
        public static extern void   groupBulkWriteClearParam  (int group_num);

        [DllImport(dll_path)]
        public static extern void   groupBulkWriteTxPacket    (int group_num);
        #endregion

        #region GroupSyncRead
        [DllImport(dll_path)]
        public static extern int    groupSyncRead             (int port_num, int protocol_version, UInt16 start_address, UInt16 data_length);

        [DllImport(dll_path)]
        public static extern bool   groupSyncReadAddParam     (int group_num, byte id);
        [DllImport(dll_path)]
        public static extern void   groupSyncReadRemoveParam  (int group_num, byte id);
        [DllImport(dll_path)]
        public static extern void   groupSyncReadClearParam   (int group_num);

        [DllImport(dll_path)]
        public static extern void   groupSyncReadTxPacket     (int group_num);
        [DllImport(dll_path)]
        public static extern void   groupSyncReadRxPacket     (int group_num);
        [DllImport(dll_path)]
        public static extern void   groupSyncReadTxRxPacket   (int group_num);

        [DllImport(dll_path)]
        public static extern bool   groupSyncReadIsAvailable  (int group_num, byte id, UInt16 address, UInt16 data_length);
        [DllImport(dll_path)]
        public static extern UInt32 groupSyncReadGetData      (int group_num, byte id, UInt16 address, UInt16 data_length);
        #endregion

        #region GroupSyncWrite
        [DllImport(dll_path)]
        public static extern int    groupSyncWrite            (int port_num, int protocol_version, UInt16 start_address, UInt16 data_length);

        [DllImport(dll_path)]
        public static extern bool   groupSyncWriteAddParam    (int group_num, byte id, UInt32 data, UInt16 data_length);
        [DllImport(dll_path)]
        public static extern void   groupSyncWriteRemoveParam (int group_num, byte id);
        [DllImport(dll_path)]
        public static extern bool   groupSyncWriteChangeParam (int group_num, byte id, UInt32 data, UInt16 data_length, UInt16 data_pos);
        [DllImport(dll_path)]
        public static extern void   groupSyncWriteClearParam  (int group_num);

        [DllImport(dll_path)]
        public static extern void   groupSyncWriteTxPacket    (int group_num);
        #endregion
      }



    public class DynamixelObject : MonoBehaviour {

        // Control table address
        public const int ADDR_MX_TORQUE_ENABLE           = 24;                  // Control table address is different in Dynamixel model
        public const int ADDR_MX_GOAL_POSITION           = 30;
        public const int ADDR_MX_PRESENT_POSITION        = 36;

        // Protocol version
        public const int PROTOCOL_VERSION                = 1;                   // See which protocol version is used in the Dynamixel

        // Default setting
        public const int DXL_ID                          = 1;                   // Dynamixel ID: 1 
        public const int BAUDRATE                        = 1000000;
        public const string DEVICENAME                   = &quot;COM9&quot;;              // Check which port is being used on your controller
                                                                                // ex) Windows: &quot;COM1&quot;   Linux: &quot;/dev/ttyUSB0&quot; Mac: &quot;/dev/tty.usbserial-*&quot;

        public const int TORQUE_ENABLE                   = 1;                   // Value for enabling the torque
        public const int TORQUE_DISABLE                  = 0;                   // Value for disabling the torque
        public const int DXL_MINIMUM_POSITION_VALUE      = 100;                 // Dynamixel will rotate between this value
        public const int DXL_MAXIMUM_POSITION_VALUE      = 4000;                // and this value (note that the Dynamixel would not move when the position value is out of movable range. s
        public const int DXL_MOVING_STATUS_THRESHOLD     = 10;                  // Dynamixel moving status threshold

        public const byte ESC_ASCII_VALUE                = 0x1b;

        public const int COMM_SUCCESS                    = 0;                   // Communication Success result value
        public const int COMM_TX_FAIL                    = -1001;               // Communication Tx Failed

        // Initialize PortHandler Structs
          // Set the port path
          // Get methods and members of PortHandlerLinux or PortHandlerWindows
          
        int port_num = dynamixel.portHandler(DEVICENAME);
        
        void start(){
            

          // Initialize PacketHandler Structs
          dynamixel.packetHandler();

          int index = 0;
          int dxl_comm_result = COMM_TX_FAIL;                                   // Communication result
          UInt16[] dxl_goal_position = new UInt16[2]{ DXL_MINIMUM_POSITION_VALUE, DXL_MAXIMUM_POSITION_VALUE };         // Goal position

          byte dxl_error = 0;                                                   // Dynamixel error
          UInt16 dxl_present_position = 0;                                      // Present position

          // Open port (COM9)
          if (dynamixel.openPort(port_num))
          {
            Debug.Log(&quot;Succeeded to open the port!&quot;);
          }
          else
          {
            Debug.Log(&quot;Failed to open the port!&quot;);

          }

          // Set port baudrate
          if (dynamixel.setBaudRate(port_num, BAUDRATE))
          {
            Debug.Log(&quot;Succeeded to change the baudrate!&quot;);
          }
          else
          {
            Debug.Log(&quot;Failed to change the baudrate!&quot;);
          }

        }
        
        void Update()
        {
            if (Input.GetKeyDown(KeyCode.Space))
            {
                //Enable motor torque
                dynamixel.write1ByteTxRx(port_num, PROTOCOL_VERSION, DXL_ID, ADDR_MX_TORQUE_ENABLE, TORQUE_ENABLE);
            }

            if (Input.GetKeyUp(KeyCode.Space))
            {
                //disable motor torque
                dynamixel.write1ByteTxRx(port_num, PROTOCOL_VERSION, DXL_ID, ADDR_MX_TORQUE_ENABLE, TORQUE_DISABLE);
            }
        }
    }
}
</code></pre>
<p>Does anyone have any suggestions?</p>
",9/2/2020 12:31,63814395.0,319,1,7,0,,11782176.0,,7/14/2019 9:35,2.0,63814395.0,"<p>Ok, I found out what was wrong!
I forgot to close the port after exiting unity's play mode.
the code below closes the port after exiting play mode:</p>
<pre><code>    [InitializeOnLoad]
public static class PlayStateNotifier
{

    static PlayStateNotifier()
    {
        EditorApplication.playModeStateChanged += ModeChanged;
    }

    static void ModeChanged(PlayModeStateChange playModeState)
    {
        if (playModeState == PlayModeStateChange.EnteredEditMode)
        {
            Debug.Log(&quot;Entered Edit mode.&quot;);
            Debug.Log(&quot;Closing dynamixel port! :D&quot;);
            dynamixel.closePort(DynamixelObject.port_num);
        }
    }
}
</code></pre>
",11782176.0,0.0,0.0,112710303.0,"Did you see following on link : USB2DYNAMIXELs purchased before July 2015 may not work with the latest FTDI drivers 2.12.00. For a fix, please see our blog post on the matter"
3394,56488829,Looking for solution how to exit the loop at the right place,|c#|robotics|,"<p>Okay guys so first I want to find three freePositions, one is sortedSystem.BooleanCellSystem[row][column] and than go back and change the row, my question is where I should put {break;} So I can Have 3 freePosition with different row. Thank you :) </p>

<pre><code>public void FindingFreeCells(SortingSystem sortedSystem, int number)
{
  int freePositions = 0;
  double tempCoef = 0;

  for (int row = 0; row &lt; sortedSystem.BooleanCellSystem.Length; row++)
  {
    for (int column = 0; column &lt; sortedSystem.BooleanCellSystem[row].Length; column++)
    {
      if (sortedSystem.BooleanCellSystem[row][column] != true)
      {
        for (int n = column; n &lt; number; n++)
        {
          if (sortedSystem.BooleanCellSystem[row][column] == true)
          {
            break;
          }
          for (int k = 1; k &lt; 3; k++)
          {
            if (sortedSystem.BooleanCellSystem[row][column + k] != true)
            {
              tempCoef += 5;
            }
          }
          if (n == number - 1)
          {
            freePositions++;
            AvailableCell temp = new AvailableCell();
            temp.FirstRow = row;
            temp.FirstColumn = column;
            temp.CellsRequired = number;
            temp.PriorityCoeff = row + column + tempCoef;
            AvailableCellsList.Add(temp);
          }
        }
      }               
    }
  }
}
</code></pre>
",6/7/2019 6:11,,60,0,9,0,,,,,,,,,,,99567430.0,"Can you help with the implementation of that structure, how it should looks."
1218,6641055,Obstacle avoidance with stereo vision,|opencv|computer-vision|robotics|,"<p>I'm working on a stereo-camera based obstacle avoidance system for a mobile robot. It'll be used indoors, so I'm working off the assumption that the ground plane is flat. We also get to design our own environment, so I can avoid specific types of obstacle that generate false positives or negatives.</p>

<p>I've already found plenty of resources for calibrating the cameras and getting the images lined up, as well as information on generating a disparity map/depth map. What I'm struggling with is techniques for detecting obstacles from this. A technique that instead worked by detecting the ground plane would be just as useful.</p>

<p>I'm working with openCV, and using the book Learning OpenCV as a reference.</p>

<p>Thanks, all</p>
",7/10/2011 12:20,6643158.0,6051,1,0,8,0.0,482404.0,"Worcester, MA",10/21/2010 0:14,89.0,6643158.0,"<p>From the literature I've read, there are three main approaches:</p>

<ol>
<li><p><strong>Ground plane approaches</strong> determine the ground plane from the stereo data and assume that all points that are not on the plane are obstacles. If you assume that the ground is the dominant plane in the image, then you may be able to find it simply a plane to the reconstructed point cloud using a robust model-fitting algorithm (such as RANSAC).</p></li>
<li><p><strong>Disparity map approaches</strong> skip converting the stereo output to a point cloud. The most popular algorithms I've seen are called v-disparity and uv-disparity. Both look for the same attributes in the disparity map, but uv-disparity can detect some types of obstacles that v-disparity alone cannot.</p></li>
<li><p><strong>Point cloud approaches</strong> project the disparity map into a three-dimensional point cloud and process those points. One example is ""inverted cone algorithm"" that uses a minimum obstacle height, maximum obstacle height, and maximum ground inclination to detect obstacles on arbitrary, non-flat, terrain.</p></li>
</ol>

<p>Of these three approaches, detecting the ground-plane is the simplest and least reliable. If your environment has sparse obstacles and a textured ground, it should be sufficient. I don't have much experience with disparity-map approaches, but the results look very promising. Finally, the Manduchi algorithm works extremely well under the widest range of conditions, including on uneven terrain. Unfortunately, it is very difficult to implement and is extremely computationally expensive.</p>

<p><strong>References:</strong></p>

<ul>
<li><strong>v-Disparity</strong>: Labayrade, R. and Aubert, D. and Tarel, J.P. Real time obstacle detection in stereovision on non flat road geometry through v-disparity representation</li>
<li><strong>uv-Disparity</strong>: Hu, Z. and Uchimura, K. UV-disparity: an efficient algorithm for stereovision based scene analysis</li>
<li><strong>Inverted Cone Algorithm:</strong> Manduchi, R. and Castano, A. and Talukder, A. and Matthies, L. Obstacle detection and terrain classification for autonomous off-road navigation</li>
</ul>

<p>There are a few papers on ground-plane obstacle detection algorithms, but I don't know of a good one off the top of my head. If you just need a starting point, you can read about my implementation for a recent project in Section 4.2.3 and Section 4.3.4 of <a href=""http://mkoval.org/downloads/projects/igvc/igvc_design.pdf"" rel=""noreferrer"">this design report</a>. There was not enough space to discuss the full implementation, but it does address some of the problems you might encounter.</p>
",111426.0,12.0,0.0,,
2680,42355760,Get centroid of point cloud data based on color using kinect v2 in ROS,|kinect|ros|point-cloud-library|robotics|kinect-v2|,"<p>I want to get the centroid of point cloud data based on color using kinect v2. Even after searching for a long time I was not able to find a package which can do this task. But since this is a general problem, I think there should be a existing package. </p>

<p>Please help. Thanks in advance!</p>
",2/20/2017 23:02,42371628.0,730,1,0,1,,5893603.0,United States,2/6/2016 23:46,48.0,42371628.0,"<p>If you are using PCL you can do </p>

<pre><code>pcl::PointXYZRGB centroid;
pcl::computeCentroid(*cloud, centroid);
</code></pre>

<p>Otherwise it is just the average of the points. For example:</p>

<pre><code>                pcl::PointXYZI centroid;

                float x = 0, y = 0, z = 0;
                for (int k = 0; k &lt; cloud-&gt;size(); k++)
                {
                   x += cloud-&gt;at(k).x;
                   y += cloud-&gt;at(k).y;
                   z += cloud-&gt;at(k).z;
                }
                centroid.x = x / (cloud-&gt;size() + 0.0);
                centroid.y = y / (cloud-&gt;size() + 0.0);
                centroid.z = z / (cloud-&gt;size() + 0.0);
</code></pre>
",6674213.0,2.0,0.0,,
1353,10075285,vehicle's obstacle avoidance sensor,|c|microcontroller|sensors|robotics|,"<p>I'm new in robotics and I would like to make a vehicle in order to play with my 3 years-old son. I have a PIC 16F917 microcontroller and a Half H bridge L293DNE. What I've tried so far in order to make the vehicle move is s C code guided by the following link: <a href=""http://www.google.gr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CCQQFjAA&amp;url=http%3A%2F%2Fwww.societyofrobots.com%2Fmember_tutorials%2Ffiles%2FLukas%2520PIC%2520Tutorial.doc&amp;ei=8tWCT4GeNZD6sgaQmYzRBA&amp;usg=AFQjCNFw6ZoQSyoKwRj3uPaLmchgBzGY4Q&amp;sig2=tLQ7VN9IJen-TXCznu4jLA"" rel=""nofollow"">http://www.google.gr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CCQQFjAA&amp;url=http%3A%2F%2Fwww.societyofrobots.com%2Fmember_tutorials%2Ffiles%2FLukas%2520PIC%2520Tutorial.doc&amp;ei=8tWCT4GeNZD6sgaQmYzRBA&amp;usg=AFQjCNFw6ZoQSyoKwRj3uPaLmchgBzGY4Q&amp;sig2=tLQ7VN9IJen-TXCznu4jLA</a>. So I was able to make the robot move forward, backward and turn left and right. What I want to do now is to find suitable sensors for obstacle avoidance. Since I have no previous experience could someone recommend the appropriate  compatible with the microcontroller sensors? If so, I would like to know in which pins of microcontroller I should connect the sensors and moreover the suitable programming order in C , for the sensors to interact with the microcontroller.</p>

<p>Thanks in advance!!!</p>
",4/9/2012 15:09,,849,1,3,2,0.0,582964.0,,1/20/2011 12:46,18.0,12331464.0,"<p>The sparkfun link in a previous comment is a good place to get sensors.  </p>

<p>Ultrasonic sensors are good for rough estimations of distance.  They have a wide beam, and the results are noisy.  Sharp IR sensors have a narrower beam and are a little more accurate.  And for very close range, (and very cheap), you can simply use an IR emitter and detector.  Typically, the IR is pulsed so that you can measure the difference between ambient IR lighting and when the pulse is on.  (Search for ""infrared proximity detector with a 555 timer"", or something along those lines).</p>
",159595.0,1.0,0.0,16551937.0,Please checkout and contribute to the new robotics stackexchange forum: http://area51.stackexchange.com/proposals/40020/robotics
4574,76058435,Trouble describing a robot's joint origin/axis using URDF and Pybullet,|python|robot|kinematics|pybullet|urdf|,"<p>I have created a complete URDF for my robot in PyBullet, but I'm having trouble getting the joints to rotate correctly. Specifically, when I try to move a joint, it doesn't rotate about the correct coordinate, and the link separates from the body of the robot.</p>
<p>Initially, I kept the joint origin and axis at their default values in the URDF and did not move any joints, which resulted in the robot being in a correct and complete static orientation. However, when I try to make a joint move in pybullet, the joint does not rotate about the correct point, and the link separates from the parent body of the robot.</p>
<p>I have already created reference axes for the servos in my SolidWorks files and attempted to enter this information into the URDF as the joint origin parameter. However, this approach has not resolved the issue.</p>
<p>I suspect that the problem lies in my translation between the parent origin coordinate, joint origin coordinate, and child origin coordinate. I need help in determining the correct joint origin/axis parameter that will allow my joints to rotate properly around the correct vector.</p>
<p>Can anyone offer any advice or suggestions on how to properly determine the joint origin parameter for the joints in my URDF and resolve this issue?</p>
<p>Python code:</p>
<pre><code>import pybullet as p
import time
import pybullet_data
import numpy as np

physicsClient = p.connect(p.GUI)#or p.DIRECT for non-graphical version
p.setAdditionalSearchPath(pybullet_data.getDataPath()) #optionally
p.setGravity(0,0,0)
groundId = p.loadURDF(&quot;plane.urdf&quot;)
robotStartPos = [0,0,.25]
robotStartOrientation = p.getQuaternionFromEuler([1.57,0,0])

urdf = &quot;C:\\Users\\danie\\OneDrive\\Desktop\\Robot_Simulation\\myrobot.urdf&quot; #change to urdf file path
robotId = p.loadURDF(urdf,robotStartPos, robotStartOrientation)

jointIndex = 2 # first joint is number 0
for i in range (10000):
    p.stepSimulation()
    p.setJointMotorControl2(robotId, jointIndex, controlMode = p.POSITION_CONTROL,  targetPosition=0.8+0.6*np.sin(i*0.01))
    time.sleep(1./240.)
    
robotPos, robotOrn = p.getBasePositionAndOrientation(robotId)
print(robotPos, robotOrn)
p.disconnect()
</code></pre>
<p>URDF File:</p>
<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;robot name=&quot;myrobot&quot;&gt;
    &lt;link name=&quot;base&quot;&gt;
        &lt;visual&gt;
            &lt;geometry&gt;
                &lt;mesh filename=&quot;base.stl&quot; scale=&quot;0.001 0.001 0.001&quot;/&gt;
            &lt;/geometry&gt;
            &lt;material name = &quot;Cyan&quot;&gt;
                &lt;color rgba=&quot;0 1.0 1.0 1.0&quot;/&gt;
            &lt;/material&gt;
        &lt;/visual&gt;
        &lt;inertial&gt;
            &lt;mass value=&quot;0.845&quot;/&gt;
            &lt;origin xyz=&quot;-.036 .058 -.123&quot;/&gt;
            &lt;inertia ixx=&quot;.0018&quot;  ixy=&quot;-.00177&quot;  ixz=&quot;0.00374&quot; iyy=&quot;.0156&quot; iyz=&quot;-.006&quot; izz=&quot;.00526&quot; /&gt; 
        &lt;/inertial&gt;
    &lt;/link&gt;

    &lt;link name=&quot;hip_left&quot;&gt;
        &lt;visual&gt;
            &lt;geometry&gt;
                &lt;mesh filename=&quot;hip_left.stl&quot; scale=&quot;0.001 0.001 0.001&quot;/&gt;
            &lt;/geometry&gt;
            &lt;material name = &quot;Orange&quot;&gt;
                &lt;color rgba=&quot;1.0 0.41 0.0 1.0&quot;/&gt;
            &lt;/material&gt;
        &lt;/visual&gt;
        &lt;inertial&gt;
            &lt;mass value=&quot;0.0588&quot;/&gt;
            &lt;origin xyz=&quot;-.036 .028 -.222&quot;/&gt;
            &lt;inertia ixx=&quot;.03&quot;  ixy=&quot;-0.00006&quot;  ixz=&quot;0.0004&quot; iyy=&quot;.002&quot; iyz=&quot;-0.000371&quot; izz=&quot;.000175&quot; /&gt; 
        &lt;/inertial&gt;
    &lt;/link&gt;
    
    &lt;link name=&quot;hip_right&quot;&gt;
        &lt;visual&gt;
            &lt;geometry&gt;
                &lt;mesh filename=&quot;hip_right.stl&quot; scale=&quot;0.001 0.001 0.001&quot;/&gt;
            &lt;/geometry&gt;
            &lt;material name = &quot;Orange&quot;&gt;
                &lt;color rgba=&quot;1.0 0.41 0.0 1.0&quot;/&gt;
            &lt;/material&gt;
        &lt;/visual&gt;
        &lt;inertial&gt;
            &lt;mass value=&quot;0.0588&quot;/&gt;
            &lt;origin xyz=&quot;-.036 .029 -.254&quot;/&gt;
            &lt;inertia ixx=&quot;.03&quot;  ixy=&quot;-0.00006&quot;  ixz=&quot;0.0004&quot; iyy=&quot;.002&quot; iyz=&quot;-0.000371&quot; izz=&quot;.000175&quot; /&gt; 
        &lt;/inertial&gt;
    &lt;/link&gt;

    &lt;link name=&quot;upper_leg_left&quot;&gt;
        &lt;visual&gt;
            &lt;geometry&gt;
                &lt;mesh filename=&quot;upper_leg_left.stl&quot; scale=&quot;0.001 0.001 0.001&quot;/&gt;
            &lt;/geometry&gt;
            &lt;material name = &quot;Green&quot;&gt;
                &lt;color rgba=&quot;0.0 1.0 0.0 1.0&quot;/&gt;
            &lt;/material&gt;
        &lt;/visual&gt;
        &lt;inertial&gt;
            &lt;mass value=&quot;0.05846&quot;/&gt;
            &lt;origin xyz=&quot;-.035 -.092 -.223&quot;/&gt;
            &lt;inertia ixx=&quot;.0035&quot;  ixy=&quot;0.0018&quot;  ixz=&quot;0.000456&quot; iyy=&quot;.0029&quot; iyz=&quot;0.00119&quot; izz=&quot;.00065&quot; /&gt; 
        &lt;/inertial&gt;
    &lt;/link&gt;

    &lt;link name=&quot;upper_leg_right&quot;&gt;
        &lt;visual&gt;
            &lt;geometry&gt;
                &lt;mesh filename=&quot;upper_leg_right.stl&quot; scale=&quot;0.001 0.001 0.001&quot;/&gt;
            &lt;/geometry&gt;
            &lt;material name = &quot;Green&quot;&gt;
                &lt;color rgba=&quot;0.0 1.0 0.0 1.0&quot;/&gt;
            &lt;/material&gt;
        &lt;/visual&gt;
        &lt;inertial&gt;
            &lt;mass value=&quot;0.05846&quot;/&gt;
            &lt;origin xyz=&quot;-.0378 -.091 -.2444&quot;/&gt;
            &lt;inertia ixx=&quot;.0035&quot;  ixy=&quot;0.0018&quot;  ixz=&quot;0.000456&quot; iyy=&quot;.0029&quot; iyz=&quot;0.00119&quot; izz=&quot;.00065&quot; /&gt;
        &lt;/inertial&gt;
    &lt;/link&gt;

    &lt;link name=&quot;lower_leg_left&quot;&gt;
        &lt;visual&gt;
            &lt;geometry&gt;
                &lt;mesh filename=&quot;lower_leg_left.stl&quot; scale=&quot;0.001 0.001 0.001&quot;/&gt;
            &lt;/geometry&gt;
            &lt;material name = &quot;Pink&quot;&gt;
                &lt;color rgba=&quot;1.0 0.0 1.0 1.0&quot;/&gt;
            &lt;/material&gt;
        &lt;/visual&gt;
        &lt;inertial&gt;
            &lt;mass value=&quot;0.05846&quot;/&gt;
            &lt;origin xyz=&quot;-.0427 -.2019 -.2223&quot;/&gt;
            &lt;inertia ixx=&quot;.00369&quot;  ixy=&quot;0.00036&quot;  ixz=&quot;0.00179&quot; iyy=&quot;.00209&quot; iyz=&quot;0.00179&quot; izz=&quot;.001823&quot; /&gt; 
        &lt;/inertial&gt;
    &lt;/link&gt;

    &lt;link name=&quot;lower_leg_right&quot;&gt;
        &lt;visual&gt;
            &lt;geometry&gt;
                &lt;mesh filename=&quot;lower_leg_right.stl&quot; scale=&quot;0.001 0.001 0.001&quot;/&gt;
            &lt;/geometry&gt;
            &lt;material name = &quot;Pink&quot;&gt;
                &lt;color rgba=&quot;1.0 0.0 1.0 1.0&quot;/&gt;
            &lt;/material&gt;
        &lt;/visual&gt;
        &lt;inertial&gt;
            &lt;mass value=&quot;0.05846&quot;/&gt;
            &lt;origin xyz=&quot;-.05398 -.202 -.0253&quot;/&gt;
            &lt;inertia ixx=&quot;.00369&quot;  ixy=&quot;0.00036&quot;  ixz=&quot;0.00179&quot; iyy=&quot;.00209&quot; iyz=&quot;0.00179&quot; izz=&quot;.001823&quot; /&gt;
        &lt;/inertial&gt;
    &lt;/link&gt;

    &lt;joint name=&quot;joint1&quot; type=&quot;revolute&quot;&gt;
        &lt;parent link=&quot;base&quot;/&gt;
        &lt;child link=&quot;hip_left&quot;/&gt;
        &lt;origin xyz=&quot;0 0 0&quot;/&gt;
        &lt;axis xyz=&quot;0 -1 0&quot;/&gt;
        &lt;limit lower=&quot;-3.14159&quot; upper=&quot;3.14159&quot; effort=&quot;10&quot; velocity=&quot;10&quot;/&gt;
    &lt;/joint&gt;
    
    &lt;joint name=&quot;joint2&quot; type=&quot;revolute&quot;&gt;
        &lt;parent link=&quot;base&quot;/&gt;
        &lt;child link=&quot;hip_right&quot;/&gt;
        &lt;origin xyz=&quot;0 0 0&quot;/&gt;
        &lt;axis xyz=&quot;0 -1 0&quot;/&gt;
        &lt;limit lower=&quot;-3.14159&quot; upper=&quot;3.14159&quot; effort=&quot;10&quot; velocity=&quot;10&quot;/&gt;
    &lt;/joint&gt;
    
    &lt;joint name=&quot;joint3&quot; type=&quot;revolute&quot;&gt;
        &lt;parent link=&quot;hip_left&quot;/&gt;
        &lt;child link=&quot;upper_leg_left&quot;/&gt;
        &lt;origin xyz=&quot;0 0 0&quot;/&gt;
        &lt;axis xyz=&quot;0 0 1&quot;/&gt;
        &lt;limit lower=&quot;-3.14159&quot; upper=&quot;3.14159&quot; effort=&quot;10&quot; velocity=&quot;10&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;joint4&quot; type=&quot;revolute&quot;&gt;
        &lt;parent link=&quot;hip_right&quot;/&gt;
        &lt;child link=&quot;upper_leg_right&quot;/&gt;
        &lt;origin xyz=&quot;0 0 0&quot;/&gt;
        &lt;axis xyz=&quot;0 0 -1&quot;/&gt;
        &lt;limit lower=&quot;-3.14159&quot; upper=&quot;3.14159&quot; effort=&quot;10&quot; velocity=&quot;10&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;joint5&quot; type=&quot;revolute&quot;&gt;
        &lt;parent link=&quot;upper_leg_left&quot;/&gt;
        &lt;child link=&quot;lower_leg_left&quot;/&gt;
        &lt;origin xyz=&quot;0 0 0&quot;/&gt;
        &lt;axis xyz=&quot;0 0 1&quot;/&gt;
        &lt;limit lower=&quot;-3.14159&quot; upper=&quot;3.14159&quot; effort=&quot;10&quot; velocity=&quot;10&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;joint6&quot; type=&quot;revolute&quot;&gt;
        &lt;parent link=&quot;upper_leg_right&quot;/&gt;
        &lt;child link=&quot;lower_leg_right&quot;/&gt;
        &lt;origin xyz=&quot;0 0 0&quot;/&gt;
        &lt;axis xyz=&quot;0 0 -1&quot;/&gt;
        &lt;limit lower=&quot;-3.14159&quot; upper=&quot;3.14159&quot; effort=&quot;10&quot; velocity=&quot;10&quot;/&gt;
    &lt;/joint&gt;
&lt;/robot&gt;
</code></pre>
<p>I tried putting in various values for the joint origins in the URDF using various translations but none helped fix the issue. I have yet to attempt to change the joint axis parameter.</p>
",4/19/2023 20:08,,220,0,0,0,,12873598.0,,2/10/2020 16:20,1.0,,,,,,,
3694,61363106,ROS move_base node is generating pre-Hydro warning,|c++|navigation|ros|robotics|catkin|,"<p>I am working with move_base from navigation stack. However, I am getting the warning that</p>

<p>""local_costmap: preHydro parameter ""static_map"" unused since ""plugins"" is provided""</p>

<p>In terms of costmap definition here are the common and local config files I have been using:</p>

<pre><code>footprint: [ [-0.15,-0.15], [0.15,-0.15], [0.15,0.15], [-0.15,0.15] ]
transform_tolerance: 0.5
map_type: costmap
obstacle_layer:
 enabled: true
 obstacle_range: 3.0
 raytrace_range: 3.5
 inflation_radius: 0.2
 track_unknown_space: false
 combination_method: 1
 observation_sources: laser_scan_sensor
 laser_scan_sensor: {sensor_frame: scanmatcher_frame, data_type: LaserScan, topic: /scan, marking: true, clearing: false}
inflation_layer:
  enabled:              true
  cost_scaling_factor:  1.0  
  inflation_radius:     0.2
obstacle_layer:
     enabled: true
     obstacle_range: 5.0
     raytrace_range: 1.0
     observation_sources: ""/scan""
     observation_persistence: 0.0
     inf_is_valid: false
     scan:
       data_type: LaserScan
       topic: scan

local_costmap:
 global_frame: map
 robot_base_frame: base_link
 update_frequency: 0.5
 publish_frequency: 0.25
 static_map: false
 rolling_window: true
 width: 50
 height: 50
 width: 8
 height: 8
 origin_x: -4
 origin_y: -4
 resolution: 0.1
 transform_tolerance: 0.5 
 plugins:
   - {name: inflation_layer,        type: ""costmap_2d::InflationLayer""}
   - {name: obstacle_layer,      type: ""costmap_2d::ObstacleLayer""}
</code></pre>

<p>Now, I have followed the navigation tutorial page which exactly addresses this issue but without success. Interestingly,my global costmap throws the same warning while receiving the map correctly with a message that ""Recieved a 250x250 map at 0.1 m/px"". My global yaml file looks like this:</p>

<pre><code>global_costmap:
 global_frame: map
 robot_base_frame: base_link
 update_frequency: 0.5
 publish_frequency: 0.25
 always_send_full_costmap: true
 width: 250
 height: 250
 origin_x: -125
 origin_y: -125
 resolution: 0.1
 static_map: true
 plugins:
   - {name: static_layer,        type: ""costmap_2d::StaticLayer""}
</code></pre>
",4/22/2020 10:40,61363290.0,5569,1,0,0,,13184375.0,,4/1/2020 13:17,18.0,61363290.0,"<p>The <code>static_map</code> parameter has been deprecated and is no longer used.</p>

<p>Just remove the <code>static_map: false</code> line inside the <code>local_costmap</code> parameters to make the warning go away.</p>
",2661982.0,0.0,1.0,,
1469,12462461,"Is it worth to learn Ada instead of another languages [c++, c#]?",|programming-languages|ada|robotics|,"<p>If Im going to make robots, which language do you recommend me? 
In our university we can choose between several languages. Most of the students are choosing Ada just because our teacher uses it. </p>

<p>After some research I found out that ada is old. What are your thoughts? 
Is is worth to learn it? </p>
",9/17/2012 15:27,12469567.0,25584,8,6,21,0.0,,,,,12462526.0,"<p>It depends on what you want to do with this language. Lisp is even older than Ada and is widely used within artificial intelligence. Learn the language you want to, learn the language which is easy for you to learn and easy for you to understand it's concepts. Then you can go further and learn more languages.</p>
",1616951.0,2.0,3.0,135693634.0,"C++ for embedded system, and C# for Windows OS application, for Ada...here is an opinion (https://www.reddit.com/r/ProgrammingLanguages/comments/eyeonv/whatever_happened_to_the_language_ada/fggvgc1/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)"
2242,29952691,Error libraries Raspbian with GoPiGo,|python|opencv|raspberry-pi|raspbian|robotics|,"<p>I am working on a GoPiGo and i'm trying to make this robot to move when the camera detect a circle. </p>

<p>The mainly problem i have here is that when i try to use the gopigo library in order to use functions as fwd(),stop(), etc, if i do NOT use sudo in the command line, and just type ""python CircleDetector_MOVEMENT.py"" it detects the gopigo library, but doesn't detect the picamera.array:</p>

<pre><code>Traceback (most recent call last):
  File ""CircleDetector_MOVEMENT.py"", line 2, in &lt;module&gt;
    from picamera.array import PiRGBArray
ImportError: No module named picamera.array
</code></pre>

<p>which i import from PIRGBarray. And when i use sudo python myprogram.py, it does NOT detect the gopigo library, and the error is the next: </p>

<pre><code>Traceback (most recent call last):
   File ""CircleDetector_MOVEMENT.py"", line 8, in &lt;module&gt;
    from gopigo import *        #Has the basic functions for controlling the          GoPiGo Robot
ImportError: No module named gopigo
</code></pre>

<p>I guess it can be something related with permission, but i have no idea how to solve it.</p>

<p>So, if you please know what can be happening here, i'll be grateful. In their forum have told me that is an I2C issue, but i'm a noob in all this whole thing and i don't know how to solve it. </p>

<p>Any Help is appreciated.</p>

<p>P.S. Here you are my code, if it helps:</p>

<pre><code>#import everything i need to get working all modules.
from picamera.array import PiRGBArray
from picamera import PiCamera
import time
import cv2
import os
import numpy as np
from gopigo import *    #Has the basic functions for controlling the GoPiGo     Robot
import sys  #Used for closing the running program
os.system('sudo modprobe bcm2835-v4l2')

h=200
w=300

camera = PiCamera()
camera.resolution = (w, h)
camera.framerate = 5
rawCapture = PiRGBArray(camera, size=(w, h))
time.sleep(0.1)


for frame in camera.capture_continuous(rawCapture, format=""bgr"",     use_video_port=True):
imagen_RGB = frame.array
copia_RGB = imagen_RGB.copy()



     gris = cv2.cvtColor(imagen_RGB, cv2.COLOR_BGR2GRAY)
     gris = cv2.medianBlur(gris,9)



    img_circulos = None
    img_circulos = cv2.HoughCircles(gris, cv2.cv.CV_HOUGH_GRADIENT, 1, 20, param1=50, param2=50, minRadius=0, maxRadius=0)


    if img_circulos is not None:

        v = 1
        img_circulos = np.round(img_circulos[0, :]).astype(""int"")


        for (x, y, r) in img_circulos:

            cv2.circle(copia_RGB, (x, y), r, (0, 255, 0), 3)
            cv2.rectangle(copia_RGB, (x - 5, y - 5),(x + 5, y + 5), (0, 128, 255, -1))
    if v == 1
       fwd()

    cv2.imshow(""Imagen Combinada"", copia_RGB)

    key = cv2.waitKey(1) &amp; 0xFF
    rawCapture.truncate(0)
    if key == ord(""q""):
        break
</code></pre>
",4/29/2015 19:20,,726,1,0,3,,4837678.0,,4/27/2015 12:31,7.0,40457954.0,"<p>Are you using a virtual environment to run the OpenCV code? If yes then you might have to copy gopigo.py to your virtual environment and run <code>python &lt;filename&gt;</code> to have it working.</p>
",7093992.0,0.0,1.0,,
1682,15498360,"Algorithm, tool or technique to represent 3D probability density functions on space",|algorithm|data-structures|computer-vision|probability|robotics|,"<p>I'm working on a project with computer vision (opencv 2.4 on c++). On this project I'm trying to detect certain features to build a map (an internal representation) of the world around. </p>

<p>The information I have available is the camera pose (6D vector with 3 position and 3 angular values), calibration values (focal length, distortion, etc) and the features detected on the object being tracked (this features are basically the contour of the object but it doesn't really matter)</p>

<p>Since the camera pose, the position of the features and other variables are subject to errors, I want to model the object as a 3D probability density function (with the probability of finding the ""object"" on a given 3D point on space, this is important since each contour has a probability associated of how likely it is that it is an actually object-contour instead of a noise-contour(bear with me)). </p>

<p><strong>Example</strong>:
If the object were a <strong>sphere</strong>, I would detect a <strong>circle</strong> (contour). Since I know the camera pose, but have no depth information, the internal representation of that object should be a fuzzy <strong>cylinder</strong> (or a cone, if the camera's perspective is included but it's not relevant). If new information is available (new images from a different location) a new contour would be detected, with it's own <em>fuzzy cylinder</em> merged with previous data. Now we should have a region where the probability of finding the object is greater in some areas and weaker somewhere else. As new information is available, the model should converge to the original object shape.</p>

<p>I hope the idea is clear now.</p>

<p>This model should be able to:</p>

<ul>
<li>Grow dynamically if needed.</li>
<li>Update efficiently as new observations are made (updating the probability inside making stronger the areas observed multiple times and weaker otherwise). Ideally the system should be able to update in real time.</li>
</ul>

<p><strong>Now the question:</strong>
How can I do to computationally represent this kind of <em>fuzzy</em> information in such a way that I can perform these tasks on it?</p>

<p>Any suitable algorithm, data structure, c++ library or tool would help.</p>
",3/19/2013 11:34,,678,1,5,3,0.0,575085.0,,1/14/2011 0:46,396.0,15547493.0,"<p>I'll answer with the computer vision equivalent of Monty Python: ""<a href=""http://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping"" rel=""nofollow"">SLAM</a>, SLAM, SLAM, SLAM!"": :-)  I'd suggest starting with Sebastian Thrun's tome. </p>

<p>However, there's older older work on the Bayesian side of active computer vision that's directly relevant to your question of geometry estimation, e.g. Whaite and Ferrie's seminal IEEE paper on uncertainty modeling (Waithe, P. and Ferrie, F. (1991). From uncertainty to visual exploration. IEEE Transactions on Pattern Analysis and Machine Intelligence, 13(10):1038–1049.). For a more general (and perhaps mathematically neater) view on this subject, see also <a href=""http://www.cs.toronto.edu/~mackay/thesis.pdf"" rel=""nofollow"">chapter 4 of D.J.C. MacKay's Ph.D. thesis</a>. </p>
",1435240.0,1.0,0.0,21945930.0,"It's not clear exactly what configurations we need to be capable of representing, in part because it's not clear exactly what modification operations are allowed.  The simplest general-purpose approach would be to simply use a 3D array of floating-point values to represent the probabilities that each voxel is present.  Some kinds of updates would be faster if you also keep a separate floating point value that stores the sum of all elements in the array, and define `p(x, y, z) = array(x, y, z) / sum`.  (This would also let you store all numbers as integers instead of FP.)"
2945,47126874,JavaScript Liquid Handling Robot,|javascript|robotics|,"<p>I have the following code  (Using <a href=""https://www.w3schools.com/js/"" rel=""nofollow noreferrer"">https://www.w3schools.com/js/</a>  Because i don't know how else to test it)</p>

<pre><code>&lt;p id=""demo""&gt;&lt;/p&gt;

&lt;script&gt;

var a,b,c,d,e,text = """",x,y;
a = 25
b = Math.floor( a / 8 )
c = a - ( b * 8)
if (c == 0)
c = 8;
d = Math.ceil ( a / 8 )
e = 8
for (y = 1; y &lt;= d; y++) {
    for (x = 1; x &lt;= e; x++) {
    text += ""&lt;br&gt;"" + x + "","" + y;
    }
}
document.getElementById(""demo"").innerHTML = text;
&lt;/script&gt;
</code></pre>

<p>I was hoping that when <strong>var a</strong> (user input) was set to any number between 1 and 96 it would give all the coordinates from 1,1 to that number of well (on a 96 well plate A1 to H12. </p>

<p>eg. var a = 25 would give 1,1 2,1 3,1 4,1 5,1 6,1 7,1 8,1 1,2 2,2 3,2 4,2 5,2 6,2 7,2 8,2 1,3 2,3 3,3 4,3 5,3 6,3 7,3 8,3 1,4 and stop. Instead it finishes off the rest of the column 2,4 3,4 4,4 5,4 6,4 7,4 8,4 and then stops.</p>

<p>How do I get it to stop in the right place??</p>

<p>Thank you</p>
",11/5/2017 21:34,47126962.0,55,1,0,-1,,5008663.0,,6/14/2015 15:48,9.0,47126962.0,"<p>This should works, hope it helps!</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>var text = """", start = 8, end = 25;

for (var i = start; i &lt; end; i++) {
    text += ""&lt;br&gt;"" + ((i % 8) + 1) + "","" + (Math.floor(i / 8) + 1)
}

document.getElementById(""demo"").innerHTML = text;</code></pre>
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;p id=""demo""&gt;&lt;/p&gt;</code></pre>
</div>
</div>
</p>
",8888888.0,0.0,4.0,,
2453,35722264,Kinect robotic arm detection,|image-processing|kinect|robotics|motion-detection|lbr-iiwa-rapi|,"<p>can i use Kinect sensor to detect the motion of a robotic arm (KUKA LBR iiwa 7R800) and calculate it's links angles in order to make it control another robotic arm.</p>
",3/1/2016 11:56,,378,1,3,0,,6002349.0,,3/1/2016 11:47,5.0,35724567.0,"<p>Of course this is possible but I don't think it is a good idea.</p>

<p>Suboptimal accuracy, lag due to processing of the 3d-data. I guess there are also cases where you cannot see all joints/links.</p>

<p>Kuka robots can output their joint angles directly. Use this data for synchronization or control both robots using the same external data.</p>

<p>Any measurement error might cause unwanted movements which in case of industrial robots can cause severe damage!</p>
",2858170.0,0.0,3.0,59123778.0,what's the motivation? what's the second robot type?
4745,77829740,WPILIB (FRC) Gradlew Issue (Gradle builds fine but gradlew doesn't),|java|gradle|robotics|wpilib|,"<p>I recently installed wpilib on my windows laptop and start an project, but when I try to build the project using <code>gradlew</code> it doesnt work:</p>
<p>Here is my <code>build.gradle</code>:</p>
<pre><code>plugins {
    id &quot;java&quot;
    id &quot;edu.wpi.first.GradleRIO&quot; version &quot;2024.1.1&quot;
}

java {
    sourceCompatibility = JavaVersion.VERSION_17
    targetCompatibility = JavaVersion.VERSION_17
}

def ROBOT_MAIN_CLASS = &quot;frc.robot.Main&quot;

// Define my targets (RoboRIO) and artifacts (deployable files)
// This is added by GradleRIO's backing project DeployUtils.
deploy {
    targets {
        roborio(getTargetTypeClass('RoboRIO')) {
            // Team number is loaded either from the .wpilib/wpilib_preferences.json
            // or from command line. If not found an exception will be thrown.
            // You can use getTeamOrDefault(team) instead of getTeamNumber if you
            // want to store a team number in this file.
            team = project.frc.getTeamNumber()
            debug = project.frc.getDebugOrDefault(false)

            artifacts {
                // First part is artifact name, 2nd is artifact type
                // getTargetTypeClass is a shortcut to get the class type using a string

                frcJava(getArtifactTypeClass('FRCJavaArtifact')) {
                }

                // Static files artifact
                frcStaticFileDeploy(getArtifactTypeClass('FileTreeArtifact')) {
                    files = project.fileTree('src/main/deploy')
                    directory = '/home/lvuser/deploy'
                }
            }
        }
    }
}

def deployArtifact = deploy.targets.roborio.artifacts.frcJava

// Set to true to use debug for JNI.
wpi.java.debugJni = false

// Set this to true to enable desktop support.
def includeDesktopSupport = false

// Defining my dependencies. In this case, WPILib (+ friends), and vendor libraries.
// Also defines JUnit 5.
dependencies {
    implementation wpi.java.deps.wpilib()
    implementation wpi.java.vendor.java()

    roborioDebug wpi.java.deps.wpilibJniDebug(wpi.platforms.roborio)
    roborioDebug wpi.java.vendor.jniDebug(wpi.platforms.roborio)

    roborioRelease wpi.java.deps.wpilibJniRelease(wpi.platforms.roborio)
    roborioRelease wpi.java.vendor.jniRelease(wpi.platforms.roborio)

    nativeDebug wpi.java.deps.wpilibJniDebug(wpi.platforms.desktop)
    nativeDebug wpi.java.vendor.jniDebug(wpi.platforms.desktop)
    simulationDebug wpi.sim.enableDebug()

    nativeRelease wpi.java.deps.wpilibJniRelease(wpi.platforms.desktop)
    nativeRelease wpi.java.vendor.jniRelease(wpi.platforms.desktop)
    simulationRelease wpi.sim.enableRelease()

    testImplementation 'org.junit.jupiter:junit-jupiter:5.10.1'
    testRuntimeOnly 'org.junit.platform:junit-platform-launcher'
}

test {
    useJUnitPlatform()
    systemProperty 'junit.jupiter.extensions.autodetection.enabled', 'true'
}

// Simulation configuration (e.g. environment variables).
wpi.sim.addGui().defaultEnabled = true
wpi.sim.addDriverstation()

// Setting up my Jar File. In this case, adding all libraries into the main jar ('fat jar')
// in order to make them all available at runtime. Also adding the manifest so WPILib
// knows where to look for our Robot Class.
jar {
    from { configurations.runtimeClasspath.collect { it.isDirectory() ? it : zipTree(it) } }
    from sourceSets.main.allSource
    manifest edu.wpi.first.gradlerio.GradleRIOPlugin.javaManifest(ROBOT_MAIN_CLASS)
    duplicatesStrategy = DuplicatesStrategy.INCLUDE
}

// Configure jar and deploy tasks
deployArtifact.jarTask = jar
wpi.java.configureExecutableTasks(jar)
wpi.java.configureTestTasks(test)

// Configure string concat to always inline compile
tasks.withType(JavaCompile) {
    options.compilerArgs.add '-XDstringConcat=inline'
}
</code></pre>
<p>Now if I build with gradle, it builds perfectly fine</p>
<p>settings.gradle:</p>
<pre><code>import org.gradle.internal.os.OperatingSystem

pluginManagement {
    repositories {
        mavenLocal()
        gradlePluginPortal()
        String frcYear = '2024'
        File frcHome
        if (OperatingSystem.current().isWindows()) {
            String publicFolder = System.getenv('PUBLIC')
            if (publicFolder == null) {
                publicFolder = &quot;C:\\Users\\Public&quot;
            }
            def homeRoot = new File(publicFolder, &quot;wpilib&quot;)
            frcHome = new File(homeRoot, frcYear)
        } else {
            def userFolder = System.getProperty(&quot;user.home&quot;)
            def homeRoot = new File(userFolder, &quot;wpilib&quot;)
            frcHome = new File(homeRoot, frcYear)
        }
        def frcHomeMaven = new File(frcHome, 'maven')
        maven {
            name 'frcHome'
            url frcHomeMaven
        }
    }
}

Properties props = System.getProperties();
props.setProperty(&quot;org.gradle.internal.native.headers.unresolved.dependencies.ignore&quot;, &quot;true&quot;);
</code></pre>
<p>I expected that it would build with gradlew as well but it doesnt,</p>
<p>heres my gradle-wrapper.properties</p>
<pre><code>distributionBase=GRADLE_USER_HOME
distributionPath=permwrapper/dists
distributionUrl=https\://services.gradle.org/distributions/gradle-8.5-bin.zip
networkTimeout=10000
validateDistributionUrl=true
zipStoreBase=GRADLE_USER_HOME
zipStorePath=permwrapper/dists
</code></pre>
<p>instead of building it gives me this:</p>
<pre><code>&gt;gradlew
Exception in thread &quot;main&quot; java.lang.ClassNotFoundException: org.gradle.launcher.GradleMain
        at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
        at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)
        at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)
        at org.gradle.wrapper.GradleWrapperMain.main(SourceFile:70)
</code></pre>
<p>I have even tried regenerating the wrapper, <code>gradle wrapper</code></p>
<p><strong>ALSO: FOR SOME REASON <code>gradlew</code> doesnt listen to commands its only building no matter what</strong></p>
<p><strong>EDIT:</strong>
I have tried my code on github codespaces and my own linux (arch linux) but windows isn't working for me, i have reinstalled wpilib multiple times not sure why its doing this to me. I have tried using a vm of windows and it works it weird why my computer doesn't work.</p>
",1/17/2024 2:56,77843380.0,110,1,0,1,,15027302.0,,1/18/2021 3:40,5.0,77843380.0,"<p>The solution to this error was that you had to remove <code>.gradle</code> folder in your home directory <code>C:\Users\{username}\</code>. Now something must happened during my installation that caused this error to happen. Everything builds correctly now.</p>
<p>NOTE: You have to reboot your computer before removing it because it won't let you remove unless you reboot.</p>
",15027302.0,0.0,0.0,,
3665,60883932,Controller_manager : [ERROR] Controler Spawner couldn't find the expected controller_manager ROS interface,|controller|runtime-error|ros|robot|gazebo-simu|,"<p>I know there is already a lot of questions on this particular error but none of the ones i found solved the issue for me...</p>

<p>I'm trying to implement the ROS differential drive controller for a two wheeled robot in gazebo, but when launching the controller spawner I get the following output :</p>

<pre><code>[INFO] [1585302057.569863, 0.000000]: Controller Spawner: Waiting for service controller_manager/load_controller
[WARN] [1585302087.735023, 40.162000]: Controller Spawner couldn't find the expected controller_manager ROS interface.
</code></pre>

<p>In fact, trying to list controller_manager services gives no output :</p>

<pre><code>$ rosservice list | grep controller_manager
$
</code></pre>

<p>I'm running ros melodic on Ubuntu 18.04.</p>

<p>Here is my config file <code>diff_drive.yaml</code>:</p>

<pre><code>wheelchair_controler:
  type        : ""diff_drive_controller/DiffDriveController""
  left_wheel  : 'left_wheel_motor'
  right_wheel : 'right_wheel_motor'
  publish_rate: 50.0               # default: 50
  pose_covariance_diagonal : [0.001, 0.001, 1000000.0, 1000000.0, 1000000.0, 1000.0]
  twist_covariance_diagonal: [0.001, 0.001, 1000000.0, 1000000.0, 1000000.0, 1000.0]

  # Wheel separation and diameter. These are both optional.
  # diff_drive_controller will attempt to read either one or both from the
  # URDF if not specified as a parameter
  wheel_separation : 0.52
  wheel_radius : 0.3048

  # Wheel separation and radius multipliers
  wheel_separation_multiplier: 1.0 # default: 1.0
  wheel_radius_multiplier    : 1.0 # default: 1.0

  # Velocity commands timeout [s], default 0.5
  cmd_vel_timeout: 0.25

  # Base frame_id
  base_frame_id: base_link #default: base_link

  # Velocity and acceleration limits
  # Whenever a min_* is unspecified, default to -max_*
  linear:
    x:
      has_velocity_limits    : true
      max_velocity           : 1.0  # m/s
      min_velocity           : -0.5 # m/s
      has_acceleration_limits: true
      max_acceleration       : 0.8  # m/s^2
      min_acceleration       : -0.4 # m/s^2
      has_jerk_limits        : true
      max_jerk               : 5.0  # m/s^3
  angular:
    z:
      has_velocity_limits    : true
      max_velocity           : 1.7  # rad/s
      has_acceleration_limits: true
      max_acceleration       : 1.5  # rad/s^2
      has_jerk_limits        : true
      max_jerk               : 2.5  # rad/s^3

</code></pre>

<p>Here is the launch file. I tried to put the spawner in a separate launch file to make sure gazebo add time to launch properly.</p>

<pre><code>&lt;?xml version=""1.0""?&gt;
&lt;launch&gt;

  &lt;!-- Controllers --&gt;
  &lt;rosparam file=""$(find wheelchair_simulation)/config/diff_drive.yaml"" command=""load"" /&gt;
  &lt;node name=""wheelchair_controller_spawner"" pkg=""controller_manager"" type=""spawner"" respawn=""false"" output=""screen"" args=""wheelchair_controler"" /&gt;


&lt;/launch&gt;

</code></pre>

<p>In my <code>.xacro</code> file I use a macro to define my wheels, joints, and gazebo tags. I also added a transmission inside it :</p>

<pre><code>&lt;xacro:macro name=""main_wheel"" params=""prefix reflect""&gt;
    &lt;link name=""main_${prefix}_wheel""&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;cylinder length=""${main_wheel_length}"" radius=""${main_wheel_radius}""/&gt;
        &lt;/geometry&gt;
        &lt;material name=""black""/&gt;
      &lt;/visual&gt;
      &lt;collision&gt;
        &lt;geometry&gt;
          &lt;cylinder length=""${main_wheel_length}"" radius=""${main_wheel_radius}""/&gt;
        &lt;/geometry&gt;
      &lt;/collision&gt;
      &lt;inertial&gt;
        &lt;xacro:cylinder_inertia length=""${main_wheel_length}"" radius=""${main_wheel_radius}"" weight=""${main_wheel_mass}""/&gt;
      &lt;/inertial&gt;
    &lt;/link&gt;

    &lt;joint name=""${prefix}_wheel_motor"" type=""continuous""&gt;
      &lt;axis xyz=""0 0 1""/&gt;
      &lt;parent link=""base_link""/&gt;
      &lt;child link=""main_${prefix}_wheel""/&gt;
      &lt;origin rpy=""${-reflect*1.5708} 0 0"" xyz=""0 ${reflect*((total_width - main_wheel_length)/2 + 0.001)} 0""/&gt;
      &lt;gazebo&gt;
        &lt;implicitSpringDamper&gt;true&lt;/implicitSpringDamper&gt;
      &lt;/gazebo&gt;
      &lt;dynamic friction=""0.1""/&gt;
    &lt;/joint&gt;

    &lt;transmission name=""${prefix}_wheel_transmission""&gt;
      &lt;type&gt;transmission_interface/SimpleTransmission&lt;/type&gt;
      &lt;joint name=""${prefix}_wheel_motor""&gt;
        &lt;hardwareInterface&gt;hardware_interface/VelocityJointInterface&lt;/hardwareInterface&gt;
      &lt;/joint&gt;
      &lt;actuator name=""${prefix}_wheel_motor""&gt;
        &lt;mechanicalReduction&gt;1&lt;/mechanicalReduction&gt;
        &lt;hardwareInterface&gt;VelocityJointInterface&lt;/hardwareInterface&gt;
      &lt;/actuator&gt;
    &lt;/transmission&gt;

    &lt;gazebo reference=""main_${prefix}_wheel""&gt;
      &lt;mu1&gt;0.8&lt;/mu1&gt;
        &lt;mu2&gt;0.8&lt;/mu2&gt;
      &lt;turnGravityOff&gt;false&lt;/turnGravityOff&gt;
      &lt;material&gt;Gazebo/Black&lt;/material&gt;
    &lt;/gazebo &gt;
  &lt;/xacro:macro&gt;
</code></pre>

<p>I made sure to install <code>gazebo_ros_controle</code>:</p>

<pre><code>$ sudo apt-get install ros-melodic-gazebo-ros-controle
</code></pre>

<p>And to link it in my description file :</p>

<pre><code>&lt;gazebo&gt;
        &lt;plugin name=""gazebo_ros_control"" filename=""libgazebo_ros_contol.so""&gt;
        &lt;/plugin&gt;
&lt;/gazebo&gt;
</code></pre>

<p>Finally, I checked the dependencies and everything looks ok :</p>

<pre><code>$ rosdep check controller_manager
All system dependencies have been satisified
</code></pre>

<p>EDIT : I add the description of the base_link and base_footprint links, in case it's necessary as I saw somewhere that the frame for the controller must have inertia</p>

<pre><code>&lt;!-- Dummy link because first link should not have any inertia. Located on the ground between both wheels for easier control --&gt;

  &lt;link name=""base_footprint""/&gt;

&lt;!-- Footprint and main inertia of the chair --&gt;

  &lt;link name=""base_link""&gt;
    &lt;visual&gt;
      &lt;geometry&gt;
        &lt;box size=""${total_length} ${total_width} ${seat_height - ground_clearence}""/&gt;
      &lt;/geometry&gt;
      &lt;origin xyz=""${-main_wheel_radius + total_length/2} 0 ${a}""/&gt;
      &lt;material name=""white""/&gt;
    &lt;/visual&gt;
    &lt;collision&gt;
      &lt;geometry&gt;
        &lt;box size=""${total_length} ${total_width} ${seat_height - ground_clearence}""/&gt;
      &lt;/geometry&gt;
      &lt;origin xyz=""${-main_wheel_radius + total_length/2} 0 ${a+0.1}""/&gt;
    &lt;/collision&gt;
    &lt;inertial&gt;
      &lt;xacro:box_inertia height=""${seat_height - ground_clearence}"" length=""${total_length}"" width=""${total_width}"" weight=""${total_mass-2*main_wheel_mass}""/&gt;
    &lt;/inertial&gt;
  &lt;/link&gt;

  &lt;joint name=""base_link_joint"" type=""fixed""&gt;
    &lt;parent link=""base_footprint""/&gt;
    &lt;child link=""base_link""/&gt;
    &lt;origin xyz=""${-total_length/2 + main_wheel_radius} 0 ${main_wheel_radius}""/&gt;
  &lt;/joint&gt;
</code></pre>

<p>I sincerely hope some of you can find the issue because I have no clue about where it comes from... Feel free to ask any missing details.</p>

<p>Thank you in advance !!</p>
",3/27/2020 10:14,,5974,2,0,3,,13134356.0,"Cannes, France",3/27/2020 9:45,25.0,63610055.0,"<p>Please check gazebo log. If there is a problem with the urdf / xacro file, gazebo is not going to initialize the robot simulation interface, and will not start the gazebo_ros_control plugin.</p>
<p>Here you have an example. Once I corrected the urdf file, the controller load and no longer I had this error.</p>
<pre class=""lang-none prettyprint-override""><code>[ INFO] [1598502340.159974962]: waitForService: Service [/gazebo/set_physics_properties] is now available.
[ INFO] [1598502340.212629654]: Physics dynamic reconfigure ready.

[ INFO] [1598502340.568106895]: Loading gazebo_ros_control plugin

[ INFO] [1598502340.568395067]: Starting gazebo_ros_control plugin in namespace: /plotter

[ INFO] [1598502340.569824694]: gazebo_ros_control plugin is waiting for model URDF in parameter [/robot_description] on the ROS param server.

[ERROR] [1598502340.698456387]: This robot has a joint named &quot;link_00__link_01&quot; which is not in the gazebo model.

[FATAL] [1598502340.698639178]: Could not initialize robot simulation interface
</code></pre>
<p>As you can see in the two last lines, gazebo is not finishing ok.</p>
",13939780.0,0.0,0.0,,
4622,76531265,"How to plan a trajectory having path in the form (x_i,y_i,theta_i), robot FK and Invers Kinematic and constraints?",|matlab|robotics|,"<p>I have the following information:
1.Inital path for the robot from start state to goal state in the form (x_i,y_i,theta_i),
2. Robot forward and inverse kinematics,
3. Some constraints to be posed on the robot related to velocity and acceleration.</p>
<p>I want to know how to utilize those known inputs to plan a trajectory then execute it by commanding robot to follow the produced trajectory.</p>
<p>I have done many searches but I still find it confusing, some utilized smoothing algorithms, other used optimization method so on so forth.</p>
",6/22/2023 11:07,,17,0,0,0,,18055837.0,,1/28/2022 8:13,8.0,,,,,,,
4620,76482257,What algorithm should I use to find the quickest path between two points while avoiding obstacles?,|dynamic|path|shortest-path|robotics|motion-planning|,"<p>I have a start and end point and several obstacles (more or less depending on the situation). I also have a robot-like system that moves in a dynamic environnment. My job is to implement an algorithm that will allow my system to arrive to the end-point avoiding the obstacles. Has anyone worked on path/motion planning before and could give me some advice ?
So far I've done a little bit of 'research' and I found that basically there are two groups of algorithms: serach-based and sampling-based. Apprently the sampling-based ones are more suitable for environments having multiple obstacles and they are also mostly used in robotics. Now testing they give a very strange trajectory, having many turns which is not at all what I want. On the other hand, search-based algorithms since they work with a grid, when we have more obstacles take a lot of memory. So, here is where I'm stuck. I was thinking of implementing RRT*, but I really don't like the path found even when I have many iterations.</p>
<p>I looked up algorithms like A*,D*,D<em>Lite,RRT,RRT</em> and PRM.</p>
",6/15/2023 12:01,,69,1,0,0,,15090954.0,,1/27/2021 11:01,8.0,76870635.0,"<p>As you have mentioned, sampling-based methods are a lot more efficient in problems with very high-dimensional state space and you don't have to worry about grid size. The observation of strange and unsmooth paths returned by a sampling-based planner is common. Usually, people perform post-processing to fix this. You can perform shortcutting (see paper: <a href=""https://ieeexplore.ieee.org/document/5509683"" rel=""nofollow noreferrer"">https://ieeexplore.ieee.org/document/5509683</a>), or you can throw the found path into an optimization-based solver (e.g. trajectory optimization) as a good initial guess that takes into account path smoothness as well.</p>
",12004417.0,1.0,0.0,,
3704,61709336,C programming for Robotics,|c|robotics|webots|,"<p>I have a robotic car in Webots simulation that needs to stop at a place whenever I press a key on keyboard. So if I press 1, it should stop at coordinate X = 140 and coordinate Z = 50. On this code, the speed goes up and down with UP and DOWN arrow. I may have to insert if statement under case 1 but don't know how to implement. The robot constantly runs on a line and should only stop or act when prompted with a keyboard command. The code given below is just a snippet, I can provide the full code if necessary. I'm a novice just starting a course on C.</p>

<pre><code>// GPS
WbDeviceTag gps;
double gps_coords[3] = {0.0, 0.0, 0.0};
double gps_speed = 0.0;

// misc variables
double speed = 0.0;
double steering_angle = 0.0;
int manual_steering = 0;
bool autodrive = true;

void print_help() {
  printf(""You can drive this car!\n"");
  printf(""Select the 3D window and then use the cursor keys to:\n"");
  printf(""
/
- steer\n"");
  printf(""[UP]/[DOWN] - accelerate/slow down\n"");
}

void set_autodrive(bool onoff) {
  if (autodrive == onoff)
    return;
  autodrive = onoff;
  switch (autodrive) {
    case false:
      printf(""switching to manual drive...\n"");
      printf(""hit [A] to return to auto-drive.\n"");
      break;
    case true:
      if (has_camera)
        printf(""switching to auto-drive...\n"");
      else
        printf(""impossible to switch auto-drive on without camera...\n"");
      break;
  }
}


void check_keyboard() {
  int key = wb_keyboard_get_key();
  switch (key) {
    case WB_KEYBOARD_UP:
      set_speed(speed + 5.0);
      break;
    case WB_KEYBOARD_DOWN:
      set_speed(speed - 5.0);
      break;
    case WB_KEYBOARD_RIGHT:
      change_manual_steer_angle(+1);
      break;
    case WB_KEYBOARD_LEFT:
      change_manual_steer_angle(-1);
      break;
    case 'A':
      set_autodrive(true);
      break;
    case '1':
      if (gps_coords[X] == ""104.9"" &amp;&amp; gps_coords[Z] == ""48.9"") {
      set_speed(speed - speed);
  }
     break;
}
</code></pre>
",5/10/2020 8:48,,351,0,2,0,,3237985.0,United Kingdom,1/26/2014 17:09,13.0,,,,,,109154005.0,"You can't put `if (gps_coords[X] == ""104.9"" && gps_coords[Z] == ""48.9"") { set_speed(speed - speed); }` inside the keyboard handler because it will only check that wen you press the number.  You need to have the keyboard handler set a flag and then in your main do `if (one_pressed == true && gps_coords[X] == ""104.9"" && gps_coords[Z] == ""48.9"") { set_speed(speed - speed); }`"
1808,18210709,Unable to run new .cfg on PlayerStage,|c|ubuntu-12.04|robotics|player-stage|,"<p>I have successfully installed Player/Stage on Ubuntu 12.04 using this blog:
<a href=""http://www.cnblogs.com/kevinGuo/archive/2012/05/03/2480077.html"" rel=""nofollow"">http://www.cnblogs.com/kevinGuo/archive/2012/05/03/2480077.html</a></p>

<p><code>player simple.cfg</code> is working fine and is showing the desired results as shown in the above mentioned blog.</p>

<p>Then, I was trying to run empty.cfg after creating empty.cfg and empty.world as explained in this tutorial:
<a href=""http://www-users.cs.york.ac.uk/jowen/player/playerstage-tutorial-manual.pdf"" rel=""nofollow"">http://www-users.cs.york.ac.uk/jowen/player/playerstage-tutorial-manual.pdf</a></p>

<p>But, it is showing this error:</p>

<pre><code>cs246@cs246:~/src/Stage-3.2.2-Source/worlds$ player empty.cfg 
Registering driver
Player v.3.0.2

* Part of the Player/Stage/Gazebo Project [http://playerstage.sourceforge.net].
* Copyright (C) 2000 - 2009 Brian Gerkey, Richard Vaughan, Andrew Howard,
* Nate Koenig, and contributors. Released under the GNU General Public License.
* Player comes with ABSOLUTELY NO WARRANTY.  This is free software, and you
* are welcome to redistribute it under certain conditions; see COPYING
* for details.

error   : Failed to load plugin libstageplugin.
error   : libtool reports error: file not found
error   : plugin search path: /home/cs246/src/Stage-3.2.2-Source/worlds:.:/usr/local/lib/
error   : failed to load plugin: libstageplugin
error   : failed to parse config file empty.cfg driver blocks
</code></pre>

<p>Can anyone help me in resolving it.</p>
",8/13/2013 13:39,,140,0,0,1,,2668817.0,,8/9/2013 18:03,58.0,,,,,,,
4487,75081222,PID Control for a DC Motor : Reducing Settling Time,|robotics|pid-controller|,"<p>I want to control the velocity of a DC motor using PID.</p>
<p>In conventional PID Control, we have
u = kp * error + ki * integral_of_error + kd * derivative of error</p>
<p>In order to reduce settling time, I was thinking of writing instead,
u = kp * error + ki * integral_of_error + kd * derivative of error + f(desired_velocity)
, where the function f returns an approximation of the correct signal value.
f(x) = x/(max_velocity) might work.</p>
<p>I'm trying to avoid using a large ki, by reducing the initial error.</p>
<p>Is this a good idea?
Does it achieve anything or would good tuning get the same results?</p>
<p>In my case, the motor's behaviour should be relatively predictable. I don't expect the resistance to motion to vary drastically.</p>
",1/11/2023 10:00,,84,1,0,0,,13056186.0,,3/13/2020 9:46,5.0,75777431.0,"<p>Your additional term f(desired_velocity) is what we call feedforward gain. This is mostly used to achieve better tracking (smaller steady-state error), but it will also make the controller more robust.</p>
<p>In your case, I would try to increase the proportional gain kp as much as possible. Doing so you'll increase the bandwidth of your controller, and so decrease its settling time. Make sure that your system remains controllable (hence stable), and that there is no oscillatory response. The latter happens often when we increase the bandwidth as it also increases noise at the same time.</p>
",20272518.0,0.0,0.0,,
1648,14818109,How to properly set projection and modelview matrices in OpenGL using camera parameters,|python|opengl|opencv|computer-vision|robotics|,"<p>I have a set of points (3D) taken from a range scanner. Sample data can be found here: <a href=""http://pastebin.com/RBfQLm56"" rel=""nofollow"">http://pastebin.com/RBfQLm56</a></p>

<p>I also have the following parameters for the scanner: </p>

<pre><code>camera matrix
[3871.88184, 0, 950.736938;
  0, 3871.88184, 976.1383059999999;
  0, 0, 1]



distortion coeffs
[0.020208003; -1.41251862; -0.00355229038; -0.00438868301; 6.55825615]



camera to reference point (transform)

[0.0225656671, 0.0194614234, 0.9995559233, 1.2656986283;

  -0.9994773883, -0.0227084301, 0.0230060289, 0.5798922567;

  0.0231460759, -0.99955269, 0.0189388219, -0.2110195758;

  0, 0, 0, 1]
</code></pre>

<p>I am trying to render these points properly using opengl but the rendering does not look right. What is the correct way to set openGL projection and modelview matrix? This is what I currently do - </p>

<pre><code>znear = 0.00001
zfar =  100
K = array([[3871.88184, 0, 950.736938],[0, 3871.88184, 976.1383059999999],[0, 0, 1]])
Rt =array([[0.0225656671, 0.0194614234, 0.9995559233, 1.2656986283],[-0.9994773883, -0.0227084301, 0.0230060289, 0.5798922567],[0.0231460759, -0.99955269, 0.0189388219, -0.2110195758]])
ren.set_projection(K,zfar,znear)
ren.set_projection_from_camera(Rt)
</code></pre>

<p>The function being used are: </p>

<pre><code>def set_projection(self,K,zfar,znear):
    glMatrixMode(GL_PROJECTION);
    glLoadIdentity();
    f_x = K[0,0]
    f_y = K[1,1]
    c_x = K[0,2]
    c_y = K[1,2]
    fovY = 1/(float(f_x)/height * 2);
    aspectRatio = (float(width)/height) * (float(f_y)/f_x);
    near = zfar
    far = znear
    frustum_height = near * fovY;
    frustum_width = frustum_height * aspectRatio;
    offset_x = (width/2 - c_x)/width * frustum_width * 2;
    offset_y = (height/2 - c_y)/height * frustum_height * 2;
    glFrustum(-frustum_width - offset_x, frustum_width - offset_x, -frustum_height - offset_y, frustum_height - offset_y, near, far);


def set_modelview_from_camera(self,Rt):
    glMatrixMode(GL_MODELVIEW)
    glLoadIdentity()
    Rx = array([[1,0,0],[0,0,-1],[0,1,0]])
    R = Rt[:,:3]
    U,S,V = linalg.svd(R)
    R = dot(U,V)
    R[0,:]=-R[0,:]
    t=Rt[:,3]
    M=eye(4)
    M[:3,:3]=dot(R,Rx)
    M[:3,3]=t
    M=M.T
    m=M.flatten()
    glLoadMatrixf(m)
</code></pre>

<p>Then I just render points (pasting snippet): </p>

<pre><code>def renderLIDAR(self,filename):
    glClear(GL_COLOR_BUFFER_BIT|GL_DEPTH_BUFFER_BIT)
    glPushMatrix();

    glEnable(GL_DEPTH_TEST)
    glClear(GL_DEPTH_BUFFER_BIT)
    glPointSize(1.0)
    f = open(filename,'r')
    f.readline() #Contains number of particles
    for line in f:
        line = line.split(' ')
        glBegin(GL_POINTS)
        glColor3f (0.0,1.0,0.0); 
        x = float(line[0])
        y = float(line[1])
        z = float(line[2])
        glVertex3f(x,y,z)
        #print x,y,z
        glEnd()

    glPopMatrix();
</code></pre>
",2/11/2013 18:04,14818668.0,1697,1,0,3,0.0,1166892.0,,1/24/2012 11:40,11.0,14818668.0,"<p>The matrices you get back, most notably the last one in your question are what in OpenGL is the composition of projection and modelview, also called Modelviewprojection, i.e.</p>

<p>MVP = P · M</p>

<p>As long as you're not interested in performing illumination calculations, you can use just that in a vertex shader, i.e.</p>

<pre><code>#version 330

uniform mat4 MVP;
in vec3 position;

void main()
{
    gl_Position = MVP * vec4(position, 1);
}
</code></pre>

<p>BTW, OpenGL and probably the library you're using as well, are using column major order, i.e. the order of the elements in memory is</p>

<pre><code>0 4 8 c
1 5 9 d
2 6 a e
3 7 b f
</code></pre>

<p>so what's written in source code must be thought as ""transposed"" (of course it is not). Since the matrix you wrote follows the same scheme you can just put it into the uniform as it is. The only question that remains are the boundaries of the NDC space used by the range scanner. But that could be taken care of with an additional matrix applied. OpenGL uses the range [-1, 1]^3 so the worst thing that can happen is, that if it's in the other popular NDC range [0, 1]^3, you'll see your geometry just squeezed into the upper left hand corner of your window, and maybe turned ""inside out"" if the Z axis goes into the other direction. Just try it, I'd say it already matches OpenGL.</p>

<p>Anyway, if you want to use it with illumination, you have to decompose it into a projection and a modelview part. Easier said than done, but a good starting point is to orthonormalize the upper left 3×3 submatrix, which yields the rotational part of the modelview 'M'. You then have to find a matrix P, that, when left multiplied with M yields the original matrix. That's an overdetermined set of linear equations, so a Gauss-Jordan scheme can do it. And if I'm not entirely mistaken, what you already got in form of that camera matrix is either the decomposed M or P (I'd go for M).</p>

<p>Once you got that you may want to get the translational part (the 4th column) into the modelview matrix as well.</p>
",524368.0,3.0,4.0,,
4040,68225798,"How stepper motor torque will behave for two different supply, 24v/5A and 36v/5A",|robotics|stepper|,"<p>How stepper motor torque will behave for two different supply, 24v/5A and 36v/5A. I am using three Nema 23 , 10kg-cm stepper motors. Using TB6600 Driver which will limit my current to rated current from the supply. It accepts 12-36v and 2.8 is the rated current.</p>
<p>I want to achieve max torque. I went through T depends on Current.
What is my motor torque behavior, when 24v/5A, 36v/5A. Speed will be very less in my use case - kind of robitics arm.</p>
",7/2/2021 13:09,,240,1,0,0,,10527120.0,,10/19/2018 3:17,14.0,68550350.0,"<p>If the TB6600 is current limiting the motor to 2.8A, then the torque will be identical for any power supply that can supply over 2.8A.</p>
<p>Internally I expect the TB6600 is using a chopper driver to limit the total average current - <a href=""https://i.stack.imgur.com/rfaoz.png"" rel=""nofollow noreferrer"">Chopper driver waveform</a> from <a href=""https://www.linearmotiontips.com/what-is-a-chopper-drive-for-a-stepper-motor/"" rel=""nofollow noreferrer"">https://www.linearmotiontips.com/what-is-a-chopper-drive-for-a-stepper-motor/</a></p>
<p>So if you increase the voltage supplied to the driver, you might see the torque ripple increase in frequency, but the average torque will still be the same.</p>
",15339016.0,0.0,0.0,,
3442,57038494,how can I use gpiozero robot library to change speeds of motors via L298N,|python-3.x|raspberry-pi2|robotics|gpiozero|,"<p>In my raspberry pi, i need to run two motors with a L298N.
I can pwm on enable pins to change speeds. But i saw that gpiozero robot library can make things a lot easier. But 
When using gpiozero robot library, how can i alter speeds of those motors by giving signel to the enable pins.</p>
",7/15/2019 11:09,61753240.0,778,2,0,1,,11390692.0,,4/21/2019 10:50,128.0,59144212.0,"<p>I have exactly the same situation.  You can of course program the motors separately but it is nice to use the robot class.
Looking into the gpiocode for this, I find that in our case the left and right tuples have a third parameter which is the pin for PWM motor speed control. (GPIO Pins  12 13 18 19 have hardware PWM support).  The first two outout pins in the tuple are to be signalled as 1, 0 for forward, 0,1 for back. 
So here is my line of code:
    Initio = Robot(left=(4, 5, 12), right=(17, 18, 13))</p>

<p>Hope it works for you!
I have some interesting code on the stocks for controlling the robot's absolute position, so it can explore its environment.</p>
",5559525.0,1.0,0.0,,
1407,11581346,Voice and face recognition libraries for .Net Gadgeteer,|voice-recognition|robotics|face-recognition|.net-micro-framework|.net-gadgeteer|,"<p>Recently, I've been looking into robotics and I've become interested in trying to make a robot of my own. It woulde be a simple one but I want it to be capable of face and speech recognition at least.</p>

<p>I've found a set called EZ-Robots that comes with face/speech recognition libraries already, but thought that .NET gadgeteer would be better (more simple, scalable...). So far, I haven't tried any programming yet, I still need to decide which set to buy (which way to go).</p>

<p>I am an experienced programmer, but absolute beginner in harware stuff.</p>

<p>My vision is that I focus mostly on software and build only basic, simple hardware. I want to interact with robot so he must recognize me (or at least tell human from other objects). I would like to speak to it, give it command etc. I live in Czech Repubic and there isn't much software available for voice recognition in my language. If I could record a command and he would recognize it, that's enough.</p>

<p>It is more of a hobby matter :).</p>

<p>For any advice, I would be grateful.</p>
",7/20/2012 14:26,,472,0,0,1,,840405.0,Czech Republic and Slovakia,7/12/2011 9:27,201.0,,,,,,,
3924,65115603,lost in 3D space - tilt values (euler?) from rotation matrix (javafx affine) only works partially,|javafx|rotation|robotics|,"<p>it is a while ago that I asked this question:
<a href=""https://stackoverflow.com/questions/48850937/javafx-how-to-apply-yaw-pitch-and-roll-deltas-not-euler-to-a-node-in-respec"">javafx - How to apply yaw, pitch and roll deltas (not euler) to a node in respect to the nodes rotation axes instead of the scene rotation axes?</a></p>
<p>Today I want to ask, how I can get the tilt (fore-back and sideways) relative to the body (not to the room) from the rotation matrix. To make the problem understandable, I took the final code from the fantastic answer of José Pereda and basicly added a method &quot;getEulersFromRotationMatrix&quot;. This is working a bit, but at some point freaks out.</p>
<p>Attached find the whole working example. The problem becomes clear with the following click path:</p>
<pre><code>// right after start
tilt fore
tilt left  // all right
tilt right
tilt back  // all right

// right after start
turn right
turn right
turn right
tilt fore
tilt back  // all right
tilt left  // bang, tilt values are completely off
</code></pre>
<p>While the buttons move the torso as expected, the tilt values (printed out under the buttons) behave wrong at some point.</p>
<pre><code>import javafx.application.Application;
import javafx.application.Platform;
import javafx.event.ActionEvent;
import javafx.event.EventHandler;
import javafx.geometry.Point3D;
import javafx.scene.Group;
import javafx.scene.Parent;
import javafx.scene.PerspectiveCamera;
import javafx.scene.Scene;
import javafx.scene.SceneAntialiasing;
import javafx.scene.SubScene;
import javafx.scene.control.Button;
import javafx.scene.control.Label;
import javafx.scene.layout.HBox;
import javafx.scene.layout.StackPane;
import javafx.scene.layout.VBox;
import javafx.scene.paint.Color;
import javafx.scene.paint.PhongMaterial;
import javafx.scene.shape.Box;
import javafx.scene.transform.Affine;
import javafx.scene.transform.Rotate;
import javafx.stage.Stage;


public class PuppetTestApp extends Application {
    
    private final int width = 800;
    private final int height = 500;
    private XGroup torsoGroup;
    private final double torsoX = 50;
    private final double torsoY = 80;

    private Label output = new Label();

    public Parent createRobot() {
        Box torso = new Box(torsoX, torsoY, 20);
        torso.setMaterial(new PhongMaterial(Color.RED));
        Box head = new Box(20, 20, 20);
        head.setMaterial(new PhongMaterial(Color.YELLOW.darker()));
        head.setTranslateY(-torsoY / 2 -10);

        Box x = new Box(200, 2, 2);
        x.setMaterial(new PhongMaterial(Color.BLUE));
        Box y = new Box(2, 200, 2);
        y.setMaterial(new PhongMaterial(Color.BLUEVIOLET));
        Box z = new Box(2, 2, 200);
        z.setMaterial(new PhongMaterial(Color.BURLYWOOD));

        torsoGroup = new XGroup();
        torsoGroup.getChildren().addAll(torso, head, x, y, z);
        return torsoGroup;
    }

    public Parent createUI() {
        HBox buttonBox = new HBox();

        Button b;
        buttonBox.getChildren().add(b = new Button(&quot;Exit&quot;));
        b.setOnAction( (ActionEvent arg0) -&gt; { Platform.exit(); } );

        buttonBox.getChildren().add(b = new Button(&quot;tilt fore&quot;));
        b.setOnAction(new TurnAction(torsoGroup.rx, 15) );

        buttonBox.getChildren().add(b = new Button(&quot;tilt back&quot;));
        b.setOnAction(new TurnAction(torsoGroup.rx, -15) );

        buttonBox.getChildren().add(b = new Button(&quot;tilt left&quot;));
        b.setOnAction(new TurnAction(torsoGroup.rz, 15) );

        buttonBox.getChildren().add(b = new Button(&quot;tilt right&quot;));
        b.setOnAction(new TurnAction(torsoGroup.rz, -15) );

        buttonBox.getChildren().add(b = new Button(&quot;turn left&quot;));
        b.setOnAction(new TurnAction(torsoGroup.ry, -28) ); // not 30 degree to avoid any gymbal lock problems

        buttonBox.getChildren().add(b = new Button(&quot;turn right&quot;));
        b.setOnAction(new TurnAction(torsoGroup.ry, 28) ); // not 30 degree to avoid any gymbal lock problems

        VBox vbox = new VBox();
        vbox.getChildren().add(buttonBox);
        vbox.getChildren().add(output);
        return vbox;
    }

    class TurnAction implements EventHandler&lt;ActionEvent&gt; {
        final Rotate rotate;
        double deltaAngle;

        public TurnAction(Rotate rotate, double targetAngle) {
            this.rotate = rotate;
            this.deltaAngle = targetAngle;
        }

        @Override
        public void handle(ActionEvent arg0) {
            addRotate(torsoGroup, rotate, deltaAngle);
        } 
    }

    private void addRotate(XGroup node, Rotate rotate, double angle) {
        Affine affine = node.getTransforms().isEmpty() ? new Affine() : new Affine(node.getTransforms().get(0));
        double A11 = affine.getMxx(), A12 = affine.getMxy(), A13 = affine.getMxz(); 
        double A21 = affine.getMyx(), A22 = affine.getMyy(), A23 = affine.getMyz(); 
        double A31 = affine.getMzx(), A32 = affine.getMzy(), A33 = affine.getMzz(); 

        Rotate newRotateX = new Rotate(angle, new Point3D(A11, A21, A31));
        Rotate newRotateY = new Rotate(angle, new Point3D(A12, A22, A32));
        Rotate newRotateZ = new Rotate(angle, new Point3D(A13, A23, A33));

        affine.prepend(rotate.getAxis() == Rotate.X_AXIS ? newRotateX : 
                rotate.getAxis() == Rotate.Y_AXIS ? newRotateY : newRotateZ);

        EulerValues euler= getEulersFromRotationMatrix(affine);
        output.setText(String.format(&quot;tilt fore/back=%3.0f    tilt sideways=%3.0f&quot;, euler.forward, euler.leftSide));
        
        node.getTransforms().setAll(affine);
    }

    public class XGroup extends Group {
        public Rotate rx = new Rotate(0, Rotate.X_AXIS);
        public Rotate ry = new Rotate(0, Rotate.Y_AXIS);
        public Rotate rz = new Rotate(0, Rotate.Z_AXIS);
    }

    @Override 
    public void start(Stage stage) throws Exception {
        Parent robot = createRobot();
        SubScene subScene = new SubScene(robot, width, height, true, SceneAntialiasing.BALANCED);
        PerspectiveCamera camera = new PerspectiveCamera(true);
        camera.setNearClip(0.01);
        camera.setFarClip(100000);
        camera.setTranslateZ(-400);
        subScene.setCamera(camera);

        Parent ui = createUI();
        StackPane combined = new StackPane(ui, subScene);
        combined.setStyle(&quot;-fx-background-color: linear-gradient(to bottom, cornsilk, midnightblue);&quot;);

        Scene scene = new Scene(combined, width, height);
        stage.setScene(scene);
        stage.show();
    }

    /**
     * Shall return the tilt values relative to the body (not relative to the room)
     * (Maybe euler angles are not the right term here, but anyway)
     */
    private EulerValues getEulersFromRotationMatrix(Affine rot) {
        double eulerX;  // turn left/right
        double eulerY;  // tilt fore/back
        double eulerZ;  // tilt sideways

        double r11 = rot.getMxx();
        double r12 = rot.getMxy();
        double r13 = rot.getMxz();

        double r21 = rot.getMyx();

        double r31 = rot.getMzx();
        double r32 = rot.getMzy();
        double r33 = rot.getMzz();

        // used instructions from https://www.gregslabaugh.net/publications/euler.pdf
        
        if (r31 != 1.0 &amp;&amp; r31 != -1.0) {
            eulerX  = -Math.asin(r31);  // already tried with the 2nd solution as well
            double cosX = Math.cos(eulerX);
            eulerY = Math.atan2(r32/cosX, r33/cosX);
            eulerZ = Math.atan2(r21/cosX, r11/cosX);
        }
        else {
            eulerZ = 0;
            if (r31 == -1) {
                eulerX = Math.PI / 2;
                eulerY = Math.atan2(r12, r13);
            }
            else {
                eulerX = -Math.PI / 2;
                eulerY = Math.atan2(-r12, -r13);
            }
        }
        
        return new EulerValues(
                eulerY / Math.PI * 180.0, 
                eulerZ / Math.PI * 180.0, 
                -eulerX / Math.PI * 180.0);     
    }
    
    public class EulerValues {
        public double leftTurn;
        public double forward;
        public double leftSide;

        public EulerValues(double forward, double leftSide, double leftTurn) {
            this.forward = forward;
            this.leftSide = leftSide;
            this.leftTurn = leftTurn;
        }
    }


    public static void main(String[] args) {
        launch(args);
    }
}

</code></pre>
<p>PS: This may look like I have close to no progress, but this is only because I try to reduce the question to the possible minimum. If you want to see how this stuff is embedded in my main project, you can watch this little video I just uploaded (but does not add anything to the question): <a href=""https://www.youtube.com/watch?v=R3t8BIHeo7k"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=R3t8BIHeo7k</a></p>
",12/2/2020 20:23,65382260.0,249,1,3,0,,2081279.0,"Munich, Germany",2/17/2013 20:41,87.0,65382260.0,"<p>I think I got it by myself now: What I computed was the &quot;default&quot; euler angles, sometimes refered to as z x' z'', where the 1st and 3th rotation is around the same axis. But what I am looking for are the angles that can be applied to the z, y' and x'' achses (in that order) to reach the position presented by the rotation matrix. (and then ignore the z rotation).</p>
<p>Or even better compute the z y' x'' eulers and the z x' y'' eulers and
only use the x' and y' values.</p>
<p>Added:
No, that was wrong. I indeed calculated the Tait-Bryan x y z rotations. So this was not the solution.</p>
<p>Ok, new explanation:</p>
<p>The rotation axes wthat I calculate are room relative rotations (not object relative rotations), and the 2nd rotation is at the vertical axe (which I am not interested in). But because it is &quot;in the middle&quot;, it can cancel out the 1st and 3th rotation, and this is what happens.</p>
<p>So the solution should be the change the rotation order, that comes out of my matrix-to-euler algorithm. But how to do this?</p>
<p>I just exchanged all &quot;y&quot; and &quot;z&quot;:</p>
<pre><code>    r11 = rot.getMxx();
    r12 = rot.getMxz();
    r13 = rot.getMxy();

    r21 = rot.getMzx();

    r31 = rot.getMyx();
    r32 = rot.getMyz();
    r33 = rot.getMyy();
</code></pre>
<p>and now it really does what I want.  :)</p>
",2081279.0,0.0,0.0,115405053.0,"Ok - I was re-testing with a different version. Back to the version I posted above: When I do 3x turn-right, then tilt-fore and tilt-back, then I get values as expected: tilt-fore/back=0 and tilt-sideways=0. But when I add on single tilt-left to that, then I get tilt-fore/back=68 and tilt-sideways=69. Isn't this completely off from what we should expect, as both tilts were zero before, and we just did a 30° tilting to one side? Or do you get different values?"
2899,46326949,Is it possible to change the OMPL source code for the V-Rep plug-in?,|robotics|motion-planning|,"<p>I develop motion planning algorithms using ompl and I'm wondering if I can somehow change the V-Rep ompl plug-in so it runs my own ompl planning algorithms (like replace RRT-Connect, FMT,... etc with my own algorithms)?</p>

<p>How should I have do this?</p>
",9/20/2017 16:09,,149,1,0,0,,8642761.0,,9/20/2017 15:49,1.0,47566014.0,"<p>Here is how to add a new planner in OMPL. OMPL is installed from source.</p>

<ol>
<li>Add the source code of the planner to <code>ompl/src/ompl/geometric/planners</code> just like all the other planners.</li>
<li>Regenerate CMake related files and make. In <code>ompl/build/Release/</code>, do 
<code>cmake . ../..</code> then <code>make -j4</code></li>
<li>Include the header file of your custom planner wherever needed.</li>
</ol>
",3504538.0,0.0,0.0,,
3893,64386937,Navigating the robot by finding the shortest way,|python|for-loop|shortest-path|robotics|,"<p>I have this task where I have to make my robot find the shortest way when I enter a destination point. I've created some functions to assign numbers (distances) for each square and counts its way back to my robot by deleting the other options. Then the robot should only follow the numbers.</p>
<p><img src=""https://i.stack.imgur.com/ZzM52.jpg"" alt=""Here's also a screenshot of the map it navigates in:
"" /></p>
<p>My code works so far, however I think I should be able to reach the same outcome by using less for loops and with a more efficient writing. I believe if I see different ways of thinking in my earlier stages, I can have a broader perspective in the future. So, any ideas on how to reach my goal with a shorter code?</p>
<pre><code>    #Your Code Starts Here
&quot;&quot;&quot;
for x in range(0, map.width):
    for y in range(0, map.height): 
        if map.blocked(x, y):
            map.value[x, y] = -1 
        else:
            map.value[x, y] = 0
&quot;&quot;&quot;
def fill_around(n,m,number):
    for x in range (n-1,n+2):
        for y in range (m-1,m+2):
            if map.value[x,y] != 0:
                pass
            elif x==n+1 and y==m+1 or x==n-1 and y==m-1 or x==n-1 and y==m+1 or x==n+1 and y==m-1:
                pass
            elif map.blocked(x,y) == True:
                map.value[x,y]= -1            
            elif x==n and y==m:
                map.value[x,y]= number
            else:
                map.value[x,y]= number+1


def till_final(final_x,final_y):
final_x=9 
final_y=1
fill_around(1,1,1)
for p in range(2,17):
    for k in range(0, map.width):
        for l in range(0, map.height):
            if map.value[final_x,final_y] ==0 and map.value[k,l]==p:
                fill_around(k,l,p)

def delete_duplicates(final_x,final_y):
for k in range(0, map.width):
    for l in range(0, map.height):
        if map.value[k,l] == map.value[final_x,final_y] and k != final_x and l != final_y:
            map.value[k,l] = 0
</code></pre>
",10/16/2020 9:56,,126,0,2,1,,10329083.0,,9/7/2018 6:27,1.0,,,,,,113854038.0,"If your code works and you are looking for advice on improving it, your question is off-topic for SO, but would probably be a good fit for https://codereview.stackexchange.com/."
4034,67731514,How do I get eulerangles from two Vector3 coordinates based on openpose 3d?,|python|robotics|openpose|,"<p>In short. I want to make following program.</p>
<p>Input: Two Vector3 coordinates
P1 = (x1, y1, z1)
P2 = (x2, y2, z2)</p>
<p>output: one Eulerangles (P1-&gt;P2 or P2-&gt;P1).</p>
<p>I'm trying to apply 3d openpose joint data to robot arm control.
3d openpose data is constructed by Vector3 (x, y, z).
but I must use EulerAngles to control a robot arm.</p>
<p>Please tell me how to calculate EulerAngles from two Vector3 coordinates.</p>
<p>The following diagram outlines what I want to do.
Sorry for the hand-drawn illustration.<br />
<a href=""https://i.stack.imgur.com/ZmqqC.jpg"" rel=""nofollow noreferrer"">outline diagram</a></p>
<p>The following is a brief summary of code</p>
<pre class=""lang-py prettyprint-override""><code>def convert_pos2angle(P1, P2):
    
    ## some code here.
    

    return angle


def main():
    #sample input
    P1 = [0, 0, 0]
    P2 = [1, 1, 1]
    
    #convert
    angle = convert_pos2angle(P1, P2)

    print(angle)

    
</code></pre>
",5/28/2021 0:42,67835293.0,161,1,2,0,,16049595.0,,5/27/2021 13:52,2.0,67835293.0,"<p>I was able to solve this problem on my own.
I found the project &quot;video2bvh&quot; on GitHub.
It Converts openpose to BVH data.
These programs work very well.</p>
<p>GitHub: <a href=""https://github.com/KevinLTT/video2bvh"" rel=""nofollow noreferrer"">https://github.com/KevinLTT/video2bvh</a></p>
",16049595.0,0.0,0.0,119724260.0,"I am sorry, but I have not made any code.
I added a diagram and brief summary of code to help people understand what I want to do."
2989,48227848,Using for / list comprehension for creating a tuple from any amount of other tuples,|python|tuples|list-comprehension|robotics|,"<p>With the following code I'm looking at how to create the <code>TRACKS[0]</code> and <code>ARM[0]</code> tuples (or even a whole set, e.g. <code>ARM</code>), as they are very similar - I think something like a list comprehension would work (as I'm picturing a for each loop).</p>

<pre><code># MOTORS: all, m1, m2, m3, m4, m5 (+, -)
MOTORS = (
  (
    (0b01010101, 0b00000001, 0b00000000),
    (0b10101010, 0b00000010, 0b00000000)
  ),
  (
    (2**0, 0, 0),
    (2**1, 0, 0)
  ),
  (
    (2**2, 0, 0),
    (2**3, 0, 0)
  ),
  (
    (2**4, 0, 0),
    (2**5, 0, 0)
  ),
  (
    (2**6, 0, 0),
    (2**7, 0, 0)
  ),
  (
    (0, 2**0, 0),
    (0, 2**1, 0)
  )
)

LED = (0,0,1)

# TRACKS: both, left, right (forward, reverse) 
TRACKS = (
    (
      (MOTORS[4][0][0] | MOTORS[5][0][0], MOTORS[4][0][1] | MOTORS[5][0][1], MOTORS[4][0][2] | MOTORS[5][0][2]),
      (MOTORS[4][1][0] | MOTORS[5][1][0], MOTORS[4][1][1] | MOTORS[5][1][1], MOTORS[4][1][2] | MOTORS[5][1][2])
    ),
    MOTORS[4],
    MOTORS[5]
  )

# ARM: all, elbow, wrist, grip (forward/open, reverse/close)  
ARM = (
  (
      (MOTORS[1][0][0] | MOTORS[2][0][0] | MOTORS[3][0][0], MOTORS[1][0][1] | MOTORS[2][0][1] | MOTORS[3][0][1], MOTORS[1][0][2] | MOTORS[2][0][2] | MOTORS[3][0][2]),
      (MOTORS[1][1][0] | MOTORS[2][1][0] | MOTORS[3][1][0], MOTORS[1][1][1] | MOTORS[2][1][1] | MOTORS[3][1][1], MOTORS[1][1][2] | MOTORS[2][1][2] | MOTORS[3][1][2])
    ),
    MOTORS[1],
    MOTORS[2],
    MOTORS[3]
  )

def motormsk (motor_id, motor_config):
  return (motor_config[motor_id][0][0] | motor_config[motor_id][1][0], motor_config[motor_id][0][1] | motor_config[motor_id][1][1], motor_config[motor_id][0][2] | motor_config[motor_id][1][2])
</code></pre>

<p>The <code>motormsk</code> function does a logical <code>OR</code> to create a mask of the values passed in and I thought that it could be used recursively to generate the mask, the function would need to take any number of tuples.</p>

<p>This configuration is used to interface with a USB motor control interface (as in the OWI-535 Robotic Arm Edge), that I'm adding virtual system config (<code>ARM</code> and <code>TRACKS</code>) to allow me to change them around / re-purpose them easily. </p>

<p>USAGE: sending <code>MOTORS[0][0]</code> starts all motors forward, <code>TRACKS[0][1]</code> starts the tracks in reverse, <code>TRACKS[1][0]</code> starts the left track forward and <code>motormsk(3, ARM)</code> stops the grip motor, etc.</p>

<p>There is a repl.it here: <a href=""https://repl.it/@zimchaa/robo-config"" rel=""nofollow noreferrer"">https://repl.it/@zimchaa/robo-config</a> - Thanks.</p>

<p>EDIT: With a suggestion to reword and a clarification of the question I've had a go at the problem for 2 elements:</p>

<pre><code>def motorcmb (motor_id_1, motor_dir_1, motor_id_2, motor_dir_2, motor_config):
  return (motor_config[motor_id_1][motor_dir_1][0] | motor_config[motor_id_2][motor_dir_2][0], motor_config[motor_id_1][motor_dir_1][1] | motor_config[motor_id_2][motor_dir_2][1], motor_config[motor_id_1][motor_dir_1][2] | motor_config[motor_id_2][motor_dir_2][2])
</code></pre>

<p>This produces: <code>motorcmb(1, 0, 2, 1, TRACKS)</code></p>

<p><code>=&gt; (64, 2, 0)</code></p>

<p>I'd still like to see what's possible / best practices for arbitrary numbers of elements.</p>
",1/12/2018 14:03,48229206.0,106,1,6,1,,9209207.0,UK,1/12/2018 13:25,3.0,48229206.0,"<p>I suggest using <code>itertools.chain()</code> to reduce a variable number of tuples to a single sequence, and then</p>

<pre><code>from operator import __or__
from functools import reduce 
x = reduce(__or__, myiterable)
</code></pre>

<p>to <code>or</code> them all together. I don't really understand the way your tuples are nested so I'm not going to try and go into specifics.</p>
",2084384.0,0.0,1.0,83439916.0,"@BoarGules - yes, essentially - although `motormsk()` does something a bit more specific - a general replacement could take an arbitrary number of motor tuples, the direction (i.e. +, -) and `OR` the components."
3056,49581278,Origin of a ROS map representation,|datagrid|maps|ros|robotics|,"<p>I am working with ROS and its map_server node.</p>

<p>I dont understand what the origin metadata info of a map means (conceptually). According to the official documetation:</p>

<blockquote>
  <p>origin : The 2-D pose of the lower-left pixel in the map, as (x, y,
  yaw), with yaw as counterclockwise rotation (yaw=0 means no rotation).
  Many parts of the system currently ignore yaw.</p>
</blockquote>

<p>It is not the initial pose of the robot? But It establish some area of interest of the occupance grid?</p>

<p>Why this value is so important for the Navigation Stack?</p>

<p>Can you give me a simple example of the same map with different origins?</p>
",3/30/2018 20:51,,4146,2,0,2,,8384050.0,,7/28/2017 21:04,9.0,51194449.0,"<p>Origin is generally the position of the robot at the beginning of the program. So yes the initial pose of the robot. When I've used it, it could be used as the original position of the robot. Generally, when using origin you create a deep copy of the current position.</p>

<pre><code>def initPose(self):
    origin = copy.deepcopy(self._current)

    q = [origin.orientation.x,
        origin.orientation.y,
        origin.orientation.z,
        origin.orientation.w]  # quaternion nonsense

    (roll, pitch, yaw) = euler_from_quaternion(q)
    return (self._current.position.x, self._current.position.y, yaw)

    # self._odom_list.waitForTransform('YOUR_STRING_HERE', 'YOUR_STRING_HERE', rospy.Time(0), rospy.Duration(1.0))
</code></pre>

<p>But I've also used origin to be the origin of a function. </p>

<pre><code>def navToPose(self, goal):
# self._odom_list.waitForTransform('map', 'base_footprint', rospy.Time(0), rospy.Duration(1.0))
# transGoal = self._odom_list.transformPose('base_footprint', goal) # transform the nav goal from the global coordinate system to the robot's coordinate system
    origin = copy.deepcopy(self._current)

    q = [origin.orientation.x,
         origin.orientation.y,
         origin.orientation.z,
         origin.orientation.w]  # quaternion nonsense

    (roll, pitch, yaw) = euler_from_quaternion(q)
     qc = [self._current.orientation.x,
           self._current.orientation.y,
           self._current.orientation.z,
           self._current.orientation.w]
    (rollc, pitchc, yawc) = euler_from_quaternion(qc)
    x = goal.pose.position.x
    y = goal.pose.position.y
    cx = origin.position.x
    cy = self._current.position.y

    print('current', cx, cy)
    print(x, y)
    theta = math.atan2(y-cy, x-cx)
    print ('angle is ', theta)
    self.rotate(theta)
    distance = (((x - cx) ** 2) + ((y - cy) ** 2)) ** .5
    print ('distance is ', distance)
    self.driveStraight(0.5, distance)
</code></pre>

<p>So generally, I've used it more as another variable. </p>

<p>Depending on how the Occupancy grid is done. Sometime the origin will refer to where it started on the grid. Allowing the program to know if it still on the map. This can create issues shown here: <a href=""https://answers.ros.org/question/285602/static-map-corner-at-origin-for-navigation-stack/"" rel=""nofollow noreferrer"">https://answers.ros.org/question/285602/static-map-corner-at-origin-for-navigation-stack/</a> (at least from what I've experienced)</p>

<p>For more information on the nav stack go: <a href=""http://wiki.ros.org/navigation"" rel=""nofollow noreferrer"">http://wiki.ros.org/navigation</a>
and here: <a href=""https://www.dis.uniroma1.it/~nardi/Didattica/CAI/matdid/robot-programming-ROS-introduction-to-navigation.pdf"" rel=""nofollow noreferrer"">https://www.dis.uniroma1.it/~nardi/Didattica/CAI/matdid/robot-programming-ROS-introduction-to-navigation.pdf</a></p>
",10026869.0,0.0,0.0,,
3633,60086354,Finding fastest path for robotic drive base,|algorithm|physics|path-finding|robotics|,"<p>I am trying to create an algorithm to find the fastest path a robot can take between 2 points in terms of time. The robot I would be using would be powered by a drive wheel on each side and have limited acceleration and velocity. I was also hoping to build in some obstacle avoidance so the robot is able to path around obstacles. I am familiar with pathfinding algorithms such as a* that find the fastest path in terms of distance between two points but this algorithm does not always find the fastest path for a robot with bounded acceleration and velocity.</p>

<p>I've been thinking about this for a couple days and honestly I'm not really sure where to start or where to find resources on the topic so any help is appreciated.</p>

<p>Example:</p>

<p><a href=""https://i.stack.imgur.com/21ZXz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/21ZXz.png"" alt=""img""></a></p>

<p>Lets say our robot is 10 units wide and each wheel has a maximum speed of 100 units/second and a maximum acceleration of 10 units/second/second.</p>

<p>At point A (x = 0, y = 0) the velocities of the robots wheels are 50 and 30 for the left and right wheels respectively and the robot is at a 30 degree angle relative to the x-axis. At point B (x = 1000, y = -600) we want the robot to be stationary and have a -75 degree angle relative to the x-axis</p>

<p>What is the most time efficient path for the robot to take from point A to point B given it's starting and ending velocities and headings while also avoiding the obstacles?</p>
",2/6/2020 1:11,,125,0,5,1,,10887179.0,,1/9/2019 1:12,3.0,,,,,,106272787.0,The search terms `hybrid a*` and `hybrid a* stanford` get some interesting hits.
2199,29168032,Extended Kalman Filter - error in update step,|matlab|prediction|robotics|kalman-filter|atan2|,"<p>I have to implement EKF without actually having good mathematical understanding of it. (Great... not...) So far I have been doing well but since I tried to implement the prediction step things started going wrong. </p>

<ol>
<li>The agent that uses EKF (red) shoots off in a random direction</li>
<li>Eventually some variables (pose, Sigma, S, K) become NaNs and the simulation fails</li>
</ol>

<p><img src=""https://i.stack.imgur.com/7S9YC.png"" alt=""enter image description here""></p>

<p>I base my code on the code from Thrun's ""Probabilistic Robotics"" on page. 204. This is the part of the code that seems to be messing things up</p>

<pre><code>% Get variables
[x,y,th] = getPose(mu_bar);
numoffeatures = size(map,2);

for f = 1:numoffeatures
    j = c(f);
    [fx,fy] = getFeatures(map,f);

    q = (fx-x).^2 + (fy-y).^2;  

    z_hat = [sqrt(q);
             atan2(fy-y,fx-x)-th;
             j];

    H = [(-fx-x)/sqrt(q) (-fy-y)/sqrt(q) 0;
          (fy-y)/q       (-fx-x)/q      -1;
                0               0        0];

    S = H*Sigma_bar*H'+Q;
    K = Sigma_bar*H'/inv(S);

    mu_bar    = mu_bar+K*(z(:,j)-z_hat);
    Sigma_bar = (eye(3)-K*H)*Sigma_bar; 
end
</code></pre>

<p>I am totally clueless... Any ideas and hints will be appreciated. Thank you.  </p>

<p><strong>UPDATE</strong></p>

<p>The reason of the agent shooting off is the 'error' when computing the difference between two angles. Those are computed using atan2. Although I know what the problem is I still can't figure out how to fix it.</p>

<p>Let's imagine that after computing atan2 for two objects I have values resulting in a = 135 and b = 45. I computed the difference between them for both possibilities 90 degrees and 270 degrees but the agent still doesn't behave the way it is supposed to. I've never really encountered atan2 before. Is my understanding of calculating the difference between atan2 values wrong? Here is the illustration of my understanding:</p>

<p><img src=""https://i.stack.imgur.com/2yFez.jpg"" alt=""enter image description here""></p>
",3/20/2015 13:45,,755,1,9,0,,1160598.0,,1/20/2012 12:19,187.0,29175515.0,"<p><code>Q</code> is the process noise?
You cannot set the process noise as </p>

<pre><code>Q = randn*eye(3);
</code></pre>

<p>because you may have negative covariance, this doesn't make sense.</p>
",2827670.0,0.0,4.0,46560281.0,"q is the euclidean distance -> if q = Inf, then one of the parameters fx,fy,x,y is also Inf.  Which one?"
3940,65679789,Obtaining input from Joystick with C# and DirectX using Tutorial,|c#|directx|visual-studio-2019|robotics|joystick|,"<p>I'm using Visual Studio 2019 my question is closely aligned with the use of this <a href=""https://www.codeproject.com/Tips/850730/Obtaining-Input-Form-a-Joystick-with-Csharp-and-Di?msg=5633819#xx5633819xx"" rel=""nofollow noreferrer"">Tutorial</a>. I've done all the steps such as:</p>
<ol>
<li>Creation of a Window Form in C#</li>
<li>Adding existing file Joystick.cs</li>
<li>Adding Reference item</li>
<li>Added Application Configuration File and pasted the code provided</li>
<li>Pasted example code into Form1.cs</li>
</ol>
<p>My question: What am I missing? I can't access the library for joystick <a href=""https://i.stack.imgur.com/ehI7w.png"" rel=""nofollow noreferrer"">Screenshot of Error</a></p>
<p>Here is the whole Form1.cs code:</p>
<pre><code>using System;
using System.Collections.Generic;
using System.ComponentModel;
using System.Data;
using System.Drawing;
using System.Linq;
using System.Text;
using System.Threading.Tasks;
using System.Windows.Forms;

namespace RunJoyStickOnLocalMachine{

public partial class Form1 : Form
{
    public Form1()
    {
        InitializeComponent();
    }

    private void Form1_Load(object sender, EventArgs e)
    {

    }
    private void joystickTimer_Tick_1(object sender, EventArgs e)
    {
        try
        {
            joystick.UpdateStatus();
            joystickButtons = joystick.buttons;

            if (joystick.Xaxis == 0)
                output.Text += &quot;Left\n&quot;;

            if (joystick.Xaxis == 65535)
                output.Text += &quot;Right\n&quot;;

            if (joystick.Yaxis == 0)
                output.Text += &quot;Up\n&quot;;

            if (joystick.Yaxis == 65535)
                output.Text += &quot;Down\n&quot;;

            for (int i = 0; i &lt; joystickButtons.Length; i++)
            {
                if (joystickButtons[i] == true)
                    output.Text += &quot;Button &quot; + i + &quot; Pressed\n&quot;;
            }
        }
        catch
        {
            joystickTimer.Enabled = false;
            connectToJoystick(joystick);
        }
    }
}
}
</code></pre>
<p>If there is anything else that I need to provide please notify me.</p>
<p>Thank You!</p>
<hr />
<p>Edit: I've solved this problem by comparing both the tutorial file and the step by step tutorial file that the blog has.</p>
",1/12/2021 7:44,65695088.0,4319,1,2,0,0.0,10965128.0,,1/25/2019 2:39,9.0,65695088.0,"<p>You can download the project from the article you mentioned to check the code.</p>
<p>Based on my test, you can use the following code in the form1.cs code after you add <code>theJoystick.cs</code> to your project.</p>
<pre><code>public partial class Form1 : Form
    {

        private Joystick joystick;        // define the type Joystick
        private bool[] joystickButtons;  // here  define the bool array
        public Form1()
        {
            InitializeComponent();
            joystick = new Joystick(this.Handle);
            connectToJoystick(joystick);
        }

        private void joystickTimer_Tick(object sender, EventArgs e)
        {
            try
            {
                joystick.UpdateStatus();
                joystickButtons = joystick.buttons;

                if (joystick.Xaxis == 0)
                    output.Text += &quot;Left\n&quot;;    // output is the name of richtextbox

                if (joystick.Xaxis == 65535)
                    output.Text += &quot;Right\n&quot;;

                if (joystick.Yaxis == 0)
                    output.Text += &quot;Up\n&quot;;

                if (joystick.Yaxis == 65535)
                    output.Text += &quot;Down\n&quot;;

                for (int i = 0; i &lt; joystickButtons.Length; i++)
                {
                    if (joystickButtons[i] == true)
                        output.Text += &quot;Button &quot; + i + &quot; Pressed\n&quot;;
                }
            }
            catch
            {
                joystickTimer.Enabled = false;
                connectToJoystick(joystick);
            }
        }
        private void enableTimer()
        {
            if (this.InvokeRequired)
            {
                BeginInvoke(new ThreadStart(delegate ()
                {
                    joystickTimer.Enabled = true;
                }));
            }
            else
                joystickTimer.Enabled = true;
        }
        private void connectToJoystick(Joystick joystick)
        {
            while (true)
            {
                string sticks = joystick.FindJoysticks();
                if (sticks != null)
                {
                    if (joystick.AcquireJoystick(sticks))
                    {
                        enableTimer();
                        break;
                    }
                }
            }
        }
    }
</code></pre>
",11507778.0,0.0,0.0,116153378.0,"Note that tutorial uses legacy Managed DirectX which is *ancient*, and only deployed by the legacy DirectX End-User Runtime. You should use [SlimDX](https://code.google.com/archive/p/slimdx/) or [SharpDX](http://sharpdx.org/) for DirectX interop instead."
2240,29615931,Building an Android app for ROS,|android|gradle|robotics|ros|,"<p>I am following <a href=""http://wiki.ros.org/ApplicationsPlatform/Clients/Android/Tutorials/Getting%20Started#Building_An_Android_App"" rel=""nofollow"">this tutorial</a> to build and android app for ROS</p>

<p>After I import this in Android studio having following problem, I am using Android studio in windows 7.</p>

<pre><code>Error:(66, 0) Gradle DSL method not found: 'android()'
Possible causes:The project 'android_apps' may be using a version of Gradle that does not contain the method.
Open Gradle wrapper fileThe build file may be missing a Gradle plugin.
Apply Gradle plugin
</code></pre>
",4/13/2015 22:09,,330,0,2,0,,474986.0,"Grenoble, France",10/13/2010 19:53,538.0,,,,,,47377663.0,Please include the specific code you are using from the tutorial so this question ages gracefully. It's possible that link might go offline or be moved in the future.
4462,74667949,Robot movement measuring using matlab video processing,|matlab|image-processing|robotics|measurement|,"<p>I'm doing robot project - It need to measure subtle movements in XY direction, while driving in Z direction .
So I was thinking of using a camera with MATLAB and blinking LED attached to a wall - that way using image subtraction I can identify the LED, and with weight matrix locate the center of the light.
Now every period of time I can log the amount of pixels the center moved right-left or up-down directions and check the accuracy of the motion.</p>
<p>But when attempting this sensing solution I had some challenges I couldn't overcome</p>
<ul>
<li>light source like LED/laser has soft edges so the center is not accurate</li>
<li>the camera is not calibrated (and I'm not sure how to calibrate it)</li>
</ul>
<p>Is there other simple solution for this problem?</p>
<p>note: the amount of motion can be proportional.</p>
",12/3/2022 15:11,,45,1,0,0,,10694333.0,world,11/23/2018 7:19,34.0,74668342.0,"<p>You might be able to improve the accuracy of the location of the led by applying some kind of peak interpolation.</p>
<p>For the calibration: Matlab offers an app for camera calibration, maybe that helps you.</p>
",17204618.0,0.0,0.0,,
1630,14514435,How to set an int to 1 if dependent on a button and in a while loop?,|c|robot|,"<p>I'm programming a robot, and unfortunately in its autonomous mode I'm having some issues.
I need to set an integer to 1 when a button is pressed, but in order for the program to recognize the button, it must be in a while loop. As you can imagine, the program ends up in an infinite loop and the integer values end up somewhere near 4,000.</p>

<pre><code> task autonomous()
   {
    while(true)
        {
    if(SensorValue[positionSelectButton] == 1)
        {
            positionSelect = positionSelect + 1;
            wait1Msec(0350);
        }
        }
   }
</code></pre>

<p>I've managed to get the value by using a wait, but I do NOT want to do this. Is there any other way I can approach this?</p>
",1/25/2013 2:32,14515103.0,128,4,2,1,,394842.0,New York,7/17/2010 20:22,304.0,14514458.0,"<p>Try remembering the current position of the button, and only take action when its state changes from off to on.</p>

<p>Depending on the hardware, you might also get a signal as though it flipped back and forth several times in a millisecond.  If that's an issue, you might want to also store the timestamp of the last time the button was activated, and then ignore repeat events during a short window after that.</p>
",459640.0,0.0,1.0,20235194.0,how about `positionSelect = 1;` ? Or set it back to 0 `SensorValue[positionSelectButton] = 0`
3948,65693285,How to load urdf.xacro in Pybullet?,|python|visualization|robotics|pybullet|,"<p>Usually in Pybullet we can do:</p>
<pre><code>robot = p.loadURDF(os.path.join(urdfRootPath, &quot;robot.urdf&quot;),useFixedBase=True)
</code></pre>
<p>to load a URDF. Is it possible to load a urdf.xacro file directly?</p>
",1/12/2021 22:56,,1736,1,0,0,0.0,7106915.0,,11/2/2016 21:05,288.0,68579422.0,"<p>I think you can load <code>.xacro</code> files directly using <code>loadURDF</code> function. Just like a bunch of examples showed <a href=""https://www.programcreek.com/python/example/122126/pybullet.loadURDF"" rel=""nofollow noreferrer"">here</a>.</p>
",8256101.0,0.0,0.0,,
2730,42667789,How to exclude poses of a wheel based robot which place behind of the porevious pose,|robotics|kalman-filter|sensor-fusion|,"<p>I am currently working on coding sensor fusion of a wheel based robot pose from GPS, Lidar, Vision and Vehicle measure. Its model is basic kinematics using EKF and no discrimination against sensors i.e. data comes in based on time stamp.  </p>

<p>I have difficulty to fuse those sensors due to following issue;
Sometimes when the latest incoming data comes in from different sensor from a sensor gave previous state, the latest pose of the robot comes in behind previous pose. Therefore data fusion does not get so smooth and zigzag-ed as a result. </p>

<p>I would like discard data which plots behind/backwards of the previous data and take data which poses always forward/ahead of previous state even when sensor to provide the data changes between timestamp t and timestamp t+1. Since the data frame is global frame, it is impossible to rely on its x coordinate in minus to achieve this. </p>

<p>Please let me know if you had some idea on this. Thank you so much in advance.
Best,</p>
",3/8/2017 9:41,,49,1,0,0,,5558413.0,,11/13/2015 10:29,26.0,43466680.0,"<h2>Preliminary warning</h2>

<p>Let me slip here a warning before suggesting posible solutions to your problem: be careful with discarding data based on your current estimate, since you never know if last measure is ""pulling pose back"" or previous one was wrong and caused your estimate to move forward too much.</p>

<h2>Posible solutions</h2>

<p>In a Kalman-like filter, observations are assumed to provide independent, uncorrelated information about state vector variables. These observations are assumed to have a random error distributed as a zero mean gaussian variable. Real life is harder, though :-(</p>

<p>Sometimes, measures are affected by a ""bias"" (a fixed term, similar to the gaussian error having a non-zero mean). e.g. tropospheric perturbations are known to introduce a position error in GPS fixes that drifts slowly over time.
If you take several sensors observing the same variable, as GPS and Lidar for for position, but they have different biases, your estimation will be jumping back and forth. Scaling problems can have a similar effect.</p>

<p>I will assume this is the root of your problem. If not, please refine your question.</p>

<p>How can you mitigate this problem? I see several alternatives:</p>

<ul>
<li><strong>Introduce a bias/scale correction term in your state vector</strong> to compensate sensor bias/drift. This is a very common trick in EKFs for inertial sensor fusion (gyro/accelerometer), that can work nice when tuned properly.</li>
<li><strong>Apply some preprocessing to sensory inputs</strong> to correct known problems. It can be difficult to tune a filter for estimating state vector and sensor parameters at the same time.</li>
<li><strong>Change how observations are interpreted</strong>. For example, use difference between consecutive position observations so that you are creating a fake odometer sensor. This greatly reduces the drift problem.</li>
<li><strong>Post-process your output</strong>. Instead of discarding observations, integrate them and keep the ""jumping"" state vector internally, but smooth the output vector to eliminate the jumps. This is done in some UAV autopilots because such jumps affect the performance of PID controllers.</li>
</ul>

<p>Finally, the most obvious and simple approach: <strong>discard observations based on some statistical test</strong>. A chi-square test of the residual can be used to determine if an observation is too far from expected values and must be discarded. Be careful with this options, though: observation rejection schemes must be completed with a state vector reinitialization logic to resutl in a stable behavior.</p>

<p>Almost all these solutions require knowning the source of each observation, so you would no longer be able to treat them indistinctly.</p>
",1076211.0,0.0,0.0,,
1894,21059100,UAV simulation in MATLAB for target tracking,|matlab|simulink|robotics|,"<p>My objective is to track a moving target on ground by an UAV and this simulation is to be done in MATLAB. I have written code for developing Urban environment plot and also put a moving point in the plot(for the target).Now for the UAV I need to implement certain guidance laws(is what I understand from resources on the Internet).So how should I proceed with it?Will I be able to write code for that also?Or should I develop a simulink model and integrate my previous code as well?
Thanks a lot in advance.</p>
",1/11/2014 6:03,,1069,1,0,0,,2417324.0,,5/24/2013 11:31,34.0,21074683.0,"<p>It should be possible to implement a simulation either with Matlab or Simulink. Which is more fit will depend on the approach you decide to take. Maybe you should decide what motion planning algorithm to implement before jumping into implementation. Wikipedia has a list of common algorithms:
<a href=""http://en.wikipedia.org/wiki/Motion_planning#Algorithms"" rel=""nofollow"">http://en.wikipedia.org/wiki/Motion_planning#Algorithms</a></p>
",3142795.0,0.0,0.0,,
2299,31898387,Void function with two arguments but not both need to be used,|java|android|null|arguments|robotics|,"<p>First Tech Challenge, A robotics organization now has an android platform instead of Lego. My team is transitioning to java but I have one simple question. If I have a function that has two arguments <code>power1</code> and <code>power2</code> and both can be set to a double but if the second one isn't specified, it shall run both motors with only the <code>power1</code> variable. Here is my code:</p>

<pre><code>package com.qualcomm.ftcrobotcontroller.opmodes;

public class BasicFunctions extends HardwareAccess {

    /*
    This file lists some basic funcions on the robot that we can use without typing large amounts of code.
    To use this code in the opmode add ""extends BasicFunctions"" to the end of the class line
     */
    public void setDriveMotorPower(double power1, double power2) {
        if (power2 = null) {
            motorLeft.setPower(power1);
            motorRight.setPower(power1);
        }

        motorLeft.setPower(power1);
        motorRight.setPower(power2);

    }

    public void init() {

    }

    @Override
    public void loop() {

    }
}
</code></pre>

<p><strong>Update:</strong> Here is a screenshot of Android Studio:</p>

<p><a href=""https://i.stack.imgur.com/wmqcY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wmqcY.png"" alt=""enter image description here""></a></p>
",8/8/2015 20:44,31898467.0,87,2,3,1,,5067982.0,,7/1/2015 4:35,21.0,31898463.0,"<p>If a function has 2 paramters, you have to send them both but there is another idea: If you don't wanna use <code>power2</code> for example you can pass it with some value that is impossible to happen like -1 or anything according to your situation then check <code>if(power2 == -1)</code> and that's better than checking for null.<br>
By the way, the method <code>.setPower()</code> for will be called 4 times because you just used <code>if</code> not <code>if else</code>. It should be like</p>

<pre><code>motorLeft.setPower(power1);
if (power2 == null) { // or -1 like I said before
        motorRight.setPower(power1);
} else {
    motorRight.setPower(power2);
}
</code></pre>
",4371389.0,0.0,3.0,51711929.0,And what is the question ?
2294,31620428,Software to model mechanical linkages,|simulation|robotics|kinematics|,"<p>I recently got interested in Theo Jensen's strandbeest, (If you haven't seen them before look them up! Such incredible engineering.) and I want to mess around with the design he has for his strandbeests' legs. However doing straight forward kinematics is waaay over my head for something like this.</p>

<p>Here's what I'm trying to model:
<a href=""https://upload.wikimedia.org/wikipedia/commons/6/61/Strandbeest-Walking-Animation.gif"" rel=""nofollow"">https://upload.wikimedia.org/wikipedia/commons/6/61/Strandbeest-Walking-Animation.gif</a>
(Can't link directly because I don't have enough reputation :/)</p>

<p>All I really need to know is the path of the 'foot', so something visual isn't necessary.</p>

<p>The final goal is to be able to apply an evolutionary algorithm to it and see if I come up with the same linkage lengths as Theo did, or maybe improve them somehow, so if I there was some software that allowed scripts to be run, that'd be ideal.</p>

<p>Sorry if the question is kind of vague, I'm not all that sure what I'm looking for. Even if there is some maths/engineering topic that would make this easier I'd love to learn.</p>

<p>Thanks!
-Oisin.</p>
",7/24/2015 21:58,31620610.0,676,1,0,1,,2254882.0,,4/7/2013 15:55,10.0,31620610.0,"<p>Well, I searched for Physics Engine, and found a promising result.</p>

<p><strong>Open Dynamics Engine</strong> seems to be an open source physics engine that could fit your needs.</p>

<blockquote>
  <p>The Open Dynamics Engine (ODE) is a free, industrial quality library for simulating articulated rigid body dynamics. Proven applications include simulating ground vehicles, legged creatures, and moving objects in VR environments. It is fast, flexible and robust, and has built-in collision detection.</p>
</blockquote>

<p>Source: <a href=""http://ode-wiki.org/wiki/index.php?title=Manual:_Introduction"" rel=""nofollow"">Wiki Introduction</a></p>

<p>There site is <a href=""http://ode.org/"" rel=""nofollow"">ode.org</a>, and it looks like you should be able to evaluate it from there. ""[S]imulating rigid body dynamics"" is what you want, right? From what I understand, it ought to fit the bill. C++ is probably a reasonable language to attempt this in. I presume you have previous programming experience? This is not what I would consider a beginner's project.</p>

<p>When you get to the evolution, search for Genetic Algorithms. They're frequently used for optimization, and could help you out considerably. Another thing to consider is what you're actually optimizing for (lowest wind speed to function, fasted movement, etc).</p>
",4430622.0,1.0,2.0,,
2724,42510907,Generate trajectories for 3 agents/robots with same tangent in specific point - MATLAB,|matlab|simulink|robotics|,"<p>I want to simulate the movements of 3 robots/agents in space and I would like to generate 3 different trajectories which have one constraint: in a certain time T the all the trajectories must have the same tangent.</p>
<p>I want something like in the following picture:
<a href=""https://i.stack.imgur.com/kBG1q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kBG1q.png"" alt=""enter image description here"" /></a></p>
<p>I need to do it through MATLAB and/or SIMULINK.</p>
",2/28/2017 14:08,,158,2,0,0,,2761849.0,,9/9/2013 14:54,630.0,42513113.0,"<p>I do not know if this is enough for what I need or not but I probably figured out something.</p>

<p>What I did is to fit a polynomial to some points and constraining the derivative of the polynomial in a certain point to be equal to 0. </p>

<p>I used the following function:
<a href=""http://www.mathworks.com/matlabcentral/fileexchange/54207-polyfix-x-y-n-xfix-yfix-xder-dydx-"" rel=""nofollow noreferrer"">http://www.mathworks.com/matlabcentral/fileexchange/54207-polyfix-x-y-n-xfix-yfix-xder-dydx-</a></p>

<p>It is pretty easy, but it saved me some work.</p>

<p>And, if you try the following: </p>

<pre><code>% point you want your functions to pass
p1 = [1 1];
p2 = [1 3];
% First function
x1 = linspace(0,4);
y1 = linspace(0,4);  
p = polyfix(x1,y1,degreePoly,p1(1),p1(2),[1],[0]);
% p = [-0.0767    0.8290   -1.4277    1.6755];
figure
plot(x1,polyval(p,x1))
xlim([0 3])
ylim([0 3])
grid on
hold on

% Second function
x2 = linspace(0,4);  
y2 = linspace(0,4);  
p = polyfix(x2,y2,degreePoly,[1],[3],[1],[0])
% p = [0.4984   -2.7132    3.9312    1.2836];
plot(x2,polyval(p,x2))
xlim([0 3])
ylim([0 3])
grid on
</code></pre>

<p>If you don't have the polyfix function and you don't want to download it you can try the same code commenting the polyfix lines:</p>

<pre><code>  p = polyfix(x1,y1,degreePoly,p1(1),p1(2),[1],[0]);
  p = polyfix(x2,y2,degreePoly,p2(1),p2(1),[1],[0]);
</code></pre>

<p>And uncommenting the lines:</p>

<pre><code>  % p = [-0.0767    0.8290   -1.4277    1.6755];
  % p = [0.4984   -2.7132    3.9312    1.2836];
</code></pre>

<p>You will get this:</p>

<p><a href=""https://i.stack.imgur.com/rm3dd.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rm3dd.jpg"" alt=""enter image description here""></a></p>

<p>Now I will use this polynomial as a position (x,y) over time of my robots and I think I should be done. The x of the polynomial will be also the time, in this way I am sure that the robots will arrive in the 0-derivative point at the same time. </p>

<p>What do you think? Does it make sense?
Thanks again.</p>
",2761849.0,0.0,0.0,,
4367,73277615,"Stereo calibration, do extrinsics change if the lens changes?",|opencv|3d|robotics|stereo-3d|calibration|,"<p>I have a stereo camera setup. Typically I would calibrate the intrinsics of each camera, and then using this result calibrate the extrinsics, so the baseline between the cameras.</p>
<p>What happens now if I change for example the focus or zoom on the lenses? Of course I will have to re-calibrate the intrinsics, but what about the extrinsics?</p>
<p>My first thought would be no, the actual body of the camera didn't move. But on my second thought, doesn't the focal point within the camera change with the changed focus? And isn't the extrinsic calibration actually the calibration between the two focal points of the cameras?</p>
<p>In short: should I re-calibrate the extrinsics of my setup after changing the intrinsics?</p>
<p>Thanks for any help!</p>
",8/8/2022 12:07,,89,1,0,0,,6465752.0,"Zürich, Schweiz",6/14/2016 17:01,19.0,73277682.0,"<p>Yes you should.</p>
<p>It's about the optical center of each camera. Different lenses put that in different places (but hopefully along the optical axis).</p>
",2602877.0,0.0,2.0,,
4049,68386445,Installing MRPT on Fedora,|robotics|mobile-robot-toolkit|,"<p>Can anyone provide a detailed procedure for installing MRPT on Fedora 33 Scientific (one of the Fedora Labs which has a KDE interface)? The MRPT installation instructions for Ubuntu mentions something about cmake/cmake-gui. Checking the man pages, F33Sci has no such thing. It must be possible to accomplish this somehow, because Fedora Robotics Lab includes MRPT. I've already tried &quot;$sudo dnf install mrpt&quot;, resulting in &quot;Error: Unable to find a match: mrpt&quot;. However, &quot;$dnf search mrpt&quot; results in a bunch of items from mrpt-base... to mrpt-stereo-camera-calibration.</p>
",7/15/2021 0:21,,88,1,0,1,,4561673.0,,2/13/2015 0:41,3.0,68453480.0,"<p>The version of MRPT that ships with Fedora is really outdated, so you do well in building from sources.</p>
<p><code>cmake-gui</code> is not 100% required, it is only mentioned in the instructions to make things easier to those preferring GUIs, but you should be able to compile using the console commands <a href=""https://docs.mrpt.org/reference/latest/compiling.html#using-the-console"" rel=""nofollow noreferrer"">here</a> (that is, the standard workflow with cmake).</p>
<p>Next, the CMake configuration process will warn you about missing libraries. Most are optional, but at least make sure of installing eigen3, opencv and wxwidgets. Those should be installed with the standard commands used in Fedora...</p>
",1631514.0,0.0,2.0,,
1131,5611290,Is it a good idea to use a single-board-computer in a UAV robot?,|robotics|,"<p>I'm not sure it's good or bad, the robot should have computer vision for SLAM. What's your idea?</p>
",4/10/2011 11:00,5611298.0,442,2,0,0,0.0,689779.0,"Toronto, ON, Canada",4/3/2011 12:10,822.0,5611298.0,"<p>Yes, that's how we did it when I was in school (albeit nine years ago). You want to focus on algorithms, not learning to program an unfamiliar platform.</p>

<p>Assuming the ""A"" stands for <em>aerial</em>, don't invest in anything you don't want crashing at high speed. And mind the vibrations.</p>
",153285.0,0.0,1.0,,
4067,69101225,PID implementation in line following bot using turtlebot,|python|opencv|ros|robotics|,"<p>I am using ROS melodic,turtlebot 2 on Ubuntu 18.04.</p>
<p>The idea is to create an environment consisting of lines as a path (slightly curved), and to program the turtlebot to follow the lines. Basically, a line following bot.</p>
<p>The idea is to click photos using the camera mounted on the turtlebot,process them to segment out the path which needs to be followed, and then control the bot's orientation, by using PID to control the angular velocity, so that it moves along the path.</p>
<p>I have created a program called <code>take_photo_mod.py</code>, which successfully keeps clicking photos and saving them as <code>image.jpg</code>, at a frequency that can be controlled.</p>
<pre><code> #!/usr/bin/env python
    from __future__ import print_function
    import sys
    import rospy
    import cv2
    from std_msgs.msg import String
    from sensor_msgs.msg import Image
    from cv_bridge import CvBridge, CvBridgeError

    class TakePhoto:
        def __init__(self):
    
            self.bridge = CvBridge()
            self.image_received = False
            img_topic = &quot;/camera/rgb/image_raw&quot;
            self.image_sub = rospy.Subscriber(img_topic, Image, self.callback)
            rospy.sleep(1)
    
        def callback(self, data):
         
            try:
                cv_image = self.bridge.imgmsg_to_cv2(data, &quot;bgr8&quot;)
            except CvBridgeError as e:
                print(e)
    
            self.image_received = True
            self.image = cv_image
    
        def take_picture(self, img_title):
            if self.image_received:
               
                cv2.imwrite(img_title, self.image)
                rospy.loginfo(&quot;received&quot;)
    
                return True
            else:
                return False
    
    if __name__ == '__main__':
    
     
        rospy.init_node('take_photo', anonymous=False)
        camera = TakePhoto()
    
        while not rospy.is_shutdown():
            
            img_title = rospy.get_param('~image_title', 'image'.jpg')
            if camera.take_picture(img_title):
                rospy.loginfo(&quot;Saved image &quot; + img_title)
            else:
                rospy.loginfo(&quot;No images received&quot;)
    
            rospy.sleep(1)
</code></pre>
<p>As for the next part, I plan to create a program <code>go_forward.py</code>, which incorporates two things:</p>
<p><strong>Part 1.</strong> <strong>The openCV aspect</strong>: processing the image clicked to segment out the path. I have successfully managed to do this:</p>
<pre><code>import cv2
import numpy as np
import math as m

def contor(lst):
    if len(lst)&gt;1:
        m = list()
        for i in lst:
            for j in i:
                m.append(j[0])
        m = np.array(m)
    else:
        m=np.array([[w//2,h-150],[w//2,h-40]])
    print(&quot;m=&quot;,m)
    m = m[m[:, 1].argsort()]
    return m

def aver(i, p):
    a = p[i]
    y = int(np.mean([a[:, 0]]))
    x = int(np.mean([a[:, 1]]))
    return (y, x)

def extract(frame):
    theta=0
    phi=0
    print(&quot;h,w =&quot;,h,w)
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    lb = np.array([0, 0, 0])
    ub = np.array([255, 50, 160])
    mask = cv2.inRange(hsv, lb, ub)
    res = cv2.bitwise_and(frame, frame, mask=mask)

   

     res2 = cv2.cvtColor(res, cv2.COLOR_BGR2GRAY)
        ret, thres = cv2.threshold(res2, 100, 255, cv2.THRESH_BINARY)
        countours, hierarchy = cv2.findContours(thres, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
        cv2.drawContours(frame, countours, -1, (255, 0, 255), 3)
    
        points = contor(countours)
        i = np.where((points[:, 1] &gt; h - 80) &amp; (points[:, 1] &lt; h))
        #print('i1', len(i[0]))
        if len(i[0]) &gt; 0:
            end = aver(i, points) #coordinates (x2,y2)
        else:
            end = (w // 2, h)
        i = np.where((points[:, 1] &gt; h - 190) &amp; (points[:, 1] &lt; h - 110))
        #print('i2', len(i[0]))
        if len(i[0]) &gt; 0:
            strt = aver(i, points) #coordinates (x1,y1)
        else:
            strt = (w // 2, h - 200)
            
    
        
        frame=cv2.line(frame,strt,end,(255,0,0),9)
        cv2.imshow(&quot;aa&quot;,frame)
        cv2.waitKey()
        cv2.destroyAllWindows()
        
        if (strt[0] - end[0]) == 0:
            phi=m.pi/2 #angle with horizontal
            theta=0 #angle with vertical
        else:
            slope = (strt[1] - end[1]) / (end[0] - strt[0])
            phi=m.atan2(slope) #angle with horizontal
            theta= (m.pi/2) - phi #angle with vertical
                
    
        return theta
    
    
    
    frame=cv2.imread(&quot;image.jpg&quot;,1)
    h, w, _ = frame.shape
    i,d=0,0
    error=extract(frame)
    print(error)
</code></pre>
<p>image.jpg ( which I was using for testing the openCV aspect. When take_photo_mod.py runs , image.jpg will keep changing at the frequency with which the bot will take photos)</p>
<p><a href=""https://i.stack.imgur.com/w3KAI.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w3KAI.jpg"" alt=""enter image description here"" /></a></p>
<p>This draws a line that approximately gives the direction along the path. Further, this calculates the angle with the angle with the <strong>vertical</strong>.</p>
<p><strong>Part 2. PID aspect</strong>: This is the part with which I am struggling. The camera is mounted on the bot, so whatever we are calculating on the image is in the bot's frame,and the orientation of the bot in the bot's frame is along the vertical. Hence the angle which we calculate with the vertical acts as the &quot;error&quot; term: the angle by which the bot should turn to be along the path. Now, I think it's the angular velocity which will serve as the control signal.</p>
<p>I already have a script that just makes the bot go forward till its terminated.It had  angular velocity fixed at 0. So,the program &quot;goforward.py&quot; will basically be the combination of the openCV aspect ,and tweaking the &quot;simply go forward&quot; script to use PID to set it's angular velocity (instead of it being fixed at 0). Then, the two programs(nodes) <code>take_photo_mod.py</code> and <code>goforward.py</code> will be launched together using a launch file.</p>
<p>What I tried for <code>goforward.py</code>:</p>
<pre><code>#!/usr/bin/env python

import rospy
import cv2
import numpy as np
import math as m
from std_msgs.msg import String
from sensor_msgs.msg import Image
from cv_bridge import CvBridge, CvBridgeError
from geometry_msgs.msg import Twist


#openCV aspect

def contor(lst):
    if len(lst)&gt;1:
        m = list()
        for i in lst:
            for j in i:
                m.append(j[0])
        m = np.array(m)
    else:
        m=np.array([[w//2,h-150],[w//2,h-40]])
    #print(&quot;m=&quot;,m)
    m = m[m[:, 1].argsort()]
    return m

def aver(i, p):
    a = p[i]
    y = int(np.mean([a[:, 0]]))
    x = int(np.mean([a[:, 1]]))
    return (y, x)


def extract(frame):
    
    h, w, _ = frame.shape
    #print(h,w)
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    lb = np.array([0, 0, 0])
    ub = np.array([255, 50, 160])
    mask = cv2.inRange(hsv, lb, ub)
    res = cv2.bitwise_and(frame, frame, mask=mask)

    res2 = cv2.cvtColor(res, cv2.COLOR_BGR2GRAY)
    ret, thres = cv2.threshold(res2, 100, 255, cv2.THRESH_BINARY)
    countours, hierarchy = cv2.findContours(thres, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    cv2.drawContours(frame, countours, -1, (255, 0, 255), 3)

    points = contor(countours)
    i = np.where((points[:, 1] &gt; h - 80) &amp; (points[:, 1] &lt; h))
    
    if len(i[0]) &gt; 0:
        end = aver(i, points)
    else:
        end = (w // 2, h)
    i = np.where((points[:, 1] &gt; h - 190) &amp; (points[:, 1] &lt; h - 110))
    
    if len(i[0]) &gt; 0:
        strt = aver(i, points)
    else:
        strt = (w // 2, h - 200)



    if (strt[0]-end[0]) == 0:
        phi=m.pi/2 #angle with horizontal
        theta = 0 #angle with vertical
    else:
        slope = (strt[1] - end[1]) / (end[0] - strt[0])
        phi=m.atan2(slope) #angle with horizontal
        theta= (m.pi/2) - phi #angle with vertical

    return theta



#PID

def PID (kp,ki,kd,dt,error):
    


#making the bot move

class GoForward():

    def _init_(self):
        rospy.init_node('GoForward', anonymous=False)
        rospy.loginfo(&quot;To stop TurtleBot CTRL + C&quot;)
        rospy.on_shutdown(self.shutdown)
        self.cmd_vel = rospy.Publisher('cmd_vel_mux/input/navi', Twist, queue_size=10)
        r = rospy.Rate(10)
        move_cmd = Twist()
        move_cmd.linear.x = 0.2
        move_cmd.angular.z = 0
        

        while not rospy.is_shutdown():
            frame=cv2.imread(&quot;image.jpg&quot;,1)
            error=extract(frame)
            w=0
            kp=0.15
            ki=0.02
            kd=0.02
            dt=0.01
            w= PID(kp,ki,kd,dt,error) 
            velocity_control=0.5 #when the bot is turning, its velocity should be slowed down.
            move_cmd.linear.x = 0.2-(veclocity_control *abs(w))
            move_cmd.angular.z = -w  #-sign because we are controlling angular velocity in -z direction (clockwise),if the error is positive, and vice versa
            self.cmd_vel.publish(move_cmd)
            r.sleep()
    


    def shutdown(self):
        rospy.loginfo(&quot;Stop TurtleBot&quot;)
        self.cmd_vel.publish(Twist())
        rospy.sleep(1)



if _name_ == '_main_':
    try:
        GoForward()
    except:
        rospy.loginfo(&quot;GoForward node terminated.&quot;)
</code></pre>
<p>The problem I'm having is to implement the PID part properly. How exactly should I approach writing the PID (...) function?</p>
",9/8/2021 10:17,,568,1,1,0,,14407957.0,,10/7/2020 14:05,57.0,69103416.0,"<p>As a comment suggested, there's a lot to tackle here, so I'll just focus on what you asked in your title. When concering PIDs in the ROS ecosystem I'd almost always suggest using the <a href=""http://wiki.ros.org/pid"" rel=""nofollow noreferrer"">PID</a> package. From your description it should do everything you need.</p>
",11245187.0,0.0,0.0,122130376.0,"that's a lot of different tasks all at once. break it down, distribute into separate questions... perhaps one at a time. honestly, if you wanted to ask about PID, this question contains too much stuff to sift through to give an answer (besides, PID tuning is likely a math/engineering question, unless you really want to talk about implementing the math)."
4565,75962469,ORB_SLAM3 Camera Calibration file,|opencv|computer-vision|robotics|slam|,"<p>I'm trying to run a a fisheye camera video feed in Monocular ORB_SLAM3 to get a trajectory, but I'm having trouble configuring the proper calibration file for it.</p>
<p><a href=""https://rpg.ifi.uzh.ch/fov.html"" rel=""nofollow noreferrer"">Fisheye Camera Intrinsicts and Image feed</a></p>
<p>Camera Intrinsicts file</p>
<pre><code>640 480 -179.471829787234 0 0.002316743975 -3.635968439375e-06 2.0546506810625e-08 320.0 240.0 1.0 0.0 0.0 256.2124 138.2261 -3.8287 23.8296 8.0091 -0.5033 6.7625 4.3653 -1.2425 -1.2663 -0.1870 0.0
</code></pre>
<p>These are examples of two formats of calibration files for ORB_SLAM3.</p>
<p><a href=""https://github.com/UZ-SLAMLab/ORB_SLAM3/blob/master/Examples_old/Monocular/TUM-VI.yaml"" rel=""nofollow noreferrer"">https://github.com/UZ-SLAMLab/ORB_SLAM3/blob/master/Examples_old/Monocular/TUM-VI.yaml</a></p>
<p><a href=""https://github.com/UZ-SLAMLab/ORB_SLAM3/blob/master/Examples_old/Monocular/TUM1.yaml"" rel=""nofollow noreferrer"">https://github.com/UZ-SLAMLab/ORB_SLAM3/blob/master/Examples_old/Monocular/TUM1.yaml</a></p>
<p>Any help or guidance would be really appreciated.</p>
<p>I tried usig the distortion parameters and both the camera types but there is something thats wrong, the negative focal length also gives an inverted trajectory but still completely inaccurate.</p>
<p><a href=""https://i.stack.imgur.com/i29Nu.png"" rel=""nofollow noreferrer"">Inverted Trajectory</a></p>
<p><a href=""https://i.stack.imgur.com/PKLC9.png"" rel=""nofollow noreferrer"">Inaccurate Trajectory</a></p>
",4/7/2023 22:59,,519,0,0,0,,21592392.0,,4/7/2023 22:37,2.0,,,,,,,
3404,56592230,In general is it ok to loop if statements with goto under else?,|lua|robotics|,"<p>So I have a task to be done which is to program the robot (AUBO) to pick different objects and place them in a certain order (Point A, B, C, D). 
I'm using some vision system known as pim60. So if an object is detected it will go and pick and the rest of the program are waypoints to drop products. The first problem is I want it to go to the next waypoint to drop the and the second thing is, the next drop point cannot be skipped until an object is detected for that drop point. </p>

<p>In my own code, I wrote a rather lengthy program like this.</p>

<pre><code>::LoopA::
script_common_interface(""SICKCamera"",""takePhoto"")
script_common_interface(""SICKCamera"",""getResult"")
Located = script_common_interface(""SICKCamera"",""partLocated"")
if(Located == 1) then
.
.
.
Drop at position A
else 
goto LoopA
end

::LoopB::
script_common_interface(""SICKCamera"",""takePhoto"")
script_common_interface(""SICKCamera"",""getResult"")
Located = script_common_interface(""SICKCamera"",""partLocated"")
if(Located == 1) then
.
.
.
Drop at position B
else 
goto LoopB
end

::LoopC::
script_common_interface(""SICKCamera"",""takePhoto"")
script_common_interface(""SICKCamera"",""getResult"")
Located = script_common_interface(""SICKCamera"",""partLocated"")
if(Located == 1) then
.
.
.
Drop at position C
else 
goto LoopC
end

::LoopD::
script_common_interface(""SICKCamera"",""takePhoto"")
script_common_interface(""SICKCamera"",""getResult"")
Located = script_common_interface(""SICKCamera"",""partLocated"")
if(Located == 1) then
.
.
.
Drop at position D
else 
goto LoopD
end
</code></pre>

<p>There is no error and the program runs as expected. However, I'm wondering if there is any better way to do it.</p>
",6/14/2019 6:00,56602237.0,117,1,6,2,,11640175.0,Singapore,6/13/2019 4:23,32.0,56602237.0,"<p>The only generally accepted use-case for <code>goto</code> is error handling, e.g. to jump forward to the cleanup code. But even for that it usually can and should be avoided.</p>

<p>You can probably do something like this:</p>

<pre><code>-- loop B
repeat
  take photo, etc.
  located = ...
until(located == 1)

Drop at position B
</code></pre>

<p>Also, if you're repeating the same code three times, you should extract it into a function, and maybe give the position as a parameter. Or at least put the whole thing into a <code>for</code> loop.</p>
",235548.0,1.0,3.0,99773559.0,My noobish program makes it appear so. And I had posted incorrect stuff after not touching the code for some time.
4210,70994989,Path Planning on a 3d point cloud,|point-clouds|robotics|motion-planning|,"<p>I have a 3d point cloud of a location on which I am trying to design a path planner for a mobile robot. Can anyone please guide me towards the right approach to take in solving the problem. For the point cloud, I have the coordinates of the obstacles on the map (their x,y,z positions). I am trying to solve the problem as a stand-alone general purpose planner for a mobile robot without using ROS.</p>
<p>My current stumbling block lies in the theoretical aspect as well since the fact that the point cloud consists of just x,y,z points, how is a path planning algorithm like A* run on such types of data where you can't define a general grid like that for a 2d case with each grid cell as a node? I have the coordinates of the obstacles on the map (their x,y,z positions).</p>
<p>Would greatly appreciate if anyone can provide me with some guidance on how to move forward.</p>
",2/5/2022 3:09,,435,1,0,0,,18085938.0,,2/1/2022 0:33,2.0,72390139.0,"<p>I am facing a similar problem. The first step should be calculating the normal vectors of the point cloud. Then, the path can be calculated based on the normal vector.</p>
",18632004.0,0.0,1.0,,
1925,22003790,NXT bluetooth pairing always failed,|bluetooth|robotics|nxt|lego-mindstorms|,"<p>Over 10 hours spend on this and really drive me crazy, any help is appreciate!</p>

<p>Tried with <strong>Android phone</strong> first:</p>

<p>Samsung NOTE2, Samsung Grand2, Samsung Glaxy S2 with android 2.2, android 4.2, androind 4.4 both with Original <strong>official</strong> NXT firmware, and latest <strong>LeJOS</strong>, If pairing from phone to NXT, the NXT would prompt window, just input the PIN same as in phone, but just <strong>quickly NXT returned to main menu</strong>, nothing more could see and still empty in Contracts list. otherwise, pairing from NXT to Phone, after Phone input the PIN, the NXT still say '<strong>line is busy</strong>' or 'unsuccessful' in LeJOS, while the Phone state the NXT is paired successfully.</p>

<p>Tried with PC to NXT, both in WinXP and Win7, I even bought a <strong>standalone</strong> Bluetooth USB Key, similar, once Input the PIN in PC same as the one in NXT set, the NXT quickly to say 'Line is busy', then the PC always take couple minutes to finish the Driver installation seems like mapping to some COM Port, then PC state paired successfully, but it never did in NXT.</p>

<p>I googled a lot, no useful information can get. Any idea?</p>
",2/25/2014 3:08,22446506.0,1672,1,0,-1,0.0,1602160.0,"Shanghai, China",8/16/2012 3:21,193.0,22446506.0,"<p>Finally I get it to work by replace the Bluetooth stack in my laptop, the experience proved the windows 7 stack can't suit, the one I used now is called bluesoleil, it also provide a management software, which I would say the UI really worried me at the beginning, it likes the very old days software, even more, it asking for license.
with this bluesoleil, I also get my arduino bluetooth sensor paired to my laptop.
I tried a generic stack provided by broadcom, but failed by it keep asking me to plug in the bluetooth adapter, and actually it's always there.
So I have to give up the paring between NXT and android phone, the andorid phone don't support replace the bluetooth stack except refresh another ROM.
Hope this information could help someone like me.</p>
",1602160.0,0.0,0.0,,
4604,76354503,How to rotate servo when there is object detected?,|python|raspberry-pi|raspberry-pi4|robotics|,"<p>currently I'm working with object detection yolov8 on raspberry pi. My project is to detect these three objects (aluminum can, plastic bottle, glass bottle). I already trained the dataset and export as best.pt file.</p>
<p>Now I got stuck on <strong>how to rotate servo when there is object detected</strong>. Here is my code.</p>
<pre><code>from ultralytics import YOLO
import cv2
from ultralytics.yolo.utils.plotting import Annotator
import torch
from gpiozero import Servo
from time import sleep
from gpiozero.pins.pigpio import PiGPIOFactory

factory = PiGPIOFactory()

servo = Servo(12, min_pulse_width=0.5/1000, max_pulse_width=2.5/1000, pin_factory=factory)

model = YOLO('best.pt')
cap = cv2.VideoCapture(0)
cap.set(3, 640)
cap.set(4, 480)

while True:
_, frame = cap.read()

img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

results = model.predict(img)

for r in results:
    
    annotator = Annotator(frame)
    
    boxes = r.boxes
    for box in boxes:
        
        b = box.xyxy[0]  # get box coordinates in (top, left, bottom, right) format
        c = box.cls
        annotator.box_label(b, model.names[int(c)])
       
      
frame = annotator.result()
  
cv2.imshow('YOLO V8 Obj Detection', frame)

#servo part

if cv2.waitKey(1) &amp; 0xFF == ord(' '):
    break

cap.release()
cv2.destroyAllWindows()
</code></pre>
",5/29/2023 3:23,,157,0,1,0,,21977022.0,,5/29/2023 3:15,2.0,,,,,,135665650.0,"It looks like you've got most things ready to go (including setting up the servo on pin 12).  It's unclear how the detected class(es) map to servo motion: e.g. what does the servo do when there are no detections ? what about multiple detections ? and for a single detection, which servo value should be selected for which class (e.g. -1, 0, 1 for alluminium, plastic, glass) ? (e.g. `servo.value = box.cls - 1.0 # rempap 0,1,2 classes to -1, 0, 1 servo values`)"
1299,9058716,How to get a direction for a robot from a method in a sensor class?,|java|sensors|robotics|,"<p>I am making a program using the LRV(Least recently visited) Algorithm. Basically, I design the algorithm for a robot to traverse through a grid (which is a 2D char array). The robot whilst traversing the grid checks whether each cell is either <code>EMPTY</code> (defined by 'O'), <code>OCCUPIED</code> ( defined by 'S' ) or <code>BLOCKED</code> (defined by 'X'). The cells can only be occupied by an object known as Sensor (this has its own class). <code>BLOCKED</code> cells cannot be traversed on. Each time the robot must move, it receives a direction from the sensor. So in the beginning the robot would be placed on the grid and it would drop a sensor and get a direction from it, or get a direction from a pre-existing sensor.</p>

<p>Now that I've explained my program, my specific question is,
I have a class Sensor that has a <code>getVisitingDirection</code> method that returns INT.
I have a counter for each direction (North, South, East and West of type INT)
Here is the class.</p>

<pre><code>package ITI1121A;
public class Sensor {

private int cColumns;
private int cRows;
private int North;
private int South;
private int West;
private int East;

public Sensor(int sX, int sY) { 

cColumns = sX;
cRows = sY;
South= -1;
North = -1;
West = -1;
East = -1;

}
/* ADD YOUR CODE HERE */
public int getX ()
{return cColumns;}
public int getY ()
{return cRows;}

public int getVisitingDirection(GridMap g1)
  boolean temp;
{
  if(cRows==0){
  //top row
    if(cColumns==0){
    temp=g1.isCellBlocked(cColumns+1,cRows);
    if (temp=false){
    return West++;
    }

    }

  }

}

public void increaseCounter(int direction)
{}

}
</code></pre>

<p>Now where I am stuck is at getVisitingDirection, I've tried to make if statements to check the top left edge of the grid ( coordinates 0,0) and yeah that's about it. 
I want the method to give a direction to the robot and then increase the counter of that direction.
Having real difficulty even getting the concept here.
Any help will be highly appreciated!
Thanks
Varun</p>
",1/30/2012 2:35,9061046.0,1918,1,5,5,0.0,1168160.0,"Toronto, ON, Canada",1/24/2012 22:55,93.0,9061046.0,"<p>I've put a function in pseudo-code that should set you in the right path.</p>

<pre><code>// lets assume binary code where 0000 represents top, right, bottom, left 
// (0011 would mean can go bottom or left)
public int getVisitingDirection()
{
    String tmpBinary = ""b""; // to initialize the field

    // check if can go up
    tmpBinary += (cCollums&gt;0&amp;&amp;checkIfUpIsBlocked()) ""1"" : ""0"";
    // TODO check if can go right (compare with size + 1)
    // check if can go bottom (compare with size +1 )
    // check if can go left (check if &gt; 0)

    // when this runs tmpBinary will be in the form of a binary representation
    // this will be passed to the robot that can then chooses where to go
    // 1111 would mean that all squares are clear to go
    // 1101 would mean top, right and left
    // etc...

}

private boolean checkIfUpIsBlocked()
{
    // TODO must check if the cell is blocked
    return true;
}
</code></pre>

<p>Do notice that you have to create the checkIfUpIsBlocked + methods.</p>

<p><strong>On top of that seams pretty good.</strong><br>
You may want to change the int fields by enums as they are easier to read and less prone to human errors.</p>

<p><strong>How to return directions with an int number?</strong><br>
You can use the binary logic and return a single int to represent multiple directions.  </p>

<pre><code>0000 (int 0)  =&gt; no possible direction
0001 (int 1)  =&gt; left direction
0010 (int 2)  =&gt; bottom direction
0011 (int 3)  =&gt; left and bottom
0100 (int 4)  =&gt; right direction
(...)
1111 (int 15) =&gt; all directions possible
</code></pre>
",67945.0,3.0,3.0,11368410.0,"placed at (3,3), check if empty, if yes then place 'S'..then get direction from 'S' for next move, so on and so forth. everytime robot detects sensor in a cell it asks for a direction"
3632,60002484,Fusing asynchronous measurements from multiple sensors,|embedded|sensors|robotics|sensor-fusion|,"<p>I have a set of 12 IMUs mounted on the end effector of my robot arm which I read using a micro controller to determine it's movement. With my controller I can read two sensors simultaneously using direct memory access. After acquiring the measurements I would like to fuse them to make up for the sensor error and generate a more reliable reading than having only one sensor.</p>

<p>After some research my understanding is that I can use a Kalman filter to reach my desired outcome, but still have the problem of all the sensor values having different time stamps, since I can read only two at a time and even if both time stamps will be synchronized perfectly, the next pair will have a different time stamp if only in the µs range. </p>

<p>Now I know controls engineering principles but am completely new to the topic of sensor fusion and google presents me with too many results to find a solution in a reasonable amount of time. 
Therefore my question, can anybody point me into the right direction by naming me a certain keyword I need to look for or literature I should work through to better understand that topic, please?</p>

<p>Thank you!</p>
",1/31/2020 10:52,60005677.0,254,1,0,0,,9395961.0,Germany,2/22/2018 11:18,3.0,60005677.0,"<p>The topic you are dealing with is not an easy one. Try to have a look at the multi-rate kalman filters.</p>

<p>The idea is that you design different kalman filters for each combination of sensor that you can available at the same time, and use it when you have the data from those sensors, while the system state is passed between the various filters.  </p>
",12753914.0,0.0,1.0,,
2102,26486572,Scan Matching Algorithm giving wrong values for translation but right value for rotation,|python|robotics|least-squares|slam-algorithm|,"<p>I've already posted it on robotics.stackexchange but I had no relevant answer.</p>

<p>I'm currently developing a SLAM software on a robot, and I tried the Scan Matching algorithm to solve the odometry problem.</p>

<p>I read this article :
<a href=""http://webdiis.unizar.es/~montesan/web/pdfs/minguez06TRO.pdf"" rel=""nofollow"">Metric-Based Iterative Closest Point Scan Matching
for Sensor Displacement Estimation</a></p>

<p>I found it really well explained, and I strictly followed the formulas given in the article to implement the algorithm.</p>

<p>You can see my implementation in python there :
<a href=""https://github.com/pierrolelivier/SLAMScanMatching/blob/master/ScanMatching.py"" rel=""nofollow"">ScanMatching.py</a></p>

<p>The problem I have is that, during my tests, the right rotation was found, but the translation was totally false. The values of translation are extremely high.</p>

<p>Do you have guys any idea of what can be the problem in my code ?</p>

<p>Otherwise, should I post my question on the Mathematics Stack Exchange ?</p>

<p>The ICP part should be correct, as I tested it many times, but the Least Square Minimization doesn't seem to give good results.</p>

<p>As you noticed, I used many bigfloat.BigFloat values, cause sometimes the max float was not big enough to contain some values.</p>
",10/21/2014 12:19,,520,1,0,1,,2648364.0,,8/3/2013 10:32,34.0,30529898.0,"<p>don't know if you already solved this issue.</p>

<p>I didn't read the full article, but I noticed it is rather old.</p>

<p>IMHO (I'm not the expert here), I would try bunching specific algorithms, like feature detection and description to get a point cloud, descriptor matcher to relate points, bundle adjustement to get the rototraslation matrix.</p>

<p>I myself am going to try sba (<a href=""http://users.ics.forth.gr/~lourakis/sba/"" rel=""nofollow"">http://users.ics.forth.gr/~lourakis/sba/</a>), or more specifically cvsba (<a href=""http://www.uco.es/investiga/grupos/ava/node/39/"" rel=""nofollow"">http://www.uco.es/investiga/grupos/ava/node/39/</a>) because I'm on opencv.</p>

<p>If you have enough cpu/gpu power, give a chance to AKAZE feature detector and descriptor.</p>
",1254714.0,0.0,0.0,,
2170,28534527,Programming a robot to explore a grid,|java|performance|grid|robot|,"<p>In my project I'm simply trying to make a robot which explores as much of the grid as it can, without taking the same path twice. Also, it has a sensor to see if an object is lying in its way(the object can only be in a corridor). But I'm having trouble trying to make the robot avoid taking the same path.</p>

<p>I tried resolving this issue by creating a 2D array for storing an integer value for each square in the grid. A value of 0 means the robot has not been on that square in the grid yet, a value of 1 means that square is blocked in the grid and a value of 2 means that the robot has been on that square before. If the robot sees that the square ahead of its current heading has value 2, then it would keep rotating to find a square with a value of 0, but if no square of value 0 around the robot exists, then it begins to backtrack.</p>

<p>My problem can be explained more clearly with this example:</p>

<p><img src=""https://i.stack.imgur.com/zgHy7.png"" alt=""enter image description here""></p>

<p>The triangle represents the robot and its start position, the bottom left corner is assumed to be the position (0,0) in my grid. The green circles represent items blocking it's path. The red square is the target for the robot. the robot can only move onto white squares in the grid.</p>

<p>When I start my programme the robot, moves forward(east since that is its current heading) till it gets to the junction just before the green circles. It looks ahead and detects an object in the way, so it rotates 90 degree anticlockwise and checks for another blockage, which again occurs hence it rotates anticlockwise again. So now the robot is at position (0,2) heading west. It can only move west to avoid leaving the grid or hitting an object, so it arrives back at its initial position but still heading west. It'll now keep rotating 90 degrees clockwise till it can find a direction which will keep it on the grid, i.e till it's facing east again. So the grid now looks like:</p>

<p><img src=""https://i.stack.imgur.com/590Dv.png"" alt=""enter image description here""></p>

<p>But now I want to ignore going ahead and onto the same path by ignoring that direction and rotating 90 degrees anticlockwise again to face north, so my robot can move north into a new path. I could simply ignore the direction and just keep rotating to find a new path, but what if I'm surrounded by been before paths and I want my robot to backtrack to its last junction. How can I efficiently do this. Also how can I efficiently detect when I need to backtrack.</p>

<p>Thank you</p>
",2/16/2015 4:27,28534606.0,1513,1,0,0,,2169454.0,,3/14/2013 10:59,68.0,28534606.0,"<p>Solving the predicament in picture 2 could be as simple as checking for other <code>white</code> squares around the robot before you make a move. In picture 2 the robot would see that the square he is facing is ""greyed out"" and decide to check all other directions and eventually find that there is an empty white square to the North of him.</p>

<p>Edit : Didn't realize it was an actual robot.</p>

<p>Since the only way of learning what is in a cell is by turning to that cell and using the sensor, the robot will have to do some amount of turning no matter what you do. When it encounters a wall or green object, it will have to turn until it finds a new path to travel. You could optimize it by disregarding the walls of the enclosure. For example, when the robot arrives back in his starting position facing west, you already know there is a wall to the south because of its coordinate position is (0,-1), which is invalid. This allows you to figure out that the open tile is to the north because you have already visited the tile to the east, requiring only one turn.</p>

<p>Furthermore, when the robot eventually travels all the way to the north, tile (0,6) you know there is a wall to the north and to the west because of its position. You could then intelligently guess that the open slot has to be to the east because the western tile (-1,6) is not valid and (0,7) is not valid either.</p>

<p>Without changing the sensor to see 2 blocks or installing more sensors on the robot (ie one on every side), there is not much more optimization that can be done due to the limited availability of information.</p>
",4080860.0,1.0,4.0,,
3373,55772405,How to store local Sub Program names in an array and call them in a loop iterating over said Array in KUKA Robotic Language,|robotics|kuka-krl|,"<p>The problem:<br/>
The main structure of the code the way I want it to be-<br/></p>

<pre><code>Def main()

decl int i
decl char arr[3]
INI

PTP HOME ...
arr[1]='w()'
arr[2]='e()'
arr[3]='l()'

for i=1 to 3
 arr[i]
endfor

END

def w()
PTP P1 ...
END

def e()
PTP P2 ...
END

def l()
PTP P3 ...
END
</code></pre>

<p>Now, as you can see, what i want to do is, have names of SubPrograms stored in an array, and basically call them one by one in a loop. (I could write the SubPrograms one by one and just remove the loop altogether,  but after calling every program i have to give a command, and i'm looking for a way where i don't have to write that command everytime, which can be done by using a loop)<br/>
<br/>
The problem is I can't figure out how to store names of the Subprgrams in an array as the above code gives a syntax error. <br/>
If there is a different way altogether of calling functions in a loop, I'd be happy to hear about it. else, I'd appreciate the help here.<br/>
<br/>
Thanks :)</p>
",4/20/2019 10:05,55870474.0,300,2,0,1,,9663758.0,,4/18/2018 10:37,21.0,55870474.0,"<p>You could implement a switch/case inside your for loop to mimic array indexing.</p>

<pre><code>Def main()

   decl int i
   decl char arr[3]
   INI

   PTP HOME ...

   for i=1 to 3
       switch i
          case 1
             w()
          case 2
             e()
          default
             l()
       endswitch
   endfor

END

def w()
   PTP P1 ...
END

def e()
   PTP P2 ...
END

def l()
   PTP P3 ...
END
</code></pre>
",8882513.0,0.0,1.0,,
2371,32683092,Trigger a relay with Java,|java|robotics|,"<p>I haven't yet started this project, but I am trying to figure out the best way a trigger a relay when a button is clicked on a UI in java. The relay will release a lock on a door when the button is clicked. I've looked at rasberri pi, but I am not familiar with that product. Can anyone suggest how I should go about doing this?</p>
",9/20/2015 19:00,,1263,1,5,0,,4513904.0,,1/31/2015 6:03,41.0,32683738.0,"<p>I've implemented this in several different ways and indeed done all sorts of device interface with Java for engineering experimentation and automation.</p>

<p>The real question here is do you have a specific relay device in mind as this is going to drive how you implement a Java interface? As examples, the two most recent Java-Relay applications I have implemented involved one of the following devices:</p>

<ul>
<li><strong>Case 1</strong>: Using Java to control low contact resistance measurement/signal SPDT(Single Pole, Double Throw) relays

<ul>
<li>Device:  <a href=""http://www.keysight.com/en/pd-1000001313:epsg:pro-pn-34970A/data-acquisition-data-logger-switch-unit?&amp;cc=US&amp;lc=eng"" rel=""nofollow"">Keysight (Agilent/HP) 34970A</a> and the <a href=""http://www.keysight.com/en/pd-1000000085%3Aepsg%3Apro-pn-34903A/20-channel-actuator-gp-switch-module-for-34970a-34972a?nid=-33237.536880682&amp;cc=US&amp;lc=eng"" rel=""nofollow"">20 Channel Switch Unit</a></li>
</ul></li>
<li><strong>Case 2</strong>: Using Java to control SPDT power relays

<ul>
<li>Device: <a href=""http://www.sainsmart.com/sainsmart-4-channel-5v-usb-relay-board-module-controller-for-automation-robotics.html"" rel=""nofollow"">SainSmart USB Relay Board</a></li>
</ul></li>
</ul>

<p>In Case 1 I used <a href=""http://rxtx.qbang.org/wiki/index.php/Main_Page"" rel=""nofollow"">RXTX</a> which is a Java serial port implementation to interface with the Agilent 34970 using a serial port.</p>

<p>In Case 2 I used the <a href=""http://www.ftdichip.com/Support/SoftwareExamples/CodeExamples/OtherPlatforms.htm"" rel=""nofollow"">JavaFTDI</a> package to interface directly with the FTDI chip onboard the relay board using BitBang mode. While I eventually go this option working, the combined lack of documentation from FTDI and SainSmart made me gouge my eyes out for days.</p>

<p>Certainly you could alternatively use a Raspberry Pi and its GPIOs to either control a separate relay purchased from <a href=""http://www.digikey.com/product-search/en/relays?k=relay"" rel=""nofollow"">Digi-Key</a> or in fact use the GPIOs themselves as a relay assuming very low voltage was used. I'm imagining two scenarios, one where the user is actually directly interfacing with the Pi or another where the Pi is on the network and through a user interface running on a separate PC the user is leveraging <a href=""https://en.wikipedia.org/wiki/Java_remote_method_invocation"" rel=""nofollow"">RMI</a> to cause the Pi to change GPIO states.</p>

<p>Really, what I would suggest is looking at what relay you want to use, and post a more specific question regarding how to interface to that relay assuming Java is your preferred language. Alternatively, you could ask what the lowest barrier to entry/learning curve/cost options there are for relays that can be controlled through Java. Without more detail, it's hard to recommend a path forward.</p>
",2665000.0,2.0,2.0,53211023.0,http://denkovi.com/usb-eight-channel-relay-board-for-automation
908,4479885,"Light weight, behavior driven multi-agent robot simulator?",|development-environment|simulation|environment|robotics|,"<p>Looking for a robot simulator that's multi-agent, light weight, behavior driven, and scriptible, visual runtime -- it's likely 2D too. There is no requirement for the logic to be output to the real world. Aside from behaviors related to sensor/motor combos - it'd be nice if it was possible to code sensor to respond to color/size/speed/etc (prey/predator/mating) and have events that happen as a result contact (death/birth/energy-gain).</p>

<p>So, far I've looked at the following, none of which have semi-complex behavior assignment, rendering and reporting:</p>

<p><strong>BugWorks:</strong> multi-agent, behavior driven, light weight, visual runtime -- but not scriptible as far as I'm able to tell; meaning you can use a GUI, and save it, but their no code output to edit directly. One thing that is nice is there's a reproduce function; although the implementation is odd, it produces one robot per click (it's not based on robot interaction) and appears to take an average of all the attributes of all robots presents; better than no function though. <a href=""http://www.informatics.sussex.ac.uk/users/christ/bugworks/"" rel=""nofollow"">More info</a></p>

<p><strong>Guido van Robot:</strong> not multi-agent, but it's got it's own simple scripting language for the robot and environment, with a debugger built in. <a href=""http://gvr.sourceforge.net/"" rel=""nofollow"">More Info</a></p>

<p><strong>Algodoo:</strong> It's got  2D Physics, point-n-click interface, but very heavy on the graphic card, so I have tested it out much. <a href=""http://www.algodoo.com"" rel=""nofollow"">More info</a></p>

<p>Any suggestions?</p>
",12/18/2010 20:01,5028010.0,367,2,0,2,0.0,471255.0,,10/10/2010 0:40,375.0,4482384.0,"<p>You can give <a href=""http://garlicsim.org"" rel=""nofollow"">GarlicSim</a> a try. It's a generic simulations framework (good for multi-agent simulation among others,) so it might not provide the specific tools you need for robot simulation, but it will make it relatively easy for you to build them.</p>
",76701.0,1.0,3.0,,
2669,42251596,Analytic calculation of Jacobian derivative(second order kinematic differentials),|c++|robotics|,"<p>I need to calculate the time derivative of the Jacobian. I want to do it analytical, I cannot calculate it numerical. I calculate <code>J_i</code> with the scheme in respect of the Denavit-Hartenberg Parameters
<code>J_i = [z_(i-1) 0]^T</code> if it is an linear-axis etc.</p>

<p>I know need a simple method to come from this <code>J</code> to <code>dJ/dt</code>. Has anyone any experience with that problem? There is a formula with derivates <code>J_i</code>, but in this formula I do not know what the parameters define, but maybe someone knows what they are:</p>

<p>linear axis:</p>

<pre><code>J_i = [(w_(i-1) x z_(i-1))  0]^T
rotation axis:
J_i = [(w_(i-1) x z_(i-1)) x r_(i-1) + z_(i-1) x ( v_(i,n9 + w_(i-1) x r_(i-1))  (w_(i-1) x z_(i-1))]^T
</code></pre>

<p>Here <code>r_i-1</code> is the distance to the TCP. I don t know what <code>w_i-1</code> and <code>v_i,n</code> is. Any help or any different suggestion?</p>

<p>Thank you guys </p>
",2/15/2017 14:06,,108,0,2,0,,7357997.0,,12/30/2016 13:07,8.0,,,,,,71662335.0,It sounds like a question more suitable for http://math.stackexchange.com/
4685,77276377,I'm almost there? 4 DOF robot arm forward kinematics,|c#|quaternions|robotics|inverse-kinematics|,"<p>I started a project to learn the forward kinematics of a custom 4 DOF robot arm I made. Essentially, I wanted to learn the basics to apply them to learning inverse kinematics <em>fingers crossed</em>. I cannot seem to grasp the rotation axis. It will <em>click</em> in my brain once I overcome whatever mental block stops me.</p>
<p>I set out to create a Joint class that would identify each joint and the rotation axis. Again, why can't my brain wrap my head around the rotation axis? Do you have any suggestions to overcome this mental blockage?</p>
<pre><code>  public class Joint {

    public double Length { get; }
    public Vector3 RotationAxis { get; }

    public Joint(double length, Vector3 rotationAxis) {

      Length = length;
      RotationAxis = Vector3.Normalize(rotationAxis);
    }
  }
</code></pre>
<p>The heart of the class is to loop through each joint and keep the rotation state while calculating the vector3 offset for the end effector. But I feel like I am misusing Quaternions. Either that or the Transform axis is incorrect. Most likely, the axis is incorrect because I can't get my head around it.</p>
<pre><code>using System;
using System.Collections.Generic;
using System.Numerics;

namespace InverseKinematicsMover {

  public class Joint {

    public double Length { get; }
    public Vector3 RotationAxis { get; }

    public Joint(double length, Vector3 rotationAxis) {

      Length = length;
      RotationAxis = Vector3.Normalize(rotationAxis);
    }
  }

  public class Forward2 {

    public List&lt;Joint&gt; Joints = new List&lt;Joint&gt;();

    public Forward2() {

      Joints = new List&lt;Joint&gt;();
    }

    float degreeToRadian(int angle) {

      // because 90 is the center for servos
      int angleDegrees = angle - 90;

      return (float)Math.PI * angleDegrees / 180.0f;
    }

    public Vector3 CalculateEndEffectorPosition(List&lt;int&gt; jointAngles) {

      Vector3 endEffectorPosition = Vector3.Zero;
      Quaternion cumulativeRotation = Quaternion.Identity;

      for (int i = 0; i &lt; Joints.Count; i++) {

        // continue to keep track of the rotation as we progress through joints
        // ie 45 degrees of the current joint is relative to 90 degrees of previous joint
        cumulativeRotation += Quaternion.CreateFromAxisAngle(Joints[i].RotationAxis, degreeToRadian(jointAngles[i]));
        
        // add to the end effector with the quaternion transformation of this joint's rotation axis
        endEffectorPosition += (float)Joints[i].Length * Vector3.Transform(Joints[i].RotationAxis, cumulativeRotation);
      }

      return endEffectorPosition;
    }
  }
}
</code></pre>
<p>Lastly, my test program uses each servo's valueo (1-180).</p>
<pre><code>      Forward2 robotArm = new Forward2();

      // Base joint rotates the arm
      robotArm.Joints.Add(new Joint(0, new Vector3(0, 1, 0)));

      // Joints extend the robot arm
      robotArm.Joints.Add(new Joint(128.0, new Vector3(1, 0, 0)));
      robotArm.Joints.Add(new Joint(148.0, new Vector3(1, 0, 0)));
      robotArm.Joints.Add(new Joint(146.0, new Vector3(1, 0, 0)));

      // Specify joint angles (in degrees) for each joint
      var  jointAngles = new List&lt;int&gt; {
        GetServoPosition(0),
        GetServoPosition(1),
        GetServoPosition(2),
        GetServoPosition(3),
      };

      // Calculate the end effector position
      Vector3 endEffectorPosition = robotArm.CalculateEndEffectorPosition(jointAngles);

      // Display the end effector position
      log($&quot;End Effector Position: X={endEffectorPosition.X}, Y={endEffectorPosition.Y}, Z={endEffectorPosition.Z}&quot;);
</code></pre>
<p>I get a strange behavior for the logged output. It appears the Y and Z axis are dependent on the X (base rotation) from not being zero. And the Y and Z axis values are heavily dependent on the X value as a multiplier for some reason.</p>
<p>I have tried configuring the Joint definitions for different Axis. The first Joint is a base rotation, as seen on common Robot Arms. The remaining joints all rotate the arm vertically (up and down).</p>
<p>I have also tried thinking the end effector accumulated value may need the length multiplied by the rotation axis during the transform.</p>
<pre><code>  endEffectorPosition += Vector3.Transform(Joints[i].RotationAxis * (float)Joints[i].Length, cumulativeRotation);
</code></pre>
<p>That provides similar results where the Y and Z values are heavily dependent on the X axis and also seem to be a multiplier of X.</p>
",10/11/2023 21:01,77288612.0,124,1,1,-1,,22725238.0,,10/11/2023 20:27,5.0,77288612.0,"<p>Ah, after some further research I figured out my issue with the axis. You see, the arm extends upward from the base when all joint values are 0. That means all joints are along the y axis relative to each other.</p>
<p>Also, the first joint was configured for the z axis rotation which is incorrect. It should be the Y axis because it rotates the arm around it.</p>
<p>The main issue was using the rotation axis in the vector transform. Because the arm extends from the y axis out of the base, it merely needs this code changed to…</p>
<pre><code>endEffectorPosition += (float)Joints[i].Length * Vector3.Transform(Vector3.UnitY, cumulativeRotation);
      }
</code></pre>
",22725238.0,0.0,0.0,136240769.0,"Welcome to Stackoverflow! I think your question can be better answered in the [StackExchange Robotics](https://robotics.stackexchange.com/), have you tried asking there?"
4075,69125009,ROS - problem adding ultrasonic data to range_sensor_layer,|ros|robotics|arduino-ultra-sonic|,"<p>I need some help with a problem I encountered while adding ultrasonic sensors to a robot (loosely based on Linorobot), already equipped with an RPlidar. Hw/Sw: Raspi3B w/ Ubuntu 16.04.6 LTS, ROS kinetic, a Teensy, 2 Nano.</p>
<p>The robot was working fine with just the lidar, but I need to be able to detect correctly glass and some reflective surfaces, so I'm adding the ultrasonic sensors.
The hardware and microcontroller (rosserial) parts seem to be working fine, I suspect it's an error from my part, maybe related to namespaces or transform frames... or maybe I'm missing something gargantuan. I checked and re-checked against online tutorials, examples and other questions similar to this one, but I couldn't identify the culprit.</p>
<p>After executing the launch files I get the standard messages (same as before trying to setup the ultrasonic sensors), plus:</p>
<pre><code>[ INFO] [1631195261.554945536]: global_costmap/sonar_layer: ALL as input_sensor_type given
[ INFO] [1631195261.596176257]: RangeSensorLayer: subscribed to topic /ultrasound_front
</code></pre>
<p>and I guess that's good.
Unfortunately from that moment onward I get (with increasingly high figures, of course):</p>
<pre><code>[ WARN] [1631195265.533631740]: No range readings received for 4.02 seconds, while expected at least every 2.00 seconds.
</code></pre>
<p>here's a sensor message (from &quot;rostopic echo /ultrasound_front&quot;):</p>
<pre><code>----
header: 
  seq: 1124
  stamp: 
    secs: 1631192726
    nsecs: 301432058
  frame_id: &quot;sonar_front&quot;
radiation_type: 0
field_of_view: 0.259999990463
min_range: 0.0
max_range: 100.0
range: 52.0
----
</code></pre>
<p>So, the topic is published and the massages should be ok...</p>
<p>My costmap_common_params.yaml:</p>
<pre><code>map_type: costmap

transform_tolerance: 1

footprint: [[-0.25, -0.25], [-0.25, 0.25], [0.25, 0.25], [0.25, -0.25]]

inflation_layer:
  inflation_radius: 0.28
  cost_scaling_factor: 3

obstacle_layer:
  obstacle_range: 2.5
  raytrace_range: 3.0
  observation_sources: scan
  observation_persistence: 0.0
  scan:
    data_type: LaserScan
    topic: scan
    marking: true
    clearing: true

sonar_layer:
  frame: sonar_front
  topics: [&quot;/ultrasound_front&quot;]
  no_readings_timeout: 2.0
  clear_on_max_reading: true
  clear_threshold: 0.2
  mark_threshold: 0.80
</code></pre>
<p>My global_costmap_params.yaml:</p>
<pre><code>global_costmap:
  global_frame: /map
  robot_base_frame: /base_footprint
  update_frequency: 1
  publish_frequency: 0.5
  static_map: true
  transform_tolerance: 1
  plugins:
    - {name: static_layer,    type: &quot;costmap_2d::StaticLayer&quot;}
    - {name: sonar_layer,   type: &quot;range_sensor_layer::RangeSensorLayer&quot;}
    - {name: obstacle_layer,  type: &quot;costmap_2d::ObstacleLayer&quot;}
    - {name: inflation_layer, type: &quot;costmap_2d::InflationLayer&quot;}
</code></pre>
<p>My local_costmap_params.yaml:</p>
<pre><code>local_costmap:
  global_frame: /odom
  robot_base_frame: /base_footprint
  update_frequency: 1
  publish_frequency: 5.0
  static_map: false
  rolling_window: true
  width: 3
  height: 3
  resolution: 0.02
  transform_tolerance: 1
  observation_persistence: 0.0

  plugins:
    - {name: obstacle_layer,  type: &quot;costmap_2d::ObstacleLayer&quot;}
    - {name: sonar_layer, type: &quot;range_sensor_layer::RangeSensorLayer&quot;}
    - {name: inflation_layer, type: &quot;costmap_2d::InflationLayer&quot;}
</code></pre>
<p>And my barebone URDF:</p>
<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;
   &lt;robot name=&quot;linorobot&quot;&gt;

    &lt;link name=&quot;base_link&quot;&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;box size=&quot;0.50 0.33 0.09&quot;/&gt;
        &lt;/geometry&gt;
        &lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0.0 0.00 0.085&quot;/&gt;
        &lt;material name=&quot;blue&quot;&gt;
          &lt;color rgba=&quot;0 0 .8 1&quot;/&gt;
        &lt;/material&gt;
      &lt;/visual&gt;
    &lt;/link&gt;

    &lt;link name=&quot;perception_deck&quot;&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;box size=&quot;0.18 0.33 0.08&quot;/&gt;
        &lt;/geometry&gt;
        &lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0.0 0.0 0.17&quot;/&gt;
        &lt;material name=&quot;blue&quot;&gt;
          &lt;color rgba=&quot;0 0 .8 1&quot;/&gt;
        &lt;/material&gt;
      &lt;/visual&gt;
    &lt;/link&gt;


    &lt;link name=&quot;wheel_left_front&quot;&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;cylinder length=&quot;0.03&quot; radius=&quot;0.06&quot;/&gt;
        &lt;/geometry&gt;
        &lt;origin rpy=&quot;1.57 0 0&quot; xyz=&quot;0.163 0.222 0.03&quot;/&gt;
        &lt;material name=&quot;black&quot;&gt;
          &lt;color rgba=&quot;0 0 0 1&quot;/&gt;
        &lt;/material&gt;
      &lt;/visual&gt;
    &lt;/link&gt;

    &lt;link name=&quot;wheel_right_front&quot;&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;cylinder length=&quot;0.03&quot; radius=&quot;0.06&quot;/&gt;
        &lt;/geometry&gt;
        &lt;origin rpy=&quot;1.57 0 0&quot; xyz=&quot;0.163 -0.222 0.03&quot;/&gt;
        &lt;material name=&quot;black&quot;&gt;
          &lt;color rgba=&quot;0 0 0 1&quot;/&gt;
        &lt;/material&gt;
      &lt;/visual&gt;
    &lt;/link&gt;

    &lt;link name=&quot;wheel_left_rear&quot;&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;cylinder length=&quot;0.03&quot; radius=&quot;0.06&quot;/&gt;
        &lt;/geometry&gt;
        &lt;origin rpy=&quot;1.57 0 0&quot; xyz=&quot;-0.163 0.222 0.03&quot;/&gt;
        &lt;material name=&quot;black&quot;&gt;
          &lt;color rgba=&quot;0 0 0 1&quot;/&gt;
        &lt;/material&gt;
      &lt;/visual&gt;
    &lt;/link&gt;

    &lt;link name=&quot;wheel_right_rear&quot;&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;cylinder length=&quot;0.03&quot; radius=&quot;0.06&quot;/&gt;
        &lt;/geometry&gt;
        &lt;origin rpy=&quot;1.57 0 0&quot; xyz=&quot;-0.163 -0.222 0.03&quot;/&gt;
        &lt;material name=&quot;black&quot;&gt;
          &lt;color rgba=&quot;0 0 0 1&quot;/&gt;
        &lt;/material&gt;
      &lt;/visual&gt;
    &lt;/link&gt;

    &lt;link name=&quot;laser&quot;&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;cylinder length=&quot;0.065&quot; radius=&quot;0.035&quot;/&gt;
        &lt;/geometry&gt;
        &lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0.0 0.0 0.2825&quot;/&gt;
        &lt;material name=&quot;black&quot;/&gt;
      &lt;/visual&gt;
    &lt;/link&gt;

    &lt;link name=&quot;chassis&quot;&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;box size=&quot;0.5 0.5 0.8&quot;/&gt;
        &lt;/geometry&gt;
        &lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0.0 0.0 0.0&quot;/&gt;
        &lt;material name=&quot;silver&quot;&gt;
          &lt;color rgba=&quot;192 192 192 0.6&quot;/&gt;
        &lt;/material&gt;
      &lt;/visual&gt;
    &lt;/link&gt;

    &lt;link name=&quot;sonar_front&quot;&gt;
      &lt;visual&gt;
       &lt;/geometry&gt;
        &lt;origin rpy=&quot;1.5708 0.2618 0&quot; xyz=&quot;-0.21 0.0 0.235&quot;/&gt;
        &lt;material name=&quot;silver&quot;&gt;
          &lt;color rgba=&quot;192 192 192 0.6&quot;/&gt;
        &lt;/material&gt;
      &lt;/visual&gt;
    &lt;/link&gt;

    &lt;link name=&quot;sonar_rear&quot;&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;box size=&quot;0.02 0.025 0.07&quot;/&gt;
        &lt;/geometry&gt;
        &lt;origin rpy=&quot;1.5708 0.2618 3.1416&quot; xyz=&quot;0.23 0.0 0.235&quot;/&gt;
        &lt;material name=&quot;silver&quot;&gt;
          &lt;color rgba=&quot;192 192 192 0.6&quot;/&gt;
        &lt;/material&gt;
      &lt;/visual&gt;
    &lt;/link&gt;

    &lt;link name=&quot;sonar_left&quot;&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;box size=&quot;0.02 0.025 0.07&quot;/&gt;
        &lt;/geometry&gt;
        &lt;origin rpy=&quot;1.5708 -0.2618 1.5708&quot; xyz=&quot;0.0 0.18 0.235&quot;/&gt;
        &lt;material name=&quot;silver&quot;&gt;
          &lt;color rgba=&quot;192 192 192 0.6&quot;/&gt;
        &lt;/material&gt;
      &lt;/visual&gt;
    &lt;/link&gt;

    &lt;link name=&quot;sonar_right&quot;&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;box size=&quot;0.02 0.025 0.07&quot;/&gt;
        &lt;/geometry&gt;
        &lt;origin rpy=&quot;1.5708 -0.2618 -1.5708&quot; xyz=&quot;0.0 -0.19 0.235&quot;/&gt;
        &lt;material name=&quot;silver&quot;&gt;
          &lt;color rgba=&quot;192 192 192 0.6&quot;/&gt;
        &lt;/material&gt;
      &lt;/visual&gt;
    &lt;/link&gt;

    &lt;joint name=&quot;base_to_wheel_left_front&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;wheel_left_front&quot;/&gt;
      &lt;origin xyz=&quot;0 0 0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_wheel_right_front&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;wheel_right_front&quot;/&gt;
      &lt;origin xyz=&quot;0 0 0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_wheel_left_rear&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;wheel_left_rear&quot;/&gt;
      &lt;origin xyz=&quot;0 0 0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_wheel_right_rear&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;wheel_right_rear&quot;/&gt;
      &lt;origin xyz=&quot;0 0 0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_laser&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;laser&quot;/&gt;
      &lt;origin xyz=&quot;0.0 0.0 0.0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_left_sonar&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;sonar_left&quot;/&gt;
      &lt;origin xyz=&quot;0.0 0.0 0.0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_right_sonar&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;sonar_right&quot;/&gt;
      &lt;origin xyz=&quot;0.0 0.0 0.0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_rear_sonar&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;sonar_rear&quot;/&gt;
      &lt;origin xyz=&quot;0.0 0.0 0.0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_front_sonar&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;sonar_front&quot;/&gt;
      &lt;origin xyz=&quot;0.0 0.0 0.0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_perception_deck&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
    &lt;joint name=&quot;base_to_laser&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;laser&quot;/&gt;
      &lt;origin xyz=&quot;0.0 0.0 0.0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_left_sonar&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;sonar_left&quot;/&gt;
      &lt;origin xyz=&quot;0.0 0.0 0.0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_right_sonar&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;sonar_right&quot;/&gt;
      &lt;origin xyz=&quot;0.0 0.0 0.0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_rear_sonar&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;sonar_rear&quot;/&gt;
      &lt;origin xyz=&quot;0.0 0.0 0.0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_front_sonar&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;sonar_front&quot;/&gt;
      &lt;origin xyz=&quot;0.0 0.0 0.0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_perception_deck&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;perception_deck&quot;/&gt;
      &lt;origin xyz=&quot;0 0 0.0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_chassis&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;chassis&quot;/&gt;
      &lt;origin xyz=&quot;0 0 0.44&quot;/&gt;
    &lt;/joint&gt;
  &lt;/robot&gt;
</code></pre>
<p>Thanks!</p>
<p>EDITS<br />
after getting the messages, &quot;rostopic hz /ultrasound_front&quot; gives:</p>
<pre><code>subscribed to [/ultrasound_front]
average rate: 3.494
    min: 0.267s max: 0.305s std dev: 0.01919s window: 3
average rate: 3.384
    min: 0.250s max: 0.353s std dev: 0.03533s window: 6
average rate: 3.362
    min: 0.250s max: 0.353s std dev: 0.02813s window: 9
average rate: 3.352
    min: 0.250s max: 0.353s std dev: 0.02625s window: 13
average rate: 3.349
    min: 0.250s max: 0.353s std dev: 0.02447s window: 16
average rate: 3.344
    min: 0.250s max: 0.353s std dev: 0.02547s window: 20
average rate: 3.341
    min: 0.250s max: 0.353s std dev: 0.02368s window: 23
average rate: 3.256
    min: 0.250s max: 0.490s std dev: 0.04349s window: 26
average rate: 3.336
    min: 0.110s max: 0.490s std dev: 0.05406s window: 30
average rate: 3.335
    min: 0.110s max: 0.490s std dev: 0.05176s window: 33
</code></pre>
<p>and so on. Publishing interval in the MCU code is 250ms.</p>
<p>&quot;max_range:1.0&quot; in &quot;rostopic echo /ultrasound_front&quot; has been corrected (was an error in the original MCU code), the behaviour doesn't change. I modified the output above to reflect the current version.</p>
<p>&quot;rostopic info /ultrasound_front&quot;, after the massages started, gives: (Thank you @BTables!)</p>
<pre><code>Type: sensor_msgs/Range

Publishers: 
 * /rosserial_NANO_sensors (http://192.168.2.54:34525/)

Subscribers: 
 * /move_base (http://192.168.2.54:40149/)
</code></pre>
",9/9/2021 21:41,,1149,1,2,2,,16873199.0,,9/9/2021 20:19,3.0,69407772.0,"<p>I finally solved some of the problems that emerged after adding the ultrasound sensors. Because of the nature of the errors, and the extremely large amount of different hw/sw configurations possible, I will put here my findings, with some more general info, hoping to help others:</p>
<ul>
<li>Double check the UNIT of MEASURE used in the range fields in the microcontroller code. For example, the library and examples I used and referred to had everything in cm.<br />
This isn't good for ROS navigation layer, the range/distance numbers passed in the messages should be in meters (min_range, max_range, range).<br />
HOWEVER the microcontroller code could be passing the data, and using some internal calculations or logic, in centimeters (like here 'https://www.intorobotics.com/how-to-use-sensor_msgs-range-ros-for-multiple-sensors-with-rosserial/', for example), so some changes are probably needed (also regarding the logic behind the clearing of the costmap, but that's a problem for another question).</li>
<li>A message rate of 20Hz should be ok, it should not produce missing data messages, sync errors, etc. However please note that it's possible this frequency has to be modified, depending on the hardware involved.</li>
<li>The costmap YAML parameter <code>clear_on_max_reading</code> behaviour depends on how the data is presented by your ultrasound sensor (or sensors) MCU code. It's a good idea to try both settings and check which one is more appropriate for your case. You can then modify the MCU code to accomodate for the library logic behind this setting (or the other way around, modifying the libraries).</li>
<li>Verify that your RVIZ configuration contains all the necessary information to visualize your ultrasound (range) sensor data (<a href=""http://wiki.ros.org/rviz/DisplayTypes/Range"" rel=""nofollow noreferrer"">http://wiki.ros.org/rviz/DisplayTypes/Range</a>)</li>
<li>The URDF usually gives clear messages if something related to the transforms and related data is not working, once the real problems are solved, it's possible to see the cones and axes in Rviz (IF the unit of measure isn't too small!), so it's easy to correct orientation and position errors.</li>
<li>Use <code>check_urdf</code> to verify the validity of the URDF file (<a href=""http://wiki.ros.org/urdf#Verification"" rel=""nofollow noreferrer"">http://wiki.ros.org/urdf#Verification</a>), and <code>urdf_to_graphiz</code> to have a visual representation with some more data, that could give some clues on malfunctions or errors (<a href=""http://wiki.ros.org/urdf#Visualization"" rel=""nofollow noreferrer"">http://wiki.ros.org/urdf#Visualization</a>). Also <code>rqt_graph</code> with <code>enable_statistics</code> set to &quot;true&quot; can give useful clues (<a href=""http://wiki.ros.org/rqt_graph"" rel=""nofollow noreferrer"">http://wiki.ros.org/rqt_graph</a>).</li>
</ul>
",16873199.0,0.0,0.0,122200468.0,"Yes, I updated the question, thanks!"
4458,74481393,Arduino mega with L298n and Motors with Encoders not registering encoders,|arduino|robotics|encoder|arduino-c++|motordriver|,"<p>I am trying to follow a tutorial from <a href=""https://www.youtube.com/watch?v=-PCuDnpgiew&amp;list=PLunhqkrRNRhYAffV8JDiFOatQXuU-NnxT&amp;index=9"" rel=""nofollow noreferrer"">youtube</a> on using ROS with Arduino to control motors, and I have connected my <a href=""https://rads.stackoverflow.com/amzn/click/com/B07BK1QL5T"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">L298N</a> with the <a href=""https://rads.stackoverflow.com/amzn/click/com/B00ME3ZH7C"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">battery</a> precisely as the video describes and have uploaded <a href=""https://github.com/joshnewans/ros_arduino_bridge"" rel=""nofollow noreferrer"">sketch</a> 1 with the supporting folder and it loads properly. The Arduino is powered properly via USB, but that connection is not shown in the diagram. When I type the &quot;e&quot; command, I get the proper response of &quot;0 0&quot; and when I do the &quot;o 255 255&quot; it says &quot;OK&quot; and drives properly but upon using &quot;e&quot; to recheck the encoders I am getting the same &quot;0 0&quot;. If anyone can spot something wrong with this, I would really appreciate the help in fixing it. Diagram and Code Below</p>
<p><a href=""https://i.stack.imgur.com/XBk5v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XBk5v.png"" alt=""Diagram"" /></a></p>
<p>Code:</p>
<pre class=""lang-cpp prettyprint-override""><code>#define USE_BASE      // Enable the base controller code
//#undef USE_BASE     // Disable the base controller code

/* Define the motor controller and encoder library you are using */
#ifdef USE_BASE
   /* The Pololu VNH5019 dual motor driver shield */
   //#define POLOLU_VNH5019

   /* The Pololu MC33926 dual motor driver shield */
   //#define POLOLU_MC33926

   /* The RoboGaia encoder shield */
   //#define ROBOGAIA
   
   /* Encoders directly attached to Arduino board */
   #define ARDUINO_ENC_COUNTER

   /* L298 Motor driver*/
   #define L298_MOTOR_DRIVER
#endif

//#define USE_SERVOS  // Enable use of PWM servos as defined in servos.h
#undef USE_SERVOS     // Disable use of PWM servos

/* Serial port baud rate */
#define BAUDRATE      57600

/* Maximum PWM signal */
#define MAX_PWM        255

#if defined(ARDUINO) &amp;&amp; ARDUINO &gt;= 100
#include &quot;Arduino.h&quot;
#else
#include &quot;WProgram.h&quot;
#endif

/* Include definition of serial commands */
#include &quot;commands.h&quot;

/* Sensor functions */
#include &quot;sensors.h&quot;

/* Include servo support if required */
#ifdef USE_SERVOS
   #include &lt;Servo.h&gt;
   #include &quot;servos.h&quot;
#endif

#ifdef USE_BASE
  /* Motor driver function definitions */
  #include &quot;motor_driver.h&quot;

  /* Encoder driver function definitions */
  #include &quot;encoder_driver.h&quot;

  /* PID parameters and functions */
  #include &quot;diff_controller.h&quot;

  /* Run the PID loop at 30 times per second */
  #define PID_RATE           30     // Hz

  /* Convert the rate into an interval */
  const int PID_INTERVAL = 1000 / PID_RATE;
  
  /* Track the next time we make a PID calculation */
  unsigned long nextPID = PID_INTERVAL;

  /* Stop the robot if it hasn't received a movement command
   in this number of milliseconds */
  #define AUTO_STOP_INTERVAL 2000
  long lastMotorCommand = AUTO_STOP_INTERVAL;
#endif

/* Variable initialization */

// A pair of varibles to help parse serial commands (thanks Fergs)
int arg = 0;
int index = 0;

// Variable to hold an input character
char chr;

// Variable to hold the current single-character command
char cmd;

// Character arrays to hold the first and second arguments
char argv1[16];
char argv2[16];

// The arguments converted to integers
long arg1;
long arg2;

/* Clear the current command parameters */
void resetCommand() {
  cmd = NULL;
  memset(argv1, 0, sizeof(argv1));
  memset(argv2, 0, sizeof(argv2));
  arg1 = 0;
  arg2 = 0;
  arg = 0;
  index = 0;
}

/* Run a command.  Commands are defined in commands.h */
int runCommand() {
  int i = 0;
  char *p = argv1;
  char *str;
  int pid_args[4];
  arg1 = atoi(argv1);
  arg2 = atoi(argv2);
  
  switch(cmd) {
  case GET_BAUDRATE:
    Serial.println(BAUDRATE);
    break;
  case ANALOG_READ:
    Serial.println(analogRead(arg1));
    break;
  case DIGITAL_READ:
    Serial.println(digitalRead(arg1));
    break;
  case ANALOG_WRITE:
    analogWrite(arg1, arg2);
    Serial.println(&quot;OK&quot;); 
    break;
  case DIGITAL_WRITE:
    if (arg2 == 0) digitalWrite(arg1, LOW);
    else if (arg2 == 1) digitalWrite(arg1, HIGH);
    Serial.println(&quot;OK&quot;); 
    break;
  case PIN_MODE:
    if (arg2 == 0) pinMode(arg1, INPUT);
    else if (arg2 == 1) pinMode(arg1, OUTPUT);
    Serial.println(&quot;OK&quot;);
    break;
  case PING:
    Serial.println(Ping(arg1));
    break;
#ifdef USE_SERVOS
  case SERVO_WRITE:
    servos[arg1].setTargetPosition(arg2);
    Serial.println(&quot;OK&quot;);
    break;
  case SERVO_READ:
    Serial.println(servos[arg1].getServo().read());
    break;
#endif
    
#ifdef USE_BASE
  case READ_ENCODERS:
    Serial.print(readEncoder(LEFT));
    Serial.print(&quot; &quot;);
    Serial.println(readEncoder(RIGHT));
    break;
   case RESET_ENCODERS:
    resetEncoders();
    resetPID();
    Serial.println(&quot;OK&quot;);
    break;
  case MOTOR_SPEEDS:
    /* Reset the auto stop timer */
    lastMotorCommand = millis();
    if (arg1 == 0 &amp;&amp; arg2 == 0) {
      setMotorSpeeds(0, 0);
      resetPID();
      moving = 0;
    }
    else moving = 1;
    leftPID.TargetTicksPerFrame = arg1;
    rightPID.TargetTicksPerFrame = arg2;
    Serial.println(&quot;OK&quot;); 
    break;
  case MOTOR_RAW_PWM:
    /* Reset the auto stop timer */
    lastMotorCommand = millis();
    resetPID();
    moving = 0; // Sneaky way to temporarily disable the PID
    setMotorSpeeds(arg1, arg2);
    Serial.println(&quot;OK&quot;); 
    break;
  case UPDATE_PID:
    while ((str = strtok_r(p, &quot;:&quot;, &amp;p)) != '\0') {
       pid_args[i] = atoi(str);
       i++;
    }
    Kp = pid_args[0];
    Kd = pid_args[1];
    Ki = pid_args[2];
    Ko = pid_args[3];
    Serial.println(&quot;OK&quot;);
    break;
#endif
  default:
    Serial.println(&quot;Invalid Command&quot;);
    break;
  }
}

/* Setup function--runs once at startup. */
void setup() {
  Serial.begin(BAUDRATE);

// Initialize the motor controller if used */
#ifdef USE_BASE
  #ifdef ARDUINO_ENC_COUNTER
    //set as inputs
    DDRD &amp;= ~(1&lt;&lt;LEFT_ENC_PIN_A);
    DDRD &amp;= ~(1&lt;&lt;LEFT_ENC_PIN_B);
    DDRC &amp;= ~(1&lt;&lt;RIGHT_ENC_PIN_A);
    DDRC &amp;= ~(1&lt;&lt;RIGHT_ENC_PIN_B);
    
    //enable pull up resistors
    PORTD |= (1&lt;&lt;LEFT_ENC_PIN_A);
    PORTD |= (1&lt;&lt;LEFT_ENC_PIN_B);
    PORTC |= (1&lt;&lt;RIGHT_ENC_PIN_A);
    PORTC |= (1&lt;&lt;RIGHT_ENC_PIN_B);
    
    // tell pin change mask to listen to left encoder pins
    PCMSK2 |= (1 &lt;&lt; LEFT_ENC_PIN_A)|(1 &lt;&lt; LEFT_ENC_PIN_B);
    // tell pin change mask to listen to right encoder pins
    PCMSK1 |= (1 &lt;&lt; RIGHT_ENC_PIN_A)|(1 &lt;&lt; RIGHT_ENC_PIN_B);
    
    // enable PCINT1 and PCINT2 interrupt in the general interrupt mask
    PCICR |= (1 &lt;&lt; PCIE1) | (1 &lt;&lt; PCIE2);
  #endif
  initMotorController();
  resetPID();
#endif

/* Attach servos if used */
  #ifdef USE_SERVOS
    int i;
    for (i = 0; i &lt; N_SERVOS; i++) {
      servos[i].initServo(
          servoPins[i],
          stepDelay[i],
          servoInitPosition[i]);
    }
  #endif
}

/* Enter the main loop.  Read and parse input from the serial port
   and run any valid commands. Run a PID calculation at the target
   interval and check for auto-stop conditions.
*/
void loop() {
  while (Serial.available() &gt; 0) {
    
    // Read the next character
    chr = Serial.read();

    // Terminate a command with a CR
    if (chr == 13) {
      if (arg == 1) argv1[index] = NULL;
      else if (arg == 2) argv2[index] = NULL;
      runCommand();
      resetCommand();
    }
    // Use spaces to delimit parts of the command
    else if (chr == ' ') {
      // Step through the arguments
      if (arg == 0) arg = 1;
      else if (arg == 1)  {
        argv1[index] = NULL;
        arg = 2;
        index = 0;
      }
      continue;
    }
    else {
      if (arg == 0) {
        // The first arg is the single-letter command
        cmd = chr;
      }
      else if (arg == 1) {
        // Subsequent arguments can be more than one character
        argv1[index] = chr;
        index++;
      }
      else if (arg == 2) {
        argv2[index] = chr;
        index++;
      }
    }
  }
  
// If we are using base control, run a PID calculation at the appropriate intervals
#ifdef USE_BASE
  if (millis() &gt; nextPID) {
    updatePID();
    nextPID += PID_INTERVAL;
  }
  
  // Check to see if we have exceeded the auto-stop interval
  if ((millis() - lastMotorCommand) &gt; AUTO_STOP_INTERVAL) {;
    setMotorSpeeds(0, 0);
    moving = 0;
  }
#endif

// Sweep servos
#ifdef USE_SERVOS
  int i;
  for (i = 0; i &lt; N_SERVOS; i++) {
    servos[i].doSweep();
  }
#endif
}
</code></pre>
<p>Encoder Pin Designations:</p>
<pre class=""lang-cpp prettyprint-override""><code>/* *************************************************************
   Encoder driver function definitions - by James Nugen
   ************************************************************ */
   
   
#ifdef ARDUINO_ENC_COUNTER
  //below can be changed, but should be PORTD pins; 
  //otherwise additional changes in the code are required
  #define LEFT_ENC_PIN_A PD2  //pin 2
  #define LEFT_ENC_PIN_B PD3  //pin 3
  
  //below can be changed, but should be PORTC pins
  #define RIGHT_ENC_PIN_A PC4  //pin A4
  #define RIGHT_ENC_PIN_B PC5   //pin A5
#endif
   
long readEncoder(int i);
void resetEncoder(int i);
void resetEncoders();

</code></pre>
<p>Encoder Driver:</p>
<pre class=""lang-cpp prettyprint-override""><code>/* *************************************************************
   Encoder definitions
   
   Add an &quot;#ifdef&quot; block to this file to include support for
   a particular encoder board or library. Then add the appropriate
   #define near the top of the main ROSArduinoBridge.ino file.
   
   ************************************************************ */
   
#ifdef USE_BASE

#ifdef ROBOGAIA
  /* The Robogaia Mega Encoder shield */
  #include &quot;MegaEncoderCounter.h&quot;

  /* Create the encoder shield object */
  MegaEncoderCounter encoders = MegaEncoderCounter(4); // Initializes the Mega Encoder Counter in the 4X Count mode
  
  /* Wrap the encoder reading function */
  long readEncoder(int i) {
    if (i == LEFT) return encoders.YAxisGetCount();
    else return encoders.XAxisGetCount();
  }

  /* Wrap the encoder reset function */
  void resetEncoder(int i) {
    if (i == LEFT) return encoders.YAxisReset();
    else return encoders.XAxisReset();
  }
#elif defined(ARDUINO_ENC_COUNTER)
  volatile long left_enc_pos = 0L;
  volatile long right_enc_pos = 0L;
  static const int8_t ENC_STATES [] = {0,1,-1,0,-1,0,0,1,1,0,0,-1,0,-1,1,0};  //encoder lookup table
    
  /* Interrupt routine for LEFT encoder, taking care of actual counting */
  ISR (PCINT2_vect){
    static uint8_t enc_last=0;
        
    enc_last &lt;&lt;=2; //shift previous state two places
    enc_last |= (PIND &amp; (3 &lt;&lt; 2)) &gt;&gt; 2; //read the current state into lowest 2 bits
  
    left_enc_pos += ENC_STATES[(enc_last &amp; 0x0f)];
  }
  
  /* Interrupt routine for RIGHT encoder, taking care of actual counting */
  ISR (PCINT1_vect){
        static uint8_t enc_last=0;
            
    enc_last &lt;&lt;=2; //shift previous state two places
    enc_last |= (PINC &amp; (3 &lt;&lt; 4)) &gt;&gt; 4; //read the current state into lowest 2 bits
  
    right_enc_pos += ENC_STATES[(enc_last &amp; 0x0f)];
  }
  
  /* Wrap the encoder reading function */
  long readEncoder(int i) {
    if (i == LEFT) return left_enc_pos;
    else return right_enc_pos;
  }

  /* Wrap the encoder reset function */
  void resetEncoder(int i) {
    if (i == LEFT){
      left_enc_pos=0L;
      return;
    } else { 
      right_enc_pos=0L;
      return;
    }
  }
#else
  #error A encoder driver must be selected!
#endif

/* Wrap the encoder reset function */
void resetEncoders() {
  resetEncoder(LEFT);
  resetEncoder(RIGHT);
}

#endif
</code></pre>
",11/17/2022 19:56,,2496,1,0,-1,,12527861.0,"West Orange, NJ, USA",12/12/2019 22:43,42.0,74674100.0,"<p>I think if you use a Mega instead of an Uno, the pin ports are different.</p>
<p>So change the port from PD4 to PE4 and PD3 to PE5. Also, change PC4 to PF4 and PC5 to PF5.</p>
<p>In the <code>Encoder.ino</code>, you also have to change the ports accordingly.</p>
<p><code>Encoder.h</code>:</p>
<pre><code>  #define LEFT_ENC_PIN_A PE4  //pin 2
  #define LEFT_ENC_PIN_B PE5  //pin 3
  
  //below can be changed, but should be PORTC pins
  #define RIGHT_ENC_PIN_A PF5  //pin A4
  #define RIGHT_ENC_PIN_B PF5   //pin A5
</code></pre>
<p><code>Encoder.ino</code>:</p>
<pre><code>  /* Interrupt routine for LEFT encoder, taking care of actual counting */
  ISR (PCINT2_vect){
    static uint8_t enc_last=0;
        
    enc_last &lt;&lt;=2; //shift previous state two places
    enc_last |= (PINE &amp; (3 &lt;&lt; 2)) &gt;&gt; 2; //read the current state into lowest 2 bits
  
    left_enc_pos += ENC_STATES[(enc_last &amp; 0x0f)];
  }
  
  /* Interrupt routine for RIGHT encoder, taking care of actual counting */
  ISR (PCINT1_vect){
        static uint8_t enc_last=0;
            
    enc_last &lt;&lt;=2; //shift previous state two places
    enc_last |= (PINF &amp; (3 &lt;&lt; 4)) &gt;&gt; 4; //read the current state into lowest 2 bits
  
    right_enc_pos += ENC_STATES[(enc_last &amp; 0x0f)];
  }
</code></pre>
",20680526.0,0.0,0.0,,
3388,55999981,Distance between two Aruco Markers in Python?,|python|opencv|computer-vision|robotics|aruco|,"<p>I am trying to calculate the distance between two Aruco markers in Python. I have code that can calculate the pose of one marker but I am not sure how to move from there. Is there anyone that has done something similar or can point me in the right direction?</p>

<p>Thank you!</p>
",5/6/2019 6:40,56002391.0,1667,1,0,2,0.0,11457959.0,,5/6/2019 6:36,7.0,56002391.0,"<p>You can find the distance between the markers by calculating the distance between the corners of the detected markers. 
The following will give you the corners and the co-ordinates of that corner.</p>

<pre><code>corners, ids, rejectedImgPoints = aruco.detectMarkers(gray, aruco_dict, parameters=arucoParameters)
x1 = int (corners[0][0][0][0]) 
y1 = int (corners[0][0][0][1])
</code></pre>

<p>Similarly you can find the co-ordinates of the corner of the other marker (x2,y2).</p>

<pre><code>import math  
def calculateDistance(x1,y1,x2,y2):  
     dist = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)  
     return dist  
print calculateDistance(x1, y1, x2, y2)
</code></pre>

<p>This code will give the distance between the two corners</p>
",10151093.0,2.0,2.0,,
2508,36960242,How does this piece of code verify a checksum?,|java|checksum|robotics|,"<p><strong>Context:</strong>
My teacher ported the Darwin-OP Framework from C++ to Java, allowing students like me to use it without having to master C++. Darwin has two controllers: the main controller runs Linux and runs the java code, and has a serial connection with the sub controller (a microcontroller) that controls all the sensors/servo's/transducers etc.</p>

<p>Darwin uses a motion.bin file in which it stores a list of 256 pages. Each page is 512 (8 * 64) bytes, and consists of 7 steps (64 bytes each) plus a page header (also 64 bytes). Each steps contains positions (a value between 0-4095) for the servo to take. So for Darwin to move his arm, he goes through (&lt;7) amount of steps until he finishes the final step.</p>

<p>Inside the page header there is a checksum of 1 byte. The Java code contains two methods in which the checksum is calculated and verified:</p>

<pre><code>private static boolean VerifyChecksum(PAGE page) {
    byte checksum = (byte)0x00;
    byte[] pagebytes = page.GetBytes();
    for (int i = 0; i &lt; pagebytes.length; i++) {
        checksum += pagebytes[i];
    }
    if (checksum != (byte)0xFF) {
        return false;
    }
    return true;
}

private static void SetChecksum(PAGE page) {
    byte checksum = (byte)0x00;
    byte[] pagebytes = page.GetBytes();
    page.header.checksum = (byte)0x00;
    for (int i = 0; i &lt; pagebytes.length; i++) {
        checksum += pagebytes[i];
    }
    page.header.checksum = (byte)((byte)0xFF - checksum);
} 
</code></pre>

<p><strong>Main question:</strong> Can someone explain how the checksum is verified? I don't understand why it checks <code>checksum != (byte)0xFF</code>. Why not just compare the calculated <code>checksum</code> to <code>page.header.checksum</code>? </p>

<p><strong>Bonus question:</strong> Why check the file integrity in the first place? Would it be that common for a page inside a .bin file to become corrupted? </p>
",4/30/2016 21:04,36960381.0,1199,2,0,1,,1534664.0,,7/18/2012 11:46,824.0,36960381.0,"<p>To compute the checksum, you perform an XOR of all the bytes in the file, then return 0xFF minus that value.<br>
The file passed in to the checksum method is the final file with 0x00 in the checksum position.</p>

<pre><code>sum = 0xFF - XOR(file)
</code></pre>

<p>For binary, addition is the same as XOR, hence the line <code>checksum += pagebytes[i];</code></p>

<p>Your professor's verification method, will XOR the entire file. Which is to say, the original argument to the checksum method, and an additional byte which is the output of the checksum method.</p>

<p>So the expected result is then:</p>

<pre><code>XOR(file, sum)
= XOR(file) + sum
= XOR(file) + 0xFF - XOR(file)
= 0xFF
</code></pre>
",103959.0,2.0,3.0,,
1216,6620778,Determining the duration of a frequency and the magnitude,|algorithm|controls|robotics|,"<p>I am working with a system in which I am getting data from a sensor (gyro) at 1KHz.</p>

<p>What I am trying to do is determine when the system is vibrating so that I can turn down the PID gains on the output.<br>
What I currently have is a high pass filter on the incoming values.  I then have set the alpha value to 1/64, which I believe should be filtering for about a 10KHz frequency.  I then take this value and then integrate if it is individual above a threshold.  When my integrated value passes another threshold, I then assume that the system is vibrating.  I also reset the integrated value every half second to ensure that it does simply grow towards the threshold.<br>
What I am trying to do with this system is make sure that it is really vibrating and not seeing a jolt.  I have tried to do this with a upper limit to how much will be added to the integrated value, but this is not really appearing to work.</p>

<p>What I am looking for is any better way to go about detecting that the system is vibrating, and not being effected by a jolt, my primary issue is that that I do not miss detect a jolt for a vibration because then that will cause the values on the PID to be lowered unnecessarily.</p>
",7/8/2011 6:37,6620873.0,137,2,0,2,0.0,144600.0,California,7/24/2009 16:05,288.0,6620873.0,"<p>FFT.  It will separate out the ""jolts"" from the vibrations, because jolts will register across all frequencies and vibrations will spike around a particular frequency.</p>
",,1.0,1.0,,
1260,7980785,Unit-testing code with unpredictable external dependencies,|java|junit|robotics|,"<p>I am involved with a project which must, among other things, controlling various laboratory instruments (robots, readers, etc...)</p>

<p>Most of these instruments are controlled either through DCOM-based drivers, the serial port, or by launching proprietary programs with various arguments. Some of these programs or drivers include simulation mode, some don't. Obviously, my development computer cannot be connected to all of the instruments, and while I can fire up virtual machines for the instruments whose drivers include a simulation mode, some stuff cannot be tested without the actual instrument.</p>

<p>Now, my own code is mostly not about the actual operations on the instruments, but about starting operations, making sure everything is fine, and synchronising between the lot of them. It is written in Java, using various libraries to interface with the instruments and their drivers.</p>

<p>I want to write unit tests for the various instrument control modules. However, because the instruments can fail in many ways (some of which are documented, some of which aren't), because the code depends on these partially random outputs, I am a bit lost regarding how to write unit tests for these parts of my code. I have considered the following solutions:</p>

<ul>
<li>only test with actual instruments connected, possibly the most accurate method, but it is not practical at all (insert plate in reader, run unit test, remove plate, run unit test, etc...), not to mention potentially dangerous,</li>
<li>use a mock object to simulate the part that actually communicates with the thing; while this one is definitely easier to implement (and run), it may not be able to reproduce the full range of potential failures (as mentioned above, a lot is undocumented, which can sometimes cause bad surprises)</li>
</ul>

<p>While I am currently thinking of going with the latter, am I missing something? Is there a better way to do this?</p>
",11/2/2011 13:03,7980991.0,543,3,0,6,0.0,313432.0,France,4/10/2010 11:04,34.0,7980840.0,"<p>If you're using mocks then you can substitute different mocks to perform differently. That is, your tests will be consistent. That's valuable since running tests against a randomly performing system is not going to give you a sense of security. Each run can/will execute a different code path.</p>

<p>Since you don't know all the failure scenarios in advance, I think there are two (non-exclusive) scenarios:</p>

<ol>
<li>Capture the details of those failures as you see them and encode further tests in your mocks to replicate these. Consequently your logging needs to be sound, to capture the failure details. As time goes on, your test set will expand to encompass and regression test these scenarios.</li>
<li>Your interfaces to these system may be able to capture all errors, but present them in a finite subset of errors. e.g. categorise all errors into (say) connection errors, timeouts etc. That way you restrict your scenarios to a small set of failures. I don't know if this is practical for your application, unfortunately.</li>
</ol>
",12960.0,4.0,0.0,,
631,3146232,"Robot, stereo vision, driving around and AForge",|robotics|aforge|,"<p>I am thinking about building a small robot that will drive around, judge distance to objects and avoid obstacles (hopefully it will) - nothing complex, just a home fun project.</p>

<p>I am planning on using two webcams to decide distances and track objects, and AForge.Net as the framework. I have never done anything AI and robotics related, so I have no idea where to start.</p>

<p>What sort of components would I need (wheels and engine, etc., that can be programmatically manipulated, programmable controller of some sort?) and where do I get all this stuff?</p>

<p>Overall, where do I start with this to make it happen? I am looking for a set of tools that would solve the basic problems and allow me to concentrate on programming of my robot.</p>
",6/30/2010 3:16,3189452.0,735,5,0,4,0.0,178980.0,Australia,9/25/2009 9:56,1175.0,3146405.0,"<p>Since you have no prior experience, you could try <a href=""http://en.wikipedia.org/wiki/Lego_Mindstorms"" rel=""nofollow noreferrer"">Lego Mindstorms</a> programmable robotics.</p>

<p>Personally, I think this is a complex project, though fascinating ;-)</p>
",108130.0,1.0,0.0,,
3708,62019726,Real Time Stepper Motors Control using ESP8266 and Blynk app,|c++|arduino|arduino-esp8266|robot|blynk|,"<p>I wanted to control 2 Stepper Motors for running a robot using the joystick of Blynk App and NodeMCU/ESP8266. But when I searched for the codes of real time controlling of Stepper Motors online I didn't get much code and most of them were not real time.</p>

<p>This is code I am currently working on:-</p>

<pre><code>#include &lt;ESP8266WiFi.h&gt;
#include &lt;BlynkSimpleEsp8266.h&gt;

#define RightMotorSpeed D7
#define RightMotorDir   D8  

const int enPin = D2;
const int enPin2 = D3;

#define LeftMotorSpeed  D6  
#define LeftMotorDir    D5


// You should get Auth Token in the Blynk App.
// Go to the Project Settings (nut icon).
// Use your own WiFi settings
char auth[] = ""LRTCZUnCI06P-pqh5rlPXRbuOUgQ_uGH"";
char ssid[] = ""Airtel_7599998800"";
char pass[] = ""air71454"";

// neutral zone settings for x and y
// joystick must move outside these boundary numbers to activate the motors
// makes it a little easier to control the wifi car
int minRange = 312;
int maxRange = 712;

// analog speeds from 0 (lowest) - 1023 (highest)
// 3 speeds used -- 0 (noSpeed), 350 (minSpeed), 850 (maxSpeed).
// use whatever speeds you want...too fast made it a pain in the ass to control
int minSpeed = 450;
int maxSpeed = 1023;
int noSpeed = 0;


void moveControl(int x, int y)
{
  // movement logic
  // move forward

   // y je vetsi jak maxrange a současně x je vetsi jak minRange a současne mensi jak max range 
  while(y &gt;= maxRange &amp;&amp; x &gt;= minRange &amp;&amp; x &lt;= maxRange) //zataci R
  {
    digitalWrite(RightMotorDir,HIGH);  
    digitalWrite(LeftMotorDir,HIGH);

    analogWrite(RightMotorSpeed,maxSpeed); 
    analogWrite(LeftMotorSpeed,maxSpeed);

    delayMicroseconds(500);

    digitalWrite(RightMotorSpeed,0); 
    digitalWrite(LeftMotorSpeed,0);

    delayMicroseconds(500);
  }

  // move forward right
  while(x &gt;= maxRange &amp;&amp; y &gt;= maxRange)   //zataci R
  {
    digitalWrite(RightMotorDir,HIGH);
    digitalWrite(LeftMotorDir,HIGH);
   analogWrite(RightMotorSpeed,minSpeed); 
    analogWrite(LeftMotorSpeed,maxSpeed);
  }

  // move forward left
  while(x &lt;= minRange &amp;&amp; y &gt;= maxRange)
  {
    digitalWrite(RightMotorDir,HIGH);
    digitalWrite(LeftMotorDir,HIGH);
    analogWrite(RightMotorSpeed,maxSpeed); 
    analogWrite(LeftMotorSpeed,minSpeed);
  }

  // neutral zone
  while(y &lt; maxRange &amp;&amp; y &gt; minRange &amp;&amp; x &lt; maxRange &amp;&amp; x &gt; minRange)
  {
    analogWrite(RightMotorSpeed,noSpeed); 
    analogWrite(LeftMotorSpeed,noSpeed);
  }

 // move back
  while(y &lt;= minRange &amp;&amp; x &gt;= minRange &amp;&amp; x &lt;= maxRange)
  {
    digitalWrite(RightMotorDir,LOW);
    digitalWrite(LeftMotorDir,LOW);
   analogWrite(RightMotorSpeed,maxSpeed); 
    analogWrite(LeftMotorSpeed,maxSpeed);
  }

  // move back and right
 while(y &lt;= minRange &amp;&amp; x &lt;= minRange)
  {
   digitalWrite(RightMotorDir,LOW);
    digitalWrite(LeftMotorDir,LOW);
    analogWrite(RightMotorSpeed,minSpeed); 
    analogWrite(LeftMotorSpeed,maxSpeed);  
  }

  // move back and left
  while(y &lt;= minRange &amp;&amp; x &gt;= maxRange)
  {
    digitalWrite(RightMotorDir,LOW);
    digitalWrite(LeftMotorDir,LOW);
    analogWrite(RightMotorSpeed,maxSpeed); 
    analogWrite(LeftMotorSpeed,minSpeed);
  }
}

void setup()
{
  // initial settings for motors off and direction forward
  pinMode(RightMotorSpeed, OUTPUT);
  pinMode(LeftMotorSpeed, OUTPUT);
  pinMode(RightMotorDir, OUTPUT);
  pinMode(LeftMotorDir, OUTPUT);
  digitalWrite(RightMotorSpeed, LOW);
  digitalWrite(LeftMotorSpeed, LOW);
  digitalWrite(RightMotorDir, HIGH);
  digitalWrite(LeftMotorDir,HIGH);



    Serial.begin(9600);

    pinMode(enPin,OUTPUT);
    digitalWrite(enPin,LOW);

    pinMode(enPin2,OUTPUT);
    digitalWrite(enPin2,LOW);



  Blynk.begin(auth, ssid, pass);
 }


void loop()
{
  Blynk.run();
}


BLYNK_WRITE(V1)
{
  int x = param[0].asInt();
  int y = param[1].asInt();
  moveControl(x,y); 
}
</code></pre>

<p>Here I have defined by 2 Stepper motors as Right and Left and since I am using TB6600 Motor Driver therefore their Pulse and Direction Pins are also Defined. That is the main reason that I am unable to use the Stepper motor Library for the code.</p>

<p>Running the code I see that Both the motors runs fine once for 3 to 5 seconds and the the Blynk Server Disconnects and Reconnects again causing the motors to stop and not creating a real time communication. Some one Please help me create a code for these 2 stepper motors to run at realtime.</p>

<p>I think that <strong>Blink.run()</strong> causes the server to reconnect and stop the motor.</p>

<p>I also searched for this cause and found that instead of Stepper motor Library I should use AccelStepper Library but that is also not achieved . Please help me with this. Any correct reference is also appreciable. Thanks in advance.</p>
",5/26/2020 10:12,,1609,1,1,0,,8913187.0,India,11/9/2017 11:08,8.0,68532470.0,"<p>Blynk requires constants pings to keep the connection alive. A while loop prevents any form of communication between your device and the Blynk server.
The preferred option is to use Timers to interactively call your functions and advance through the code. Another method is to force a server ping.</p>
<p>For example, you could add the following and call softDelay(1) within each of your while loops.</p>
<pre><code>void softDelay(uint32_t t) {
  unsigned long currentTime = millis();`
  unsigned long newTime = currentTime + t;
  while (currentTime &lt;= newTime)
  {
     Blynk.run();
     timer.run();
     currentTime = millis();
  }
}
</code></pre>
",16530042.0,0.0,0.0,109699787.0,"I think you should find out what real time means. I doubt that most code for constrolling stepper motors is not realtime as you claim. also note that it is not a very good idea to search for complete solutions and tutorials on your specific project. rather break your project down into sub-problems which are more common. you won't find a tutorial on how to look like Arnold Schwarzenegger with pink hair, but you'll find plenty of resources on how to do body building and on how to dye hair properly."
235,786997,Multi threading using NXT,|multithreading|robotics|lego-mindstorms|,"<p>At my robotics club we are trying to get the multi-threading capability to work on LEGO Mindstorms NXT, but it seems that the threads are interfering with each other and causing the program to stop entirely.</p>

<p>Does anyone know how to correctly implement multi-threading on the NXT visual programming environment.</p>
",4/24/2009 18:15,,3038,2,0,4,0.0,60723.0,,1/30/2009 17:23,110.0,787028.0,"<p><strong>Does anyone know how to correctly implement multi-threading on the NXT visual programming environment?</strong></p>

<p>Yes, lots of people know how to implement multi-threading on the NXT visual programming environment.</p>

<p>Unless you are more specific, though, you aren't going to find out what's wrong with your particular problem.  There are lots of gotchas, though.</p>

<p>For instance, if you have two threads inside a loop, the loop will not loop until BOTH threads complete.</p>

<p>There are many others - too many to enumerate here - so start debugging your program piece by piece, and post examples of your 'code' (screenshots?) and then describe what it's doing, and what you want it to do.</p>
",2915.0,1.0,0.0,,
2545,37649397,Python pip install qibuild exception then I installed pip yet.,|python|python-2.7|pip|robotics|debian-based|,"<p>I'm new on this forum and I will ask you to be friendly with my first publication. Thank you!!</p>

<p>So, here is my problem. I installed pip and now I try to install qibuild with it. But I've got a problem.
This is the command that I run:</p>

<pre><code>pip install qibuild
</code></pre>

<p>And this the error that I have:</p>

<pre><code>Downloading/unpacking qibuild
Cleaning up...
Exception:
Traceback (most recent call last):
File ""/usr/lib/python2.7/dist-packages/pip/basecommand.py"", line 122, in  main
status = self.run(options, args)
File ""/usr/lib/python2.7/dist-packages/pip/commands/install.py"", line 290, in run
requirement_set.prepare_files(finder, force_root_egg_info=self.bundle,bundle=self.bundle)
File ""/usr/lib/python2.7/dist-packages/pip/req.py"", line 1178, in prepare_files
url = finder.find_requirement(req_to_install, upgrade=self.upgrade)
File ""/usr/lib/python2.7/dist-packages/pip/index.py"", line 194, in find_requirement
page = self._get_page(main_index_url, req)
File ""/usr/lib/python2.7/dist-packages/pip/index.py"", line 568, in _get_page
session=self.session,
File ""/usr/lib/python2.7/dist-packages/pip/index.py"", line 694, in get_page
req, link, ""connection error: %s"" % exc, url,
TypeError: __str__ returned non-string (type SysCallError)

Storing debug log for failure in /root/.pip/pip.log
</code></pre>

<p>I searched on the net and I didn't find anything like that. Someone have an idea of what I can do to fix this problem? </p>
",6/6/2016 4:05,,282,1,0,0,,6428577.0,,6/6/2016 3:46,0.0,39495954.0,"<p>Can you check the time on your system. Sometimes it so happens when your system time is inaccurate the pip installation fails. I have faced this issue once.</p>

<p>Change your system date to current date in accordance with your time zone ,
Hope this helps</p>
",6473089.0,0.0,0.0,,
1895,21061463,Entegrating existing .mfile to the .mdl simulink,|matlab|simulink|robotics|,"<p>I convert my .m file above as a function like below ,  my input is nothing and my output is q. But I have a problem. When I put my created function block to the simulink and connect to the display screen , matlab gives me some errors like;</p>

<p>*<strong>Try and catch are not supported for code generation.
Function 'tb_optparse.m' (#80.5667.6083), line 157, column 25:
""try""
Launch diagnostic report.*</strong></p>

<p><em><strong>Function call failed.
Function 'MATLAB Function' (#94.848.897), line 37, column 3:
""mstraj(path, [15 15 15], [], [1 0 1], 0.02 , 0.2)""
Launch diagnostic report.</em></strong></p>

<p><strong>Errors occurred during parsing of MATLAB function 'MATLAB Function'(#93)</strong> </p>

<p>How can I fix these errors? Thanks</p>

<pre><code>function output = fcn()


%mdl_puma560    %to create puma robot

for type=1:3  % main for loop. It turns 3 times. At first, it sets the path
    %           to x-y plane and draw the robot, at second for y-z plane
    %           and then for x-z plane

  if type==1 

% The path of robot for x-y plane    
path=[0 0 1;0 0 0;0 2 0 ;0.5 1 0 ;1 2 0;1 0 0;1.5 0 1;1.5 0 0;
      1.5 2 0;2.2 2 0;2.5 1.6 0;2.5 0.4 0;2.2 0 0;1.5 0 0;0 0 1];


 elseif type==2   

% Same thing as first part    
path=[-0.5 0 0;0 0 0;0 0 1;0 -0.5 0.5;0 -1 1;0 -1 0;-0.5 -1.2 0;0 -1.2 0;
    0 -1.2 1;0 -1.7 1;0 -2 0.7;0 -2 0.3;0 -1.7 0;0 -1.2 0];


 elseif type==3

 % Same thing as first and second part     
path=[0 -0.5 0;0 0 0;0 0 1;0.5 0 0.5;1 0 1;1 0 0;1.3 -0.5 0;1.3 0 0;
    1.3 0 1;1.7 0 1;2 0 0.7;2 0 0.3;1.7 0 0;1.3 0 0];


  end



% I created a trajectory

p=mstraj(path, [15 15 15], [], [1 0 1], 0.02 , 0.2);

% [15 15 15] means the maximum speed in x,y,z directions.
% [1 0 1] means the initial coordinates
% 0.02 means acceleration time
% 0.2 means smoothness of robot


numrows(p)*0.2;    % 200 ms sample interval
Tp=transl(0.1*p);  % Scale factor of robot
Tp=homtrans( transl(0.4,0,0),Tp);  % Origin of the letter
q=p560.ikine6s(Tp) ;  % The inverse kinematic


% for i=1:length(q)
% %q matrix has 280 rows and 6 columns. So this for loop turns 280 times
% % At every turns , it plots one part of movement. q(1,:), q(2,:), ...  
% 
%     p560.plot(q(i,:))
% 
% end

end

output=q;
</code></pre>
",1/11/2014 11:02,,322,1,0,-1,,3177900.0,,1/9/2014 14:14,14.0,21065743.0,"<p>Well as the error message says, it looks like your function <code>mstraj</code> is calling <code>try/catch</code> which isn't supported for code generation (MATLAB functions in Simulink are first converted to C code when you run the model).</p>

<p>Have a look at <a href=""http://www.mathworks.co.uk/help/simulink/ug/calling-matlab-functions.html"" rel=""nofollow"">Call MATLAB Functions</a> in the documentation for ways to work around this using <code>coder.extrinsic</code>. Extrinsic functions return data of type <code>mxArray</code> so you will need to convert it to whatever the data type of <code>p</code> (see <strong>Converting mxArrays to Known Types</strong> in the documentation page above).</p>

<p>In you case, it would probably look something like:</p>

<pre><code>function output = fcn()

coder.extrinsic('mstraj');

% etc...

p = 0; % Define p as a scalar of type double (change to required data type if not appropriate)
p=mstraj(path, [15 15 15], [], [1 0 1], 0.02 , 0.2);

% etc...
</code></pre>
",2257388.0,0.0,0.0,,
2514,37075632,"PID controller affect on a differential driving robot when the parameters (Kp, Ki, and Kd) are increased individually. [full Q written below]",|robotics|pid-controller|,"<p>Question: <strong>A PID controller has three parameters Kp, Ki and Kd which could affect the output performance. A differential driving robot is controlled by a PID controller. The heading information is sensed by a compass sensor. The moving forward speed is kept constant. The PID controller is able to control the heading information to follow a given direction. Explain the outcome on the differential driving robot performance when the three parameters are increased individually.</strong></p>

<p>This is a question that has come up in a past paper but most likely won't show up this year but it still worries me. It's the only question that has me thinking for quite some time. I'd love an answer in simple terms. Most stuff i've read on the internet don't make much sense to me as it goes heavy into the detail and off topic for my case.</p>

<p><strong>My take on this:</strong></p>

<p>I know that the <strong>proportional</strong> term, Kp, is entirely based on the error and that, let's say, double the error would mean doubling Kp (applying proportional force). This therefore implies that increasing Kp is a result of the robot heading in the wrong direction so Kp is increased to ensure the robot goes on the right direction or at least tries to reduce the error as time passes so an increase in Kp would affect the robot in such a way to adjust the heading of the robot so it stays on the right path.</p>

<p>The <strong>derivative</strong> term, Kd, is based on the rate of change of the error so an increase in Kd implies that the rate of change of error has increased over time so double the error would result in double the force. An increase by double the change in the robot's heading would take place if the robot's heading is doubled in error from the previous feedback result. Kd causes the robot to react faster as the error increases.</p>

<p>An increase in the <strong>integral</strong> term, Ki, means that the error is increased over time. The integral accounts for the sum of error over time. Even a small increase in the error would increase the integral so the robot would have to head in the right direction for an equal amount of time for the integral to balance to zero.</p>

<p><em>I would appreciate a much better answer and it would be great to be confident for a similar upcoming question in the finals.</em></p>

<p>Side note: i've posted this question on the Robotics part earlier but seeing that the questions there are hardly ever noticed, i've also posted it here. </p>
",5/6/2016 15:01,37122552.0,756,1,0,0,,5920532.0,,2/12/2016 21:09,67.0,37122552.0,"<p>I would highly recommend reading this article <a href=""http://eas.uccs.edu/~cwang/ECE4330F12/PID-without-a-PhD.pdf"" rel=""nofollow"">PID Without a PhD</a> it gives a great explanation along with some implementation details. The best part is the numerous graphs. They show you what changing the P, I, or D term does while holding the others constant.</p>

<p>And if you want real world Application  Atmel provides example code on their site (for 8 bit MCU) that perfectly mirrors the PID without a PhD article. It follows this code from AVR's website exactly (they make the ATMega32p microcontroller chip on the Arduino UNO boards) <a href=""http://www.atmel.com/Images/doc2558.pdf"" rel=""nofollow"">PDF explanation</a> and <a href=""http://www.atmel.com/images/AVR221.zip"" rel=""nofollow"">Atmel Code in C</a></p>

<p>But here is a general explanation the way I understand it.</p>

<p><strong>Proportional:</strong> This is a proportional relationship between the error and the target. Something like <code>Pk(target - actual)</code> Its simply a scaling factor on the error. It decides how quickly the system should react to an error (if it is of any help, you can think of it like amplifier slew rate). A large value will quickly try to fix errors, and a slow value will take longer. With Higher values though, we get into an overshoot condition and that's where the next terms come into play</p>

<p><strong>Integral:</strong> This is meant to account for errors in the past. In fact it is the sum of all past errors. This is often useful for things like a small dc/constant offset that a Proportional controller can't fix on its own. Imagine, you give a step input of 1, and after a while the output settles at .9 and its clear its not going anywhere. The integral portion will see this error is always ~.1 too small so it will add it back in, to hopefully bring control closer to 1. THis term usually helps to stabilize the response curve. Since it is taken over a long period of time, it should reduce noise and any fast changes (like those found in overshoot/ringing conditions). Because it's aggregate, it is a very sensitive measurement and is usually very small when compared to other terms. A lower value will make changes happen very slowly, and create a very smooth response(this can also cause ""wind-up"" see the article)</p>

<p><strong>Derivative:</strong> This is supposed to account for the ""future"". It uses the slope of the most recent samples. Remember this is the slope, it has nothing to do with the position error(<code>current-goal</code>), it is <code>previous measured position - current measured position</code>. This is most sensitive to noise and when it is too high often causes ringing. A higher value encourages change since we are ""<em>amplifying</em>"" the slope.</p>

<p>I hope that helps. Maybe someone else can offer another viewpoint, but that's typically how I think about it.</p>
",2705382.0,1.0,0.0,,
2823,45093135,Controller for robotic arm in computer vision system,|c++|c|controller|robotics|,"<p>I'm working on a project of building a computer vision system. I have an embedded computer (Matrox 4Sight GPm) for running a C++ (OpenCV) program which I tested it with my laptop's built-in camera and it works.</p>

<p>And the idea is that when the certain conditions are met, the vision system will output a signal to trigger a robotic arm to perform a task.</p>

<p>Since I'm learning things from scratch, I wonder do I need an extra controller for controlling the arm?</p>

<p>If yes, what do I need to add in my computer vision coding part and how's the controller's code will look like (in C or C++) so that the vision can communicate with the controller to control the robotic arm?</p>

<p>If no (the embedded computer can control the robotic arm), what code do I need to add to make it happen?</p>

<p>I know this is a vague question,  but any direction for me to look into will be greatly appreciated! Thank you.</p>
",7/14/2017 1:19,45096834.0,218,1,2,0,,5957244.0,,2/21/2016 0:20,30.0,45096834.0,"<p>I went through the datasheet of <strong>Matrox 4Sight GPm</strong> and it seems a very powerful platform.
It has one <strong>RS-232</strong> and one <strong>RS-485</strong> port for serial communication, and also consists of one <strong>FPGA</strong> with <strong>Digital I/O's</strong> that can take input commands from <strong>Intel HM76 PCH</strong> processor.</p>

<p>In my opinion, an extra microcontroller should not be needed to drive a robotic arm. If robotic arm consists of simple D.C. motors, you need to figure out how to control the <strong>Digital I/O's</strong> of FPGA and interface a simple motor driver IC such as <strong>L293D</strong> or <strong>L298</strong> to that <strong>Digital I/O's</strong>.</p>

<p>If the robotic arm consists of servo motors then, in that case, you definitely need a microcontroller which has <strong>PWM</strong> on it. You need to program the <strong>RS-232</strong> of <strong>Matrox 4Sight GPm</strong> to send some custom commands to the microcontroller on the <strong>UART</strong> and you can write a simple program for microcontroller to drive the servo motors of that arm using <strong>PWM</strong> depending on the command received over the <strong>RS-232</strong> serial channel from <strong>Matrox 4Sight GPm</strong>.</p>

<p>I hope I have cleared some of your doubts.</p>
",2805824.0,1.0,1.0,77163420.0,"Most of the controllers are coded in C, that's why I also include C. And ppl who are familiar with C and controller might have some suggestions."
3729,62630556,"Mapping, and autonomous drones",|artificial-intelligence|augmented-reality|robotics|,"<p>What is a good software to be able to navigate a drone in an augmented reality, is there anything that exists for individual and educational use in this area?</p>
",6/29/2020 2:58,,61,1,0,0,,13830207.0,"Fortuna, CA, USA",6/28/2020 17:40,6.0,62701790.0,"<p>I'm not sure this is the platform to ask this question, but here is an answer. There were a few companies that tackled this issue in the past, dARwing was doing exactly that and edgybees did something similar. Both companies are no longer in this field.</p>
",3360466.0,0.0,1.0,,
2637,41645196,Robot Obstacle Recording/Avoiding,|java|while-loop|robotics|,"<p>I'm trying to make this robot go in random directions until it reaches an obstacle. Then it should record that obstacle (Obstacle = 1,2,3 etc) and switch direction. This should go on until the timer expires.</p>

<pre><code>public static void main(String args[]) throws Exception{

    Robot therobot = new Robot();

    int x = 10000;
    int obstacles = 0;
    Random rand = new Random();
    int r1 = rand.nextInt(255) + 1;
    int r2 = rand.nextInt(255) + 1;

    therobot.setWheelVelocities(100,100);
    long before = System.currentTimeMillis();

    while (System.currentTimeMillis() - before &lt; x){
        Thread.sleep(x);
        if( therobot.isObstacle() ==true || therobot.isTapped() == true)
        {
            r1 = rand.nextInt(255) - 255;
            r2 = rand.nextInt(255) - 255;
            obstacles = obstacles++;

            therobot.setWheelVelocities(r1, r2);
        }
    }
    System.out.println(obstacles);

    therobot.stopWheels();
    therobot.quit();
}
</code></pre>

<p>But this doesn't seem to work. It just goes until the timer expires but it won't stop or record anything.</p>

<p>What am I missing ? </p>
",1/13/2017 23:50,41645234.0,282,1,9,0,0.0,7133825.0,,11/8/2016 21:11,15.0,41645234.0,"<p>Having</p>

<pre><code>int x = 10000;
long before = System.currentTimeMillis();
while (System.currentTimeMillis() - before &lt; x){
    Thread.sleep(x);
    // Processing
}
</code></pre>

<p>The first iteration of while loop takes entire time duration of 10 seconds because of <code>Thread.sleep(10000)</code>.</p>

<p>Sleep amount should be significantly lesser than total duration.</p>
",2327745.0,0.0,4.0,70490511.0,"Yes, sorry, I didn't write it down here."
2590,40584193,Why calculate jacobians in ekf-slam,|robotics|kalman-filter|slam|,"<p>I know it is a very basic question but I want to know why do we calculate the Jacobian matrices in EKF-SLAM, I have tried so hard to understand this, well it won't be that hard but I want to know it. I was wondering if anyone could help me on this.</p>
",11/14/2016 8:01,41658660.0,422,1,0,1,,6831268.0,,9/14/2016 13:58,100.0,41658660.0,"<p>The Kalman filter operates on linear systems.  The steps update two parts in parallel:  The state <code>x</code>, and the error covariances <code>P</code>.  In a linear system we predict the next <code>x</code> by <code>Fx</code>.  It turns out that you can compute the exact covariance of <code>Fx</code> as <code>FPF^T</code>.  In a non-linear system, we can update <code>x</code> as <code>f(x)</code>, but how do we update <code>P</code>?  There are two popular approaches:</p>

<ol>
<li>In the EKF, we choose a linear approximation of <code>f()</code> at <code>x</code>, and then use the usual method <code>FPF^T</code>.</li>
<li>In the UKF, we build an approximation of the distribution of <code>x</code> with covariance <code>P</code>.  The approximation is a set of points called <em>sigma points</em>.  Then we propagate those states through our real <code>f(sigma_point)</code> and we measure the resulting distribution's variance.</li>
</ol>

<p>You are concerned with the EKF (case 1).  What is a good linear approximation of a function?  If you zoom way in on a curve, it starts to look like a straight line, with a slope that's the derivative of the function at that point.  If that sounds strange, look at <a href=""https://en.wikipedia.org/wiki/Taylor_series"" rel=""nofollow noreferrer"" title=""the Wikipedia entry on Taylor series"">Taylor series</a>.  The multi-variate equivalent is called the Jacobian.  So we evaluate the Jacobian of <code>f()</code> at <code>x</code> to give us an <code>F</code>.  Now <code>Fx != f(x)</code>, but that's okay as long as the changes we're making to <code>x</code> are small (small enough that our approximated <code>F</code> wouldn't change much from before to after).</p>

<p>The main problem with the EKF approximation is that when we use the approximation to update the distributions after the measurement step, it tends to make the resulting covariance <code>P</code> too low.  It acts like corrections ""work"" in a linear way.  The actual update will depart slightly from the linear approximation and not be quite as good.  These small amounts of overconfidence build up as the KF iterates and have to be offset by adding some fictitious process noise to <code>Q</code>.</p>
",479989.0,3.0,0.0,,
4720,77607266,Implementing Kalman Filter for Improved Measurement Accuracy in Autonomous Two-Wheeled Robot with Encoders,|pid|robotics|kalman-filter|encoder|,"<p>So i am a beginner in the field of robotics, and I have successfully created an autonomous two-wheeled robot equipped with two encoders. I've implemented a PI regulation on speed, and while the robot performs reasonably well, I still encounter some measurement errors. To enhance its performance, I'm considering the implementation of a Kalman filter.</p>
<p>While I have a theoretical understanding of the Kalman filter, I'm a bit confused about the practical implementation. Should I apply a simple Kalman filter to each encoder independently to smooth the measurements, or is it more appropriate to use the filter on the x and y positions of the robot? . Any guidance on the best approach for implementing the Kalman filter in this context would be greatly appreciated.</p>
",12/5/2023 15:12,,54,0,1,1,,23046277.0,,12/5/2023 14:46,0.0,,,,,,136829548.0,"At the end of the day, is the (x, y) position of the robot what you care about? If so, then it probably makes sense to have those be the states of the filter. Are there any other measurements that you get about the position of the vehicle? If the encoders is all that you have, a Kalman Filter probably won't help you all that much. The dynamics in the filter would be driven by the encoder, but there are no measurements to correct the state estimate, so uncertainty would just keep growing over time."
1449,11794299,Distance and Angle robot control,|algorithm|robotics|,"<p>I am sorry if it is not the right Stack Exchange website i should ask this question on, but it seems to me that this is closely related to software programming more than electrican engineering ! If it should be elsewhere, please tell me.</p>

<p>Let's assume we want to control a 2 motorized wheels robot to go from point A to point B. Let's assume i want to control my robot by providing it the distance and angle. </p>

<p>I could first do the control on my angle, and then on the distance, so the robot first turns around its center (by providing positive order to one wheel, negative to the other one), and then travel until the point B is reached (by providing two positive oders to the wheels). </p>

<p>However, if i want the robot to do it in one move, i want the two different control values to be controlled at the same time. This way the robot does have a nice curve until it reaches the destination. Doing this, i provide two positive orders to the wheels, and i add on one side a positive order, and a negative one to the other side.</p>

<p>With this last solution, i have some troubles to understand how to reach the right direction.</p>

<p>If, for example, i want to go 1meter backward. I will set my order as point B : (1m,180°). With the first solution, no problem, the angle is done first, and when it is done, the 1m are done.
With the second solution, i move WHILE i turn, therefore, the curve is way bigger than the ordered 1m, and it stops after 1m, not at the point B.</p>

<p>How should i address this concern ? Do you have any advice, or maybe did i not understand well this technique ? I tried to simulate a control system with a little XNA game to try out solutions even without any robot, so feel free to give any advices you may think interesting !</p>

<p>Al_th</p>
",8/3/2012 10:53,11794398.0,2190,3,0,2,0.0,1503324.0,France,7/5/2012 8:09,188.0,11794398.0,"<p>I'm not fully qualified to answer this myself, but there is some great material about this on Udacity: <a href=""http://www.udacity.com/view#Course/cs373/CourseRev/apr2012/Unit/510040/Nugget/515042"" rel=""nofollow"">http://www.udacity.com/view#Course/cs373/CourseRev/apr2012/Unit/510040/Nugget/515042</a>. Sebastian Thrun (who runs Google's autonomous car project) will explain this much better than I ever could.</p>

<p>Edit: the example in the video assumes a bicycle model (two wheels only), so it's not directly applicable to your case. However, you might get some valueable info that you can utilize.</p>
",177628.0,1.0,1.0,,
4625,76698876,Issue while trying to controlling LEGO Spike Prime motors using Visual Studio Code,|python|error-handling|command-line|integration|robotics|,"<p>I'm using the LEGO SPIKE Prime extension for Visual Studio Code from the GitHub blog (<a href=""https://github.com/sanjayseshan/spikeprime-vscode"" rel=""nofollow noreferrer"">https://github.com/sanjayseshan/spikeprime-vscode</a>) to develop a project. However, I'm encountering a &quot;TypeError: argument of type 'NoneType' is not iterable&quot; error. I'm seeking assistance in resolving this issue.</p>
<p>Project Description:
I'm working on a project that involves controlling the LEGO SPIKE Prime robot using the extension for Visual Studio Code. The extension provides various features and functionalities for interacting with the LEGO SPIKE Prime hardware.</p>
<p>Error Message:
When running the following command in the command line, I encounter the TypeError:</p>
<pre><code>C:\Users\***\Desktop\lego\levov2&gt;python C:\spiketools\spikejsonrpc.py -t COM17 upload &quot;c:\Users\***\Desktop\lego\levov2\teste.py&quot; 1
TypeError: argument of type 'NoneType' is not iterable
</code></pre>
<p>(Note: The username has been obscured for privacy reasons)</p>
<p>Issue:
The error occurs when executing the above command using the LEGO SPIKE Prime extension's &quot;spikejsonrpc.py&quot; script. It seems that the 'NoneType' error is related to an iterable object not being available during the execution.</p>
<p>Expected Behavior:
I expected the command to upload the &quot;teste.py&quot; file to the LEGO SPIKE Prime hub connected to the specified COM port without any errors.</p>
<p>Additional Information:
I have followed the installation steps provided in the GitHub blog for setting up the LEGO SPIKE Prime extension.
The LEGO SPIKE Prime hub is properly connected to COM17 and functional.
The &quot;teste.py&quot; file exists in the specified directory.
Request:
I would appreciate any insights or suggestions on how to resolve this TypeError when using the LEGO SPIKE Prime extension for Visual Studio Code. If there are any specific configurations or additional steps that need to be taken to ensure the proper functionality of the extension and the &quot;spikejsonrpc.py&quot; script, please let me know.</p>
<p>Thank you for your assistance!</p>
",7/16/2023 14:51,,427,0,1,0,,22235563.0,,7/16/2023 12:43,1.0,,,,,,135223136.0,Please provide enough code so others can better understand or reproduce the problem.
2506,36891435,Contour Detection in Opencv Python 2.7,|python-2.7|opencv|image-processing|automation|robotics|,"<p>I have been trying to identify the contours of red colored objects in Open Cv (python 2.7) and we have succeeded in identifying them. But, I want to detect the position of the red colored object (left or right) and I have not succeeded in doing so. If anyone could give me the code or steps to do so, I would be really thankful.</p>

<p>Our current code for identifying red colored objects is as follows:</p>

<pre><code>import numpy as np
import cv2
cap = cv2.VideoCapture(0)
while(1):
  _, frame = cap.read()
  hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
  lower_color = np.array([0, 50, 50])
  upper_color = np.array([60, 255, 255])
  mask = cv2.inRange(hsv, lower_color, upper_color)
  mask = cv2.erode(mask, None, iterations=2)
  mask = cv2.dilate(mask, None, iterations=2)
  res = cv2.bitwise_and(frame, frame, mask=mask)

  cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]
  cv2.drawContours(frame, cnts, 0, (127, 255, 0), 3)
 print cnts
 cv2.imshow('frame', frame)
 cv2.imshow('mask', mask)
 cv2.imshow('res', res)
 cv2.imshow('contours', frame)



 k = cv2.waitKey(5) &amp; 0xFF
 if k == 27:
    print ""release""

    break
 cap.release()
 cv2.destroyAllWindows()
</code></pre>
",4/27/2016 13:34,,1371,1,0,2,,6261639.0,,4/27/2016 13:08,7.0,36892486.0,"<p>Well, You are one step away of getting the position. You can create a <a href=""http://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=boundingrect#boundingrect"" rel=""nofollow""><code>boundingrect</code></a> around the contour(s) and then you can calculate it's center to obtain the coordinates of the object.</p>

<p>You can also try <a href=""http://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=minenclosingcircle#minenclosingcircle"" rel=""nofollow""><code>minEnclosingCircle</code></a> That will give you the center and the radius of it. This could be a little more direct to find the center :)</p>

<p><a href=""http://docs.opencv.org/2.4/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.html"" rel=""nofollow"">Here</a> you can find a small tutorial of both functions, but in c++.</p>

<p>in python would be something like this</p>

<pre><code>...  
cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]
cv2.drawContours(frame, cnts, 0, (127, 255, 0), 3)
(x,y),radius = cv2.minEnclosingCircle(cnts[0])
center = (int(x),int(y))
radius = int(radius)
cv2.circle(frame, center, radius, (255, 0, 0), 3)
...
</code></pre>

<p>in this case, center will be the position of your object. This code only takes in account the first contour in the array...</p>
",888688.0,1.0,2.0,,
4559,75794882,Dynamically configure my simulation (matlab-simulink),|matlab|simulation|simulink|robotics|,"<p>I want to create 3 different controllers for the same system. I want to simulate them all in the same environment, and run each simulation with a different controller, or with 2 of the controllers and change mid simulation.</p>
<p>What i want to achieve is the following (without necessarily the switch case block), where some logic would replace the constant block:</p>
<p><a href=""https://i.stack.imgur.com/cs8FW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cs8FW.png"" alt=""enter image description here"" /></a></p>
<p>I tried using a <a href=""https://www.mathworks.com/help/simulink/slref/switchcase.html"" rel=""nofollow noreferrer"">switch case block</a>, however, my simulation fails because the sample time of the switch case block is different from the sample times in other components (derivatives for continuous PID, discrete PID, filters for continuous PID).</p>
<p>How can configure my simulation dynamically to use different controllers ( differenct blocks)?</p>
",3/20/2023 20:09,75802149.0,145,1,0,0,,17330440.0,,11/4/2021 17:58,16.0,75802149.0,"<p>What i ended up doing is the following:</p>
<p><strong>1:</strong> I used a <a href=""https://www.mathworks.com/help/simulink/slref/multiportswitch.html"" rel=""nofollow noreferrer"">multiport switch</a>. The switch doesn't rely on sampling time, so there are no conflicts between PID sampling times and controller switching.</p>
<p><strong>2:</strong> I have a timeseries signal to activate the controller i want. The multiport switch accepts a integer signal, and it fails otherwise. To achieve that, the timeseries signal is constructed with the following method:</p>
<pre><code>Select_Controller = [0, 2;Tchange - Tsampling 2;Tchange 1  ];;
</code></pre>
<p>and then this signal is passed from a ZOH block with sampling rate <code>Tsampling</code>. Thus the signal is always an integer value.</p>
<p>This is the simulink diagram:
<a href=""https://i.stack.imgur.com/Uaen2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Uaen2.png"" alt=""enter image description here"" /></a></p>
<p>which results on those signals:
<a href=""https://i.stack.imgur.com/fKVhx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fKVhx.png"" alt=""enter image description here"" /></a></p>
<p>The command was:</p>
<pre><code>Select_Controller = [0, 2;(3 - 1e-3) 2;3 1  ];;
</code></pre>
",17330440.0,0.0,0.0,,
4502,75367988,"Java error ""OpenCV Assertion failed: (-215:Assertion failed) npoints >= 0 && (depth == CV_32F || depth == CV_32S)""",|java|opencv|image-processing|contour|robotics|,"<p>When I try to run this openCV code on an FTC robot:</p>
<pre><code>        List&lt;MatOfPoint&gt; contours = new ArrayList&lt;&gt;();
        Imgproc.findContours(thresholded, contours, new Mat(), Imgproc.RETR_EXTERNAL, Imgproc.CHAIN_APPROX_SIMPLE);
        contoursAttr = contours;

        MatOfPoint biggestContour = new MatOfPoint();

        for (MatOfPoint curContour : contours) {
            if (Imgproc.contourArea(curContour) &gt; Imgproc.contourArea(biggestContour)) {
                biggestContour = curContour;
            }
        }
</code></pre>
<p>I get the error in the title, namely &quot;OpenCV Assertion failed: (-215:Assertion failed) npoints &gt;= 0 &amp;&amp; (depth == CV_32F || depth == CV_32S)&quot;. I've seen other answers for this question, but they almost all talk about signatures changing across versions, and are somewhat python-specific. However, given that <code>contours</code> is a <code>List&lt;MatOfPoint&gt;</code>, I feel like this should work??</p>
<p>Thanks.</p>
",2/7/2023 0:54,75371933.0,61,1,0,1,,12240158.0,"Los Angeles, CA, USA",10/18/2019 18:37,9.0,75371933.0,"<p>In the first iteration of the loop, <code>biggestContour</code> is created using default constructor, and has the wrong OpenCV depth.</p>
<p>The following code creates <code>MatOfPoint</code> object with default constructor:</p>
<pre><code>MatOfPoint biggestContour = new MatOfPoint();
</code></pre>
<p>In the first iteration, the following code computes <code>contourArea</code> over the &quot;new&quot; <code>MatOfPoint</code>:</p>
<pre><code>if (Imgproc.contourArea(curContour) &gt; Imgproc.contourArea(biggestContour)
</code></pre>
<p>The result is the error &quot;OpenCV Assertion failed... &amp;&amp; (depth == CV_32F || depth == CV_32S)&quot;<br />
The reason for getting the specific assertion is because the depth of the &quot;new&quot; <code>MatOfPoint</code> is <code>CV_8UC1</code> (not <code>CV_32F</code> and not <code>CV_32S</code>).</p>
<hr />
<p>Suggested solution:<br />
Initialize <code>biggestContour</code> to the first contour in the list:</p>
<pre><code>MatOfPoint biggestContour = contours.get(0); //new MatOfPoint();

for (MatOfPoint curContour : contours) {
    if (Imgproc.contourArea(curContour) &gt; Imgproc.contourArea(biggestContour)) {
        biggestContour = curContour;
    }
}        
</code></pre>
<hr />
<p>It is also a good practice to verify that <code>contours</code> is not empty:</p>
<pre><code>if (!contours.isEmpty())
{
    MatOfPoint biggestContour = contours.get(0);//new MatOfPoint();
    
    for (MatOfPoint curContour : contours) {
        if (Imgproc.contourArea(curContour) &gt; Imgproc.contourArea(biggestContour)) {
            biggestContour = curContour;
        }
    }
}
</code></pre>
",4926757.0,1.0,0.0,,
1732,16664330,Balancing robot PID tuning,|c++|arduino|robotics|pid-controller|,"<p>I'm trying to build a two-wheeled balancing robot for fun. I have all of the hardware built and put together, and I think I have it coded as well. I'm using an IMU with gyro and accelerometers to find my tilt angle with a complimentary filter for smoothing the signal. The input signal from the IMU seems pretty smooth, as in less than 0.7 variance + or - the actual tilt angle.</p>

<p>My IMU sampling rate is 50&nbsp;Hz and I do a <a href=""http://en.wikipedia.org/wiki/PID_controller"" rel=""nofollow noreferrer"">PID</a> calculation at 50&nbsp;Hz too, which I think should be fast enough.</p>

<p>Basically, I'm using the PID library found at <em><a href=""http://playground.arduino.cc//Code/PIDLibrary"" rel=""nofollow noreferrer"">PID Library </a></em>.</p>

<p>When I set the P value to something low then the wheels go in the right direction.</p>

<p>When I set the P value to something large then I get an output like the graph.</p>

<p><img src=""https://i.stack.imgur.com/mjJ5J.jpg"" alt=""Enter image description here""></p>
",5/21/2013 7:22,,3763,4,5,3,0.0,1268168.0,"Menlo Park, CA, USA",3/14/2012 5:48,245.0,16712118.0,"<p>If you haven't already, I'd suggest you do a search on the terms <a href=""https://www.google.com/search?q=arduino%20PID"" rel=""nofollow"">Arduino PID</a> (obvious suggestion but lots of people have been down this road). I remember when that PID library was being written, the author posted quite a bit with tutorials, etc. (<a href=""http://brettbeauregard.com/blog/2011/04/improving-the-beginners-pid-introduction/"" rel=""nofollow"">example</a>). Also I came across this <a href=""http://playground.arduino.cc/Code/PIDAutotuneLibrary"" rel=""nofollow"">PIDAutotuneLibrary</a>.</p>

<p>I wrote my own PID routines but also had a heck of a time tuning and never got it quite right.</p>
",840992.0,0.0,0.0,23992677.0,"Steven, I find your project fascinating! I assume IMU is Inertial Measurement Unit? @JoachimPileborg above is correct, lots of code and hard to deduce logic without full knowledge about your hardware. I see you are starting with PID = 0,0,0. My suspicion these values should be different. When P is high you say the wheels go in the wrong direction, and that sounds like your software logic is faulty. I suggest you describe the project on a web site if possible then post a link here. I for one am interested. Good luck."
4629,76714383,Mir 100 pause and continue via rest api,|python-3.x|rest|robotics|,"<p>I've been working with mir 100 and using Python to make rest API requests and now I'm trying to pause and unpause the robot via rest API, I'm not working in mir fleet, just one robot, I try to use the action post to make the request to pause the robot via Postman to make some tests, the robot status code was 201 (success) but the robot doesn't stops, someone know how I should make the request correctly?</p>
<p>This is the JSON that I am using:</p>
<pre><code>{
&quot;allowed_methods&quot;:null,
  &quot;descriptions&quot;: [
    null
  ],
  &quot;help&quot;: &quot;string&quot;,
  &quot;parameters&quot;: [
    &quot;pause&quot;
  ],
  &quot;mission_group_id&quot;: &quot;2a4c6731-1fd4-11ee-b4c4-94c691a733c6&quot;,
  &quot;name&quot;: &quot;pause&quot;,
  &quot;action_type&quot;: &quot;pause&quot;,
  &quot;description&quot;: &quot;string&quot;
}
</code></pre>
<p>And this is the request url: http://ip/api/v2.0.0/actions/pause</p>
",7/18/2023 15:28,,192,1,0,0,,,,,,77239306.0,"<p>I've also been trying to play/pause the mission via RestAPI.</p>
<p>I'm unable to pause the robot from /actions/pause API call.</p>
<p>I can do it from the Put call to /status. The API documentation lets you &quot;try it out&quot; and outlines the parameters for the body to send. You can omit whatever you don't need. I determined the values by playing/pausing the robot and execute the get/status call from the robot's API documentation webpage. Get to it via the robot interface at help/API documentation, then log in and launch as the user for the calls.</p>
<p>The body parameters are:</p>
<pre><code>{
 &quot;map_id&quot;: &quot;string&quot;,
  &quot;mode_id&quot;: 0,
  &quot;state_id&quot;: 0,
  &quot;web_session_id&quot;: &quot;string&quot;,
  &quot;position&quot;: {},
  &quot;serial_number&quot;: &quot;string&quot;,
  &quot;name&quot;: &quot;string&quot;,
  &quot;answer&quot;: &quot;string&quot;,
  &quot;guid&quot;: &quot;string&quot;,
  &quot;datetime&quot;: &quot;2023-10-05T17:12:08.182Z&quot;,
  &quot;clear_error&quot;: true
}
</code></pre>
<p>You can just send the state id, no comma at the end:</p>
<pre><code>{
  &quot;state_id&quot;: 0
}
</code></pre>
<p>Play is state 3 and pause is 4.</p>
",22690906.0,0.0,0.0,,
4123,69498883,How to locate a urdf file using parser.AddModelFromFile(full_name); in Drake,|c++|robotics|drake|,"<pre><code>#include &quot;drake/geometry/scene_graph.h&quot;
#include &quot;drake/multibody/parsing/parser.h&quot;
#include &quot;drake/common/find_resource.h&quot;

int main(void) {
// Building a floating-base plant
    drake::multibody::MultibodyPlant&lt;double&gt; plant_{0.0};
    drake::geometry::SceneGraph&lt;double&gt; scene_graph;
    std::string full_name = drake::FindResourceOrThrow(
        &quot;model/model.urdf&quot;);
    drake::multibody::Parser parser(&amp;plant_, &amp;scene_graph);
    parser.AddModelFromFile(full_name);
    plant_.Finalize();
    return 1;
}
</code></pre>
<p>The above code give me the following error:</p>
<pre><code>terminate called after throwing an instance of 'std::runtime_error'
  what():  Drake resource_path 'model/model.urdf' does not start with drake/.
Aborted (core dumped)
</code></pre>
<p>My Directory layout is:</p>
<ul>
<li>Project Folder
<ul>
<li>model
<ul>
<li>urdf</li>
</ul>
</li>
<li>src
<ul>
<li>main.cpp</li>
<li>CmakeLists.txt</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>my src CmakeLists.txt is:</p>
<pre><code>cmake_minimum_required(VERSION 3.16.3)

list(APPEND CMAKE_MODULE_PATH &quot;${CMAKE_SOURCE_DIR}&quot;)
message(&quot;CMAKE Directory : &quot; ${PROJECT_SOURCE_DIR})

project(IK VERSION 1.0)

# specify the C++ standard
set(CMAKE_CXX_STANDARD 11)
set(CMAKE_CXX_STANDARD_REQUIRED True)

find_package (Eigen3 3.3 REQUIRED NO_MODULE)
find_package(drake CONFIG REQUIRED)

# specify the main source file
set(SOURCE main.cpp)
add_executable(${PROJECT_NAME} ${SOURCE})

target_link_libraries(&quot;${PROJECT_NAME}&quot; 
    Eigen3::Eigen
    drake::drake}
</code></pre>
<p>Essentially, my objective is to use a custom urdf and build a model and use it for the inverse kinematics class.</p>
",10/8/2021 16:15,69499161.0,176,1,0,0,,15412613.0,,3/17/2021 5:33,10.0,69499161.0,"<p>I think you have a very easy problem to resolve. You're currently using <code>drake::FindResourceOrThrow()</code>. If you look at <a href=""https://drake.mit.edu/doxygen_cxx/namespacedrake.html#a535bb41fc091d96d7de06b87e68c9afb"" rel=""nofollow noreferrer"">its documentation</a>, you'll note:</p>
<blockquote>
<p>The resource_path refers to the relative path within the Drake source repository, prepended with drake/.</p>
</blockquote>
<p>You have a urdf in an arbitrary location. In that case, just pass a filesystem path without calling <code>FindResource()</code> (or any of its variants). Whether it is an absolute or relative path will depend on where your executable ends up w.r.t. the urdf in your built/installed system.</p>
",7686256.0,1.0,1.0,,
3527,57940967,"In Peter Corke‘s Robot Book, the same Function `rpy2r()` yields different results?",|matlab|robotics|,"<p><img src=""https://i.stack.imgur.com/hdCFF.png"" alt=""enter image description here""></p>

<p>but matlab result:</p>

<pre><code>&gt;&gt; R2=rpy2r(0.1,0.2,0.3)

R2 =

    0.9363   -0.2751    0.2184
    0.2896    0.9564   -0.0370
   -0.1987    0.0978    0.9752

&gt;&gt; [theta,v]=tr2angvec(R)

theta =

    0.3655


v =

    0.1886    0.5834    0.7900

&gt;&gt; [v,lambda]=eig(R)

v =

   0.6944 + 0.0000i   0.6944 + 0.0000i   0.1886 + 0.0000i
  -0.0792 - 0.5688i  -0.0792 + 0.5688i   0.5834 + 0.0000i
  -0.1073 + 0.4200i  -0.1073 - 0.4200i   0.7900 + 0.0000i


lambda =

   0.9339 + 0.3574i   0.0000 + 0.0000i   0.0000 + 0.0000i
   0.0000 + 0.0000i   0.9339 - 0.3574i   0.0000 + 0.0000i
   0.0000 + 0.0000i   0.0000 + 0.0000i   1.0000 + 0.0000i
</code></pre>

<p>As the pic shows, the same function gives different outputs, has any one else found the same issue?</p>
",9/15/2019 3:41,,179,1,0,0,,12056753.0,,9/12/2019 6:45,2.0,57948106.0,"<p>I can't read Chinese, but apparently the function <code>tr2angvec</code>only returns one rotation angle and its corresponding vector. Matlab's <code>eig</code> returns the full matrix of eigenvectors and a diagonal matrix with eigenvalues. </p>

<p>Because of this, the last column given by <code>eig</code> is the vector provided by <code>tr2angvec</code>.  Note that <code>theta=0.3655</code> while the real eigenvalue is one (because rotations do not change length). Then notice that the real part of the imaginary eigenvalues is <code>0.9339</code>, and that <code>acos(0.9339)=0.365626358 rad</code>. </p>

<p>Then recall that the imaginary eigenvalues of a rotation matrix are <code>cos(theta) + i sin(theta)</code> and <code>cos(theta) - i sin(theta)</code>.</p>
",3007075.0,0.0,1.0,,
2003,24719506,Mindstorms NXT Programming opposite facing ultrasonic and light sensors,|robotics|lego-mindstorms|nxt|,"<p>I'm not usually one to ask for help, but after weeks of trying to get this to work, I'm reaching out for help as a ditch attempt.</p>

<p>I've been having trouble with programming my LEGO Sumo bot. Because of this, I started to learn the program more and more. I've still had this one problem though. How would I program my sumo bot to utilize opposite facing ultrasonic sensors? The closest I've gotten was having two normal 'spin seek destroy back up' loops running in parallel with another loop of the same idea only with opposite directions and the other sensor ports. </p>

<p>The problem with that is the robot seems to want to do each seek and destroy loop in a pattern. Front, back, front, back, and so on. This presents problems and negates the entire purpose of having both sensors. The other problem is when the back ultrasonic sensor is triggered first, the robot wants to spin in circles to seek and move the direction of the back ultrasonic at the same time. So it will jump backwards and turn back and forth in a stuttering motion.</p>

<p>My hopes is to have the robot spin and move toward an object that the ultrasonic sensor sees. Regardless of which ultrasonic sensor is triggered. After the target is seen the robot will move until the light sensor sees white, and move in the opposite direction. </p>

<p>I can provide more information if necessary.</p>

<p>I hope I'm okay in asking this here, it really is my last effort.</p>

<p>This is the full code:
<img src=""https://i.stack.imgur.com/sC1D8.png"" alt=""full code""></p>

<p>This is the code inside each loop:
<img src=""https://i.stack.imgur.com/XYSCr.png"" alt=""inside each loop""></p>
",7/13/2014 4:16,,3052,1,6,0,,3833554.0,,7/13/2014 3:57,10.0,30516761.0,"<p>Check both sensors in a loop and use the output of both sensors to decide which action to take.</p>
",2250588.0,0.0,0.0,38341231.0,"It sounds like you need to check both sensors in one ""spin seek destroy back up"" loop."
3660,60640639,Minibrick8 doesnt connect to computer via RS-232 serial cable when there is a Snap-action-switch (microswitch) attatched,|robotics|dmx512|,"<p>I am using a MiniBrick8 (from Gilderfluke &amp; Company in Burbank, California), as part of a lighting/ projector rig. The MiniBrick8 needs an input from a button (using a switch) to start a program running (PCMacs). At the moment the MiniBrick8 is also connected to a BrightSign Media Player.</p>

<p>Minibrick8 doesnt connect to computer via RS-232 serial cable when there is a Snap-action-switch (microswitch) attatched and drawing power from the '9-24 VDC' screw output.</p>

<p>The switch has two output parts: NC, NO, and a power input (COM). (NC and NO being normally closed and normally open respectively).</p>

<p>The NC is connected to ground, so when the switch is not pressed down power is routed directly from the VDC through the switch into ground. Then when the button is pressed power is instead routed from the switch into the MiniBrick8's A-input.</p>

<p>I then take the A-inputs return cable and ground it.</p>

<p>Maybe the fact that while the switch is not pressed down current is constantly flowing through the switch into ground? Is this a problem?</p>

<p>Thank you for your help.</p>
",3/11/2020 16:32,60726352.0,22,1,0,0,,8564483.0,"Dorset, United Kingdom",9/5/2017 15:28,8.0,60726352.0,"<p>SOLVED:</p>

<p>It was perhaps as I thought.</p>

<p>I changed the switch to get a power input into the NO (normally open) connector, leaving the NC (normally closed) connector without any cable attached, connecting the output via the COM connector to one of the A-trigger/input terminals (the other going to ground).</p>

<p>I had previously thought (incorrectly obviously) that all the switch's connectors had to have wires connecting them to something.</p>

<p>Here is a source: <a href=""https://youtu.be/pf_Mngbx32w"" rel=""nofollow noreferrer"">YouTube Video: https://youtu.be/pf_Mngbx32w</a></p>

<p>^^ This helped me figure it out.</p>
",8564483.0,1.0,0.0,,
3588,58836477,Robot won't connect to a new connection on our SQL server to AWS,|sql|automation|robotics|rpa|g1ant|,"<p>I’ve been connecting the G1ANT robot to SQL server and it works fine.
However now I've created a new connection on our SQL server to AWS but it's not connecting to it.</p>

<p>This connection string works fine:</p>

<p><code>Server=GARDSQLDEV01;Database=ColumbusMk1;Trusted_Connection=True;</code></p>

<p>This connection string does not work:</p>

<p><code>Server=columbus-dev.csalh0f00gat.eu-west-2.rds.amazonaws.com,1433;Database=ColumbusMk1; User ID=sa; Password=***;   Trusted_Connection=True;</code></p>

<p>Replaced the password with *** for security reasons.</p>

<p>Please help.</p>
",11/13/2019 11:49,58836538.0,42,1,0,1,,11697489.0,,6/25/2019 12:00,7.0,58836538.0,"<p>Use special character ‴long text‴ for long connection strings with spaces. It should work fine:</p>

<pre><code>database ‴Server=columbus-dev.csalh0f00gat.eu-west-2.rds.amazonaws.com,
1433;Database=ColumbusMk1; User ID=sa; Password=********;   Trusted_Connection=True;‴
</code></pre>

<p>Good luck! </p>
",11261776.0,0.0,0.0,,
1327,9533311,Referencing objects through a tcp/ip connection - for robotics,|c#|tcp|robotics|,"<p>I've begun to design a framework for a robot I'm going to build. The control-software is - for reasons of later portability and also for the challenge herein - an attempt at mimicking the human system (of course drastically simplified). Thus the framework has a nervous system, consisting of a brain and spinal cord, the latter through which the brain controls the sensors and 'limbs' of the peripheral nervous system - i.e. the robots camera, microphones, motorcontrols and so on.</p>

<p>My challenge is in that I can't figure out how to send commands from the brain - via the spinal cord - to the sensor objects, initialized by the peripheral nervous system controller. In my implementation, the spinal cord is a tcp/ip server, accepting connections from the sensors and sending them up to the brain. How would something like this be accomplished? The brain has an awareness of the sensory objects, but it shouldn't be able to instantiate them - this is only for the peripheral nervous system. So how to call functions on those sensor objects, from the brain via the spinal cord, to the peripheral nervous system and finally to the sensor in question? </p>
",3/2/2012 12:29,9533516.0,238,3,0,0,,1208500.0,,2/14/2012 7:04,340.0,9533500.0,"<p>In Java you could do something like this with RMI. In C# please take a look at CORBA.</p>

<p>Doing things like this with a tcp/ip-server (I guess you want to open a socket and then parse command through it) is pain in the neck. If you want to do this else, you have to define Commands which are sended, received, parsed and the right method is called.</p>

<p>If you are able to set up sth. like a IIS you could probably write a Webservice in C#. The Actors would consume the Methods of the Webservice and ""remotely"" call them.</p>
",619619.0,0.0,1.0,,
4281,71697482,Incorrect distance travelled using optical wheel encoder,|python|raspberry-pi|raspbian|robotics|encoder|,"<p>I started using a raspberry pi 3b+ about 2 months ago, so I'm fairly new. I'm working on a project, in which I have assembled a 4 wheeled small car, it has 4 DC motors, a 3b+, an L289D H bridge and an optical wheel encoder.</p>
<p>I'm in my initial stages of the project where I am testing the kit, whether it travels the correct distance or not. Below is the code that I'm using to test the process:</p>
<pre><code>import RPi.GPIO as GPIO
import time
from time import sleep

GPIO.setmode(GPIO.BOARD)
GPIO.setup(11, GPIO.IN, pull_up_down=GPIO.PUD_UP)

GPIO.setup(13, GPIO.OUT)   #clockwise left
GPIO.setup(15, GPIO.OUT)   #anticlockwise right
GPIO.setup(16, GPIO.OUT)   #anticlockwise left
GPIO.setup(18, GPIO.OUT)   #clockwise right

stateLast = GPIO.input(11)
rotationCount=0
stateCount=0
stateCountTotal=0
flag=0

circ=62.4*3.14 #mm
statesPerRotation=20
distancePerStep= circ/statesPerRotation

GPIO.output(13, GPIO.HIGH)
GPIO.output(18, GPIO.HIGH)
flag=&quot;true&quot;
while flag==&quot;true&quot;:
    stateCurrent = GPIO.input(11)
    if stateCurrent != stateLast:
        stateLast = stateCurrent
        stateCount += 1
        stateCountTotal += 1
        
    if stateCount == statesPerRotation:
        rotationCount += 1
        stateCount = 0
        
    distance = distancePerStep * stateCountTotal
    print(&quot;Distance&quot;,distance)
    if distance &gt; 50:
        flag = &quot;false&quot;

GPIO.output(13, GPIO.LOW)
GPIO.output(18, GPIO.LOW)
</code></pre>
<p>What's happening is that upon running the code, the kit is not travelling the distance I enter in the code above. When I use lower values of distance, the kit travels accurately, but as soon as I increase the distance to greater than 200mm for example, the kit falls short of the actual distance it is supposed to travel. For 300mm, it stops at around 250mm on ground, but the interesting thing is that upon printing the distance from the code, it runs till 300mm. I am failing to understand that why is the code showing that the kit has travelled 300mm but in reality it always stops at around 220mm-250mm.</p>
<p>Another interesting thing I've noticed is that when I use Thonny to run the code, the distance travelled is always inconsistent, no matter the value of distance. For example, if I use distance 20mm, sometimes it ran 10mm, at other moments 30mm, and never accurate. When I ran the code.py file from the terminal, the distance it travelled became consistent, but still inaccurate as stated in the above paragraph.</p>
<p>I'm using remote desktop connection to access the 3b+. I have used another wheel encoder to ensure there's no problem with it and have also checked the battery voltage, which is good. Other than that I have tested the kit on different surfaces as well, but nothing has worked till now.</p>
<p>I would appreciate if anyone helps me out with this.</p>
<p>Thanks!</p>
",3/31/2022 18:27,,132,1,8,0,,18646904.0,,3/31/2022 18:15,1.0,71697907.0,"<pre><code># real-time processing requires fast non blocking code. Try the following 
# and see if the performance improves.
import RPi.GPIO as GPIO
import time
from time import sleep

GPIO.setmode(GPIO.BOARD)
GPIO.setup(11, GPIO.IN, pull_up_down=GPIO.PUD_UP)

GPIO.setup(13, GPIO.OUT)  # clockwise left
GPIO.setup(15, GPIO.OUT)  # anticlockwise right
GPIO.setup(16, GPIO.OUT)  # anticlockwise left
GPIO.setup(18, GPIO.OUT)  # clockwise right

stateLast = GPIO.input(11)
rotationCount = 0
stateCount = 0
stateCountTotal = 0
flag = 0

circ = 62.4 * 3.14  # mm
statesPerRotation = 20
distancePerStep = circ / statesPerRotation

GPIO.output(13, GPIO.HIGH)
GPIO.output(18, GPIO.HIGH)
# flag = &quot;true&quot;

while True: # Use boolean logic not string evaluation
    stateCurrent = GPIO.input(11)
    if stateCurrent != stateLast:
        stateLast = stateCurrent
        stateCount += 1
        stateCountTotal += 1

    if stateCount == statesPerRotation:
        rotationCount += 1
        stateCount = 0

    distance = distancePerStep * stateCountTotal
    # print(&quot;Distance&quot;, distance)  # Do not print in th e loop
    if distance &gt; 50:
        break
print(&quot;Distance&quot;, distance)
GPIO.output(13, GPIO.LOW)
GPIO.output(18, GPIO.LOW)
</code></pre>
",7803702.0,0.0,0.0,126710961.0,"Yes, you need to understand what the actual frequency of the optical encoder is. You should match the sampling by your code to slightly greater than twice that frequency."
2946,47127628,Is Parse in JavaScript possible?,|javascript|robotics|,"<p>I have tried searching and haven't been able to see anything that can parse in JavaScript.</p>

<p>It would only have to parse a .txt (tab delimited) file with the following:</p>

<p>L123____Donald____Duck____247.09</p>

<p>S234____Mickey____Mouse___356.09</p>

<p>F456____Daffy_____Duck____1650.36</p>

<p>N876____Minnie____Mouse___60.45</p>

<p>It would have to pull out the number, first initial of first name, first initial of second name and then the number at the end (concentration). The first 3 L123, D and D could all be given a var, but the concentration number would have to get a separate one as I have to do some funky calculations and the assign it function in a liquid handling robot.</p>

<p>Any help on this gratefully received, even if it's only partial as I like to try and work things out ( sometimes ;-) )</p>

<p>I use this to test things at the moment because I don't know what else to use: <a href=""https://www.w3schools.com/js/"" rel=""nofollow noreferrer"">https://www.w3schools.com/js/</a></p>

<p>Thank you</p>
",11/5/2017 23:09,,48,1,4,0,,5008663.0,,6/14/2015 15:48,9.0,47127643.0,"<pre><code>var input = /* your data */;
var table = input.split(""\n"").map(line =&gt; line.split(""\t"")); // here's your table
</code></pre>
",283863.0,2.0,0.0,81205433.0,"Yep, parsing usually involves regular expressions"
4180,70290926,How do wifi bulbs pair with the mobile,|wifi|robotics|atmega|smartphone|smart-device|,"<p>I have been working on making a commercial-equivalent smart WiFi bulb. I cannot understand how the bulb pairs with the app on smartphone and gains access to the Internet through my personal modem. The bulb cannot communicate through the personal WiFi before getting paired, as it doesn't have the credentials. I tried to experiment with my existing commercial bulb and while putting it in reset mode, noticed -</p>
<ol>
<li>There wasn't any WiFi (that could be the bulb's AP) visible in the available networks list of my PC. Well, I don't know if it was a hidden network.</li>
<li>There wasn't any Bluetooth device visible in the list that could possibly be the bulb.</li>
</ol>
<p>So I want to know how the bulb communicates with the app -</p>
<ol>
<li>Does it act as AP (Access Point) and the app connects to it and gives it the credentials.</li>
<li>Else, does the bulb use any other type of communication to pair with the app.</li>
<li>Or, does it somehow just get to the smartphone and it all &quot;mysteriously&quot; works.</li>
</ol>
<p>I would be happy if someone could explain the process to me. Also, the app scans for the available devices whenever I want to pair to a new device. Then, I can select the desired device and pair it. So, please also explain (if possible) how the scanning works.</p>
<blockquote>
<p>No need to get to each technical detail, I just want to know the process. I will ask later if I need the tech details.</p>
</blockquote>
",12/9/2021 13:40,,52,0,3,0,,17635924.0,,12/9/2021 12:53,2.0,,,,,,129267324.0,You should read this article → [How do I ask a good question?](https://stackoverflow.com/help/how-to-ask)
1457,12042285,Problems making Carmen Robotics toolkit in Fedora,|makefile|fedora|robotics|carmen|,"<p>I'm running into issues installing the Carmen Robotics toolkit in Fedora;</p>

<h2>Making:</h2>

<p>When I type <code>make</code>, I get the following error message</p>

<pre><code>---- Copying global/carmen-std.ini to carmen.ini 

   ***********
   E X P O R T
   ***********

---- Copying ipc.h to [path]/carmen-0.7.4-beta/include/carmen
</code></pre>

<p>... many similar lines  </p>

<pre><code>---- Copying param_interface.h to [path]/carmen-0.7.4-beta/include/carmen
Makefile:7: *** missing separator.  Stop.
make: *** [export] Error 255
</code></pre>

<p>I've googled around and saw that this can be caused by spaces instead of tabs at the beginnings of lines. There is no such issue anywhere near line 7 of the makefile.
Doing make -d gives a lot of output, which ends with:</p>

<pre><code>Updating goal targets.... Considering target file `export'.  
File `export' does not exist.  
Finished prerequisites of target file `export'.  
Must remake target `export'.  
Invoking recipe from ../Makefile.rules:285 to update target `export'.  
Putting child 0x174b8e0 (export) PID 5816 on the chain.  
Live child 0x174b8e0 (export) PID 5816  
Reaping winning child 0x174b8e0 PID 5816  
Live child 0x174b8e0 (export) PID 5819  
Reaping winning child 0x174b8e0 PID 5819  
Removing child 0x174b8e0 PID 5819 from chain.  
Successfully remade target file `export'.  
GNU Make 3.82 Built for x86_64-redhat-linux-gnu Copyright (C) 2010  
Free Software Foundation, Inc. License GPLv3+:  
GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;  
This is free software: you are free to change and redistribute it.  
There is NO WARRANTY, to the extent permitted by law.  
Reading makefiles...  
Reading makefile `Makefile'...   
Reading makefile `../Makefile.conf' (search path) (no ~ expansion)...  
Reading makefile `../Makefile.vars' (search path) (no ~ expansion)...  
Makefile:7: *** missing separator.  Stop.  
Reaping losing child 0xda4940 PID 5794   
make: *** [export] Error 255  
Removing child 0xda4940 PID 5794 from chain.  
</code></pre>

<p>I've heard that getting to Carmen to compile can be a terrible experience, but I didn't expect that it would give me <em>this</em> much trouble, especially since I'd done it successfully on another computer in the past.</p>

<p><em><strong>I can't even make clean</em></strong></p>

<p>Does anyone have sage wisdom to offer on this topic?</p>
",8/20/2012 17:33,12044070.0,195,1,1,0,,1530111.0,"Medford, MA",7/16/2012 21:27,90.0,12044070.0,"<p>I downgraded from Make 3.82 to 3.81 and this issue went away.</p>
",1530111.0,0.0,0.0,16078848.0,"Solved one problem
> Searching for linux kernel headers... not found
The problem here was that 
a) --headers didn't seem to be working.
I edited the config file to point to /usr/src/kernels/$DISTRO/include/"
3004,48455762,How to code EEPROM with potentiometer in Arduino,|c++|arduino|robotics|eeprom|servo|,"<p>I have A code I got from a site. I've been trying to add EEPROM in that code but I can't get it right. I tried adding EEPROM.write and EEPROm.read I also add another servo</p>

<p>Can someone help me with it, <strong>Adding EEPROM in the code?</strong> Thank you in advance :) </p>

<p>here's the code (Simple Robotic arm with play and record function):</p>

<pre><code>#include &lt;Servo.h&gt;
#include &lt;EEPROM.h&gt;

const int PinButton1 = 2;  // pin 2   
const int PinButton2 = 3;  // pin 3
int mode = 1;     // case 1 program robot arm. case 2 replay positions 
int bounce = 0;
volatile int buttonPress = 0;
unsigned long lastButtonPressTime = 0;
volatile unsigned long bounceTime = 0;
Servo Arm0Servo;      //servo objects
Servo Arm1Servo;      // Ihave renumbered servos 26 1 2017
Servo Arm2Servo;
Servo Arm3Servo;
Servo Arm4Servo;
int pos1,pos2,pos3,pos4,pos0;
int PosArm[5] ;       // array 
int ArmPos[100][5];   // array to hold arm positions up to 100
int PosCount = 0;     //  to count number of positions increased when button     pressed
int PosCountMax = 0;  //  number of positions recorded when double button push initiates replay mode
int PosReached = 0;   // used to check that arm has moved from one recorded position to next position
int addr = 0;
boolean recorded = false;

void setup() {  
     for(int i = 0; i &lt;100 ; i++ ){
     for(int p = 0; p &lt;5 ; p++ ){
         ArmPos[i][p] = -1;
     }
 }
pinMode(PinButton1 , OUTPUT);
digitalWrite(PinButton1 ,LOW);
pinMode(PinButton2, INPUT_PULLUP);
//  I have made a small change here due to problem using some Arduinos
//attachInterrupt( digitalPinToInterrupt(PinButton2),ButtonPress , LOW );
// digitalPinToInterrupt(PinButton2) may not be defined!
attachInterrupt( 1,ButtonPress , LOW );   // interupt to capture button presses

// attach servos to relervent pins on arduino nano
Arm0Servo.attach(12); // grip    90 to 180 open    limits of servo movement
Arm1Servo.attach(11); // elbow      to 130 up
Arm2Servo.attach(10); // shoulder   10 to 50 down
Arm3Servo.attach(9);  // turn    0  to 180 right 
Arm4Servo.attach(8);  // turn    0  to 180 right
}

void loop() {
  switch(mode){
    case 1 :  //  program robot arm. 1 press to remember position. 2 presses to progress next case 2 replay mode
              // analogRead(pin) that reads poteniometers on training arm
    PosArm[0] = map(analogRead(0),480,1024,180,10); // map (480,1024 value from potentiometer to 180,90 value sent to servo)
    EEPROM.write(addr, PosArm[0]);
    addr++;
    PosArm[1] = map(analogRead(1),480,1024,180,10);
    EEPROM.write(addr, PosArm[1]);
    addr++;
    PosArm[2] = map(analogRead(2),480,1024,180,10);
    EEPROM.write(addr, PosArm[2]);
    addr++;
    PosArm[3] = map(analogRead(3),480,1024,180,10);
    EEPROM.write(addr, PosArm[3]);
    addr++;
    PosArm[4] = map(analogRead(4),480,1024,180,10);
    EEPROM.write(addr, PosArm[4]);
             addr++;
    MoveArm();                          // call method  
    if(buttonPress == 1){               // flag set by interupt when button is pressed          
         buttonPress = 0;               // reset flag 
         if( millis() &gt; (lastButtonPressTime + 1000)){    // only one button press in one secound
             // record  position of arm PosArm to array[100][] of armpositions        
             ArmPos[PosCount][0] = PosArm[0];

             ArmPos[PosCount][1] = PosArm[1];

             ArmPos[PosCount][2] = PosArm[2];

             ArmPos[PosCount][3] = PosArm[3];

             ArmPos[PosCount][4] = PosArm[4];


             if (addr == 512) {
               EEPROM.write(addr, 255);
               break;
             }

             if( PosCount &lt; 100) {      // stop recording if over 100 positions recorded (memory limitations)
                 PosCount++;         
             }
         }else{                         //  more than one button press
             mode = 2;                  // go to next phase
             PosCountMax  = PosCount;   // set number of arm positions recorded
             PosCount = 0;              // reset count ready to read  arm position array from begining 
         }
    lastButtonPressTime = millis();              
    }
  break;
case 2 :        // read arm position array
    PosReached = 0;
    for(int i = 0; i &lt;5 ; i++ ){   //adjust servo positions   
        // we move the servos in small steps and the delay(20)  makes arm motion smooth and slow
        if( PosArm[i] &gt; ArmPos[PosCount][i]) PosArm[i] = PosArm[i] - 1;  // if actual position is greater than requird position reduce position by 1
        if( PosArm[i] &lt; ArmPos[PosCount][i]) PosArm[i] = PosArm[i] + 1;  // if actual position is less than required position value increase position valuue by 1
        if( PosArm[i] == ArmPos[PosCount][i]) PosReached++;              // check if servo  has reached required position/angle
    }

    if(PosReached == 5) PosCount++;        // if all 4 servos have reached position  then increase array index (PosCount)to next position in array
    if(PosCount == PosCountMax) PosCount = 0;   //  if end of array reached reset index to 0 and repeat.
    Play();   // physically move arm to updated position, this is broken into small steps
    delay(5);   // pause between moves so over all motion is slowed down
  break;
default :
  break;
  }    
}

void MoveArm() {   // write arm position data to servos
  for (int i = 0 ; i &lt; 5 ; i++) {
  if (PosArm[i] &lt; 5) PosArm[i] = 5;  // limit servo movement to prevent hitting end stops
  if (PosArm[i] &gt; 175) PosArm[i] = 175;  // servo.write limited 5 - 175  
  }
  Arm0Servo.write(PosArm[0]);
  Arm1Servo.write(PosArm[1]);
  Arm2Servo.write(PosArm[2]);
  Arm3Servo.write(PosArm[3]);
  Arm4Servo.write(PosArm[4]);
  Play();
  delay(5);
}
void Play() {   // write arm position data to servos
  for (int i = 0 ; i &lt; 5 ; i++) {
  if (PosArm[i] &lt; 5) PosArm[i] = 5;  // limit servo movement to prevent hitting end stops
  if (PosArm[i] &gt; 175) PosArm[i] = 175;  // servo.write limited 5 - 175  
  }

  if (addr == 0) {
  PosArm[0] = EEPROM.read(addr);
  pos0 = EEPROM.read(addr+5);
  addr++;

  PosArm[1] = EEPROM.read(addr);
  pos1 = EEPROM.read(addr+5);
  addr++;

  PosArm[2] = EEPROM.read(addr);
  pos2 = EEPROM.read(addr+5);
  addr++;

  PosArm[3] = EEPROM.read(addr);
  pos3 = EEPROM.read(addr+5);
  addr++;

  PosArm[4] = EEPROM.read(addr);
  pos4 = EEPROM.read(addr+5);
  addr++;

  }

  }
   void ButtonPress(){   //  interupt to capture button press
   if(micros() &gt; (bounceTime + 3000)){ // debounce timer 
    bounceTime = micros();          // ingnore interupts due to button bounce
    buttonPress = 1;     // flag for button pressed
}
</code></pre>

<p>}</p>
",1/26/2018 4:21,,702,1,6,-1,,7245083.0,,12/3/2016 12:46,23.0,48460095.0,"<p>The code that you posted will never attempt to read data from the EEPROM due to the following problem:</p>

<p>When your microcontroller is started, the variable <code>mode</code> is set to 1. If you look carefully at the <code>loop</code> function you can see that your code will enter the <code>case 1</code> branch at least once where it basically writes data to the EEPROM and - more importantly - will set <code>addr</code> to a value other than 0, due to the <code>addr++</code> instructions (unless you run the code so long that you encounter an integer overflow).</p>

<p>The only place where you attempt to read from the EEPROM is in the <code>Play</code> function. Due to the <code>if (addr == 0)</code> condition in this function you will never execute the <code>EEPROM.read</code> instructions as <code>addr</code> will not be 0 as explained above.</p>
",,1.0,1.0,83906103.0,done editing it.
3417,56883538,Simulink model 'to workspace' output,|matlab|simulink|robotics|pid-controller|,"<p>I am trying to control motor torque and am using a workspace variable in Simulink and want to output similar variable to workspace.</p>

<p>I have size(T_u)=[3, 91] whereas the output I am getting from the simulation has size [91, 90]</p>

<p>I am unable to understand why this is so.</p>

<p>Code that I am using:</p>

<pre><code>load('Motor_Param.mat')

t = 1:0.1:10;
T_o = [0.05*(10-t);0.04*(10-t);0.03*(10-t)];
T_d = zeros(size(T_o));
T_e = (T_d - T_o);
C_PD = pid(100,0,10,100);
T_u = zeros(size(T_e));
for k=1:size(T_e,1)
    T_u(k,:) = lsim(C_PD,T_e(k,:),t);
%T_u(1,:)= -45.0450000000000    -44.5444552724092   -44.0439110892737   -43.5433674500493   -43.0428243541925   -42.5422818011600   -42.0417397904094   -41.5411983213986   -41.0406573935862   -40.5401170064312   -40.0395771593933   -39.5390378519326   -39.0384990835098   -38.5379608535861   -38.0374231616233   -37.5368860070837   -37.0363493894301   -36.5358133081260   -36.0352777626353   -35.5347427524223   -35.0342082769522   -34.5336743356904   -34.0331409281029   -33.5326080536564   -33.0320757118181   -32.5315439020554   -32.0310126238368   -31.5304818766308   -31.0299516599067   -30.5294219731343   -30.0288928157839   -29.5283641873264   -29.0278360872332   -28.5273085149760   -28.0267814700274   -27.5262549518604   -27.0257289599483   -26.5252034937652   -26.0246785527857   -25.5241541364848   -25.0236302443380   -24.5231068758215   -24.0225840304120   -23.5220617075865   -23.0215399068228   -22.5210186275990   -22.0204978693939   -21.5199776316868   -21.0194579139572   -20.5189387156857   -20.0184200363529   -19.5179018754402   -19.0173842324294   -18.5168671068029   -18.0163504980435   -17.5158344056347   -17.0153188290603   -16.5148037678048   -16.0142892213531   -15.5137751891906   -15.0132616708034   -14.5127486656779   -14.0122361733011   -13.5117241931606   -13.0112127247442   -12.5107017675407   -12.0101913210389   -11.5096813847285   -11.0091719580996   -10.5086630406426   -10.0081546318487   -9.50764673120954   -9.00713933821711   -8.50663245236405   -8.00612607314350   -7.50562020004906   -7.00511483257487   -6.50460997021554   -6.00410561246623   -5.50360175882257   -5.00309840878072   -4.50259556183731   -4.00209321748951   -3.50159137523496   -3.00109003457184   -2.50058919499879   -2.00008885601498   -1.49958901712007   -0.999089677814209  -0.498590837598075  0.00190750402718064

    a = sim('Motor_Control','SimulationMode','normal');
    out = a.get('T_l')
end
</code></pre>

<p>Link to .mat and .slx files is: <a href=""https://drive.google.com/open?id=1kGeA4Cmt8mEeM3ku_C4NtXclVlHsssuw"" rel=""nofollow noreferrer"">https://drive.google.com/open?id=1kGeA4Cmt8mEeM3ku_C4NtXclVlHsssuw</a></p>
",7/4/2019 8:19,,2931,2,3,0,,11713385.0,,6/28/2019 11:48,15.0,56884632.0,"<p>If you set the <code>Save format</code> in the <code>To Workspace</code> block to <code>Timeseries</code> the output will have the dimensions of the signal times the number of timesteps.</p>

<p>In your case I activated the option <code>Display-&gt;Signals &amp; Ports-&gt;Signal dimensions</code> and the signal dimensions in your model look like this:</p>

<p><a href=""https://i.stack.imgur.com/byFbb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/byFbb.png"" alt=""Signal dimensions in Simulink model""></a></p>

<p>So the signal that you output to the workspace has the size <code>90</code>. Now if I print <code>size(out.Data)</code> I get </p>

<pre><code>ans = 138  90
</code></pre>

<p>where 90 is the signal dimension and 138 is the number of timesteps in your Simulink model.</p>

<p>You could now use the last row of the data (which has the length 90) and add it to your array.</p>
",3562088.0,0.0,1.0,100314942.0,"Hi @Capri as I don't have the Control System Toolbox installed, could you provide the arrays `C_PI`, `T_u` and `T_l` for `k=1`?"
1571,14002701,how to reprogram an old computer rom and use it as rom memory for another task?,|microcontroller|robotics|rom|,"<p>I've ripped open an old Pentium desktop. The main board is a Zida 5svx. I got to know from the manual (which i downloaded from the internet) the location of the ROM chip on the board, and took it out. It was mentioned in the manual that the chip was a Flash EEPROM.</p>

<p>Now, what I am interested in is this: Is there a way to erase the ROM and flash it with, say a C program to blink an LED (i know this might put you into a fit of laughter, but read on all the same), or control a motor?</p>

<p>I also want to know if I can construct a mega-sized micro-controller with the left-over Pentium, some MBs of RAM, and this ROM.</p>

<p>Any suggestions?</p>

<p>P.S: I know that such a uC will require appropriate power supply setup and things.</p>
",12/22/2012 12:08,,1293,1,5,2,,1592082.0,,8/11/2012 10:24,109.0,35949961.0,"<p>The key is in getting and deeply studying the manufacturer's datasheets for each device you remove and wish to reuse.  I am supposing that since you asked the question that you did that you are not a professional electrical engineer - that's OK, but you will need to do hours, days, or weeks of study to truly understand the datasheets well enough to successfully reuse your motherboard chips because they are written for professional engineers with years of experience, and unfortunately they were not written to be understood by hobbyists.  If you can succeed in acquiring and thoroughly understanding all of the datasheets (and the related user's guides as well for the more complex chips) then you have made it to the point where you might be able to start a custom design based on your recovered parts, on paper at least.  In order to test your design and insure that each part of it is working will require at least an oscilloscope and volt meter - and the knowledge of how to use them.  An understanding of basic electronics is essential, you will not succeed without it.  Very good soldering/rework/assembly skills will be required as well if you hope to have your design truly work - you can do everything else right and it can still fail if your skills in this area are lacking.  There is simply not enough time for me to advise you on everything you will need to know - but if you are motivated, dedicated, and you don't give up when setbacks and roadblocks occur (and trust me, they occur all too frequently for even the best engineers and best designs) - meaning that you are not easily frustrated when things don't work - then you have a chance at success.  I wish you all the best, and try to have fun while doing it (important in case fun is all you ever get out of your project).  :)</p>
",6045061.0,0.0,0.0,19397251.0,Thanks dwelch. I think that ought to open some doors...
4422,73980976,"Python Universal Robots UR-RTDE's API - Getting sporadic ""RTDEReceiveInterface Exception: End of file""",|python|api|exception|robotics|,"<p>Been using happily <a href=""https://pypi.org/project/ur-rtde/"" rel=""nofollow noreferrer"">ur-rtde</a> and lately started getting &quot;RTDEReceiveInterface Exception: End of file&quot; messages in different places of the code.</p>
<p>It seems it happens only when the robot is at rest.</p>
<p>Anyone sees this?</p>
",10/6/2022 23:39,,345,0,0,1,,722453.0,,4/24/2011 8:34,185.0,,,,,,,
3546,58625888,Generate probability density after single sensor measurement,|robotics|,"<p>In MCL localization, the robot is able to generate a density map after receiving a single sensor scan. An example is given on page 10 of the following link.</p>

<p><a href=""http://robots.stanford.edu/papers/thrun.robust-mcl.pdf"" rel=""nofollow noreferrer"">http://robots.stanford.edu/papers/thrun.robust-mcl.pdf</a></p>

<p>I wonder how Figure 3 is generated given only a sensor scan and a wall map?</p>
",10/30/2019 12:56,,29,1,0,0,,3284573.0,"Taichung City, Taiwan",2/7/2014 15:54,75.0,58629570.0,"<blockquote>
  <p>Fig. 3c probability distribution for different poses</p>
</blockquote>

<p>It indicates probability of scan <code>o</code> being taken at different locations {x1, x2, ...xn}. </p>

<p><a href=""https://i.stack.imgur.com/M50m7.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M50m7.gif"" alt=""enter image description here""></a></p>

<p>In <code>Fig.3c</code>, these probabilities are plotted such that higher the probability higher the darkness.</p>

<p>I guess these positions of particle are taken from <code>Fig. 6a</code> as example. You can see in <code>Fig. 3c</code> that particles in the corridor have high probability (more dark). </p>
",1595504.0,0.0,0.0,,
3578,58744003,Custom addon not displayed in the addons menu in G1ANT studio,|c#|automation|robotics|rpa|g1ant|,"<p>I am trying to  create a new addon but the addon is not being displayed in the addons menu in G1ANT Studio. Even other addons installed from the marketplace are also not displayed. I am using the latest version. I have tried running G1ANT studio as administrator. Yet it makes no difference. </p>

<p>Here is the Addon.cs file of my addon:</p>

<pre><code>using System.Collections.Generic;
using System.Linq;
using System.Text;
using G1ANT.Language;

// Please remember to refresh G1ANT.Language.dll in references

namespace G1ANT.Addon.LibreOffice
{
    [Addon(Name = ""libreoffice"", Tooltip = ""Provides commands to automate LibreOffice"")]
    [Copyright(Author = ""G1ANT LTD"", Copyright = ""G1ANT LTD"", Email = ""support@g1ant.com"", Website = ""www.g1ant.com"")]
    [License(Type = ""LGPL"", ResourceName = ""License.txt"")]
    [CommandGroup(Name = ""calc"", Tooltip = ""Commands connected with creating editing and generally working on calc"")]
    public class LibreOfficeAddon : Language.Addon
    {

        public override void Check()
        {
            base.Check();
            // Check integrity of your Addon
            // Throw exception if this Addon needs something that doesn't exists
        }

        public override void LoadDlls()
        {
            base.LoadDlls();
            // All dlls embeded in resources will be loaded automatically,
            // but you can load here some additional dlls:

            // Assembly.Load(""..."")
        }

        public override void Initialize()
        {
            base.Initialize();
            // Insert some code here to initialize Addon's objects
        }

        public override void Dispose()
        {
            base.Dispose();
            // Insert some code here which will dispose all unnecessary objects when this Addon will be unloaded
        }
    }
}
</code></pre>

<p>The addon also references some other DLLs as dependencies. </p>
",11/7/2019 7:38,58818729.0,90,2,0,2,,11998441.0,"Chennai, Tamil Nadu, India",8/30/2019 8:39,42.0,58818077.0,"<p>There are no errors in your code. Have you ever compiled the HelloWorld example from this tutorial? <a href=""https://github.com/G1ANT-Robot/G1ANT.Addon.Tutorials/tree/master/G1ANT.Addon.Command.HelloWorld"" rel=""nofollow noreferrer"">https://github.com/G1ANT-Robot/G1ANT.Addon.Tutorials/tree/master/G1ANT.Addon.Command.HelloWorld</a></p>

<p>Remember
1. All dlls in the solution should be marked as ""Resource"" and will be embeded into your addon
2. The target .NET Framework of your project should be 4.6.1</p>
",12360767.0,3.0,1.0,,
2888,45996174,Control a robot with REST API with JS and Mongodb,|mysql|rest|raspberry-pi|robotics|,"<p>I tried to write a control connection between my robot and PC or Android. 
Ok. So right now it looks like so:
There is an local apache2 server on my raspberry pi and my robot checks every time my MySQL database which is not efficient especially because MySQL is slow</p>

<p>I need a new solution to improve speed of communication beetween my robot and PC or Android app.</p>

<p>Is it efficient if I setup REST server on my raspberry pi with MongoDB in JS? MongoDB should be much faster than my current MySQL solution. And If I had REST service I can use it to communicate android app or webapp or my pc to my robot.</p>

<p>Could you give me any tip if that helps with my case? Or the current solution client-server with MySQL will give me the same effect. Or there is any other better solution to ensure control connection between raspberry pi robot and different clients.</p>
",9/1/2017 8:32,,137,0,5,0,,3904216.0,,8/3/2014 15:59,392.0,,,,,,78972636.0,Is the webserver necessary? What about  querying the database directly from the controller? Is there a way to do that?
4463,74683769,Microbit doesn't run flashed program when seated in Max:bot,|robotics|bbc-microbit|,"<p>My son bought a <a href=""https://www.pbtech.com/au/product/SBCDFR0005/DFRobot-STEM-Education-Boson-ROB0147-Maxbot-DIY-Pr"" rel=""nofollow noreferrer"">Max:bot DIY Programmable Robot Kit</a>, which uses a BBC Microbit.</p>
<p><a href=""https://i.stack.imgur.com/2q5Jf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2q5Jf.png"" alt=""Max:bot"" /></a></p>
<p>You can see above where the Microbit slots in.</p>
<h3>What works ...</h3>
<p>To set the scene of our problem, we'll set aside the Max:bot for a moment and just consider the Microbit in isolation ...</p>
<p>Using Microsoft MakeCode, we can code a simple program to drive the LEDs on the Microbit.  With the Microbit unseated from the Max:bot, and connected to a Mac over USB, we can successfully flash the Microbit with our program.  As the USB cable provides power for the Microbit, the program begins running, and illuminates the LEDs as expected.  If we press the Microbit reset button, the program runs from the beginning as expected.</p>
<p>Furthermore, if we remove the USB cable (removing the power source), and then re-cable the USB cable (providing power once more), the Microbit immediately runs the program.  This shows to us we have successfully written the program to the Microbit's flash memory as it is persistent across power on/off cycles.</p>
<p>I note that when we power the Microbit via the USB cable, a yellow LED on the rear of the Microbit near the USB port is illuminated.</p>
<h3>What the problem is ...</h3>
<p>Let's now bring the Max:bot back into the fold ...</p>
<p>The Max:bot has a battery pack and (it would seem) provides power to the Microbit independently of the USB cable.</p>
<p>If we do not have the USB cable plugged into the Microbit, and we seat the Microbit in the Max:bot connector, and we turn on the Max:bot, then the Microbit appears to power on but does not run the program stored in its flash.</p>
<p>The Microbit instead initialises with this sequence shown on its LEDs.</p>
<p><a href=""https://i.stack.imgur.com/YlZUS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YlZUS.png"" alt=""boot"" /></a></p>
<p>I note that when powered via the Max:bot, the aforementioned yellow LED on the rear of the Microbit near the USB port does not turn on.</p>
<p>The question is then, why doesn't the Microbit run the flashed program when it is seated in and powered by the Max:bot.</p>
<h3>A final observation ...</h3>
<p>The following sequence does run the program:</p>
<ul>
<li>have the Max:bot powered off</li>
<li>cable the USB to the Mac (Microbit powers on, loads program from flash)</li>
<li>turn on the Max:bot</li>
<li>uncable the USB</li>
</ul>
<p>The question then is why is the USB required to be connected for the Microbit to boot from flash?  (Because it's not practical to do so when you've got a program that actually drives the bot around).</p>
",12/5/2022 5:34,74751567.0,79,1,3,1,,1212960.0,,2/16/2012 3:44,1316.0,74751567.0,"<p>From the images you have shared of the LEDs on the micro:bit when it is in the Max:bit it appears to be entering &quot;<a href=""https://makecode.microbit.org/v0/reference/bluetooth/bluetooth-pairing"" rel=""nofollow noreferrer"">Bluetooth Pairing Mode</a>&quot;. This mode is entered when holding down buttons <code>A</code> and <code>B</code> on the front of your micro:bit while powering on the device. The signal for those buttons are also available on the <a href=""https://makecode.microbit.org/device/pins"" rel=""nofollow noreferrer"">edge connector</a>.</p>
<p>My assumption here is that Max:bit is using <code>P5</code> and <code>P11</code> and so it appears to the micro:bit that button A and button B are being held down when Max:bit is powered.</p>
<p>Looking at the assembly guide it would appear that there are some LED strips that might be the culprit for this.</p>
<p><a href=""https://i.stack.imgur.com/saMjI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/saMjI.png"" alt=""enter image description here"" /></a></p>
",7721752.0,1.0,0.0,131822110.0,Thank you @ukBaz I will investigate that angle.
3977,66340344,Compile the Robot Control Library for a different beaglebone cape,|beagleboneblack|robotics|,"<p>How should I go about modifying and/or compiling the <a href=""http://strawsondesign.com/docs/librobotcontrol/"" rel=""nofollow noreferrer"">Robot Control Library</a> for use with a different beaglebone cape that uses slightly different pin assignments?</p>
<p>My primary reason for wanting to re-use the Robot Control Library is the ability to read a fourth encoder via the PRU.  Beyond that, I only need access to the encoder and pwm modules.</p>
",2/23/2021 20:07,66394399.0,372,1,1,1,0.0,3830019.0,,7/11/2014 15:21,12.0,66394399.0,"<h1>TL;DR</h1>
<p>Modifying the PRU firmware to read the encoder signal from a different pin was easy.  Figuring out how to assemble a working device tree for the combination of features I needed was way harder.</p>
<p>I would welcome any feedback on how I <strong>should</strong> have done this, or how I could improve upon what I currently have.</p>
<h1>Robot Control Library + Motor Cape</h1>
<p>The Robotics Cape and the BeagleBone Blue provide a turnkey solution for servo controlling four motors,
IFF you are satisfied with driving them at 8V (e.g. a 2S LIPO battery).  The Motor Cape can handle a
higher drive voltage (and more current), but does not include encoders.  Plugging encoders into the P8 &amp; P9
headers on the Motor Cape is simple enough, but the BeagleBone itself only has 3 encoder counters (eQEP).<br />
The Robot Control Library solves this problem by reading the fourth encoder with PRU0.  However, some of the
pins conflict between the Motor Cape and what the Robot Control Library expects on the Robotics Cape.</p>
<p>So, how hard could it be to use the Robot Control Library to read encoders and drive motors with a slightly
different pinout on the Motor Cape?  Probably not difficult at all if you are already competent with BeagleBone
device tree overlays, which I am not...</p>
<h1>It all starts with a plan -- Pin Selection</h1>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Pin</th>
<th>PRU Bit</th>
<th>Robotics Cape</th>
<th>Motor Cape</th>
</tr>
</thead>
<tbody>
<tr>
<td>P8_15</td>
<td>15</td>
<td>Enc 4B</td>
<td>--</td>
</tr>
<tr>
<td>P8_16</td>
<td>14</td>
<td>Enc 4A</td>
<td>M2 Dir</td>
</tr>
<tr>
<td>P9_25</td>
<td>7</td>
<td>IMU</td>
<td>--</td>
</tr>
</tbody>
</table>
</div>
<p>The Robot Control Library expects the fourth encoder to show up on P8_15 and P8_16, but the
Motor Cape has P8_16 wired as a direction signal.  There are only 12 pins than be configured
as inputs to PRU0 and I eventually selected  P9_25 because I did not need the IMU functionality.</p>
<p>The best reference I found for what pins can be used for what purposes were these pdfs:</p>
<ul>
<li><a href=""https://ofitselfso.com/BeagleNotes/BeagleboneBlackP8HeaderPinMuxModes.pdf"" rel=""nofollow noreferrer"">https://ofitselfso.com/BeagleNotes/BeagleboneBlackP8HeaderPinMuxModes.pdf</a></li>
<li><a href=""https://ofitselfso.com/BeagleNotes/BeagleboneBlackP9HeaderPinMuxModes.pdf"" rel=""nofollow noreferrer"">https://ofitselfso.com/BeagleNotes/BeagleboneBlackP9HeaderPinMuxModes.pdf</a></li>
</ul>
<h1>The Easy Part -- Modifying the PRU Code</h1>
<p>The Robot Control Library defines the encoder signal input bits in <code>pru_firmware/src/pur0-encoder.asm</code> as</p>
<pre><code>; Encoder counting definitions
; these pin definitions are specific to SD-101D Robotics Cape
    .asg    r0,         OLD     ; keep last known values of chA and B in memory
    .asg    r1,         EXOR    ; place to store the XOR of old with new AB vals
    .asg    14,         A
    .asg    15,         B
</code></pre>
<p>This can be modified to look for the A channel on bit 7 (of register 31, which is used for all inputs) as</p>
<pre><code>; Encoder counting definitions
; these pin definitions are specific to SD-101D Robotics Cape
    .asg    r0,         OLD     ; keep last known values of chA and B in memory
    .asg    r1,         EXOR    ; place to store the XOR of old with new AB vals
    .asg    07,         A
    .asg    15,         B
</code></pre>
<p><strong>N.B.</strong> The PRU firmware has to be separately compiled by by running <code>make</code> and <code>sudo make install</code>
inside the <code>pru_firmware</code> directory.  It is <strong>not</strong> compiled automatically as part as part of building the rest
of the library from the top-level Makefile.</p>
<h3>Helpful Tip: What version am I actually running?</h3>
<p>There are instructions on modifying the repored version of <code>librobotcontrol</code> in
<code>library/version_updating_howto.txt</code>.  I followed these instructions to create my own
&quot;private&quot; version number so that I could confirm that I was actually running my modified
version of the libray.  This version is reported by <code>rc_test_drivers</code>.</p>
<p>However... as noted above, the PRU firmware was not getting compiled by the top-level Makefile,
so for a while I was running my &quot;new&quot; version of <code>librobotcontrol</code> with &quot;old&quot; firmware in the PRU.</p>
<h1>The Part that Almost Worked -- The Device Tree</h1>
<p>I found references in both the documentation and code for <code>librobotcontrol</code> that device tree overlays
were no longer needed because the Robotics Cape used its own device tree.</p>
<blockquote>
<p>The overlay is deprecated now, instead the cape gets its own complete device tree.</p>
</blockquote>
<p>I also observed that running the recommended <code>configure_robotics_dt.sh</code> replaced <code>/boot/uEnv.txt</code> with
the following simplified version that loads a single device tree binary (.dtb)</p>
<pre><code>uname_r=4.19.94-ti-r42
dtb=am335x-boneblack-roboticscape.dtb
cmdline=coherent_pool=1M
</code></pre>
<p>My favorite reference that I found for general information about the device tree, pinmux, etc was
<a href=""http://www.ofitselfso.com/BeagleNotes/AboutTheDeviceTree.pdf"" rel=""nofollow noreferrer"">http://www.ofitselfso.com/BeagleNotes/AboutTheDeviceTree.pdf</a>  However, I now realize that some of the
details are a bit out of date, so be careful.</p>
<p>Because I had very little idea of where else to start, I set out to modify the robotics cape device tree
just enough to eliminate the conflicts with the Motor Cape.  I forked and cloned
<a href=""https://github.com/beagleboard/BeagleBoard-DeviceTrees"" rel=""nofollow noreferrer"">https://github.com/beagleboard/BeagleBoard-DeviceTrees</a> and created two new files</p>
<ul>
<li><code>am335x-boneblack-custom.dts</code>
<ul>
<li>copy of <code>am335x-boneblack-roboticscape.dts</code></li>
<li>changed <code>model</code> to make the new device tree recognizable</li>
<li>changed <code>#include</code> to point to <code>am335x-custom.dtsi</code> instead of <code>am335x-roboticscape.dtsi</code></li>
</ul>
</li>
<li><code>am335x-custom.dtsi</code>
<ul>
<li>copy of <code>am335x-roboticscape.dtsi</code></li>
<li>deleted a whole bunch of stuff I thought I didn't need anymore</li>
<li>routed P9_25 (instead of P8_16) to PRU0</li>
</ul>
</li>
</ul>
<p>Before</p>
<pre><code>            /* PRU encoder input */
            0x03c 0x36  /* P8_15,PRU0_r31_15,MODE6 */
            0x038 0x36  /* P8_16,PRU0_r31_14,MODE6 */
</code></pre>
<p>After</p>
<pre><code>            /* PRU encoder input */
            0x03c 0x36  /* P8_15,PRU0_r31_15,MODE6 */
            0x1ac 0x36  /* P9_25,PRU0_r31_7,MODE6 */
</code></pre>
<p>After compiling and installing the modified device trees (<code>make</code>, <code>sudo make install</code> in the
<code>BeagleBoard-DeviceTrees</code> repo), I modified <code>/boot/uEnv.txt</code> to call my new custom device tree</p>
<pre><code>uname_r=4.19.94-ti-r42
dtb=am335x-boneblack-custom.dtb
cmdline=coherent_pool=1M
</code></pre>
<p>I was able to boot the BeagleBone with no cape installed, plug encoders directly into the desired pins
on P8_and P9 (including enc4a on P9_25) and read all four encoders using <code>sudo rc_test_encoders</code>.
I thought I had won and went to bed...</p>
<h3>Motor Cape Won't Boot</h3>
<p>After a good night's sleep, I plugged the Motor Cape onto the BeagleBone expecting nothing to change since
I was only passing encoder signals directly through the P8 and P9 headers.  I thought the next step would be
to make similar tweaks to a few of the pwm direction pins.</p>
<p>However, the BeagleBone refused to boot my custom device tree with the MotorCape installed.  I went back to
the &quot;standard&quot; <code>am335x-boneblack-roboticscape.dtb</code> device tree and observed that it would not boot either with
the Motor Cape intalled.  I also became suspicious that the &quot;factory&quot; installation of the Robotics Cape might have
been using overlays after all</p>
<p>I had been torn from the beginning about whether I should be starting from the Robotics Cape device tree and removing
things I did not need in order to eliminate resource conflicts, versus starting with the &quot;naked&quot; BeagleBone device tree
and adding the things that I did need.  Whether accurate or not, in my mind that kind-of mapped into trying to specify
a full device tree versus providing an overlay to apply on top of the base device tree.  The latter seemed like
the conceptually more correct path, so once the Motor Cape failed to boot with the robotics-cape-derived device tree,
I decided to bite the bullet and try to figure out device tree overlays.</p>
<h3>Unanswered Questions</h3>
<ul>
<li>[ ] Why won't the BB boot from <code>am335x-boneblack-roboticscape.dtb</code> with the motor cape installed?  What is the actual error?</li>
<li>[ ] Does a &quot;normal&quot; installation of <code>librobotcontrol</code> install the simplified <code>uEnv.txt</code> above or does it use an overlay?  Does it work?</li>
</ul>
<p>I did not yet have USB-to-TTL serial cable that could fit under an installed cape, so I know very little about
why or how this was failing to boot.</p>
<h1>The Part that Finally Worked -- Device Tree Overlays</h1>
<p>I eventually figured out that the collection of device tree overlays is avialable both at
<a href=""https://github.com/beagleboard/bb.org-overlays"" rel=""nofollow noreferrer"">https://github.com/beagleboard/bb.org-overlays</a> and in the <code>v4.19.x-ti-overlays</code> branch at
<a href=""https://github.com/beagleboard/BeagleBoard-DeviceTrees"" rel=""nofollow noreferrer"">https://github.com/beagleboard/BeagleBoard-DeviceTrees</a>.  I suspect that this might be an
in-progress migration, but there was more documentation associated with the <code>bb.org-overlays</code>
repository so that is what I chose to use.</p>
<p>A few documentation links that I wish I had found earlier:</p>
<ul>
<li><a href=""https://elinux.org/Beagleboard:BeagleBoneBlack_Debian#U-Boot_Overlays"" rel=""nofollow noreferrer"">https://elinux.org/Beagleboard:BeagleBoneBlack_Debian#U-Boot_Overlays</a></li>
<li><a href=""https://github.com/cdsteinkuehler/beaglebone-universal-io"" rel=""nofollow noreferrer"">https://github.com/cdsteinkuehler/beaglebone-universal-io</a></li>
<li><a href=""https://vadl.github.io/beagleboneblack/2016/07/29/setting-up-bbb-gpio"" rel=""nofollow noreferrer"">https://vadl.github.io/beagleboneblack/2016/07/29/setting-up-bbb-gpio</a></li>
</ul>
<p>I created forked, cloned, and branched the <code>bb.org-overlays</code> repo and created a new overlay at
<code>src/arm/CustomCape-00A0.dts</code> by hacking together pieces from <code>BBORG_MOTOR-00A2.dts</code> and <code>RoboticsCape-00A0.dts</code></p>
<pre><code>/*
 * Device Tree Overlay for custom cape trying to reuse Robot Control Library's
 * reading of 4x optical encoders.
 */

/*
pinmux control byte map courtesy of http://beaglebone.cameon.net/
Bit 5: 1 - Input, 0 - Output
Bit 4: 1 - Pull up, 0 - Pull down
Bit 3: 1 - Pull disabled, 0 - Pull enabled
Bit 2 \
Bit 1 |- Mode
Bit 0 /
 */

/dts-v1/;
/plugin/;

/ {
    compatible = &quot;ti,beaglebone-black&quot;;

    /* identification */
    part-number = &quot;CustomCape&quot;;

    /* version */
    version = &quot;00A0&quot;;

    exclusive-use =
        &quot;P8.11&quot;,    /*QEP_2B*/
        &quot;P8.12&quot;,    /*QEP_2A*/
        &quot;P8.16&quot;,    /*PRU_ENCODER_A*/
        &quot;P8.33&quot;,    /*QEP_1B*/
        &quot;P8.35&quot;,    /*QEP_1A*/
        &quot;P9.27&quot;,    /*QEP_0B*/
        &quot;P9.41&quot;,    /*MOT_STBY*/
        &quot;P9.42&quot;;    /*QEP_0A*/

    /*
     * Helper to show loaded overlays under: /proc/device-tree/chosen/overlays/
     */
    fragment@0 {
        target-path=&quot;/&quot;;
        __overlay__ {
            chosen {
                overlays {
                    CustomCape-00A0 = __TIMESTAMP__;
                };
            };
        };
    };

fragment@1 {
    target = &lt;&amp;am33xx_pinmux&gt;;
    __overlay__ {
        /****************************************
        *           pinmux helper
        ****************************************/
        mux_helper_pins: pins {
            pinctrl-single,pins = &lt;

            /* EQEP */
            0x1A0 0x31  /* P9_42,EQEP0A, MODE1 */
            0x1A4 0x31  /* P9_27,EQEP0B, MODE1 */
            0x0D4 0x32  /* P8_33,EQEP1B, MODE2 */
            0x0D0 0x32  /* P8_35,EQEP1A, MODE2 */
            0x030 0x34  /* P8_12,EQEP2A_in, MODE4 */
            0x034 0x34  /* P8_11,EQEP2B_in, MODE4 */

            /* PRU encoder input */
            0x03c 0x36  /* P8_15,PRU0_r31_15,MODE6 */
            0x1ac 0x36  /* P9_25,PRU0_r31_7,MODE6 */
            &gt;;
        };
    };
};

/****************************************
    Pinmux Helper
    activates the pinmux helper list of pin modes
****************************************/
fragment@2 {
    target = &lt;&amp;ocp&gt;;
        __overlay__ {
            test_helper: helper {
            compatible = &quot;bone-pinmux-helper&quot;;
            pinctrl-names = &quot;default&quot;;
            pinctrl-0 = &lt;&amp;mux_helper_pins&gt;;
            status = &quot;okay&quot;;
        };
    };
};


    /*
     * Free up the pins used by the cape from the pinmux helpers.
     */
    fragment@3 {
        target = &lt;&amp;ocp&gt;;
        __overlay__ {
            P8_11_pinmux { status = &quot;disabled&quot;; };  /* enc3b */
            P8_12_pinmux { status = &quot;disabled&quot;; };  /* enc3a */
            P8_15_pinmux { status = &quot;disabled&quot;; };  /* enc4b */
            P8_33_pinmux { status = &quot;disabled&quot;; };  /* enc0  */
            P8_35_pinmux { status = &quot;disabled&quot;; };  /* enc0  */
            P9_25_pinmux { status = &quot;disabled&quot;; };  /* enc4a */
            P9_27_pinmux { status = &quot;disabled&quot;; };  /* enc1b */
            P9_92_pinmux { status = &quot;disabled&quot;; };  /* enc1a */
        };
    };

/****************************************
        Encoders
****************************************/
fragment@9 {
    target = &lt;&amp;eqep0&gt;;
    __overlay__ {
        count_mode = &lt;0&gt;;  /* 0 - Quadrature mode, normal 90 phase offset cha &amp; chb.  1 - Direction mode.  cha input = clock, chb input = direction */
        swap_inputs = &lt;0&gt;; /* Are channel A and channel B swapped? (0 - no, 1 - yes) */
        invert_qa = &lt;1&gt;;   /* Should we invert the channel A input?  */
        invert_qb = &lt;1&gt;;   /* Should we invert the channel B input? I invert these because my encoder outputs drive transistors that pull down the pins */
        invert_qi = &lt;0&gt;;   /* Should we invert the index input? */
        invert_qs = &lt;0&gt;;   /* Should we invert the strobe input? */

        status = &quot;okay&quot;;
    };
};

fragment@10 {
    target = &lt;&amp;eqep1&gt;;
    __overlay__ {
        count_mode = &lt;0&gt;;  /* 0 - Quadrature mode, normal 90 phase offset cha &amp; chb.  1 - Direction mode.  cha input = clock, chb input = direction */
        swap_inputs = &lt;0&gt;; /* Are channel A and channel B swapped? (0 - no, 1 - yes) */
        invert_qa = &lt;1&gt;;   /* Should we invert the channel A input?  */
        invert_qb = &lt;1&gt;;   /* Should we invert the channel B input? I invert these because my encoder outputs drive transistors that pull down the pins */
        invert_qi = &lt;0&gt;;   /* Should we invert the index input? */
        invert_qs = &lt;0&gt;;   /* Should we invert the strobe input? */
        status = &quot;okay&quot;;
    };
};

fragment@11 {
    target = &lt;&amp;eqep2&gt;;
    __overlay__ {
        count_mode = &lt;0&gt;;  /* 0 - Quadrature mode, normal 90 phase offset cha &amp; chb.  1 - Direction mode.  cha input = clock, chb input = direction */
        swap_inputs = &lt;0&gt;; /* Are channel A and channel B swapped? (0 - no, 1 - yes) */
        invert_qa = &lt;1&gt;;   /* Should we invert the channel A input?  */
        invert_qb = &lt;1&gt;;   /* Should we invert the channel B input? I invert these because my encoder outputs drive transistors that pull down the pins */
        invert_qi = &lt;0&gt;;   /* Should we invert the index input? */
        invert_qs = &lt;0&gt;;   /* Should we invert the strobe input? */
        status = &quot;okay&quot;;
    };
};


/****************************************
        PRU
****************************************/
fragment@31 {
    target = &lt;&amp;pruss&gt;;
    __overlay__ {
        status = &quot;okay&quot;;
    };
};
};

</code></pre>
<p>I added my custom overlay to <code>/boot/uEnv.txt</code> and disabled the video and audio overlays</p>
<pre><code>uname_r=4.19.94-ti-r42
#uuid=
#dtb=

###U-Boot Overlays###
###Documentation: http://elinux.org/Beagleboard:BeagleBoneBlack_Debian#U-Boot_Overlays
###Master Enable
enable_uboot_overlays=1
###
###Additional custom capes
uboot_overlay_addr4=/lib/firmware/CustomCape-00A0.dtbo
###
###Custom Cape
#dtb_overlay=/lib/firmware/&lt;file8&gt;.dtbo
###
###Disable auto loading of virtual capes (emmc/video/wireless/adc)
#disable_uboot_overlay_emmc=1
disable_uboot_overlay_video=1
disable_uboot_overlay_audio=1
#disable_uboot_overlay_wireless=1
#disable_uboot_overlay_adc=1
###
###PRUSS OPTIONS
###pru_rproc (4.19.x-ti kernel)
uboot_overlay_pru=/lib/firmware/AM335X-PRU-RPROC-4-19-TI-00A0.dtbo
###
###Cape Universal Enable
enable_uboot_cape_universal=1
###
###U-Boot Overlays###

cmdline=coherent_pool=1M net.ifnames=0 lpj=1990656 rng_core.default_quality=100 quiet
</code></pre>
<p>I make no claims of optimality or even correctness, but this configuration boots with or without the Motor Cape
installed and I can read all four encoders with <code>rc_test_encoders</code>.  When the Motor Cape is installed, uBoot
is correctly picking up and applying the <code>BBORG_MOTOR-00A2</code> overlay.  I honestly thought that I would need a lot more
configuration of the PRU to get the PRU-based encoder counter from the Robot Control Library working, but this seeems
to do the trick.</p>
<p>I would welcome any feedback on how I <strong>should</strong> have done this, or how I could improve upon what I currently have.</p>
<h3>Helpful Tip: Watch the serial terminal!</h3>
<p>I am embarrassed that I even attempted to debug device tree booting issues without first having a serial terminal
open to the beaglebone so that I could observe the boot sequence.  With the help of some 5-minute epoxy, I was
eventually able to make a 90-degree header to bring the JTAG port out from under an installed cape.</p>
<p><a href=""https://elinux.org/Beagleboard:BeagleBone_Black_Serial"" rel=""nofollow noreferrer"">https://elinux.org/Beagleboard:BeagleBone_Black_Serial</a></p>
",3830019.0,3.0,0.0,117331116.0,"You've asked a question without providing any relevant details whatsoever, in which case the answer would be ""follow instructions to update the code to match your specifications""."
4623,76626687,Formula derivation in the CHOMP: Gradient Optimization Techniques for Efficient Motion Planning article,|robotics|convex-optimization|motion-planning|,"<p>Has anyone read this article, i.e., <em>CHOMP: Gradient Optimization Techniques for Efficient Motion Planning</em> article?</p>
<p>I'm confused about the derivation of equations 2-3, which is shown in the figure below.</p>
<p>A detailed procedure is expected. Besides, which chapter of Convex Optimization (S. Boyd and L. Vandenberghe) does this derivation belong to？</p>
<p>From which chapter of the book can I learn the derivation process?</p>
<p>Pic:</p>
<p><a href=""https://i.stack.imgur.com/rfKfT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rfKfT.png"" alt=""enter image description here"" /></a></p>
",7/6/2023 7:58,,31,0,0,0,,19644538.0,,7/29/2022 1:56,26.0,,,,,,,
4659,77046789,Very low accuracy and high loss with LSTM model,|python|deep-learning|lstm|recurrent-neural-network|robotics|,"<p>I try to learn the inverse kinematics of a robotic manipulator. To do that I have a simulator with which I acquired data.</p>
<p>My dataset is composed of positions in X, Y and Z and actuator variables (6 of them). That represents around 310 000 rows. Therefore, the inputs of my NN are the positions (3 inputs) and the outputs the actuator variables (6 outputs). Just before (when my dataset were much simpler, 3 outputs only) I managed to obtain good results with a MLP. However, now that the dataset is much more complicated, I did not manage to obtain a similar result.</p>
<p>I decided to explore a bit the option of RNN and particularly the LSTM. However, I still have some difficulties to obtain convenient results. In fact, at this moment I only achieve 0.17 of accuracy and 0.06 loss (a good value of loss would be 0.0001).</p>
<p>Here is my code :</p>
<pre><code>import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

# Chargement des données
data = np.genfromtxt('/dataprocess.csv', delimiter=',')
scaler = MinMaxScaler()
data[1:,[7,8,9,10,11,12,13,14,15]] = scaler.fit_transform(data[1:,[7,8,9,10,11,12,13,14,15]])

# Diviser les données en ensembles d'entraînement et de test
trainset, testset = train_test_split(data[1:, [7,8,9,10,11,12,13,14,15]], test_size=0.2, random_state=0)


# Préparation des données en séquences
seq_length = 10  # Longueur de la séquence
X_train_seq = [trainset[i:i+seq_length, [6, 7, 8]] for i in range(len(trainset) - seq_length)]
Y_train_seq = [trainset[i+seq_length, [0, 1, 2, 3, 4, 5]] for i in range(len(trainset) - seq_length)]
X_test_seq = [testset[i:i+seq_length, [6, 7, 8]] for i in range(len(testset) - seq_length)]
Y_test_seq = [testset[i+seq_length, [0, 1, 2, 3, 4, 5]] for i in range(len(testset) - seq_length)]

X_train_seq = np.array(X_train_seq)
Y_train_seq = np.array(Y_train_seq)
X_test_seq = np.array(X_test_seq)
Y_test_seq = np.array(Y_test_seq)

print(&quot;X train seq shape : &quot;, np.array(X_train_seq).shape)

# Création du modèle RNN avec tf.keras.Model
# input_layer = tf.keras.layers.Input(shape=(3,))  # Entrée : x, y, z
input_layer = tf.keras.layers.Input(shape=(seq_length, 3))  # Entrée : séquence de x, y, z
rnn_layer = tf.keras.layers.LSTM(256, activation='relu', return_sequences=True)(input_layer)  # Couche SimpleRNN
hidden_layer2 = tf.keras.layers.LSTM(256, activation='relu', return_sequences=True)(rnn_layer)  # Couche intermédiaire
hidden_layer3 = tf.keras.layers.LSTM(256, activation='relu', return_sequences=True)(hidden_layer2)  # Couche intermédiaire
hidden_layer4 = tf.keras.layers.LSTM(256, activation='relu', return_sequences=True)(hidden_layer3)  # Couche intermédiaire
hidden_layer5 = tf.keras.layers.LSTM(256, activation='relu', return_sequences=True)(hidden_layer4)  # Couche intermédiaire
hidden_layer6 = tf.keras.layers.LSTM(256, activation='relu', return_sequences=True)(hidden_layer5)  # Couche intermédiaire
output_layer = tf.keras.layers.Dense(6)(hidden_layer6)  # 6 sorties pour les variables d'actionnement



model = tf.keras.Model(inputs=input_layer, outputs=output_layer)

# Compilation du modèle
model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])

# Entraînement du modèle
History = model.fit(X_train_seq, Y_train_seq, validation_data=(X_test_seq, Y_test_seq), epochs=100, batch_size=512, verbose=2)
# History = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=500, batch_size=1024, verbose=2)

# Évaluation du modèle sur les données de test
loss, accuracy = model.evaluate(X_test_seq, Y_test_seq, verbose=0)
# loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)
print(&quot;Test accuracy:&quot;, accuracy)
print(&quot;Test loss:&quot;, loss)
</code></pre>
",9/5/2023 17:59,,113,0,3,0,,22505446.0,,9/5/2023 17:38,3.0,,,,,,135839879.0,@Jeppe yes i’ve tried lots of different combination. From one LSTM layer with 10 neurons to 10 LSTM layers with 512 neurons. I also changed batch size and epochs but the results is still the same
3230,53311457,Is best first search optimal and complete?,|artificial-intelligence|path-finding|robotics|heuristics|best-first-search|,"<p>I have some doubts regarding best first search algorithm. The pseudocode that I have is the following:
<a href=""https://i.stack.imgur.com/OgPtS.png"" rel=""noreferrer"">best first search pseudocode</a></p>

<p>First doubt: is it complete? I have read that it is not because it can enter in a dead end, but I don't know when can happen, because if the algorithm chooses a node that has not more neighbours it does not get stucked in it because this node is remove from the open list and in the next iteration the following node of the open list is treated and the search continues.</p>

<p>Second doubt: is it optimal? I thought that if it is visiting the nodes closer to the goal along the search process, then the solution would be the shortest, but it is not in that way and I do not know the reason for that and therefore, the reason that makes this algorithm not optimal.</p>

<p>The heuristic I was using is the straight line distance between two points.</p>

<p>Thanks for your help!!</p>
",11/15/2018 2:12,,11896,2,0,6,,5604964.0,Spain,11/25/2015 15:53,43.0,53321809.0,"<p>In general case best first search algorithm is complete as in worst case scenario it will search the whole space (worst option). Now, it should be also optimal - given the heuristic function is admissible - meaning it does not overestimate the cost of the path from any of the nodes to goal. (It also needs to be consistent - that means that it adheres to triangle inequality, if it is not then the algorithm would not be complete - as it could enter a cycle)</p>

<p>Checking your algorithm I do not see how the heuristic function is calculated. Also I do not see there is calculated the cost of the path to get to the particular node.
So, it needs to calculate the actual cost of the path to reach a particular node and then it needs to add a heuristics estimate of the cost of the path from the node towards goal.</p>

<p>The formula is <code>f(n)=g(n)+h(n)</code> where g(n) is the cost of the path to reach the node and h(n) is the heuristics estimating the cost of the cheapest path from n to the goal. </p>

<p>Check the implementation of <a href=""https://en.wikipedia.org/wiki/A*_search_algorithm"" rel=""nofollow noreferrer"">A* algorithm</a> which is an example of best first search on path planning.</p>

<p><strong>TLDR</strong> In best first search, you need to calculate the cost of a node as a sum of the cost of the path to get to that node and the heuristic function that estimate the cost of the path from that node to the goal. If the heuristic function will be admissible and consistent the algorithm will be optimal and complete.</p>
",6779656.0,3.0,2.0,,
2469,36403200,Same coordinate system between two objects C#,|c#|robotics|coordinate-systems|kinect-sdk|,"<p>I am writing a code using C#. I use a Kinect SDK V2 and a robot, for example my robot model is an IRB 1600. From the Kinect sensor I take a human point cloud when a human is detecting from the camera and from the robot I take one position (X-Y-Z position) that tells me where the robot is every time I ask it.</p>
<p>My problem is that the camera and robot have got different coordinate systems the sensor have the sender of the camera and the robot has his bases. I want to create a same coordinate system between them for distance calculation between human and robot. Are any methods to do that? tutorials?</p>
",4/4/2016 12:50,,87,1,0,1,,5436280.0,,10/12/2015 10:30,19.0,36409262.0,"<p>I think you just need to make a calibration...</p>

<p>Somewhere you need to say that your 3D coordinate system starts at (0,0,0) and then re-convert all the positions based on this coordinate system. Shouldn't be hard to implement.</p>

<p>Nevertheless, if you have both 3D positions of the two sensors, a simple calculation in 3D gives you the distance.</p>

<p>Distance=sqrt(X2-X1)^2+(Y2-Y1)^2+(Z2-Z1)^2)</p>
",2205242.0,0.0,0.0,,
4778,78212426,Arduino ESP-01 doesn't works,|arduino|robotics|,"<p>A fatal esptool.py error occurred: Failed to connect to ESP8266: Timed out waiting for packet header</p>
<p>I have this code and this components:
<a href=""https://i.stack.imgur.com/i4Sf8.png"" rel=""nofollow noreferrer"">enter image description here</a><a href=""https://i.stack.imgur.com/scuB4.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I don't know what I should do any suggestions?</p>
<p>I'm just trying to connect and upload this simple code to the esp via FTDI usb</p>
",3/23/2024 20:44,,19,0,1,0,,23759528.0,,3/23/2024 20:39,1.0,,,,,,137888784.0,"If I'm asking it's because I haven't gotten the answer on Google, It's common sense."
4429,74116961,Pepper (SoftBank Robotics): Set moveTo speed in Pepper robot via Python (no ROS),|robotics|pepper|nao-robot|slam|,"<p>I'm working on Pepper robot by SoftBank Robotics and I'm trying to write a Python script that dynamically manages the Pepper robot speed .
I'm studying <a href=""http://doc.aldebaran.com/2-5/naoqi/motion/almotion.html"" rel=""nofollow noreferrer"">ALMotion</a> and <a href=""http://doc.aldebaran.com/2-5/naoqi/motion/alnavigation.html"" rel=""nofollow noreferrer"">ALNavigation</a> APIs, but I'm not able to find a solution.</p>
<p>Do you have some ideas?</p>
",10/18/2022 20:14,74135983.0,188,1,0,0,,11858777.0,"Bari, BA, Italia",7/30/2019 15:58,36.0,74135983.0,"<p><a href=""http://doc.aldebaran.com/2-5/naoqi/motion/control-walk-api.html#almotionproxy-move1"" rel=""nofollow noreferrer"">ALMotion::move</a> and <a href=""http://doc.aldebaran.com/2-5/naoqi/motion/control-walk-api.html#almotionproxy-movetoward1"" rel=""nofollow noreferrer"">ALMotion::moveToward</a> contain velocity/speed as a normalized value from the interval &lt;0; 1&gt; (and &lt;-1; 0&gt; for the opposite direction).</p>
<p>See e.g. the <a href=""https://gitlab.fi.muni.cz/nlp/dialog_coming"" rel=""nofollow noreferrer"">Come with me</a> app to see examples how Pepper's hand joint angles are transformed to its movement speed.</p>
",10822884.0,2.0,0.0,,
1359,10420966,Manipulator/camera calibration issue (linear algebra oriented),|python|linear-algebra|robotics|calibration|,"<p>I'm working on a research project involving a microscope (with a camera connected to the view port; the video feed is streamed to an application we're developing) and a manipulator arm. The microscope and manipulator arm are both controlled by a Luigs &amp; Neumann control box (very obsolete - the computer interfaces with it with a serial cable and its response time is slowwww.) The microscope can be moved in 3 dimensions; X, Y, and Z, whose axes are at right angles to one another. When the box is queried, it will return decimal values for the position of each axis of each device. Each device can be sent a command to move to a specific position, with sub-micrometer precision.</p>

<p>The manipulator arm, however, is adjustable in all 3 dimensions, and thus there is no guarantee that any of its axes are aligned at right angles. We need to be able to look at the video stream from the camera, and then click on a point on the screen where we want the tip of the manipulator arm to move to. Thus, the two coordinate systems have to be calibrated.</p>

<p>Right now, we have achieved calibration by moving the microscope/camera's position to the tip of the manipulator arm, setting that as the synchronization point between the two coordinate systems, and moving the manipulator arm +250um in the X direction, moving the microscope to the tip of the manipulator arm at this new position, and then using the differences between these values to define a 3d vector that corresponds to the distance and direction moved by the manipulator, per unit in the microscope coordinate system. This is repeated for each axis of the manipulator arm. </p>

<p>Once this data is obtained, in order to move the manipulator arm to a specific location in the microscope coordinate system, a system of equations can be solved by the program which determines how much it needs to move the manipulator in each axis to move it to the center point of the screen. This works pretty reliably so far.</p>

<p>The issue we're running into here is that due to the slow response time of the equipment, it can take 5-10 minutes to complete the calibration process, which is complicated by the fact that the tip of the manipulator arm must be changed occasionally during an experiment, requiring the calibration process to be repeated. Our research is rather time sensitive and this creates a major bottleneck in the process.</p>

<p>My linear algebra is a little patchy, but it seems like if we measure the units traveled by the tip of the manipulator arm per unit in the microscope coordinate system and have this just hard coded into the program (for now), it might be possible to move all 3 axes of the manipulator a specific amount at once, and then to derive the vectors for each axis from this information. I'm not really sure how to go about doing this (or if it's even possible to do this), and any advice would be greatly appreciated. If there's any additional information you need, or if you need clarification on anything please let me know.</p>
",5/2/2012 20:14,,321,1,3,0,,1370967.0,,5/2/2012 20:00,120.0,10421975.0,"<p>You really need four data points to characterize three independent axes of movement.</p>

<p>Can you can add some other constraints, ie are the manipulator axes orthogonal to <em>each other</em>, even if not fixed relative to the stage's axes? Do you know the manipulator's alignment <em>roughly</em>, even if not exactly?</p>

<p>What takes the most time - moving the stage to re-center? Can you move the manipulator and stage at the same time? How wide is the microscope's field of view? How much distance-distortion is there near the edges of the view - does it actually have to be re-centered each time to be accurate? Maybe we could come up with a reverse-screen-distortion mapping instead?</p>
",33258.0,0.0,4.0,13448482.0,"Are you sure that the manipulator arm is controlled in x, y and z? Usually robot arms are controlled by either absolute or relative angle of each of the joints. Or is the control box already converting (x,y,z) to joint angles?"
4041,68257668,Correct tf frames setting in ndt_matching,|ros|robotics|,"<p>ndt_matching succeeded in autoware, but the vehicle model cannot be set correctly.</p>
<ol>
<li>How do I set the correct angle for the vehicle model?</li>
<li>What does the frame &quot;mobility&quot; mean?</li>
</ol>
<p>tf.launch</p>
<pre><code>&lt;node pkg=&quot;tf&quot;  type=&quot;static_transform_publisher&quot; name=&quot;world_to_map&quot; args=&quot;0 0 0 0 0 0 /world /map 10&quot; /&gt;
&lt;node pkg=&quot;tf&quot;  type=&quot;static_transform_publisher&quot; name=&quot;map_to_points_map&quot; args=&quot;0 0 0 0 0 0 /map /points_map 10&quot; /&gt;
&lt;node pkg=&quot;tf&quot;  type=&quot;static_transform_publisher&quot; name=&quot;velodyne_to_lidar_top&quot; args=&quot;0 0 0 0 0 0 /velodyne /lidar_top 10&quot; /&gt;
</code></pre>
<p><a href=""https://i.stack.imgur.com/s77bf.png"" rel=""nofollow noreferrer"">Image for RViz</a></p>
<p><a href=""https://i.stack.imgur.com/Crr17.png"" rel=""nofollow noreferrer"">Image for TF Tree</a></p>
",7/5/2021 14:15,68463650.0,384,1,5,1,,9724286.0,,5/1/2018 5:54,4.0,68463650.0,"<p>The settings in the TF file were correct.
To change the angle of the vehicle model, I made the following settings.</p>
<ol>
<li>Change the yaw setting of <code>Baselink to Localizer</code> in the <code>Setup</code> tab (in the direction you want the vehicle model to point).</li>
<li>Set the yaw setting of <code>ndt_matching</code> to offset it.(if baselink angle(1) is -1.55, here it is +1.55)</li>
</ol>
<p>I wrote an article about these issues, Thank you JWCS!</p>
<p><a href=""https://medium.com/yodayoda/localization-with-autoware-3e745f1dfe5d"" rel=""nofollow noreferrer"">https://medium.com/yodayoda/localization-with-autoware-3e745f1dfe5d</a></p>
",9724286.0,1.0,0.0,120741210.0,Cool... Would you like to post an answer that describes what you did to fix it?
2936,46906286,Is the C language supported as a V-REP script,|simulation|robotics|,"<p>I am quite new to V-rep and while I'm reading the documentation, in the section Regular API, all the provided script functions are written for both Lua and C, but officially, Lua is the supported language for scripting.</p>

<p>My question is, can I write the scripts in C ?</p>
",10/24/2017 9:02,,32,1,0,0,,1483023.0,,6/26/2012 14:11,42.0,49824463.0,"<p>No, the only thing you can do is to use the remote API to write an external C++ program that handles the simulation.</p>
",9106031.0,1.0,0.0,,
3678,61001497,How to publish odom (nav_msgs/Odometry) from MD49 encoders outputs?,|python|ros|robotics|odometry|,"<p>I am using MD49 Motor Drive with its motors</p>

<p><a href=""https://www.robot-electronics.co.uk/htm/md49tech.htm"" rel=""nofollow noreferrer"">https://www.robot-electronics.co.uk/htm/md49tech.htm</a></p>

<p><a href=""http://wiki.ros.org/md49_base_controller"" rel=""nofollow noreferrer"">http://wiki.ros.org/md49_base_controller</a></p>

<p>How to subscribe (encoder_l and encoder_r) from md49_base_controller package and publish (vx , vy ,and vth ) as a form odom (nav_msgs/Odometry) ?</p>

<p><strong>There are two problem:</strong></p>

<p>1-The first is that the encoders outputs are not correct ""the package needs to be modified.</p>

<p>2-The second is the I want to create a package that subscribe the right and left wheels encoder counts (encoder_l and encoder_r) and publish (vx , vy ,and vth) as a form odom (nav_msgs/Odometry) to be compatable wth imu MPU9250</p>

<p><a href=""http://wiki.ros.org/robot_pose_ekf"" rel=""nofollow noreferrer"">http://wiki.ros.org/robot_pose_ekf</a></p>

<p><strong>The proposed package is:</strong></p>

<p>1- We have to convert  (encoder_l and encoder_r)  to (RPM_l and RPM_r) as follow:</p>

<pre><code>if (speed_l&gt;128){newposition1 = encoder_l;}
else if  (speed_l&lt;128){ newposition1 = 0xFFFFFFFF-encoder_l;}
else if  (speed_l==128) {newposition1=0;}

newtime1 = millis();
RPM_l = ((newposition1-oldposition1)*1000*60)/((newtime1-oldtime1)*980);
oldposition1 = newposition1;
oldtime1 = newtime1;
delay(250);

if (speed_r&gt;128){ newposition2 = encoder_r;}
else if  (speed_r&lt;128){ newposition2 = 0xFFFFFFFF-encoder_r;}
else if   (speed_r==128) { newposition2=0;}
newtime2 = millis();
RPM_r = ((newposition2-oldposition2)*1000*60)/((newtime2-oldtime2)*980);
oldposition2 = newposition2;
oldtime2= newtime2;
delay(250);
</code></pre>

<p>2- We have to convert (RPM_l and RPM_r) to (vx, vy, and vth) as follow:</p>

<pre><code>vx=(r/2)*RPM_l*math.cos(th)+(r/2)*RPM_r*math.cos(th);
vx=(r/2)*RPM_l*math.sin(th)+(r/2)*RPM_r*math.sin(th);
vth=(r/B)*omega_l-(r/B)*omega_r;
</code></pre>

<p>Hint: r and B are wheel radius and vehicle width respectively.</p>

<p>3- The odom (nav_msgs/Odometry) package is:</p>

<pre><code>#!/usr/bin/env python

import math
from math import sin, cos, pi

import rospy
import tf
from nav_msgs.msg import Odometry
from geometry_msgs.msg import Point, Pose, Quaternion, Twist, Vector3
from md49_messages.msg import md49_encoders

rospy.init_node('odometry_publisher')

odom_pub = rospy.Publisher(""odom"", Odometry, queue_size=50)
odom_broadcaster = tf.TransformBroadcaster()


x = 0.0
y = 0.0
th = 0.0

vx =0.1
vy = -0.1
vth = 0.1

current_time = rospy.Time.now()
last_time = rospy.Time.now()

r = rospy.Rate(1.0)
while not rospy.is_shutdown():
    current_time = rospy.Time.now()


    # compute odometry in a typical way given the velocities of the robot
    dt = (current_time - last_time).to_sec()
    delta_x = (vx * cos(th) - vy * sin(th)) * dt
    delta_y = (vx * sin(th) + vy * cos(th)) * dt
    delta_th = vth * dt

    x += delta_x
    y += delta_y
    th += delta_th

    # since all odometry is 6DOF we'll need a quaternion created from yaw
    odom_quat = tf.transformations.quaternion_from_euler(0, 0, th)

    # first, we'll publish the transform over tf
    odom_broadcaster.sendTransform(
        (x, y, 0.),
        odom_quat,
        current_time,
        ""base_link"",
        ""odom""
    )

    # next, we'll publish the odometry message over ROS
    odom = Odometry()
    odom.header.stamp = current_time
    odom.header.frame_id = ""odom""

    # set the position
    odom.pose.pose = Pose(Point(x, y, 0.), Quaternion(*odom_quat))

    # set the velocity
    odom.child_frame_id = ""base_link""
    odom.twist.twist = Twist(Vector3(vx, vy, 0), Vector3(0, 0, vth))

    # publish the message
    odom_pub.publish(odom)

    last_time = current_time
    r.sleep()
</code></pre>
",4/2/2020 21:32,,3802,2,0,0,,6020636.0,,3/4/2016 23:16,33.0,61122286.0,"<p>First off, you need to import nav_msgs/Odometry as the following:</p>

<p><strong>from nav_msgs.msg import Odometry</strong></p>

<p>You must have a function that performs those conversions and then in rospy.Subscriber import those variables, like this:</p>

<pre><code>def example(data):
    data.vx=&lt;conversion&gt;
    data.vth=&lt;conversion&gt;

def listener():
    rospy.Subscriber('*topic*', Odometry, example)
    rospy.spin()

if __name__ == 'main':
    listener()
</code></pre>

<p>I think this would work</p>
",13185389.0,0.0,1.0,,
4002,67072289,Eye-In-Hand Calibration OpenCV,|python|opencv|camera-calibration|robotics|,"<p>I have a setup where a (2D) camera is mounted on the end-effector of a robot arm - similar to the OpenCV <a href=""https://docs.opencv.org/master/d9/d0c/group__calib3d.html"" rel=""nofollow noreferrer"">documentation</a>:
<a href=""https://i.stack.imgur.com/pRWnu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pRWnu.png"" alt=""enter image description here"" /></a></p>
<p>I want to calibrate the camera and find the transformation from camera to end-effector.
I have already calibrated the camera using this OpenCV guide, <a href=""https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_calib3d/py_calibration/py_calibration.html"" rel=""nofollow noreferrer"">Camera Calibration</a>, with a checkerboard where the undistorted images are obtained.</p>
<p>My problem is about finding the transformation from camera to end-effector. I can see that OpenCV has a function, <a href=""https://docs.opencv.org/master/d9/d0c/group__calib3d.html"" rel=""nofollow noreferrer"">calibrateHandEye()</a>, which supposely should achieve this. I already have the &quot;gripper2base&quot; vectors and are missing the &quot;target2cam&quot; vectors. Should this be based on the size of the checkerboard squares or what am I missing?
Any guidance in the right direction will be appreciated.</p>
",4/13/2021 9:35,67155501.0,6224,1,4,3,0.0,11166171.0,,3/7/2019 14:53,33.0,67155501.0,"<p>You are close to the answer.</p>
<p>Yes, it is based on the size of the checkerboard. But instead of directly taking those parameters and an image, this function is taking target2cam. How to get target2cam? Just simply move your robot arm above the chessboard so that the camera can see the chessboard and take a picture. From the picture of the chessboard and camera intrinsics, you can find target2cam. Calculating the extrinsic from the chessboard is already given in opencv.</p>
<p>Repeat this a couple of times at different robot poses and collect multiple target2cam. Put them calibrateHandEye() and you will get what you need.</p>
",2104452.0,4.0,1.0,118704183.0,"Are you implementing everything in ROS, in ROS we can set the TF of camera and the EF."
3572,58681419,Ultra96 or Pynq-Z2 kit?,|python|arm|iot|robotics|,"<p>Which one is more suitable for my project? I want to build a human gesture mimicking Robotic arm. The application is to be built using an open source PYNQ framework. It requires me to choose either one of the kits Ultra96 or PYNQ-Z2 kit for my project so that they can provide me with it. I am unable to find the exact difference between the functionalities of the two, and when to use which? Please help me.</p>
",11/3/2019 15:02,,198,1,0,0,,6024781.0,,3/6/2016 8:44,6.0,61090911.0,"<p>I have been using the Z2 board and it would seem to work just fine for your project. Although I can't speak for the ultra96 it's worth noting that it is a community board and so is not officially supported by Pynq. </p>
",1748173.0,0.0,0.0,,
1772,17103286,Any alternatives to libhand?,|opengl|computer-vision|robotics|ogre|libhand|,"<p><strong><a href=""http://www.libhand.org/"" rel=""nofollow"">LibHand</a></strong> seems to be tightly integrated with <strong>OGRE</strong> for rendering. And extending any OGRE program beyond the basics is an uphill task.</p>

<p>Are there any alternatives to LibHand? I'm hoping to use it with simple OpenGL programs.</p>
",6/14/2013 7:29,,447,0,2,1,,1630.0,"Santa Clara, CA, USA",8/17/2008 17:03,7263.0,,,,,,24756268.0,"As an experienced SO user, you should know that we're not a replacement for Google."
3937,65649719,PID Control: Is adding a delay before the next loop a good idea?,|c++|robotics|control-theory|pid-controller|,"<p>I am implementing PID control in c++ to make a differential drive robot turn an accurate number of degrees, but I am having many issues.</p>
<p><strong>Exiting control loop early due to fast loop runtime</strong></p>
<p>If the robot measures its error to be less than .5 degrees, it exits the control loop and consider the turn &quot;finished&quot; (the .5 is a random value that I might change at some point). It appears that the control loop is running so quickly that the robot can turn at a very high speed, turn <em>past</em> the setpoint, and exit the loop/cut motor powers, because it <em>was</em> at the setpoint for a short instant. I know that this is the entire purpose of PID control, to accurately reach the setpoint without overshooting, but this problem is making it very difficult to tune the PID constants. For example, I try to find a value of kp such that there is steady oscillation, but there is never any oscillation because the robot thinks it has &quot;finished&quot; once it passes the setpoint. To fix this, I have implemented a system where the robot has to be at the setpoint for a certain period of time before exiting, and this has been effective, allowing oscillation to occur, but the issue of exiting the loop early seems like an unusual problem and my solution may be incorrect.</p>
<p><strong>D term has no effect due to fast runtime</strong></p>
<p>Once I had the robot oscillating in a controlled manner using only P, I tried to add D to prevent overshoot. However, this was having no effect for the majority of the time, because the control loop is running so quickly that 19 loops out of 20, the rate of change of error is 0: the robot did not move or did not move enough for it to be measured in that time. I printed the change in error and the derivative term each loop to confirm this and I could see that these would both be 0 for around 20 loop cycles before taking a reasonable value and then back to 0 for another 20 cycles. Like I said, I think that this is because the loop cycles are so quick that the robot literally hasn't moved enough for any sort of noticeable change in error. This was a big problem because it meant that the D term had essentially no effect on robot movement because it was almost always 0. To fix this problem, I tried using the last non-zero value of the derivative in place of any 0 values, but this didn't work well, and the robot would oscillate randomly if the last derivative didn't represent the current rate of change of error.</p>
<p><strong>Note: I am also using a small feedforward for the static coefficient of friction, and I call this feedforward &quot;f&quot;</strong></p>
<p><strong>Should I add a delay?</strong></p>
<p>I realized that I think the source of both of these issues is the loop running very very quickly, so something I thought of was adding a wait statement at the end of the loop. However, it seems like an overall bad solution to intentionally slow down a loop. Is this a good idea?</p>
<pre><code>turnHeading(double finalAngle, double kp, double ki, double kd, double f){
    std::clock_t timer;
    timer = std::clock();

    double pastTime = 0;
    double currentTime = ((std::clock() - timer) / (double)CLOCKS_PER_SEC);

    const double initialHeading = getHeading();
    finalAngle = angleWrapDeg(finalAngle);

    const double initialAngleDiff = initialHeading - finalAngle;
    double error = angleDiff(getHeading(), finalAngle);
    double pastError = error;

    double firstTimeAtSetpoint = 0;
    double timeAtSetPoint = 0;
    bool atSetpoint = false;

    double integral = 0;
    double derivative = 0;
    double lastNonZeroD = 0;

    while (timeAtSetPoint &lt; .05)
    {
        updatePos(encoderL.read(), encoderR.read());
        error = angleDiff(getHeading(), finalAngle);

        currentTime = ((std::clock() - timer) / (double)CLOCKS_PER_SEC);
        double dt = currentTime - pastTime;

        double proportional = error / fabs(initialAngleDiff);
        integral += dt * ((error + pastError) / 2.0);
        double derivative = (error - pastError) / dt;
        
        //FAILED METHOD OF USING LAST NON-0 VALUE OF DERIVATIVE
        // if(epsilonEquals(derivative, 0))
        // {
        //     derivative = lastNonZeroD;
        // }
        // else
        // {
        //     lastNonZeroD = derivative;
        // }

        double power = kp * proportional + ki * integral + kd * derivative;

        if (power &gt; 0)
        {
            setMotorPowers(-power - f, power + f);
        }
        else
        {
            setMotorPowers(-power + f, power - f);
        }

        if (fabs(error) &lt; 2)
        {
            if (!atSetpoint)
            {
                atSetpoint = true;
                firstTimeAtSetpoint = currentTime;
            }
            else //at setpoint
            {
                timeAtSetPoint = currentTime - firstTimeAtSetpoint;
            }
        }
        else //no longer at setpoint
        {
            atSetpoint = false;
            timeAtSetPoint = 0;
        }
        pastTime = currentTime;
        pastError = error;
    }
    setMotorPowers(0, 0);
}

turnHeading(90, .37, 0, .00004, .12);
</code></pre>
",1/10/2021 2:49,,202,0,3,0,,14486787.0,Austin,10/20/2020 16:15,5.0,,,,,,116072604.0,"You might get more results posting the in the Electrical Engineering group. Lots of experienced control theory people there. Also, if you log position, time, and motor control you can get a lot of info to adjust feedback constants."
4585,76226178,Self balancing robot - how to obtain the elapsed angle from gyroscopic data?,|python|raspberry-pi|robotics|mpu6050|,"<p>I am trying to complete my University project building and programming a self balancing robot in python (using RPi) but have hit a road block in my knowledge.</p>
<p>From the MPU6050 I am able to obtain both acceleration and gyroscopic data, furthermore the angles from them (using atan2 function for accel angles and integrating dt for gyro angles). However I cannot think of a way to code this data so that in a loop of given time (dt) it will turn the change in angle (from gyro data) into the total elapsed angle.</p>
<p>I need to be able to loop the program so that it will run for time (dt), store this data, run the program again but expect + or - the previous stored data.
I will then be able to use both angles from the gyro and accel data and run it through a complimentary filter.</p>
<p>I have uploaded the relevant code below.</p>
<pre><code>import smbus                    #import SMBus module of I2C
import time
from time import sleep
import math

#some MPU6050 Registers and their Address
PWR_MGMT_1   = 0x6B
SMPLRT_DIV   = 0x19
CONFIG       = 0x1A 
GYRO_CONFIG  = 0x1B
INT_ENABLE   = 0x38
ACCEL_XOUT_H = 0x3B
ACCEL_YOUT_H = 0x3D
ACCEL_ZOUT_H = 0x3F
GYRO_XOUT_H  = 0x43
GYRO_YOUT_H  = 0x45
GYRO_ZOUT_H  = 0x47

DT = 0.02
rad_to_deg = 180/(math.pi)



def MPU_Init():
    #write to sample rate register
    bus.write_byte_data(Device_Address, SMPLRT_DIV, 7)
    
    #Write to power management register
    bus.write_byte_data(Device_Address, PWR_MGMT_1, 1)
    
    #Write to Configuration register
    bus.write_byte_data(Device_Address, CONFIG, 0)
    
    #Write to Gyro configuration register
    bus.write_byte_data(Device_Address, GYRO_CONFIG, 24)
    
    #Write to interrupt enable register
    bus.write_byte_data(Device_Address, INT_ENABLE, 1)

def read_raw_data(addr):
    #Accelero and Gyro value are 16-bit
        high = bus.read_byte_data(Device_Address, addr)
        low = bus.read_byte_data(Device_Address, addr+1)
    
        #concatenate higher and lower value
        value = ((high &lt;&lt; 8) | low)
        
        #to get signed value from mpu6050
        if(value &gt; 32768):
                value = value - 65536
        return value


bus = smbus.SMBus(1)    # or bus = smbus.SMBus(0) for older version boards
Device_Address = 0x68   # MPU6050 device address
MPU_Init()

print (&quot; Reading Data of Gyroscope and Accelerometer&quot;)


    
while True:
        #Read Accelerometer raw value
        acc_x = read_raw_data(ACCEL_XOUT_H)
        acc_y = read_raw_data(ACCEL_YOUT_H)
        acc_z = read_raw_data(ACCEL_ZOUT_H)
        
        #Read Gyroscope raw value
        gyro_x = read_raw_data(GYRO_XOUT_H)
        gyro_y = read_raw_data(GYRO_YOUT_H)
        gyro_z = read_raw_data(GYRO_ZOUT_H)
        
        #Full scale range +/- 250 degree/C as per sensitivity scale factor
        Ax = acc_x/16384
        Ay = acc_y/16384
        Az = acc_z/16384
        
        Gx = gyro_x/131.0
        Gy = gyro_y/131.0
        Gz = gyro_z/131.0
        
        gyro_x_angle= Gy * DT 

        accel_x_angle = (math.atan2(Az,Ax)-(math.pi/2))*rad_to_deg

        
time.sleep(DT)
</code></pre>
<p>there seems to be many similar self balancing robot projects but very few programmed in python that i can find
Any help would be much appreciated.</p>
<p>I have experimented with lists, for loops, while loops but cannot seem to obtain the required results.</p>
",5/11/2023 9:35,,101,0,0,0,,21879238.0,,5/11/2023 9:08,2.0,,,,,,,
2893,46136105,Getting a point in the local frame of a simulated robot,|robotics|game-development|coordinate-transformation|kinematics|homogenous-transformation|,"<p>This is for a robot simulation that I'm currently working on. 
If you look at the diagram provided, you'll see two co-ordinate frames. <strong>Frame A</strong>, <strong>Frame B</strong>; and you will find a point <em>p</em>. </p>

<p>Co-ordinate frame <strong>A</strong> is the world-frame, and frame <strong>B</strong> is the local frame for my robot (where the <em>x</em>-axis is the heading-direction of the robot, as per convention). The robot is able to rotate and drive around in the world.</p>

<p>My goal here is to find the point <em>p</em>, expressed in terms of frame <strong>A</strong>, and re-express it in terms of the frame <strong>B</strong>. </p>

<p><a href=""https://i.stack.imgur.com/UjSz6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UjSz6.png"" alt=""My set-up""></a></p>

<p>The standard equation that I would use to implement this would be as follows: </p>

<pre><code>point_in_frameB_x = point_in_frameA_x * cos(theta) - point_in_frameA_y * sin(theta) + t_x
point_in_frameB_y = point_in_frameA_x * sin(theta) + point_in_frameA_y * cos(theta) + t_y
</code></pre>

<p>Where <code>t_x</code> and <code>t_y</code> make up the translation transformation of frame <strong>B</strong> to frame <strong>A</strong>. </p>

<p>However, there are some complications here that prevent me from getting my desired results:</p>

<p>Since the robot can rotate around (with its default pose being with a heading direction north--and this has a rotation of 0 radians), I don't know how I would define <code>t_x</code> and <code>t_y</code> in the above code. Because if the robot has a heading direction (i.e. x-axis) parallel to the y-axis of frame <strong>A</strong>, the translation vector would be different from the situation where the robot has a heading direction parallel to the x-axis of frame <strong>A</strong>.</p>

<p><em>You would notice that the transformation from frame <strong>A</strong> to frame <strong>B</strong> isn't straightforward. I'm using this convention simply because I'm implementing this simulator which uses this convention for its image-frame.</em></p>

<p>Any help would be greatly appreciated. Thanks in advance.</p>
",9/9/2017 23:53,,96,1,0,0,,848423.0,,7/17/2011 6:05,122.0,46173021.0,"<p>Follow <a href=""https://en.wikipedia.org/wiki/Right-hand_rule"" rel=""nofollow noreferrer"">right hand rule</a> for assigning coordinate frames. In your picture axes of either frame A or frame B must be changed. </p>

<p><a href=""https://i.stack.imgur.com/xd1WU.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xd1WU.jpg"" alt=""enter image description here""></a></p>
",1595504.0,0.0,2.0,,
3615,59229077,Custom Python Transpiler,|python|assembly|robotics|transpiler|lego-mindstorms|,"<p><br/>
I am participating in FLL (FIRST LEGO League, a Mindstorm robotics competition) and this year they piloted a program allow teams to code in Python. I have found a way to disassemble the LEGO Mindstorm block language to the LEGO assembly code. How can I write a python-based transpiler to convert the python to the assembly code or vice versa? I will try to upload the assembly code. </p>

<p>Many thanks for any help you can provide!</p>
",12/7/2019 18:32,,167,0,2,0,,11846654.0,"New York, NY, USA",7/27/2019 22:35,33.0,,,,,,104671444.0,maybe ask author of [Transcrypt](https://www.transcrypt.org/) which transpiles Python to JavaScript
4141,69752391,Detecting an empty circular/rectangular on a surface using a pointcloud,|c++|point-cloud-library|point-clouds|robotics|realsense|,"<p>I am using the Intel Realsense L515 lidar to obtain a pointcloud. I need to detect an empty  area (no objects blocking it) on a surface (i.e. a table or a wall) so that a robotic arm has enough room to apply pressure to the surface at that point.</p>
<p>So far I have segmented the pointcloud in PCL in order to find a plane, to first detect the surface.
Afterwards I try segmenting for a 10-20cm circle anywhere in the area in order to attempt to find an empty space for the robot to operate.</p>
<p><a href=""https://i.stack.imgur.com/ElwEF.jpg"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/ElwEF.jpg</a></p>
<p>However as you can see, the circle (yellow points) is hollow instead of a full disc (so there could be objects lying inside it) and it has holes in it so it's not really an empty place on the table as the holes are created by objects eliminated in the planar segmentation stage (in this case that's my foot in the middle of the circle).</p>
<p>Is there any way to obtain the true non-object cluttered area on a planar surface?</p>
<p>Current code:</p>
<pre><code>//Plane segmentation processing start
pcl::ModelCoefficients::Ptr coefficients (new pcl::ModelCoefficients);
pcl::PointIndices::Ptr inliers (new pcl::PointIndices);
// Create the plane segmentation object
pcl::SACSegmentation&lt;pcl::PointXYZ&gt; seg;
// Optional
seg.setOptimizeCoefficients (true);
// Mandatory
seg.setModelType (pcl::SACMODEL_PLANE);
seg.setMethodType (pcl::SAC_RANSAC);
seg.setDistanceThreshold (0.005);

// Create the filtering object
pcl::ExtractIndices&lt;pcl::PointXYZ&gt; extract;

// Segment the largest planar component from the remaining cloud
seg.setInputCloud(cloud);
pcl::ScopeTime scopeTime(&quot;Test loop plane&quot;);
{
    seg.segment(*inliers, *coefficients);
}
if (inliers-&gt;indices.size() == 0)
{
    std::cerr &lt;&lt; &quot;Could not estimate a planar model for the given dataset.&quot; &lt;&lt; std::endl;
}

// Extract the inliers
extract.setInputCloud(cloud);
extract.setIndices(inliers);
extract.setNegative(false);
extract.filter(*cloud_p);
std::cerr &lt;&lt; &quot;PointCloud representing the planar component: &quot; &lt;&lt; cloud_p-&gt;width * cloud_p-&gt;height &lt;&lt; &quot; data points.&quot; &lt;&lt; std::endl;

//Circle segmentation processing start
pcl::ModelCoefficients::Ptr coefficients_c (new pcl::ModelCoefficients);
pcl::PointIndices::Ptr inliers_c (new pcl::PointIndices);
// Create the circle segmentation object
pcl::SACSegmentation&lt;pcl::PointXYZ&gt; seg_c;
// Optional
seg_c.setOptimizeCoefficients (true);
// Mandatory
seg_c.setModelType (pcl::SACMODEL_CIRCLE2D);
seg_c.setMethodType (pcl::SAC_RANSAC);
seg_c.setDistanceThreshold (0.01);
seg_c.setRadiusLimits(0.1,0.2);

// Create the filtering object
pcl::ExtractIndices&lt;pcl::PointXYZ&gt; extract_c;

// Segment a circle component from the remaining cloud
seg_c.setInputCloud(cloud_p);
pcl::ScopeTime scopeTime2(&quot;Test loop circle&quot;);
{
    seg_c.segment(*inliers_c, *coefficients_c);
}
if (inliers_c-&gt;indices.size() == 0)
{
    std::cerr &lt;&lt; &quot;Could not estimate a circle model for the given dataset.&quot; &lt;&lt; std::endl;
}
std::cerr &lt;&lt; &quot;Circle coefficients: \nCenter coordinates: &quot; &lt;&lt; coefficients_c-&gt;values[0] &lt;&lt; &quot; &quot; &lt;&lt; coefficients_c-&gt;values[1] &lt;&lt; &quot; &quot; &lt;&lt; coefficients_c-&gt;values[2] &lt;&lt; &quot; &quot;;

// Extract the inliers
extract_c.setInputCloud(cloud_p);
extract_c.setIndices(inliers_c);
extract_c.setNegative(false);
extract_c.filter(*cloud_c);
std::cerr &lt;&lt; &quot;PointCloud representing the circle component: &quot; &lt;&lt; cloud_c-&gt;width * cloud_c-&gt;height &lt;&lt; &quot; data points.&quot; &lt;&lt; std::endl;
</code></pre>
",10/28/2021 10:25,,653,0,2,2,,17269511.0,,10/28/2021 9:56,7.0,,,,,,123422583.0,"Thank you, I will look into it. However, I expect I might need a way to compensate for the fact that the density of the points is higher closer to the camera than it is farther away from it perhaps, but I will see how impactful that is while testing. What is the parameter that specifies the size of the voxel? I can't seem to find that information in the documentation. Is it the resolution?"
3621,59447623,Webots - Open multiple windows side by side,|robotics|webots|,"<p>I'd like to edit my proto files and my project files side by side in two different windows (I've a big screen / monitor). However seems like I can only create a ""New world"" which will open in the same window, and there is no ""Window"" menu either. Is it possible to have multiple webots world windows simultaneously open?</p>

<p><a href=""https://i.stack.imgur.com/5dZjM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5dZjM.png"" alt=""enter image description here""></a></p>

<p>Thank you!</p>

<p>P.S. I use macOS.</p>
",12/22/2019 19:55,59450666.0,123,1,0,2,,1210650.0,"Seattle, WA, USA",2/15/2012 6:49,483.0,59450666.0,"<p>It's not possible to open several world files simultaneously in Webots. However, you can open several instances of Webots with different world files. But within the same instance of Webots you can edit as many PROTO files as you like. You should simply open them from the <code>File</code> menu, <code>Open Text File...</code>.</p>
",810268.0,1.0,1.0,,
3695,61379723,Recovery behavior clears the obstacle layer,|c++|navigation|mapping|ros|robot|,"<p>I am using RP-Lidar /scan topic together with move_base from navigation stack. Although the obstacle layer is parameterized to get the LaserScan type data from /scan topic, I am recieving the message that ""Recover behavior will clear layer 'obstacles'"". I would like to mention that the UniTest over /scan topic and /odom topic are working fine. Thus in my RVIZ, the obstacle are not shown neither the planner takes them into account to prevent a collision.
For clarity here is my common config file:</p>

<pre><code>footprint: [ [-0.15,-0.15], [0.15,-0.15], [0.15,0.15], [-0.15,0.15] ]
transform_tolerance: 0.5
map_type: costmap
obstacle_layer:
 enabled: true
 obstacle_range: 3.0
 raytrace_range: 3.5
 inflation_radius: 0.2
 track_unknown_space: false
 combination_method: 1
 observation_sources: laser_scan_sensor
 laser_scan_sensor: {sensor_frame: scanmatcher_frame, data_type: LaserScan, topic: /scan, marking: true, clearing: false}
inflation_layer:
  enabled:              true
  cost_scaling_factor:  1.0  
  inflation_radius:     0.2
obstacle_layer:
     enabled: true
     obstacle_range: 5.0
     raytrace_range: 1.0
     observation_sources: ""/scan""
     observation_persistence: 0.0
     inf_is_valid: false
     scan:
       data_type: LaserScan
       topic: scan  
</code></pre>
",4/23/2020 5:08,61400149.0,1761,1,0,0,,13184375.0,,4/1/2020 13:17,18.0,61400149.0,"<p>Your <code>observation_sources</code> should be <code>scan</code> not <code>""/scan""</code>.</p>

<p>As mentioned in the <a href=""http://wiki.ros.org/costmap_2d/hydro/obstacles"" rel=""nofollow noreferrer"">Obstacle Layer wiki</a>:</p>

<blockquote>
  <p>A list of observation source names separated by spaces. This defines
  each of the  namespaces defined below. Each source_name
  in observation_sources defines a namespace in which parameters can be
  set:</p>
  
  <p>~//topic (string, default: source_name)</p>
  
  <p>The topic on which sensor data comes in for this source. Defaults to
  the name of the source.</p>
</blockquote>
",2661982.0,1.0,0.0,,
4467,74726722,How to extract points that belong to a vertical plane from a point cloud using PCL?,|c++|point-cloud-library|robotics|lidar|ransac|,"<p>I want to write a program that takes a point cloud data and extract the set of points that belong to a vertical plane. The process mustn’t take more than 100 milliseconds. What is the best way to do this?</p>
<p>I tried using RANSAC filter but, it's slow and also the result is not good.</p>
<pre><code> pcl::SACSegmentation&lt;pcl::PointXYZ&gt; seg;
    seg.setOptimizeCoefficients(true);
    seg.setModelType(pcl::SACMODEL_PLANE);
    seg.setMethodType(pcl::SAC_RANSAC);
    seg.setMaxIterations(1000);
    seg.setDistanceThreshold(0.01);
</code></pre>
",12/8/2022 7:17,74741042.0,452,1,2,0,,20066403.0,,9/23/2022 2:21,2.0,74741042.0,"<p>First of all, I would recommend to use the latest PCL release (or even the master branch), compiled from source. Compared to PCL 1.9.1, this includes several speed improvements in the sample consensus module. Additionally, by compiling from source, you make sure that you can use everything your computer is capable of (e.g. SIMD instructions).
With the latest PCL release (or master branch) you can also use the parallel RANSAC implementation by calling <code>seg.setNumberOfThreads(0)</code>.
If this is still too slow, you can try to downsample the cloud before passing it to <code>SACSegmentation</code>, e.g. with <a href=""https://pointclouds.org/documentation/classpcl_1_1_random_sample.html"" rel=""nofollow noreferrer"">https://pointclouds.org/documentation/classpcl_1_1_random_sample.html</a></p>
<p>If you only want vertical planes (that is, planes that are parallel to a specified axis), you should use <code>SACMODEL_PARALLEL_PLANE</code> instead of <code>SACMODEL_PLANE</code> and call <a href=""https://pointclouds.org/documentation/classpcl_1_1_s_a_c_segmentation.html#a23abc3e522ccb2b2846a6c9b0cf7b7d3"" rel=""nofollow noreferrer"">https://pointclouds.org/documentation/classpcl_1_1_s_a_c_segmentation.html#a23abc3e522ccb2b2846a6c9b0cf7b7d3</a> and <a href=""https://pointclouds.org/documentation/classpcl_1_1_s_a_c_segmentation.html#a7a2dc31039a1717f83ca281f6970eb18"" rel=""nofollow noreferrer"">https://pointclouds.org/documentation/classpcl_1_1_s_a_c_segmentation.html#a7a2dc31039a1717f83ca281f6970eb18</a></p>
",6540043.0,0.0,0.0,131888874.0,"What do you mean by ""slow""? Have you measured the time it takes? What do you mean with ""the result is not good""? Please be more precise here so I can make a good suggestion. Which PCL version are you using? There are several speed improvements in the latest PCL versions, and also a parallel RANSAC implementation."
1954,23207910,Basic continuous 2D map exploration ( with obstacles ) algorithms,|algorithm|mapping|2d|robotics|,"<p>Here is the problem: There is a map which size is anywhere from 200*200 pixels to 1000*1000 pixels. Each pixel is a third of an inch in length/width.  </p>

<p>The map has walls coming from it (which can be of any size), and can be pre-processed by any means. However, when the problem starts, a robot (of pixel size 18*18) is placed in an unknown location, along with several obstacles and one goal, all of which are in an unknown location.</p>

<p>If the robot bumps into any wall/object/goal it immediately dies. As such it has a simple laser scanner that perfectly sees an 80*80 pixel square ahead of it,centered on the robot.</p>

<p>I have already solved the problem of localizing the robot and determining its position (within a small error) on the grid. I am having a bit of trouble with making a good algorithm for going across the entire map until the goal is found.</p>

<p>I have the idea of making the robot go to the lower right, and somehow sweeping left to right, avoiding obstacles, and walls, until the goal is found, however I am unsure of a good way to do that.</p>

<p>Are there any decent algorithms for such a thing, or are there better ones for what I am trying to do?</p>
",4/21/2014 23:48,,2368,2,0,1,0.0,3513412.0,,4/9/2014 2:49,2.0,23208064.0,"<p>The one and only algorithm that comes to my mind is a simple Lee algorithm. <a href=""http://www.oop.rwth-aachen.de/documents/oop-2007/sss-oop-2007.pdf"" rel=""nofollow"">Here's</a> a pretty decent tutorial about what it does and how it works.</p>

<p>Using this algorithm you should be able to find all the obstacles and eventually find the goal, also you will find the shortest path to the goal.</p>

<p>The only big difference is that you have to move an 80x80 object instead of a 1x1 object, but I will let you deal with the way you will implement this.</p>
",2253278.0,0.0,0.0,,
3461,57675404,Data structure to store a grid (that will have negative indices),|c++|data-structures|ros|robotics|,"<p>I'm studying robotics at the university and I have to implement on my own SLAM algorithm. To do it I will use ROS, Gazebo and C++.</p>

<p>I have a doubt about what data structure I have to use to store the map (and what I'm going to store it, but this is another story).</p>

<p>I have thought to represent the map as a 2D grid and robot's start location is (0,0). But I don't know where exactly is the robot on the world that I have to map. It could be at the top left corner, at the middle of the world, or in any other unknonw location inside the world.</p>

<p>Each cell of the grid will be 1x1 meters. I will use a laser to know where are the obstacles. Using current robot's location, I will set to 1 on all the cells that represent an obstacle. For example, it laser detects an obstacle at 2 meters in front of the robot, I will set to 1 the cell at (0,2).</p>

<p>Using a vector, or a 2D matrix, here is a problem, because, vector and matrices indices start at 0, and there could be more room behind the robot to map. And that room will have an obstacle at (-1,-3).</p>

<p>On this data structure, I will need to store the cells that have an obstacle and the cells that I know they are free.</p>

<p>Which kind of data structure will I have to use?</p>

<p><strong>UPDATE:</strong><br/></p>

<p>The process to store the map will be the following:</p>

<ol>
<li>Robot starts at (0,0) cell. It will detect the obstacles and store them in the map.</li>
<li>Robot moves to (1,0) cell. And again, detect and store the obstacles in the map.</li>
<li>Continue moving to free cells and storing the obstacles it founds.</li>
</ol>

<p>The robot will detect the obstacles that are in front of it and to the sides, but never behind it.</p>

<p>My problem comes when the robot detects an obstacle on a negative cell (like (0,-1). I don't know how to store that obstacle if I have previously stored only the obstacle on ""positive"" cells. So, maybe the ""offset"", it is not a solution here (or maybe I'm wrong).</p>
",8/27/2019 13:07,,1516,6,11,2,0.0,68571.0,"Barcelona, Spain",2/19/2009 19:03,4696.0,57675538.0,"<p>You can use a <code>std::set</code> to represent a grid layout by using a <code>position</code> class you create. It contains a <code>x</code> and <code>y</code> variable and can therefore be used to intuitively be used to find points inside the grid. You can also use a <code>std::map</code> if you want to store information about a certain location inside the grid.</p>

<p>Please don't forget to fulfill the C++ named requirements for <a href=""https://en.cppreference.com/w/cpp/container/set/set"" rel=""nofollow noreferrer""><code>set</code></a>/<a href=""https://en.cppreference.com/w/cpp/container/map/map"" rel=""nofollow noreferrer""><code>map</code></a> such as <a href=""https://en.cppreference.com/w/cpp/named_req/Compare"" rel=""nofollow noreferrer"">Compare</a> if you don't want to provide a comparison operator externally.</p>

<p>example:
position.h</p>

<pre><code>/* this class is used to store the position of things
 * it is made up by a horizontal and a vertical position.
 */
class position{
private:
    int32_t horizontalPosition;
    int32_t verticalPosition;
public:
    position::position(const int hPos = 0,const int vPos = 0) : horizontalPosition{hPos}, verticalPosition{vPos}{}
    position::position(position&amp; inputPos) : position(inputPos.getHorPos(),inputPos.getVerPos()){}
    position::position(const position&amp; inputPos) : position((inputPos).getHorPos(),(inputPos).getVerPos()){}

    //insertion operator, it enables the use of cout on this object: cout &lt;&lt; position(0,0) &lt;&lt; endl;
    friend std::ostream&amp; operator&lt;&lt;(std::ostream&amp; os, const position&amp; dt){
        os &lt;&lt; dt.getHorPos() &lt;&lt; "","" &lt;&lt; dt.getVerPos();
        return os;
    }

    //greater than operator
    bool operator&gt;(const position&amp; rh) const noexcept{
        uint64_t ans1 = static_cast&lt;uint64_t&gt;(getVerPos()) | static_cast&lt;uint64_t&gt;(getHorPos())&lt;&lt;32;
        uint64_t ans2 = static_cast&lt;uint64_t&gt;(rh.getVerPos()) | static_cast&lt;uint64_t&gt;(rh.getHorPos())&lt;&lt;32;

        return(ans1 &lt; ans2);
    }

    //lesser than operator
    bool operator&lt;(const position&amp; rh) const noexcept{
        uint64_t ans1 = static_cast&lt;uint64_t&gt;(getVerPos()) | static_cast&lt;uint64_t&gt;(getHorPos())&lt;&lt;32;
        uint64_t ans2 = static_cast&lt;uint64_t&gt;(rh.getVerPos()) | static_cast&lt;uint64_t&gt;(rh.getHorPos())&lt;&lt;32;

        return(ans1 &gt; ans2);
    }

    //equal comparison operator
    bool operator==(const position&amp; inputPos)const noexcept {
        return((getHorPos() == inputPos.getHorPos()) &amp;&amp; (getVerPos() == inputPos.getVerPos()));
    }

    //not equal comparison operator
    bool operator!=(const position&amp; inputPos)const noexcept {
        return((getHorPos() != inputPos.getHorPos()) || (getVerPos() != inputPos.getVerPos()));
    }

    void movNorth(void) noexcept{
        ++verticalPosition;
    }
    void movEast(void) noexcept{
        ++horizontalPosition;
    }
    void movSouth(void) noexcept{
        --verticalPosition;
    }
    void movWest(void) noexcept{
        --horizontalPosition;
    }

    position getNorthPosition(void)const noexcept{
        position aPosition(*this);
        aPosition.movNorth();
        return(aPosition);
    }
    position getEastPosition(void)const noexcept{
        position aPosition(*this);
        aPosition.movEast();
        return(aPosition);
    }
    position getSouthPosition(void)const noexcept{
        position aPosition(*this);
        aPosition.movSouth();
        return(aPosition);
    }
    position getWestPosition(void)const noexcept{
        position aPosition(*this);
        aPosition.movWest();
        return(aPosition);
    }

    int32_t getVerPos(void) const noexcept {
        return(verticalPosition);
    }
    int32_t getHorPos(void) const noexcept {
        return(horizontalPosition);
    }
};
</code></pre>

<pre><code>std::set&lt;position&gt; gridNoData;
std::map&lt;position, bool&gt; gridWithData;

gridNoData.insert(point(1,1));
gridWithData.insert(point(1,1),true);

gridNoData.insert(point(0,0));
gridWithData.insert(point(0,0),true);

auto search = gridNoData.find(point(0,0));
if (search != gridNoData.end()) {
    std::cout &lt;&lt; ""0,0 exists"" &lt;&lt; '\n';
} else {
    std::cout &lt;&lt; ""0,0 doesn't exist\n"";
}

auto search = gridWithData.find(point(0,0));
if (search != gridWithData.end()) {
    std::cout &lt;&lt; ""0,0 exists with value"" &lt;&lt; search-&gt;second  &lt;&lt; '\n';
} else {
    std::cout &lt;&lt; ""0,0 doesn't exist\n"";
}
</code></pre>

<p><br>
The above class was used by me in a similar setting and we used a <code>std::map</code> defined as: </p>

<pre><code>std::map&lt;position,directionalState&gt; exploredMap;
</code></pre>

<p>To store if we had found any walls at a certain position.
<br>
By using this <code>std::map</code> based method you avoid having to do math to know what offset you have to have inside an 2D array (or some structure like that). It also allows you to move freely as there is no chance that you'll travel outside of the predefined bounds you set at construction. This structure is also more space efficient against a 2D array as this structure only saves the areas where the robot has been. This is also a C++ way of doing things: relying on the STL instead of creating your own 2D map using C constructs.</p>
",8548828.0,1.0,1.0,101798646.0,I have updated my question to better clarify my problem.
2567,39688964,Robots moving boxes,|c#|robot|,"<p>This is an exam question I couldn't solve and I need to solve it because I can face it in my next exam again ( and it decides whether you get a D or an A).
<br>
<br><b>The problem:</b><br><br></p>
<blockquote>
<p>&quot;Two robots R1 and R2 carry boxes around a factory. R1 can carry 1 or 3 or 5 boxes at once, whereas R2 can carry 2 or 4 boxes at once. If there are 34 boxes, write a C# program that finds every movement combination of robot R1 and R2 carrying all the boxes. The movements occur in such a way that one robot may move after the other one (R1 gets the boxes, carries them in the required destination, and then R2 can go next). Also show which combination allows carrying all the boxes with minimal movement.</p>
<p>Possible combination: (R1=5,R2=4), (R1=3,R2=4), (R1=3,R2=2), (R1=3,R2=2), (R1=3,R2=2), (R1=1,R2=2)&quot;</p>
</blockquote>
<p>The problem is that <b>I don't even know where to start</b>. I wrote some possible combinations hoping that I might get a clue to start somewhere. I tried a program, but it didn't work (printed the numbers of boxes until the number of boxes after being taken from the robots was not negative: boxes-(r1+r2)&gt;=0, which is one specific case out of every possible combination)</p>
<p>I found a program from an older student who sent me the following windows form code:<br></p>
<pre><code>            private void button1_Click(object sender, EventArgs e)
    {
        int Min = 34;
        string stMin = &quot;&quot;;
        for(int i1=0;i1&lt;=34;i1++)
            for (int i2 = 0; i2 &lt;= 34; i2++)
                for (int i3 = 0; i3 &lt;= 34; i3++)
                    for (int j1 = 0; j1 &lt;= 34; j1++)
                        for (int j2 = 0; j2 &lt;= 34; j2++)
                            for (int j3 = 0; j3 &lt;= 34; j3++)
                            {
                                if (i1 * 3 + i2 * 4 + i3 * 5 + j1 * 1 + j2 * 2 + j3 * 3 == 34 &amp;&amp; i1 &gt; 0 &amp;&amp; i2 &gt; 0 &amp;&amp; i3 &gt; 0 &amp;&amp; j1 &gt; 0 &amp;&amp; j2 &gt; 0 &amp;&amp; j3 &gt; 0 &amp;&amp; (i1 + i2 + i3 == j1 + j2 + j3))
                                {
                                    if(i1+i2+i3+j1+j2+j3&lt;Min)
                                    {
                                        Min = i1 + i2 + i3 + j1 + j2 + j3;
                                        stMin = &quot;R1 =&gt;&quot; + i1 + &quot; x 3, &quot; + i2 + &quot; x 4 &quot; + i3 + &quot; x 5 &quot; + &quot;R2 =&gt;&quot; + j1
                                            + &quot; x 1 &quot; + j2 + &quot; x 2 &quot; + j3 + &quot; x 3 &quot;; 
                                    }
                                    string st = &quot;R1 =&gt;&quot; + i1 + &quot; x 3, &quot; + i2 + &quot; x 4 &quot; + i3 + &quot; x 5 &quot; + &quot;R2 =&gt;&quot; + j1
                                            + &quot; x 1 &quot; + j2 + &quot; x 2 &quot; + j3 + &quot; x 3 &quot;;
                                    listBox1.Items.Add(st);
                                }
                                listBox1.Items.Add(&quot;==========Min=========&quot;);
                                listBox1.Items.Add(stMin);
                            }
    }
</code></pre>
<p>I analyzed it for 3 days but I don't know how this code works. Asked him for explanation but he says it's not his code, doesn't remember where he got it nor knows if it even works.
I also asked friends and colleagues but no one knows how to solve it.</p>
<p>I would appreciate if someone could give me an idea or a piece of code to start with the solution (writing the full code would be great, and no I won't copy paste it into my exam, I will look up to understand every step of the code).
<br>Side info: I am a novice programmer. My professor taugh us basic stuff like reading input from users, using loops and creating classes. My self-learning didn't reach such a complex problem, so please explain your solution as deep and specific as possible.</p>
<p>Thank you in advance</p>
",9/25/2016 16:05,39804790.0,575,1,10,0,0.0,5749161.0,,1/5/2016 18:47,11.0,39804790.0,"<p>Well, since you are still curious let's solve the probem.</p>

<p>It is a classic one and as usual the first thing is to be very clear about the conditions:</p>

<ul>
<li><p>The are these possible combination: (R1=5,R2=4), (R1=3,R2=4), (R1=3,R2=2), (R1=3,R2=2), (R1=3,R2=2), (R1=1,R2=2)</p></li>
<li><p>The starting box count is 34</p></li>
</ul>

<p>This implies that we are <strong>done</strong> when all boxes are gone and also that we are on an <strong>invalid</strong> combination of moves if we have 1 or 2 boxes left, since these can't be moved by any of our combinations.</p>

<p>Let's next create a nice data structure to hold the allowed combinations so we can use it in a loop; let's call those <em>'full moves'</em>:</p>

<pre><code>Dictionary&lt;string, int&gt; fullMoves = new Dictionary&lt;string, int&gt;();
</code></pre>

<p>We will store the moves as strings and will also store the total count of boxes moved by each combination..</p>

<p>Next we need to fill the data structure:</p>

<pre><code>for (int i = 1; i &lt;= 5; i += 2)
    for (int j = 2; j &lt;= 4; j += 2)
        fullMoves.Add(i + ""-"" + j + ""  "", i + j);
</code></pre>

<p>Test this with the debugger to see that is works!</p>

<p>Now let's get down to business: We need a function to do the real work.</p>

<p>As I have explained in my comments this is a typical problem for a <strong>recursive</strong> solution. All the choices create a tree of paths to take (i.e. sequences of full moves) and trees (almost) always call for a recursive aproach. It will call itself over and over again, passing out the current state of affairs until is is done, i.e. has reached a 'halting condition'.</p>

<p>The abstract goal is this:</p>

<ul>
<li>test the situation, and if it is 

<ul>
<li>invalid, discard it</li>
<li>finished, add to a list of solutions</li>
<li>not done: do work (add new options) and repeat for all current paths</li>
</ul></li>
</ul>

<p>Here is a piece of code that does just that. It passes on both a list of current paths and of correct solutions found so far. Also the new path and the count of boxes still left. </p>

<pre><code>void moveBoxes(int count, string curMove, List&lt;string&gt; curMoveList, List&lt;string&gt; solutions)
{
    // test for halting conditions:
    // 1) count == 0: we're done with this solution
    if (count == 0)
    {
        solutions.Add(curMove);
        return;
    }
    // 2) less than three boxes: invalid solution:
    else if (count &lt; 3)
    {
        curMoveList.Remove(curMove);
        return;
    }
    // keep moving..:
    foreach (string cm in curMoveList)
        foreach (string k in fullMoves.Keys)
        {
            int bc = count - fullMoves[k];
            moveBoxes( bc, curMove + k,  curMoveList, solutions );
        }
}
</code></pre>

<p>You can see that is is rather short. This is one feature often found in recursive solution. Another is that it takes a little practice to wrap your head around. Look <a href=""https://stackoverflow.com/questions/30272291/how-can-i-read-listview-column-headers-and-their-values/30272730?s=2|0.2044#30272730"">here for a simpler example</a>, which collects <code>TextBoxes</code> from nested containers in a form! </p>

<p>Let's put it to the test! Here is a testbed that runs the code for a number of box numbers and writes out to the console  how many solutions you get for each starting number. The solution for the last starting number (34) is also written out to a <code>TextBox</code>.</p>

<pre><code>List&lt;string&gt; solutions = new List&lt;string&gt;();
List&lt;string&gt; curMoveList = new List&lt;string&gt;();
for (int ccc = 10; ccc &lt;= 34; ccc++)
{
    solutions = new List&lt;string&gt;();
    curMoveList = new List&lt;string&gt;(); 

    int count = ccc;
    curMoveList.Add("""");

    moveBoxes(count, """", curMoveList, solutions);

    Console.WriteLine(ccc + "" boxes can be moved in "" + solutions.Count + "" ways.\r\n"");
}
StringBuilder sb = new StringBuilder();
foreach (string s in solutions) sb.Append(s.Length / 5 + "" moves:  "" + s + ""\r\n"");
textBox1.Text = sb.ToString();
</code></pre>

<p>Here is the console output:</p>

<blockquote>
  <p>10 boxes can be moved in 8 ways.<br> 11 boxes can be moved in 6
  ways.<br> 12 boxes can be moved in 11 ways.<br> 13 boxes can be moved
  in 18 ways.<br> 14 boxes can be moved in 16 ways.<br> 15 boxes can be
  moved in 36 ways.<br> 16 boxes can be moved in 36 ways.<br> 17 boxes
  can be moved in 58 ways.<br> 18 boxes can be moved in 86 ways.<br> 19
  boxes can be moved in 98 ways.<br> 20 boxes can be moved in 172
  ways.<br> 21 boxes can be moved in 201 ways.<br> 22 boxes can be moved
  in 304 ways.<br> 23 boxes can be moved in 432 ways.<br> 24 boxes can
  be moved in 549 ways.<br> 25 boxes can be moved in 856 ways.<br> 26
  boxes can be moved in 1088 ways.<br> 27 boxes can be moved in 1587
  ways.<br> 28 boxes can be moved in 2220 ways.<br> 29 boxes can be
  moved in 2966 ways.<br> 30 boxes can be moved in 4364 ways.<br> 31
  boxes can be moved in 5798 ways.<br> 32 boxes can be moved in 8284
  ways.<br> 33 boxes can be moved in 11529 ways.<br> 34 boxes can be
  moved in 15760 ways.<br></p>
</blockquote>

<p>Maje sure to collect the output in a <code>Stringbuilder</code>; creating <code>15k</code> strings is expensive and adding them directly to a <code>TextBox</code> take a <strong>real long</strong> time..</p>

<p><strong>Bonus* questions:</strong> </p>

<ul>
<li>Why do I divide by 5 at the end ?-)</li>
<li>Why is the number of solutions <strong>not</strong> always increasing ?-)</li>
</ul>
",3152130.0,2.0,6.0,66679497.0,"Yes. More hints: you know that there are 6 full moves. You can combine them in a string. You need a List<string> next. . For each recursion the recursive function passes on the curren t list and the current count of remaining boxes. And it branches out be adding 6 more string, expanding the current one by each of the 6 full moves. it termnates when less than 3 boxes are left. 0 or 1 os ok, 2 is an illegal result, which must be deleted. finally count the lengths of the list strings. for easier coding put the six moves with a name and their box count into a Dictionary<string, int>.."
3531,58100832,How is it possible for the RPA bot to look constantly in a folder if there is a new File?,|robotics|blueprism|rpa|,"<p>This file is stored in ""Microsoft Azure Storage Explorer"". It should be added to the queue.</p>

<p>It is not sure when the file is added, because it is created by an other application, that has nothing to do with rpa.</p>
",9/25/2019 14:32,,457,1,2,-1,,12040775.0,,9/9/2019 9:25,32.0,58115548.0,"<p>Well, you have action ""Utility - File Management: File exist?"", and you have action ""Utility - General:Sleep"". If you combine these two, then you'll have the answer.</p>
",5779258.0,3.0,1.0,102617719.0,"I mean, how this works. How can I tell blueprism: ""please look constantly if there is a new file. If there is a new file, conduct process XY""."
4223,71254308,"How can i find the position of ""boundary boxed"" object with lidar and camera?",|opencv|ros|robotics|gazebo-simu|pcl|,"<p>This question is related to my final project. In gazebo simulation environment, I am trying to detect obstacles' colors and calculate the distance between robot and obstacles. I am currently identifying their colors with the help of OpenCV methods (<a href=""https://i.stack.imgur.com/V0Pyz.png"" rel=""nofollow noreferrer"">object with boundary box</a>) but I don't know how can i calculate their distances between robot. I have my robot's position. I will not use stereo. I know the size of the obstacles. Waiting for your suggestions and ideas. Thank you!</p>
<p>My robot's topics :</p>
<ul>
<li>cameras/camera/camera_info <em>(Type: sensor_msgs/CameraInfo)</em></li>
<li>cameras/camera/image_raw <em>(Type: sensor_msgs/Image)</em></li>
<li>sensors/lidars/points <em>(Type: sensor_msgs/PointCloud2)</em></li>
</ul>
",2/24/2022 15:24,71259367.0,556,1,2,1,,18010735.0,,1/23/2022 16:44,54.0,71259367.0,"<p>You can project the point cloud into image space, e.g., with OpenCV (as in <a href=""https://stackoverflow.com/questions/25244603/opencvs-projectpoints-function"">here</a>). That way, you can filter all points that are within the bounding box in the image space. Of course, projection errors because of differences between both sensors need to be addressed, e.g., by removing the lower and upper quartile of points regarding the distance to the LiDAR sensor. You can use the remaining points to estimate the distance, eventually.</p>
<p>We have such a system running and it works just fine.</p>
",5296179.0,0.0,0.0,125951364.0,"Sorry for insufficient information. I don't have the positions of the all obstacles. I am trying to find that. I just have my robot's position. I am not using stereo vision. I just want to fuse lidar and my camera.  Camera space is not calibrated Actually, i dont know what that it means. I know the size of the obstacles. I edited my question."
4563,75940959,Bad Latency with v4l2_camera_node in ROS2,|raspberry-pi|ros|robotics|v4l2|,"<p>I am working on a project that provides a camera feed from a microcontroller running ROS2 to a Unity Scene. Currently I am using the v4l2 package and running the v4ls_camera_node to send the data to the Unity scene, however, I am getting terrible latency of about 12 sec. I am not great at working with image processing and was curious if anyone else has ran into this problem and knew of any solutions to try.</p>
<p>I tried changing some of the parameters with no success.</p>
",4/5/2023 14:55,,104,0,1,0,,21177078.0,,2/9/2023 3:28,4.0,,,,,,133958082.0,Please provide enough code so others can better understand or reproduce the problem.
804,3928909,Drive Sequence for Sanyo B00224 Stepper Motor,|microcontroller|arduino|robotics|motordriver|,"<p>I was given a Sanyo #B00224 4 wire stepper motor, and for the life of me cannot determine the drive sequence, e.g. what order to power the coils and in which direction. As far as I can tell it is a bi polar stepper motor and should be drive-able with:</p>

<pre><code>  Winding 1a 1100110011001100110011001
  Winding 1b 0011001100110011001100110
  Winding 2a 0110011001100110011001100
  Winding 2b 1001100110011001100110011
</code></pre>

<p>or:</p>

<pre><code>  Winding 1a 1000100010001000100010001
  Winding 1b 0010001000100010001000100
  Winding 2a 0100010001000100010001000
  Winding 2b 0001000100010001000100010
</code></pre>

<p>Where 1 is power and 0 is ground.</p>

<p>I am really just looking for a datasheet on this motor or any information you might have.</p>

<p>Thanks!</p>
",10/13/2010 23:36,3938164.0,897,1,0,0,,460137.0,"Burlington, VT",9/28/2010 3:19,23.0,3938164.0,"<p>Since it's such a cheap stepper motor, you probably won't be able to get a datasheet on it.</p>

<p>Most 4-wire stepper motors are bipolar, so that's a good assumption. I think that this would make the drive pattern the first one.</p>

<p>You may find that if you are using an Arduino, it just cannot provide the power that you need to make those motors turn.  You may need to use some sort of external motor driver (H-bridge).  I believe that the Arduino motor shield has this functionality built in.  I would try a higher voltage than 5 for some immediate troubleshooting (9 or 12V).  One of the online stores has this motor listed as a 12Vdc stepper, so that's probably the problem.</p>

<p><a href=""http://www.allelectronics.com/make-a-store/item/SMT-125/4-WIRE-STEPPER-MOTOR-W/GEARS/1.html"" rel=""nofollow"">http://www.allelectronics.com/make-a-store/item/SMT-125/4-WIRE-STEPPER-MOTOR-W/GEARS/1.html</a></p>

<p>If you are still having trouble identifying the wires, you can ohm them out with a multimeter to determine the pairs.  To find forward and reverse polarity, you should be able to swap the pairs until the motor correctly drives.</p>
",460065.0,0.0,2.0,,
4452,74301643,About accesing depth images using Python OpenCV,|python|opencv|ros|robotics|realsense|,"<p>I have a array called <code>depth_msg</code> which has a lot of numbers that represents the depth values of a image. When I convert them in <code>cv2</code> message format,</p>
<p>I get this depth image,
<a href=""https://i.stack.imgur.com/YXVU7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YXVU7.png"" alt=""enter image description here"" /></a></p>
<p>And the actual image is ,
<a href=""https://i.stack.imgur.com/WO6lg.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WO6lg.jpg"" alt=""enter image description here"" /></a></p>
<p>How can I get the depth value of the red and yellow bell peppers with the help of the depth image?</p>
<p>My code is,</p>
<pre><code>import cv2
import numpy as np
from cv_bridge import CvBridge

def depth_clbck(depth_msg):

    bridge = CvBridge()
    image = bridge.imgmsg_to_cv2(depth_msg,&quot;32FC1&quot;)
    depth_array = np.array(image, dtype=np.float32)
    depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(image, alpha=0.05), cv2.COLORMAP_HSV)

    cv2.imshow('image', np.array(depth_colormap))
    cv2.waitKey()
</code></pre>
<p>How can I get the depth values of the red and yellow bell-peppers with the help of this depth array and this depth image?</p>
",11/3/2022 10:36,,1966,1,0,0,,13906165.0,,7/10/2020 13:29,6.0,74310544.0,"<p>In order to get the depth value for the red and yellow peeprs in thei mage, you will first have to segment the blobs for the two peppers fro mthe image. Once you have the segmented mask, you can simply obtain through slicing.</p>
<p>For segmentation, you can either manually segment out the peppers or using a image segmentation algorithm - this link shall help you get started:</p>
<p><a href=""https://scikit-image.org/docs/stable/api/skimage.segmentation.html"" rel=""nofollow noreferrer"">https://scikit-image.org/docs/stable/api/skimage.segmentation.html</a></p>
<p>Once a segmentation mask <em>mask</em> is obtained, <em>depth_array[mask&gt;0]</em> will fetch you the depth pixels.</p>
",16744609.0,0.0,0.0,,
4431,74133720,"How to understand ""pre"" in Preintegration of IMU?",|computer-vision|robotics|slam|,"<p>I see <em>preintegration</em> in IMU-fused SLAM literatures, which mention that <em>preintegration is useful in avoiding to recompute the IMU integration between two consecutive keyframes</em>.</p>
<p>However, when I see some open-sourced SLAM code, e.g. OrbSLAM3, I haven't found anything special in preintegration because it just looks like an ordinary integration (with same time-intervals, same interpolation, same full caculation over multiple intervals), and I don't see an expected pre-calculated value that is reused time and time again.</p>
<p>So my question, is <em>preintegration</em> just an alias of the integration of IMU, or else how to correctly understand &quot;<strong>pre</strong>&quot;?</p>
",10/20/2022 2:12,74188571.0,335,1,2,1,,14286161.0,Beijing,9/16/2020 7:53,27.0,74188571.0,"<p>Finally I've made clear of <em><strong>Preintegration</strong></em>.</p>
<p>It is a misunderstanding to think of <em>Preintegration</em> as a pre-calculated value that is reusable in each process for calculating any new integration.</p>
<p><strong>In fact</strong>, <em>Preintegration</em> actually means that people <strong>temporarily</strong> calculate integrations <strong>in a rough way</strong> for incoming any time instant (say, instant <em>0, 1, 2, ..., k-1</em>) and, at a key instant <em>k</em>, a bundle adjustment can be performed on the <em>pre-integrations</em> over time instants [<em>0, k</em>] <strong>without recomputing</strong> the integration on time instant <em>0, 1, 2, ..., k-1</em>.</p>
",14286161.0,1.0,0.0,130889103.0,"@CrisLuengo These two sites are with potential value, thanks!"
3598,58904495,"FTC Robotics code is slow to start, becasue of ""problem with ""imu""""",|java|robotics|,"<p>I am running some code for my FTC robotics team, but when I press init, an orange error appears: ""problem with ""imu"""". There is no variable in our code named imu, and it is an orange runtime error, which is usually caused by a problem with the rev hub. After pressing run, there is a long delay before anything happens, after which the program runs as normal. What does this error mean?</p>
",11/17/2019 19:44,59814602.0,296,1,2,1,,11842161.0,"Moscow, ID",7/26/2019 14:51,25.0,59814602.0,"<p>If you are using two rev hubs that are connect together check to make sure that in the I2C Bus 0 there is not two instances of IMU. Because the IMU is used for tasks like compass and gyro if you have two imu's in your configuration the app gets confused.</p>
",11986567.0,1.0,0.0,104071790.0,Do you know what IMU stands for?
1701,15730886,How to normalize fitness scores?,|java|neural-network|genetic-algorithm|robotics|,"<p>I'm evolving a population of neural networks and I've been struggling with normalizing fitness scores (to values in range 0 to 1), so that the number on its own is most meaningful. The issue is that agents are tested under different conditions - they participate in different games, and for each game a different fitness function is used. Fitness functions look more or less like this:</p>

<pre><code>agentsFitness[indiv][0] += Util.mean(speed) * (games[0].getConstant(0) - Math.sqrt((Math.abs((speed[LEFT] - speed[RIGHT]))) * (games[0].getConstant(1) - Util.normalize(0, 4000, maxIRActivation))));
</code></pre>

<p>but each one will take different inputs. I can easily normalize numbers for each of them separately because I can estimate the maxima and minima of the input. Some of them will be in range of (-30,000, 360,000) and some (0, 900).</p>

<p>Part that I find difficult is that the agents may be tested on two, three or more games at the same time, so their fitness score will be a sum of the scores on all of the games. Additionally, new games can be introduced/evolved. Hard-coding normalization's min and max is not suitable here.</p>

<p>If I try to use very large max and min, I end up with a scores in a range (0.40, 0.45) for the games that have smaller input values which hides the underlying diversity of the scores.</p>

<p>Any suggestions on how these fitness scores could be normalized will be greatly appreciated.</p>
",3/31/2013 15:15,15738248.0,2001,2,0,2,,1770971.0,"Bay Area, CA, United States",10/24/2012 10:52,730.0,15730998.0,"<p>You could use the standard normalized scores:
For each population (in this case each input collection) you can calculate the score of an individual by subtracting the population mean from it, and then dividing it by their standard deviation.</p>

<p>This does not leave you with numbers between 0 and 1 but does allow you to compare two populations with each other</p>
",2062285.0,0.0,0.0,,
4578,76087052,Segmentation fault (core dumped) error when running Python script with pykinect_azure and MoveIt,|python|segmentation-fault|ros|kinect|robotics|,"<p>I'm running a Python script that uses <code>pykinect_azure</code> to track the position of the user's left wrist, and then tries to move a robot arm using the MoveIt library. However, when I run the script, I run into the following error:</p>
<blockquote>
<p>[ INFO] [1682280101.642647859]: Loading robot model 'panda'...
Segmentation fault (core dumped)</p>
</blockquote>
<p>Here is the code I'm using:</p>
<pre class=""lang-py prettyprint-override""><code>import sys
import cv2
import math
import rospy
import pykinect_azure as pykinect
import moveit_commander
from moveit_commander import MoveGroupCommander

pykinect.module_path = '/usr/lib/x86_64-linux-gnu/libk4a.so'

if __name__ == &quot;__main__&quot;:
    # Initialize Kinect and MoveIt libraries
    pykinect.initialize_libraries(track_body=True)
    moveit_commander.roscpp_initialize(sys.argv)
    rospy.init_node('move_robot')

    device_config = pykinect.default_configuration
    device_config.color_resolution = pykinect.K4A_COLOR_RESOLUTION_OFF
    device_config.depth_mode = pykinect.K4A_DEPTH_MODE_WFOV_2X2BINNED
    device = pykinect.start_device(config=device_config)
    bodyTracker = pykinect.start_body_tracker()
    arm = MoveGroupCommander('panda_arm')

    # Initialize variables
    cv2.namedWindow('Depth image with skeleton', cv2.WINDOW_NORMAL)
    left_wrist_init = None
    left_wrist_prev = None

    while True:
        # Update Kinect
        capture = device.update()
        body_frame = bodyTracker.update()
        ret_depth, depth_color_image = capture.get_colored_depth_image()
        ret_color, body_image_color = body_frame.get_segmentation_image()

        if not ret_depth or not ret_color:
            continue

        # Get left wrist position for each detected body
        num_bodies = body_frame.get_num_bodies()
        for body_idx in range(num_bodies):
            body = body_frame.get_body(body_idx)
            if body.is_valid:
                left_wrist_joint = body.joints[pykinect.K4ABT_JOINT_WRIST_LEFT]

                if left_wrist_init is None:
                    left_wrist_init = left_wrist_joint.position
                    left_wrist_prev = left_wrist_joint.position
                else:
                    #  Calculate the difference in wrist position
                    if left_wrist_joint.position is None:
                        continue
                    wrist_diff = [left_wrist_joint.position[i] - left_wrist_prev[i] for i in range(3)]

                    # Update the target pose of the robot arm
                    current_pose = arm.get_current_pose().pose
                    current_pose.position.x += wrist_diff[0]
                    current_pose.position.y += wrist_diff[1]
                    current_pose.position.z += wrist_diff[2]

                    # Move the robot arm to the new position
                    arm.set_pose_target(current_pose)
                    arm.go()

                    # Update the previous wrist position
                    left_wrist_prev = left_wrist_joint.position
    device.stop()
    moveit_commander.roscpp_shutdown()
</code></pre>
<p>Any ideas on what could cause this? When I use the libraries individually the code runs just fine, but now when combining the two the error occurs. I've tries to use the debugger to narrow it down, but it never catches the error but instead just shuts down shortly after it started. Thanks for any answer!</p>
",4/23/2023 20:12,,183,0,0,1,,16431723.0,,7/12/2021 12:48,23.0,,,,,,,
4424,74063852,How to move a robot arm in a circle,|trigonometry|robotics|kinematics|,"<p>I have a 6 joint robot arm, and I want to move it in a circle. I want parameters to choose the radius, degrees, and resolution/quality of the circle.</p>
<p>How do I do this?</p>
<p><a href=""https://i.stack.imgur.com/YqbUA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YqbUA.png"" alt=""arm"" /></a></p>
",10/14/2022 2:58,74063853.0,403,1,0,0,,15789222.0,"Canada, Earth",4/29/2021 0:58,27.0,74063853.0,"<h2>A quick trig review:</h2>
<p>The hypotenuse is opposite the right angle of the triangle.</p>
<p><a href=""https://i.stack.imgur.com/mczX9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mczX9.png"" alt=""enter image description here"" /></a></p>
<p>The ratio of the height to the hypotenuse is called the sine.</p>
<p>The ratio of the base to the hypotenuse is called the cosine.</p>
<h2>Generating (x,y) coordinates of a circle</h2>
<p>The circle is centered at the point (0,0).
The radius of the circle is 1.
Angles are measured starting from the x-axis.
If we draw a line from the point (0,0) at an angle <em><strong>a</strong></em> from the x-axis, the line will intersect the circle at the point <em><strong>P</strong></em>.</p>
<p><a href=""https://i.stack.imgur.com/vTa1S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vTa1S.png"" alt=""image"" /></a></p>
<p>To generate the coordinates along a circle, let's start with a small example.
We'll use <em><strong>r</strong></em> to refer to the radius of the circle and <em><strong>a</strong></em> to refer to the angles spanned starting from the x-axis.</p>
<p>Let's start with just the five following angles: 0, 90, 180, 270 and 360.</p>
<p>(0 and 360 degrees are the same angle, which is on the positive x-axis).</p>
<pre><code>r = 1

a = 0, 90, 180, 270, 360 (angles in degrees)
</code></pre>
<p>Then, to generate the X and y coordinates along the circle, we use the following equations for each of the angles:</p>
<pre><code>x = r * cos(a)
y = r * sin(a)
</code></pre>
<p>These are the x and y coordinates calculated from the two equations above:</p>
<pre><code>(1, 0)
(0, 1)
(-1, 0)
(0, -1)
(1,0)
</code></pre>
<p>Here's what that looks like on a graph:</p>
<p><a href=""https://i.stack.imgur.com/768cw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/768cw.png"" alt=""image"" /></a></p>
<p>In the above examples, we're only using 4 points, so it doesn't look much like a circle yet.
However, if we use 17 points, we can see the coordinates are approaching a circular shape:</p>
<p><a href=""https://i.stack.imgur.com/kqfNP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kqfNP.png"" alt=""image"" /></a></p>
<h2>Here is a visualization of the math (sin cos wave):</h2>
<p><a href=""https://i.stack.imgur.com/M55sB.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M55sB.gif"" alt=""gif"" /></a></p>
<h2>Here is the Arduino code for a circular movement:</h2>
<pre><code>void moveCircle(float radius, float degrees, float resolution, bool direction)
{
  for (float i = 0; i &lt;= degrees; i += resolution)
  {
    // Get X and Y
    float X = radius * cos(i * DEG_TO_RAD);
    float Y = radius * sin(i * DEG_TO_RAD);

    if (direction)
    {
      // Move circle vertically
      moveTz(X);
      moveTy(Y);
    }
    else
    {
      // Move circle horizontally
      moveTx(X);
      moveZ(Y);
    }
  }
}
</code></pre>
<p>I recommend testing this code by creating a graph in Microsoft Excel to verify that the x y coordinates create a circle.</p>
",15789222.0,0.0,0.0,,
3191,51995191,Using Flexible Collision Library (FCL),|c++11|collision-detection|robotics|,"<p>I am trying to implement self-Collision checks on a manipulator arm using the <em>Flexible Collision Library</em> and its difficult to find examples or tutorials. </p>

<p>The documentation available at for is not verbose enough with examples for integration and I just couldn't wrap my head around it. </p>

<p>Where can I find any suitable resources? </p>
",8/23/2018 22:38,,1717,0,2,0,,4796617.0,"San Francisco, CA, USA",4/16/2015 12:46,48.0,,,,,,94969703.0,"Maybe you can start from reading the code snippets from https://github.com/flexible-collision-library/fcl#interfaces and the related interfaces, which is well documented in the header files."
4644,76830384,How to transfer latest state between threads in rust without using a mutex,|multithreading|rust|real-time|robotics|rust-tokio|,"<p>I am working on a Rust project where I have two distinct threads that serve specific purposes. One is a real-time thread which interfaces directly with a piece of hardware and maintains strict timing requirements, while the other is a web server thread that provides an API for querying the current state of said hardware.</p>
<p>I need a mechanism through which the web server thread can acquire the most recent state of the hardware from the real-time thread. The challenge here is to avoid the use of mutexes, as the real-time thread cannot afford to be blocked waiting for the mutex.</p>
<p>Here are a few solutions I have considered and their corresponding challenges:</p>
<ol>
<li><code>rwlock</code>: But the writer thread (real-time thread) would need to wait if any reader has a lock on it.</li>
<li>Double Buffering: The thread that does the swap needs a mutable reference to the entire struct, which causes borrow checker problems.</li>
<li><code>mpsc Channel</code>: If there are no web requests being made, the channel could fill up quickly, leading to wasted memory.</li>
</ol>
",8/3/2023 17:32,,110,2,2,3,,13163184.0,Canada,3/31/2020 7:10,2.0,76830453.0,"<p>You can use the <a href=""https://docs.rs/arc-swap"" rel=""nofollow noreferrer""><code>arc-swap</code></a> crate, that provides the <a href=""https://docs.rs/arc-swap/latest/arc_swap/type.ArcSwap.html"" rel=""nofollow noreferrer""><code>ArcSwap</code></a> type (and related types): an <code>Arc</code> that can be replaced and loaded in a lock-free fashion.</p>
",7884305.0,2.0,1.0,135452127.0,"@KevinReid It's a struct representing all the states aggregated from all parts of a robot, about 10 or so f32s and some booleans some enums"
3375,55787126,Can someone explain how to add the slider object so it controls its own individual servo motor?,|user-interface|arduino|processing|computer-science|robotics|,"<p>The problem is that every time I add a slider object and try to connect it to a different servo motor through a different pin the sliders both only control the same servo motor. It won't allow for me to add a servo motor that controls each servo motor independently through the user-interface. I'm using Processing as the interface and Arduino as the IDE. </p>

<p>I have tried adding other slider objects but they all still control the same servo. I do not know if the issue is through Arrduino or Processing. When I add the other sliders, I connect them to their own pins but it still doesn't allow for them to be controlled individually.</p>

<p>Processing code:</p>

<pre><code>import processing.serial.*;
import cc.arduino.*;
import controlP5.*;
ControlP5 controlP5;
Arduino arduino;
int servoAngle = 90;
void setup() {

 size(400,400);

 println(Arduino.list());
 arduino = new Arduino(this, Arduino.list()[0], 57600);

 for (int i = 0; i &lt;= 13; i++)
 arduino.pinMode(i, Arduino.OUTPUT);

 controlP5 = new ControlP5(this);
 controlP5.addSlider(""servoAngle"",0,180,servoAngle,20,10,180,20);

}
void draw() {
 arduino.analogWrite(9, servoAngle);
 //delay(15);
}
</code></pre>

<p>Arduino code:</p>

<pre><code>    #include &lt;Servo.h&gt;
    #include &lt;Firmata.h&gt;
    Servo servos[MAX_SERVOS];
    byte servoPinMap[TOTAL_PINS];
    byte servoCount = 0;

void analogWriteCallback(byte pin, int value)
{
  if (IS_PIN_DIGITAL(pin)) {
    servos[servoPinMap[pin]].write(value);
  }
}

void systemResetCallback()
{
  servoCount = 0;
}

void setup()
{
  byte pin;

  Firmata.setFirmwareVersion(FIRMATA_FIRMWARE_MAJOR_VERSION, 
  FIRMATA_FIRMWARE_MINOR_VERSION);
  Firmata.attach(ANALOG_MESSAGE, analogWriteCallback);
  Firmata.attach(SYSTEM_RESET, systemResetCallback);

  Firmata.begin(57600);
  systemResetCallback();

  // attach servos from first digital pin up to max number of
  // servos supported for the board
  for (pin = 0; pin &lt; TOTAL_PINS; pin++) {
    if (IS_PIN_DIGITAL(pin)) {
      if (servoCount &lt; MAX_SERVOS) {
        servoPinMap[pin] = servoCount;
        servos[servoPinMap[pin]].attach(PIN_TO_DIGITAL(pin));
        servoCount++;
      }
    }
  }
}

void loop()
{
  while (Firmata.available())
    Firmata.processInput();
}
</code></pre>

<p>I would like to be able to add 3 more sliders that to the one already created but be able to control 4 servo motors, each controlled by its own slider, but the result I'm getting is that each additional slider is controlling the same motors.</p>
",4/21/2019 21:59,,285,1,0,0,,11392380.0,"Chicago, IL, USA",4/21/2019 21:38,1.0,56297074.0,"<p>totally depend on which hardware you are running.
but i think you have to change a few places</p>

<p>(1) decelar all trackbar value and the pwm</p>

<pre><code> int servoAngle = 90; int servoAngle1 = 90; int servoAngle2 = 90; int servoAngle3 = 90;
</code></pre>

<p>(2)
is add track bar</p>

<pre><code>controlP5.addSlider(""servoAngle"",0,180,servoAngle,20,10,180,20);
controlP5.addSlider(""servoAngle1"",0,180,servoAngle1,20,10,180,20);
controlP5.addSlider(""servoAngle2"",0,180,servoAngle2,20,10,180,20);
controlP5.addSlider(""servoAngle3"",0,180,servoAngle3,20,10,180,20);
</code></pre>

<p>(3) link up the hardware pin. totally hardware depended, not sure which 1 u are using. ill jus tput here as sample</p>

<pre><code> arduino.analogWrite(9, servoAngle);
 arduino.analogWrite(10, servoAngle);
 arduino.analogWrite(11, servoAngle);
 arduino.analogWrite(12, servoAngle);
 //delay(15);
}
</code></pre>

<p>that should more or less do it. You know many years ago, we have to write our own processing to arduino serial protocol to do this.</p>
",11530294.0,-1.0,0.0,,
3714,62046666,Find 3D coordinate with respect to the camera using 2D image coordinates,|matlab|image-processing|robotics|coordinate-systems|,"<p>I need to calculate the X,Y coordinates in the world with respect to the camera using u,v coordinates in the 2D image. I am using an S7 edge camera to send a 720x480 video feed to MATLAB.</p>

<p>What I know: Z i.e the depth of the object from the camera, size of the camera pixels (1.4um), focal length (4.2mm)</p>

<p>Let's say the image point is at (u,v) = (400,400).</p>

<p>My approach is as follows:</p>

<ol>
<li>Subtract the pixel value of center point (240,360) from the u,v pixel coordinates of the point in the image. This should give us the pixel coordinates with respect to the camera's optical axis (z axis). The origin is now at the center of the image. So new coordinates are: (160, -40)</li>
<li>Multiply the new u,v pixel values with pixel size to obtain the distance of the point from the origin in physical units. Let's call it (x,y). We get (x,y) = (0.224,-0.056) in mm units.</li>
<li>Use the formula X = xZ/f &amp; Y = yZ/f to calculate X,Y coordinates in the real world with respect to the camera's optical axis.</li>
</ol>

<p>Is my approach correct? </p>
",5/27/2020 15:19,,7616,1,0,2,0.0,8555995.0,,9/3/2017 20:44,27.0,62047722.0,"<p>Your approach is going in the right way, but it would be easier if you use a more standardize approach. What we usually do is use <a href=""https://en.wikipedia.org/wiki/Pinhole_camera_model"" rel=""nofollow noreferrer"">Pinhole Camera Model</a> to give you a transformation between the world coordinates <code>[X, Y, Z]</code> to the pixel <code>[x, y]</code>. Take a look in <a href=""https://staff.fnwi.uva.nl/r.vandenboomgaard/IPCV20172018/LectureNotes/CV/PinholeCamera/PinholeCamera.html"" rel=""nofollow noreferrer"">this guide</a> which describes step-by-step the process of building your transformation.</p>

<p>Basically you have to define you Internal Camera Matrix to do the transformation:</p>

<p><a href=""https://i.stack.imgur.com/1UHnW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1UHnW.png"" alt=""enter image description here""></a></p>

<ul>
<li><strong>fx and fy</strong> are your focal length scaled to use as pixel distance. You can calculate this with your FOV and the total pixel in each direction. Take a look <a href=""https://photo.stackexchange.com/questions/97213/finding-focal-length-from-image-size-and-fov"">here</a> and <a href=""http://www.cs.toronto.edu/~jepson/csc420/notes/imageProjection.pdf"" rel=""nofollow noreferrer"">here</a> for more info. </li>
<li><p><strong>u0 and v0</strong> are the piercing point. Since our pixels are not centered in the <code>[0, 0]</code> these parameters represents a translation to the center of the image. (intersection of the optical axis with the image plane provided in pixel coordinates).</p></li>
<li><p>If you need, you can also add a the <strong>skew factor a</strong>, which you can use to correct shear effects of your camera. Then, the Internal Camera Matrix will be:</p></li>
</ul>

<p><a href=""https://i.stack.imgur.com/cBp9s.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cBp9s.png"" alt=""enter image description here""></a></p>

<p>Since your depth is fixed, just fix your Z and continue the transformation without a problem. </p>

<p><strong>Remember:</strong> If you want the inverse transformation (camera to world) just invert you Camera Matrix and be happy!</p>

<p><a href=""https://www.mathworks.com/help/vision/ug/camera-calibration.html#bu0nh2_"" rel=""nofollow noreferrer"">Matlab has also a very good guide for this transformation. Take a look.</a></p>
",11522398.0,4.0,4.0,,
2838,45709339,How to specify a column name and data item in Blueprism?,|c#|sql|vbscript|automation|robotics|,"<p>I'm using Blueprism to work with SQL queries and in my query, the column name is referenced like this <code>[columnName]</code> which is the right way and data items are referenced this way <code>[dataItem]</code>. </p>

<p>For example, ""SELECT * FROM table WHERE <code>[columnName] = [dataItem]</code>"" </p>

<p>The problem with the above is that the BluePrism reads both as columns and outputs an error saying dataItem is not a column name. How do I reference a data item in a query? Blueprism uses C#/VB Script.</p>
",8/16/2017 9:03,45710964.0,997,1,0,2,,4633221.0,,3/4/2015 17:07,25.0,45710964.0,"<p>I figured out what the problem was. I had to append the sql query and the variable. </p>

<p><code>""SELECT * FROM [Customer Details] WHERE [Name] ='"" &amp; [Test] &amp;  ""' ""</code></p>
",4633221.0,0.0,0.0,,
4663,77101051,Integrating Raspberry Camera with ROS2,|camera|ubuntu-20.04|raspberry-pi4|robotics|ros2|,"<p>I am trying to integrate raspberrypi camera into my robotic project. I have tried modules 2 and 3.  same result. I'm using ROS-FOXY( ubuntu 20.04) and ROS HUMBLE(ubuntu 22).
I installed  the driver using</p>
<pre><code>sudo apt install libraspberrypi-bin v4l-utils ros-foxy-v4l2-camera ros-foxy-image-transport-plugins
</code></pre>
<p>vcgencmd get_camera shows <em><strong>&quot; 0 detected 0 supported &quot;</strong></em> both in ros-foxy and ros-humble.</p>
<p>The cameras were detected when with raspberrypi OS, but not with Ubuntu.
<em><strong>using v4l2-ctl --list-devices</strong></em> shows</p>
<pre><code>Cannot open device /dev/video0, exiting.
</code></pre>
<p>Anybody with a clue as to what is missing?</p>
",9/13/2023 23:43,,311,0,0,0,,6351900.0,"Ontario, Canada",5/18/2016 15:31,40.0,,,,,,,
780,3636814,Deciding on the covariance for a Kalman Filter matrixes,|math|probability|robotics|,"<p>I am beginning to explore using probability in my robotics applications. My goal is to progress to full SLAM, but I am starting with a more simple Kalman Filter to work my way up.</p>

<p>I am using Extended Kalman Filter, with state as [X,Y,Theta]. I use control input [Distance, Vector], and I have an array of 76 laser ranges [Distance,Theta] as my measurement input.</p>

<p>I am having trouble knowing how to decide on the covariance to use in my Gaussian function. Because my measurements are uncertain (The laser is about 1cm accurate at &lt; 1meter, but can be up to 5cm accurate at ranges higher) I do not know how to create the 'function' to estimate the probability of this. I know this function is supposed to 'linearize' to be used, but I'm not sure how to go about this.</p>

<p>I am reasonably confident on how to decide on the function for my state Gaussian, I am happy to use a plain old mean=0,variance=1 on this.. This should work no? I would appreciate some help from people understanding Kalman Filters, because I think I may be missing something.</p>
",9/3/2010 14:39,3636999.0,551,2,0,5,0.0,282090.0,,1/10/2010 12:34,138.0,3636999.0,"<p><a href=""http://www.roboticsproceedings.org/rss01/p38.pdf"" rel=""nofollow noreferrer"">This</a> paper could be a good starting point for you, but you might just choose to manually tweak the values. That's probably good enough for your application.</p>
",436667.0,4.0,1.0,,
4124,69553909,How can I get z position of hector_quadrotor in ROS?,|c++|simulation|ros|robotics|,"<p>I am trying to get z position of hector_quadrotor in simulation. I can get X and Y axis coordinates but I couldn't get Z coordinate. I tried to get it by using GPS but the values are not correct. So I want to get Z coordinate by using barometer or another sensor.</p>
<p>Here the a part of pose_estimation_node.cpp (You can find full version on github source):</p>
<pre><code>void PoseEstimationNode::gpsCallback(const sensor_msgs::NavSatFixConstPtr&amp; gps, const 
geometry_msgs::Vector3StampedConstPtr&amp; gps_velocity) {
  boost::shared_ptr&lt;GPS&gt; m = boost::static_pointer_cast&lt;GPS&gt;(pose_estimation_-&gt;getMeasurement(&quot;gps&quot;));

  if (gps-&gt;status.status == sensor_msgs::NavSatStatus::STATUS_NO_FIX) {
    if (m-&gt;getStatusFlags() &gt; 0) m-&gt;reset(pose_estimation_-&gt;state());
    return;
  }

  GPS::Update update;
  update.latitude = gps-&gt;latitude * M_PI/180.0;
  update.longitude = gps-&gt;longitude * M_PI/180.0;
  update.velocity_north =  gps_velocity-&gt;vector.x;
  update.velocity_east  = -gps_velocity-&gt;vector.y;
  m-&gt;add(update);

  if (gps_pose_publisher_ || sensor_pose_publisher_) {
    geometry_msgs::PoseStamped gps_pose;
    pose_estimation_-&gt;getHeader(gps_pose.header);
    gps_pose.header.stamp = gps-&gt;header.stamp;
    GPSModel::MeasurementVector y = m-&gt;getVector(update, pose_estimation_-&gt;state());

    if (gps_pose_publisher_) {
      gps_pose.pose.position.x = y(0);
      gps_pose.pose.position.y = y(1);
      gps_pose.pose.position.z = gps-&gt;altitude - pose_estimation_-&gt;globalReference()- &gt;position().altitude;
      double track = atan2(gps_velocity-&gt;vector.y, gps_velocity-&gt;vector.x);
      gps_pose.pose.orientation.w = cos(track/2);
      gps_pose.pose.orientation.z = sin(track/2);
      gps_pose_publisher_.publish(gps_pose);
    }

    sensor_pose_.pose.position.x = y(0);
    sensor_pose_.pose.position.y = y(1);
    &quot;&quot;&quot;I add it here&quot;&quot;&quot; 
  }
}
</code></pre>
<p>If I add -----&gt;   sensor_pose_.pose.position.z = gps-&gt;altitude,</p>
<p>I can get a Z coordinate on RVIZ simulation or gnome-terminal. But as I said, the values are very meanless (negative values).</p>
<p>Also ------&gt; gps_pose.pose.position.z = gps-&gt;altitude - pose_estimation_-&gt;globalReference()-
&gt;position().altitude;</p>
<p>It is not working because position().altitude return NAN.
There are another measurement method in pose_estimation_node.cpp like barometer. How can I use barometer value.</p>
<p>Here the another part of pose_estimation_node.cpp:</p>
<pre><code>#if defined(USE_HECTOR_UAV_MSGS)
void PoseEstimationNode::baroCallback(const hector_uav_msgs::AltimeterConstPtr&amp; 
altimeter) {
  boost::shared_ptr&lt;Baro&gt; m = boost::static_pointer_cast&lt;Baro&gt;(pose_estimation_-&gt;getMeasurement(&quot;baro&quot;));
  m-&gt;add(Baro::Update(altimeter-&gt;pressure, altimeter-&gt;qnh));
}

#else
void PoseEstimationNode::heightCallback(const geometry_msgs::PointStampedConstPtr&amp; 
height) {
boost::shared_ptr&lt;Height&gt; m = boost::static_pointer_cast&lt;Height&gt;(pose_estimation_-&gt;getMeasurement(&quot;height&quot;));

Height::MeasurementVector update;
update(0) = height-&gt;point.z;
m-&gt;add(Height::Update(update));

if (sensor_pose_publisher_) {
sensor_pose_.pose.position.z = height-&gt;point.z - m-&gt;getElevation();
  }
}
#endif

void PoseEstimationNode::magneticCallback(const geometry_msgs::Vector3StampedConstPtr&amp; magnetic) {
boost::shared_ptr&lt;Magnetic&gt; m = boost::static_pointer_cast&lt;Magnetic&gt;(pose_estimation_-&gt;getMeasurement(&quot;magnetic&quot;));

Magnetic::MeasurementVector update;
update.x() = magnetic-&gt;vector.x;
update.y() = magnetic-&gt;vector.y;
update.z() = magnetic-&gt;vector.z;
m-&gt;add(Magnetic::Update(update));

if (sensor_pose_publisher_) {
sensor_pose_yaw_ = -(m-&gt;getModel()-&gt;getTrueHeading(pose_estimation_-&gt;state(), update) - pose_estimation_-&gt;globalReference()-&gt;heading());
}
}
</code></pre>
",10/13/2021 10:31,69557069.0,120,1,0,1,,16475993.0,,7/18/2021 19:32,8.0,69557069.0,"<p>To use a barometer you need to actually add that sensor to your robot. What you're looking at is only the callback meaning such messages are supported. If you want to add a new sensor to your robot I'd suggest looking at <a href=""http://gazebosim.org/tutorials/?tut=add_laser"" rel=""nofollow noreferrer"">this tutorial</a>.</p>
<p>All that being said, if you're getting incorrect altitude values you need to go back and recheck your transforms because the localization built into hector should work fine.</p>
",11245187.0,0.0,0.0,,
4208,70837669,"How can I parse ""package://"" in a URDF file path?",|python|path|robotics|urdf|,"<p>I have a robot URDF that points to mesh files using &quot;package://&quot;.</p>
<pre><code>  &lt;geometry&gt;
    &lt;mesh filename=&quot;package://a1_rw/meshes/hip.dae&quot; scale=&quot;1 1 1&quot;/&gt;
  &lt;/geometry&gt;
</code></pre>
<p>I would like to use urdfpy to parse this URDF. However, it is unable to interpret the meaning of &quot;package://&quot;.</p>
<pre><code>import os
from urdfpy import URDF

a1_rw = {
    &quot;model&quot;: &quot;a1&quot;,
    &quot;csvpath&quot;: &quot;a1_rw/urdf/a1_rw.csv&quot;,
    &quot;urdfpath&quot;: &quot;a1_rw/urdf/a1_rw.urdf&quot;
}

model = a1_rw
curdir = os.getcwd()
path_parent = os.path.dirname(curdir)
print(&quot;path parent = &quot;, path_parent)
model_path = model[&quot;urdfpath&quot;]
robot = URDF.load(os.path.join(path_parent, model_path))
</code></pre>
<p>Here is the error message:</p>
<pre><code>$ python3.8 calc_parallax.py
path parent =  /home/ben/Documents/git_workspace/a1_test
Traceback (most recent call last):
  File &quot;calc_parallax.py&quot;, line 18, in &lt;module&gt;
    robot = URDF.load(os.path.join(path_parent, model_path))
  File &quot;/home/ben/.local/lib/python3.8/site-packages/urdfpy/urdf.py&quot;, line 3729, in load
    return URDF._from_xml(node, path)
  File &quot;/home/ben/.local/lib/python3.8/site-packages/urdfpy/urdf.py&quot;, line 3926, in _from_xml
    kwargs = cls._parse(node, path)
  File &quot;/home/ben/.local/lib/python3.8/site-packages/urdfpy/urdf.py&quot;, line 161, in _parse
    kwargs.update(cls._parse_simple_elements(node, path))
  File &quot;/home/ben/.local/lib/python3.8/site-packages/urdfpy/urdf.py&quot;, line 137, in _parse_simple_elements
    v = [t._from_xml(n, path) for n in vs]
  File &quot;/home/ben/.local/lib/python3.8/site-packages/urdfpy/urdf.py&quot;, line 137, in &lt;listcomp&gt;
    v = [t._from_xml(n, path) for n in vs]
  File &quot;/home/ben/.local/lib/python3.8/site-packages/urdfpy/urdf.py&quot;, line 181, in _from_xml
    return cls(**cls._parse(node, path))
  File &quot;/home/ben/.local/lib/python3.8/site-packages/urdfpy/urdf.py&quot;, line 161, in _parse
    kwargs.update(cls._parse_simple_elements(node, path))
  File &quot;/home/ben/.local/lib/python3.8/site-packages/urdfpy/urdf.py&quot;, line 137, in _parse_simple_elements
    v = [t._from_xml(n, path) for n in vs]
  File &quot;/home/ben/.local/lib/python3.8/site-packages/urdfpy/urdf.py&quot;, line 137, in &lt;listcomp&gt;
    v = [t._from_xml(n, path) for n in vs]
  File &quot;/home/ben/.local/lib/python3.8/site-packages/urdfpy/urdf.py&quot;, line 1146, in _from_xml
    kwargs = cls._parse(node, path)
  File &quot;/home/ben/.local/lib/python3.8/site-packages/urdfpy/urdf.py&quot;, line 161, in _parse
    kwargs.update(cls._parse_simple_elements(node, path))
  File &quot;/home/ben/.local/lib/python3.8/site-packages/urdfpy/urdf.py&quot;, line 127, in _parse_simple_elements
    v = t._from_xml(v, path)
  File &quot;/home/ben/.local/lib/python3.8/site-packages/urdfpy/urdf.py&quot;, line 181, in _from_xml
    return cls(**cls._parse(node, path))
  File &quot;/home/ben/.local/lib/python3.8/site-packages/urdfpy/urdf.py&quot;, line 161, in _parse
    kwargs.update(cls._parse_simple_elements(node, path))
  File &quot;/home/ben/.local/lib/python3.8/site-packages/urdfpy/urdf.py&quot;, line 127, in _parse_simple_elements
    v = t._from_xml(v, path)
  File &quot;/home/ben/.local/lib/python3.8/site-packages/urdfpy/urdf.py&quot;, line 581, in _from_xml
    meshes = load_meshes(fn)
  File &quot;/home/ben/.local/lib/python3.8/site-packages/urdfpy/utils.py&quot;, line 225, in load_meshes
    meshes = trimesh.load(filename)
  File &quot;/home/ben/.local/lib/python3.8/site-packages/trimesh/exchange/load.py&quot;, line 111, in load
    ) = parse_file_args(file_obj=file_obj,
  File &quot;/home/ben/.local/lib/python3.8/site-packages/trimesh/exchange/load.py&quot;, line 623, in parse_file_args
    raise ValueError('string is not a file: {}'.format(file_obj))
ValueError: string is not a file: /home/ben/Documents/git_workspace/a1_test/a1_rw/urdf/package://a1_rw/meshes/trunk.dae
</code></pre>
<p>Is there any way to get urdfpy (or another urdf parser) to parse this correctly?</p>
",1/24/2022 17:06,75057263.0,1265,1,0,5,,9922981.0,"Boston, MA",6/11/2018 2:44,9.0,75057263.0,"<p>The behavior you're observing is expected. The documentation for <a href=""https://urdfpy.readthedocs.io/en/latest/generated/urdfpy.URDF.html#urdfpy.URDF.load"" rel=""nofollow noreferrer""><code>urdfpy.URDF.load()</code></a> specifically states:</p>
<blockquote>
<p>Any paths in the URDF should be specified as relative paths to the <code>.urdf</code> file instead of as ROS resources.</p>
</blockquote>
<p>If you want to keep using the same library, the only way to sort this out is to replace the strings in the <code>.urdf</code> file. To do so I suggest using a <a href=""https://docs.python.org/3/library/tempfile.html"" rel=""nofollow noreferrer"">temporary file</a> so that it is automatically discarded once you've loaded your URDF and doesn't actually impact your original urdf.</p>
<pre class=""lang-py prettyprint-override""><code>from pathlib import Path
from urdfpy import URDF
import tempfile

# URDF file (pathlib is a little nicer but not mandatory)
urdf_file_path = Path(&quot;path/to/my/file.urdf&quot;)

# Define how you replace your string. Adjust it so it fits your file organization
ros_url_prefix = &quot;package://&quot;
abs_path_prefix = &quot;/path/to/my/meshes/folder&quot;

# Start with openning a temp dir (context manger makes it easy to handle)
with tempfile.TemporaryDirectory() as tmpdirname:
  
    # Where your tmpfile will be
    tmp_file_path = Path(tmpdirname)/urdf_file_path.name

    # Write each line in fout replacing the ros url prefix with abs path
    with open(urdf_file_path, 'r') as fin:
        with open(tmp_file_path, 'w') as fout:
            for line in fin:
                fout.write(line.replace(ros_url_prefix, abs_path_prefix))

    # Load the urdf from the corrected tmp file
    robot_urdf = load(tmp_file_path)

# Here we get out of tmpfile context manager, so the tmpdir and all its content is erased
robot_urdf.show() # The robot urdf is still accessible
</code></pre>
",12256622.0,1.0,0.0,,
3600,58934308,fast light and accurate person-detection algorithm to run on raspberry pi,|python|computer-vision|raspberry-pi3|object-detection|robotics|,"<p>Hope you are doing well.</p>

<p>I am trying to build a following robot which follows a person.
I have a raspberry pi and and a calibrated stereo camera setup.Using the camera setup,i can find depth value of any pixel with respect to the reference frame of the camera. </p>

<p>My plan is to use feed from the camera to detect person and then using the stereo camera to find the average depth value thus calculating distance and from that calculate the position of the person with respect to the camera and run the motors of my robot accordingly using PID.</p>

<p>Now i have the robot running and person detection using HOGdescriptor that comes opencv.But the problem is,even with nomax suppression, the detector is not stable to implement on a robot as too many false positives and loss of tracking occurs pretty often.</p>

<p><strong>So my question is,can u guys suggest a good way to track only people. Mayb a light NN of some sort,as i plan to run it on a raspberry pi 3b+.
I am using intel d435 as my depth camera. 
TIA</strong></p>
",11/19/2019 12:37,58934824.0,1003,2,1,0,,11381978.0,,4/19/2019 3:55,36.0,58934824.0,"<p>You can use pretrained model. Nowadays there's <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"" rel=""nofollow noreferrer"">plenty</a> of them to choose from. There're also lighter versions for mobile devices. Check <a href=""https://medium.com/@madhawavidanapathirana/real-time-human-detection-in-computer-vision-part-2-c7eda27115c6"" rel=""nofollow noreferrer"">this</a> blog post. It's also worth to check <a href=""https://www.tensorflow.org/lite"" rel=""nofollow noreferrer"">TensorFlow Lite</a>. Some architectures will give you boundig boxes, some masks. Guess you'd be more interested in masks.</p>
",9230562.0,0.0,0.0,104127910.0,Maybe d435 can't help you match and some sort of IR camera will suit better.
1687,15548139,Embedded Systems Bit Count,|arm|avr|robotics|,"<h2>I do apologize if this is a duplicate even though I did search around here for a similar question, I only found one.</h2>

<p>So my programming team in my Engineering class currently use a 32-bit 72MHz ARM Cortex-M3 microprocessor. We're all seniors in high school, and we're struggling to use the libraries and whatnot, mostly due to poor docs from the manufacturer of the Bioloid Premium we're using. However we are about to purchase an 8-bit 16MHz AVR microcontroller because it has a wider range of support online and an easier-to-use library + more documentation. My question here is, would the decreased bit-count as well as the lower processor speed really matter to us? We're not going to be doing a lot of process-intensive programming, but more like a basic robotics class.
So, main differences between an 8-bit 16MHz AVR microprocessor and a 32-bit 72MHz ARM Cortex-M3 microprocessor?
Also, (if it holds any relevancy):</p>

<ol>
<li>We're using a Bioloid Premium by Robotis w/ CM530 (ARM), about to switch to CM510 (AVR).</li>
<li>We'll be using Embedded C instead of Robotis' RoboPlus IDE as our instruction set.</li>
</ol>

<p><em>I have googled around, found out what a bit-count was, and more about it's impact on processor speed, but not a lot of documents about it give a clear and concise answer and that's why I came here, because it's for clear and concise answers. (So please don't tell me to Google it when I've spent the past twenty minutes doing so.)</em></p>
",3/21/2013 12:52,15639663.0,314,2,2,1,,1928611.0,,12/25/2012 18:36,16.0,15564169.0,"<blockquote>
  <p>We're using a Bioloid Premium by Robotis w/ CM530 (ARM), about to
  switch to CM510 (AVR). We'll be using Embedded C instead of Robotis'
  RoboPlus IDE as our instruction set.</p>
</blockquote>

<p>I looked around at the products you refer to, and your question seems to be missing the issues you should really be concerned with.</p>

<p>The Bioloid Premium kit looks pretty sweet, with all the parts put together and configured for you already. Much of robotics courses are usually concerned with designing the hardware. You are not going to be doing any of that. So your tasks really come down to programming the hardware you are given.</p>

<p>That said, there is a world of difference between the RoboPlus IDE, which seems similar to the Lego Mindstorms drag and drop interface, and writing code in C using AVR Studio!</p>

<p>I have used AVR Studio before, but there was a major change in versions recently. You might need to modify the example programs to work in the latest version, and you will probably need some help with that.</p>

<p>It looks like they supply you with enough example code to use the periperpherals, but I don't see right away how to write a main() function to do something like follow a plan. Perhaps, there are some examples online.</p>

<p>But to answer your question, you are probably not going to run into any limitations in terms of processor capacity. They switched to a cheaper and more powerful processor to write the newer version of their control software, but the old hardware will be great, too. Working in C, you will become familiar with how to actually use an MCU, and that knowledge will transfer to other chips. The AVR family is a great one to start with. It has lots of features and is pretty sensible in how it works, with lots of documentation and third-party support. Definitely download the <a href=""http://www.atmel.com/devices/atmega2561.aspx?tab=documents"" rel=""nofollow"">datasheet</a> from Atmel for the chip you are using, although it is a dense and difficult read. You will only need to read parts of it. Also, check out the <a href=""http://www.avrfreaks.net/"" rel=""nofollow"">AVR Freaks</a> forums.</p>

<p>This sounds like a fantastic high school course. Have fun with it! </p>
",66363.0,2.0,0.0,22042439.0,"The answer I think really depends on what ""basic robotics"" is meant: for some basic robotics are circuits that blink leds, for others basic robotics are autonomous learning robots that sweep your house, do your laundry and chase your cat. These different robots require different processing power, which will affect the MCU choices."
2809,44874441,How to convert a rotation matrix to axis angle form?,|matlab|robotics|rotational-matrices|,"<pre><code>theta=acos((trace(R)-1)/2);
if trace(R)==3
    vec = [0 0 0];
    axang=[0 0 0 0];
    vec(1)=R(3,2)-R(2,3);
    vec(2)=R(1,3)-R(3,1);
    vec(3)=R(2,1)-R(1,2);
    vec=(1/(2*sin(theta)))*vec;       
    axang = [vec, theta];
elseif trace(R)==-1
    vec=[0 0 0;0 0 0];
    axang=[0 0 0 0;0 0 0 0];
    X=[0 0];
    Y=[0 0];
    Z=[0 0];
    Y(1)=sqrt((R(2,2)+1)/2);
    Y(2)=-Y(1);
    X(1)=R(2,1)/(2*Y(1));
    X(2)=R(2,1)/(2*Y(2));
    Z(1)=R(2,3)/(2*Y(1));
    Z(2)=R(2,3)/(2*Y(2));
    vec(1,:)=[X(1) Y(1) Z(1)];
    vec(2,:)=[X(2) Y(2) Z(2)];
    axang(1,:)=[vec(1,:), theta];
    axang(2,:)=[vec(2,:), theta];
else 
    vec = [0 0 0];
    axang=[0 0 0 0];
    vec(1)=R(3,2)-R(2,3);
    vec(2)=R(1,3)-R(3,1);
    vec(3)=R(2,1)-R(1,2);
    vec=(1/(2*sin(theta)))*vec;       
    axang = [vec, theta];
end
</code></pre>

<p>So this was my code but it didn't work when the rotation matrix is </p>

<pre><code>R = [-1 0  0;
     0  -1 0;
     0  0  1]
</code></pre>

<p>What is wrong with the code ? <code>axang</code> is a vector that stores axis in the first three positions and the angle in the last position.</p>
",7/2/2017 19:28,,1987,3,2,1,0.0,8245200.0,"Kolkata, West Bengal, India",7/2/2017 18:59,19.0,44874714.0,"<p>It seems to me that you are looking for a conversion of a rotation matrix to quaternions, which is a built-in feature of Matlab if you installed the Robotics System Toolbox, i.e. <a href=""https://mathworks.com/help/robotics/ref/rotm2quat.html"" rel=""nofollow noreferrer""><code>rotm2quat</code>:</a></p>

<pre><code>axang = rotm2quat(R)
</code></pre>

<p>Note that the output format is slightly different as <a href=""https://mathworks.com/help/robotics/ref/rotm2quat.html#bun2_xa-5"" rel=""nofollow noreferrer"">documented by Matlab</a>:</p>

<blockquote>
  <p>Unit quaternion, returned as an n-by-4 matrix containing n
  quaternions. Each quaternion, one per row, is of the form q = [w x y
  z], with w as the scalar number.</p>
</blockquote>

<p>Therefore you may need to swap the columns as follows:</p>

<pre><code>axang = axang(:, [2 3 4 1]);
</code></pre>
",7621674.0,2.0,0.0,76729193.0,Aah. @SardarUsama thanks. New to stack overflow. Fixed my problem anyway. I was dividing by 0 in one case. You ended up helping me when you said 'Provide the relation between rotation matrix and axis angle form'. Thanks anyway.
1891,21044101,How to entegrate existing .m file into the simulink .mdl file,|matlab|model|simulink|robotics|,"<p>I'm using robotic toolbox by Peter Corke in Matlab .  I have .m file for puma560 robot (it is for robot trajectory. The robot follows given path). When I try to use  for ex.  ""sl_ctorque""  simulink file which is in robotic toolbox(it is about computed torque method) , I couldn't entegrate my .m file into the simulink file. My .m file is given below. So if anyone know how to do this idea, I'd appreciate it. Thanks!</p>

<pre><code>clear;clc; 
mdl_puma560    %to create puma robot

for type=1:3  % main for loop. It turns 3 times. At first, it sets the path
    %           to x-y plane and draw the robot, at second for y-z plane
    %           and then for x-z plane

  if type==1 

% The path of robot for x-y plane    
path=[0 0 1;0 0 0;0 2 0 ;0.5 1 0 ;1 2 0;1 0 0;1.5 0 1;1.5 0 0;
      1.5 2 0;2.2 2 0;2.5 1.6 0;2.5 0.4 0;2.2 0 0;1.5 0 0;0 0 1];


 elseif type==2   

% Same thing as first part    
path=[-0.5 0 0;0 0 0;0 0 1;0 -0.5 0.5;0 -1 1;0 -1 0;-0.5 -1.2 0;0 -1.2 0;
    0 -1.2 1;0 -1.7 1;0 -2 0.7;0 -2 0.3;0 -1.7 0;0 -1.2 0];


 elseif type==3

 % Same thing as first and second part     
path=[0 -0.5 0;0 0 0;0 0 1;0.5 0 0.5;1 0 1;1 0 0;1.3 -0.5 0;1.3 0 0;
    1.3 0 1;1.7 0 1;2 0 0.7;2 0 0.3;1.7 0 0;1.3 0 0];


  end



% I created a trajectory

p=mstraj(path, [15 15 15], [], [1 0 1], 0.02 , 0.2);

% [15 15 15] means the maximum speed in x,y,z directions.
% [1 0 1] means the initial coordinates
% 0.02 means acceleration time
% 0.2 means smoothness of robot


numrows(p)*0.2;    % 200 ms sample interval
Tp=transl(0.1*p);  % Scale factor of robot
Tp=homtrans( transl(0.4,0,0),Tp);  % Origin of the letter
q=p560.ikine6s(Tp);   % The inverse kinematic


for i=1:length(q)
% q matrix has 280 rows and 6 columns. So this for loop turns 280 times
% At every turns , it plots one part of movement. q(1,:), q(2,:), ...  

    p560.plot(q(i,:))


end

end
</code></pre>
",1/10/2014 12:12,21045432.0,1209,1,3,2,,3177900.0,,1/9/2014 14:14,14.0,21045432.0,"<p>You need to write your m file as a function and then use the <strong>MATLAB Function Block</strong>.
The MATLAB Function block allows you to add MATLAB® functions to Simulink® models for deployment to desktop and embedded processors. This capability is useful for coding algorithms that are better stated in the textual language of the MATLAB software than in the graphical language of the Simulink product. </p>

<p><img src=""https://i.stack.imgur.com/iwoAA.png"" alt=""enter image description here""></p>

<p>Then you can open the block as paste your function:</p>

<p><img src=""https://i.stack.imgur.com/fjGtl.png"" alt=""enter image description here""></p>

<p>to see an example check out <a href=""http://www.mathworks.com/help/simulink/ug/creating-an-example-model-that-uses-a-matlab-function-block.html"" rel=""nofollow noreferrer"">this page</a>.</p>
",1926629.0,1.0,0.0,31642350.0,"the way it was asked, it sounded as it was specific to the robotics toolbox. See NKN's answer for integrate MATLAB functions in Simulink."
3223,53070070,Create a moving marker. Robot Operating System (ROS),|openstreetmap|ros|path-finding|robotics|planning|,"<p>I'm trying to plan routes in ROS using OSM data and show the results in Rviz, using Python. Until now, my result is the following one:</p>

<p>Correct path computed by the algorithm shown in Rviz:
<a href=""https://i.stack.imgur.com/dxwVy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dxwVy.png"" alt=""Correct path computed by the algorithm shown in Rviz""></a></p>

<p>Now, what I need is to create a marker or something that follows this highlighted path (simulating a car moving on it). </p>

<p>The idea of my project is to simulate a car moving in the correct path and if the car goes to other street while moving because any reason (I will probably intentionally indicate by code that the car deviates), the algorithm is executed again to replan the route from the place the car is to the same final position.</p>

<p>Is there any way to do that simulation of the car moving in rviz? </p>

<p>I am very grateful for your contributions!</p>
",10/30/2018 17:46,,551,1,0,0,,5604964.0,Spain,11/25/2015 15:53,43.0,53084491.0,"<p>You can create your own models using <a href=""http://wiki.ros.org/robot_model/Tutorials"" rel=""nofollow noreferrer"">Robot_model</a> Package and the set it's <code>base_frame</code> to the frame thats moving alongside your highlighted path. </p>

<p>also you can use any existing models(It may not look like the thing you want)
and if you don't know your moving frame you can use Axe inside Rviz to represent the location of the frame at any moment</p>
",7350738.0,1.0,0.0,,
3630,59866125,"Find the next 3D point given a starting point, a orientation quaternion, and a distance travelled",|math|quaternions|robotics|slam|,"<p>What is the formula I need to use to find the second 3D point (P1) given:</p>

<ol>
<li>The first point P0 = [x0, y0, z0]</li>
<li>An orientation quaternion Q0 = [q0, q1, q2, q3]</li>
<li>The distance traveled S</li>
</ol>

<p>I'm guessing that the distance traveled S needs to be split up into it's constituent X, Y and Z components. Is there an easy way to do this using quaternions?</p>
",1/22/2020 18:27,59872425.0,837,1,0,0,0.0,12448898.0,"Vancouver, BC, Canada",11/28/2019 0:20,20.0,59872425.0,"<p>Components of direction vector (forward-vector) are:</p>

<pre><code>x = 2 * (q1*q3 + q0*q2)
y = 2 * (q2*q3 - q0*q1)
z = 1 - 2 * (q1*q1 + q2*q2)
</code></pre>

<p>This formula is calculated from Quaternion-to-Matrix (below) with multiplication by <code>(0,0,1)</code> vector. </p>

<p>Normalize <code>D=(x,y,z)</code> if it is not unit, and calculate <code>P_New.x= P0.x + S * D.x</code> and other components.</p>

<hr>

<p>To get up- and left- vector of orientation (perhaps your orientation refers to another base frame orientation - OX or OY as forward), use another columns of  the matrix cited below:</p>

<p><a href=""https://www.euclideanspace.com/maths/geometry/rotations/conversions/quaternionToMatrix/index.htm"" rel=""nofollow noreferrer"">Link:</a>
Quaternion multiplication and orthogonal matrix multiplication can both be used to represent rotation. If a quaternion is represented by <code>qw + i qx + j qy + k qz</code> , then the equivalent matrix, to represent the same rotation, is:</p>

<pre><code>1 - 2*qy2 - 2*qz2   2*qx*qy - 2*qz*qw   2*qx*qz + 2*qy*qw
2*qx*qy + 2*qz*qw   1 - 2*qx2 - 2*qz2   2*qy*qz - 2*qx*qw
2*qx*qz - 2*qy*qw   2*qy*qz + 2*qx*qw   1 - 2*qx2 - 2*qy2
</code></pre>
",844416.0,0.0,3.0,,
3145,50849484,Translate degrees into 3-motor movement,|python|math|robotics|,"<p>I have a robot with three motors (red), each with an omni-directional (blue) (<a href=""https://www.vexrobotics.com/media/catalog/product/cache/1/image/9df78eab33525d08d6e5fb8d27136e95/2/1/217-2584.jpg"" rel=""nofollow noreferrer"">see example of omniwheel here</a>). If I want the robot to move forward, I can activate both motor 1 and 3, and the wheel at 2 will spin freely, perpendicular to the direction its motor can spin.</p>

<p>How could I write a function that takes degrees as an input and outputs 3 values ranging from 0-1 (no motor speed, full motor speed), so the robot faces the same direction while moving towards the true bearing specified in degrees?</p>

<p>E.g. 45 degrees is inputted and the robot moves North-East, relative to North in the diagram, while maintaining a constant rotation.</p>

<p>Thanks. </p>

<p><a href=""https://i.stack.imgur.com/QD865.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QD865.png"" alt=""Diagram - blue=wheels, red=motors""></a></p>
",6/14/2018 4:10,50849964.0,472,3,1,2,0.0,7675033.0,Australia,3/7/2017 22:12,112.0,50849922.0,"<p>Some thoughts: </p>

<p>Motor rotation velocities V1..V3 should be in ranges -1..1, where -1 is full clockwise rotation, 1 is full CCW rotatation.</p>

<p>Motors create moment of rotation. To exclude rotation of robot, sum of moments should be zero. For equal legs</p>

<pre><code>  V1 + V2 + V3 = 0
</code></pre>

<p>When moments are compensated, every motor causes force along OX or OY axis corresponding to projection of it's velocity onto axis. To provide moving in direction Fi with speed S:</p>

<pre><code>- V1 * Sqrt(3)/2 + V2  - V3 * Sqrt(3)/2 = S * Cos (Fi)   //OX axis
 -V1 / 2 + V3 / 2  = S * Sin (Fi)                         //OY axis   
</code></pre>

<p>Checking for moving up </p>

<pre><code>   Fi  = Pi/2
   V1, V2, V3 = -1, 0, 1
   V1 + V2 + V3 = 0  //moment is OK
   Sqrt(3)/2 - Sqrt(3)/2 = 0  //zero speed or OX
   1/2 + 1/2 = S  //S speed for OY
</code></pre>

<p>In general: solve system of three linear equations, get V1, V2, V3</p>

<pre><code>  V1 + V2 + V3 = 0
- V1 * Sqrt(3)/2 + V2  - V3 * Sqrt(3)/2 = S * Cos (Fi)  
 -V1 / 2 + V3 / 2  = S * Sin (Fi)        
</code></pre>

<p>Solving for 45 degrees gives</p>

<pre><code>   &gt; solve({v1+v2+v3=0,-0.87*v1+v2-0.87*v3=0.71,-0.5*v1+0.5*v3=0.71},{v1,v2,v3});
   {v1 = -.90, v2 = .38, v3 = .52}
</code></pre>

<p>Some transformations to get closed-form solution:</p>

<pre><code>  V2 = -V1 - V3
- V1 * Sqrt(3)/2 -V1 - V3  - V3 * Sqrt(3)/2 = S * Cos (Fi)  
  V1 + V3 =  - 2 * S * Cos (Fi) / (2 + Sqrt(3))
  V3 - V1 =  2 * S * Sin(Fi)
  and finally
  V3 = S * (Sin(Fi) - Cos (Fi) / (2 + Sqrt(3)))
  V1 = - S * (Sin(Fi) + Cos (Fi) / (2 + Sqrt(3))) 
  V2 = -V1 - V3
</code></pre>

<p>If after calculations some velocity has absolute value V that exceeds 1.0, divide all velocities by this value to ensure they are in range    </p>

<p>(of course, real-life dynamic is more complex)</p>
",844416.0,2.0,0.0,88701978.0,"Isn't it just that the speed of the outer edge of a wheel due to rotation (along the motor axis) should be equal to the magnitude of the component of the travel velocity in the direction perpendicular to the motor axis? Cool setup though, I learned that omni wheels exist."
4568,76017501,Roboclaw : [ERROR] : cmd: 59 failed crc_check,|raspberry-pi|ros|robotics|,"<p>I have a problem with my robot. Whenever I try to launch it, I keep getting an error message: [ERROR] cmd:59 failed crc_check (and sometimes [ERROR] cmd:49 failed crc_check).</p>
<p>Although I can see the roboclaw_node in the rqt_graph, but I am unable to send any commands to the motors.
I have a Roboclaw ST 2x45A with the correct configuration.
Can someone please help me identify the problem and suggest a solution?</p>
<p>For the execution and the error I have this:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/ubuntu/catkin_ws/src/drivers/ros_bno055_driver/nodes/bno055_node.py&quot;, line 165, in &lt;module&gt;
    main()
  File &quot;/home/ubuntu/catkin_ws/src/drivers/ros_bno055_driver/nodes/bno055_node.py&quot;, line 136, in main
    node = BNO055Driver()
  File &quot;/home/ubuntu/catkin_ws/src/drivers/ros_bno055_driver/nodes/bno055_node.py&quot;, line 28, in __init__
    if not self.device.begin():
  File &quot;/home/ubuntu/catkin_ws/src/drivers/ros_bno055_driver/nodes/Adafruit_BNO055/BNO055.py&quot;, line 382, in begin
    self._config_mode()
  File &quot;/home/ubuntu/catkin_ws/src/drivers/ros_bno055_driver/nodes/Adafruit_BNO055/BNO055.py&quot;, line 358, in _config_mode
    self.set_mode(OPERATION_MODE_CONFIG)
  File &quot;/home/ubuntu/catkin_ws/src/drivers/ros_bno055_driver/nodes/Adafruit_BNO055/BNO055.py&quot;, line 416, in set_mode
    self._write_byte(BNO055_OPR_MODE_ADDR, mode &amp; 0xFF)
  File &quot;/home/ubuntu/catkin_ws/src/drivers/ros_bno055_driver/nodes/Adafruit_BNO055/BNO055.py&quot;, line 302, in _write_byte
    self._i2c_device.write8(address, value)
  File &quot;/home/ubuntu/catkin_ws/src/drivers/ros_bno055_driver/nodes/Adafruit_GPIO/I2C.py&quot;, line 116, in write8
    self._bus.write_byte_data(self._address, register, value)
  File &quot;/home/ubuntu/catkin_ws/src/drivers/ros_bno055_driver/nodes/Adafruit_PureIO/smbus.py&quot;, line 256, in write_byte_data
    self._device.write(data)
OSError: [Errno 121] Remote I/O error
[ INFO] [1658396096.440849967]: Done initializing likelihood field model.
[bno055_driver-2] process has died [pid 1770, exit code 1, cmd /home/ubuntu/catkin_ws/src/drivers/ros_bno055_driver/nodes/bno055_node.py __name:=bno055_driver __log:=/home/ubuntu/.ros/log/5d988550-08d8-11ed-b5aa-e3ef9a600f4e/bno055_driver-2.log].
log file: /home/ubuntu/.ros/log/5d988550-08d8-11ed-b5aa-e3ef9a600f4e/bno055_driver-2*.log
[WARN] [1658396097.779578]: EXTERNAL_PID expected
[WARN] [1658396097.822070]: Roboclaw Firmware &quot;'USB Roboclaw 2x45a v4.2.8\\n'&quot;
[WARN] [1658396097.833447]: M1 PID 0.000000,0.000000,0.000000,0.000000
[WARN] [1658396097.837850]: M2 PID 0.000000,0.000000,0.000000,0.000000
[WARN] [1658396097.848505]: M1MaxCurrent 10.00 A M2MaxCurrent 10.00 A triggered after 0.200000 s
[ERROR] [1658396098.954525]: cmd: 59 failed crc_check
[ERROR] [1658396098.962611]: Main batt voltage high hysteresis
[WARN] [1658396098.967473]: M1 over current
[WARN] [1658396098.982044]: M2 over current
[WARN] [1658396098.991308]: Main batt voltage high
[ERROR] [1658396099.001628]: cmd: 59 failed crc_check
[ERROR] [1658396099.006135]: End of : Main batt voltage high hysteresis
[WARN] [1658396099.011149]: End of : M1 over current
[WARN] [1658396099.017205]: End of : M2 over current
[WARN] [1658396099.024713]: End of : Main batt voltage high
[ERROR] [1658396099.032435]: cmd: 59 failed crc_check
[ERROR] [1658396099.039953]: cmd: 59 failed crc_check
[ WARN] [1658396099.046330150]: Timed out waiting for transform from base_link to map to become available before running costmap, tf error: canTransform: target_frame map does not exist.. canTransform returned after 0.100352 timeout was 0.1.
[ERROR] [1658396099.054249]: cmd: 59 failed crc_check
[ERROR] [1658396099.068091]: cmd: 59 failed crc_check
[ERROR] [1658396099.093577]: cmd: 59 failed crc_check
[ERROR] [1658396099.119525]: cmd: 59 failed crc_check
[ERROR] [1658396099.147087]: cmd: 59 failed crc_check
[ERROR] [1658396099.167921]: cmd: 59 failed crc_check
[ERROR] [1658396099.193518]: cmd: 59 failed crc_check
[ERROR] [1658396099.222741]: cmd: 59 failed crc_check
Warning: TF_REPEATED_DATA ignoring data with redundant timestamp for frame base_footprint at time 1658396099.246406 according to authority unknown_publisher
         at line 280 in /home/ubuntu/catkin_ws/src/geometry2/tf2/src/buffer_core.cpp
Warning: TF_REPEATED_DATA ignoring data with redundant timestamp for frame base_footprint at time 1658396099.246406 according to authority unknown_publisher
         at line 280 in /home/ubuntu/catkin_ws/src/geometry2/tf2/src/buffer_core.cpp
Warning: TF_REPEATED_DATA ignoring data with redundant timestamp for frame base_footprint at time 1658396099.246406 according to authority unknown_publisher
         at line 280 in /home/ubuntu/catkin_ws/src/geometry2/tf2/src/buffer_core.cpp
[ WARN] [1658396099.256928502]: global_costmap: Parameter &quot;plugins&quot; not provided, loading pre-Hydro parameters
[ERROR] [1658396099.258143]: cmd: 59 failed crc_check
[ERROR] [1658396099.268142]: cmd: 59 failed crc_check
[ERROR] [1658396099.293706]: cmd: 59 failed crc_check
[ INFO] [1658396099.319486335]: global_costmap: Using plugin &quot;static_layer&quot;
[ERROR] [1658396099.320640]: cmd: 59 failed crc_check
[ERROR] [1658396099.352751]: cmd: 59 failed crc_check
[ INFO] [1658396099.357522298]: Requesting the map...
[ERROR] [1658396099.367956]: cmd: 59 failed crc_check
[ERROR] [1658396099.393685]: cmd: 59 failed crc_check
[ERROR] [1658396099.418762]: cmd: 59 failed crc_check
[ERROR] [1658396099.455218]: cmd: 59 failed crc_check
[ERROR] [1658396099.467973]: cmd: 59 failed crc_check
[ERROR] [1658396099.497122]: cmd: 59 failed crc_check
[ERROR] [1658396099.518128]: cmd: 59 failed crc_check
[ERROR] [1658396099.553966]: cmd: 59 failed crc_check
[ERROR] [1658396099.568980]: cmd: 59 failed crc_check
[ INFO] [1658396099.575256002]: Resizing costmap to 2048 X 2048 at 0.050000 m/pix
[ERROR] [1658396099.592809]: cmd: 59 failed crc_check
[ERROR] [1658396099.617774]: cmd: 59 failed crc_check
[ERROR] [1658396099.647493]: cmd: 59 failed crc_check
[ INFO] [1658396099.666048372]: Received a 2048 X 2048 map at 0.050000 m/pix
[ERROR] [1658396099.668508]: cmd: 59 failed crc_check
[ INFO] [1658396099.682784391]: global_costmap: Using plugin &quot;obstacle_layer&quot;
[ERROR] [1658396099.694193]: cmd: 59 failed crc_check
[ INFO] [1658396099.701562409]:     Subscribed to Topics: laser_scan_sensor
[ERROR] [1658396099.719585]: cmd: 59 failed crc_check
[ERROR] [1658396099.744177]: cmd: 59 failed crc_check
[ INFO] [1658396099.767514632]: global_costmap: Using plugin &quot;inflation_layer&quot;
[ERROR] [1658396099.768384]: cmd: 59 failed crc_check
[ERROR] [1658396099.793876]: cmd: 59 failed crc_check
[ERROR] [1658396099.819689]: cmd: 59 failed crc_check
[ERROR] [1658396099.848546]: cmd: 59 failed crc_check
[ERROR] [1658396099.868187]: cmd: 59 failed crc_check
[ERROR] [1658396099.894135]: cmd: 59 failed crc_check
[ERROR] [1658396099.930328]: cmd: 59 failed crc_check
[ERROR] [1658396099.945882]: cmd: 59 failed crc_check```


</code></pre>
",4/14/2023 17:10,,89,0,1,0,,21644642.0,,4/14/2023 16:54,2.0,,,,,,134096975.0,Please provide enough code so others can better understand or reproduce the problem.
1105,5540608,Tegra based robotics platform,|android|serial-port|arm|robotics|i2c|,"<p>I am looking into the possibility of developing a Tegra based robotics platform running Android. To do this I need to be able to preform serial, I2C, and possibly PWM communications, does the Tegra platform have allow this? And does Android support access to this kind of hardware level communication?</p>
",4/4/2011 15:18,,220,2,0,0,,127896.0,,6/23/2009 22:33,170.0,6579235.0,"<p>For communicating with hardware from the Android, I suggest looking at the <a href=""http://developer.android.com/guide/topics/usb/adk.html"" rel=""nofollow"">Android Open Accessory Development Kit</a>. This will allow you to communicate with external devices (e.g. a <a href=""http://shop.moderndevice.com/products/freeduino-usb-host-board"" rel=""nofollow"">Freeduino</a>) over the phone's USB port. Besides allowing you to get the project working without opening the phone, this is generally the path of least resistance.</p>
",111426.0,0.0,0.0,,
3173,51448305,Should samples from np.random.normal sum to zero?,|python|numpy|statistics|robotics|normal-distribution|,"<p>I am working on the motion model of a robot. In every time step, the robot's motion is measured, then I sample the normal distribution with the measurement as the mean and a small sigma value for covariance in order to simulate noise. This noisy motion is then added to the robot's previous state estimate.</p>

<p>But when I keep the robot still, these noisy measurements seem to accumulate and the robot ""thinks it's moving.""</p>

<p>Shouldn't these random samples not accumulate, but sum to zero?</p>

<p>In other words, would you expect the following to be true:</p>

<pre><code>0 ~ np.sum([np.random.normal(0, 0.1) for _ in range(1000)])
</code></pre>

<p>I have tried writing out the above in an explicit loop and seeding the random number generator with a different number before taking every sample, but the sums still deviate far from zero.</p>

<p>Is this simply a limitation of random number generators, or am I misunderstanding the fact(?) that many samples from the normal distribution should sum to zero?</p>
",7/20/2018 18:16,51448556.0,561,1,4,1,,10011845.0,,6/29/2018 16:02,10.0,51448556.0,"<p>The short answer to your question is no. Be careful not to conflate the sum of an array of independent random variables and the mean of those independent random variables.</p>

<p>Per the article that @Hongyu Wang referenced in his comment, let's verify the following:</p>

<p>""If X and Y are independent random variables that are normally distributed, then their sum is also normally distributed.""</p>

<p>Effectively, this is what you have done. You have created an array of independent random variables and taken their sum, which in turn, should be normally distributed.</p>

<p>I have slightly modified your code to demonstrate:</p>

<pre><code>import random, numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

x = [np.sum([np.random.normal(0,0.1) for _ in range(1000)]) for _ in range(1000)]

sns.distplot(x)
plt.show()
</code></pre>

<p>Which yields: <a href=""https://i.stack.imgur.com/lKvQX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lKvQX.png"" alt=""enter image description here""></a></p>

<p>You can verify that your normal distribution is correctly distributed about a mean of <code>0</code>, by doing:</p>

<pre><code>np.mean([np.random.normal(0, 0.1) for _ in range(1000)])
</code></pre>
",8146556.0,1.0,0.0,89867187.0,"No. https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables, notice the std increases"
4493,75192356,unable to get the messages from the mqtt topic to ros topic. ||Mqtt to Ros bridge,|python-3.x|mqtt|ros|robotics|ros2|,"<p>Issue: Trying to get mqtt messages from mqtt_topic to ros topic.
I am trying to control my robot which is in ros environment from the outside server using mqtt protocol. I got connected with the outside server. But i didn't get the messages from the mqtt_topic</p>
<p><img src=""https://i.stack.imgur.com/xcXPY.png"" alt=""enter image description here"" /></p>
<p>I am currently using mqtt bridge package https://github.com/groove-x/mqtt_bridge
Please help me to overcome this issue or please recommend some other mqtt ros bridge packages.</p>
",1/21/2023 9:32,,182,0,4,0,,21054569.0,,1/21/2023 9:16,4.0,,,,,,132690590.0,Please show `demo.launch` and the code you use to verify that nothing is received or the `rostopic echo` call you use instead. Your screenshot is not overly useful (and should be text as hardillb says).
3342,55258526,Accelerometer too noisy,|python|robotics|,"<p>I am working on a group project for building a robot and we are trying to implement an accelerometer that triggers an alarm if the robot is picked up/pushed on the way. Right now our accelerometer is very noisy and not very reliable. Could someone point me in the right direction on how to fix this?
Current code:</p>

<pre><code> def accel_alarm(self):
        cur_accel = self.read_smooth_accel()
        accel_x, accel_y, accel_z = cur_accel
        base_x, base_y, base_z = (-32, 31, 1075)  # values when robot is static
        if abs(accel_x - base_x) &gt; 350 or abs(
                accel_y - base_y) &gt; 350 or abs(accel_z - base_z) &gt; 350:
            if not (self.alarm):
                self.send_alarm()
                self.alarm = True
                print(
                    ""[accel_alarm] ALARM STATE "" + str(accel_x) + "" X "" + str(
                        accel_y) + "" Y  "" + str(accel_z) + "" Z"")

    # Smooth accelerometer output by taking the average of the last n values
    # where n = len(self.accel_data)
    def read_smooth_accel(self):
        cur_accel, _ = self.accel.read()
        self.accel_data.pop()
        self.accel_data.appendleft(cur_accel)
        # For the first len(accel_data) values the average is not
        # representative - just return current value
        if [0, 0, 0] in self.accel_data:
            return cur_accel

        av_accel = [sum(i) / float(len(i)) for i in zip(*self.accel_data)]
        return av_accel
</code></pre>

<p>The second method is trying to tackle the noisiness but it is still not very nice... </p>
",3/20/2019 10:26,,40,0,4,0,,10585692.0,,10/31/2018 11:53,16.0,,,,,,97255737.0,"Basically, when it is turned on now and the robot is even static, it prints out the coordinates to the console every millisecond. We want to try to reduce the number of outputs to only appear when the robot moves more than 10cm in direction."
2579,39719865,ROS custom message and numpy arrays,|numpy|ros|robotics|,"<p>I want custom message to contain numpy arrays(I mean creation of .msg file and compiling it).
As tutorial said we have to use <code>numpy_msg(type)</code> wrapper to be able to send numpy arrays. But is it possible to include it into my own .msg file?</p>
",9/27/2016 8:33,39743101.0,2345,1,0,0,,2157254.0,,3/11/2013 14:43,178.0,39743101.0,"<p>The message file doesn't change, you still use the ROS-style arrays (e.g., <code>float32[]</code>).</p>
<p>The <code>numpy_msg</code> wrapper just enables your publisher and subscribers to directly use numpy objects instead of having to do the conversion yourself.</p>
<p>Make sure to watch out for these warnings:</p>
<blockquote>
<p>all of your array data must be initialized as numpy arrays</p>
<p>every numerical array in the Message must be initialized with a numpy array of the correct data type.</p>
</blockquote>
",256798.0,1.0,0.0,,
3722,62414292,NVIDIA Jetson Nano with Realsense 435i via Isaac - Camera not found,|robotics|nvidia-jetson|realsense|slam|nvidia-jetson-nano|,"<p>I posted about this over on the Isaac forums, but listing it here for visibility as well. I am trying to get the Isaac Realsense examples working on a Jetson Nano with my 435i (firmware downgraded to 5.11.15 per the Isaac documentation), but I've been unable to so far. I've got a Nano flashed with Jetpack4.3 and have installed all dependencies on both the desktop and the Nano. The realsense-viewer works fine, so I know the camera is functioning properly and is being detected by the Nano. However, when I run <code>./apps/samples/realsense_camera/realsense_camera</code> it throws an error:</p>

<pre><code>ERROR engine/alice/components/Codelet.cpp@229: Component 'camera/realsense' of type 'isaac::RealsenseCamera' reported FAILURE:

    No device connected, please connect a RealSense device
ERROR engine/alice/backend/event_manager.cpp@42: Stopping node 'camera' because it reached status 'FAILURE'
</code></pre>

<p>I've attached the log of this output as well. I get the same error running locally on my desktop, but that's running through WSL so I was willing to write that off. Any suggestions would be greatly appreciated!</p>

<pre><code>0m2020-06-15 17:18:20.620 INFO  engine/alice/tools/websight.cpp@166: Loading websight...0m
33m2020-06-15 17:18:20.621 WARN  engine/alice/backend/application_json_loader.cpp@174: This application does not have an explicit scheduler configuration. One will be autogenerated to the best of the system's abilities if possible.0m
0m2020-06-15 17:18:20.622 INFO  engine/alice/backend/redis_backend.cpp@40: Successfully connected to Redis server.
0m
33m2020-06-15 17:18:20.623 WARN  engine/alice/backend/backend.cpp@201: This application does not have an execution group configuration. One will be autogenerated to the best of the systems abilities if possible.0m
33m2020-06-15 17:18:20.623 WARN  engine/gems/scheduler/scheduler.cpp@337: No default execution groups specified. Attempting to create scheduler configuration for 4 remaining cores. This may be non optimal for the system and application.0m
0m2020-06-15 17:18:20.623 INFO  engine/gems/scheduler/scheduler.cpp@290: Scheduler execution groups are:0m
0m2020-06-15 17:18:20.623 INFO  engine/gems/scheduler/scheduler.cpp@299: __BlockerGroup__: Cores = [3], Workers = No0m
0m2020-06-15 17:18:20.623 INFO  engine/gems/scheduler/scheduler.cpp@299: __WorkerGroup__: Cores = [0, 1, 2], Workers = Yes0m
0m2020-06-15 17:18:20.660 INFO  engine/alice/backend/modules.cpp@226: Loaded module 'packages/realsense/librealsense_module.so': Now has 45 components total0m
0m2020-06-15 17:18:20.679 INFO  engine/alice/backend/modules.cpp@226: Loaded module 'packages/rgbd_processing/librgbd_processing_module.so': Now has 51 components total0m
0m2020-06-15 17:18:20.696 INFO  engine/alice/backend/modules.cpp@226: Loaded module 'packages/sight/libsight_module.so': Now has 54 components total0m
0m2020-06-15 17:18:20.720 INFO  engine/alice/backend/modules.cpp@226: Loaded module 'packages/viewers/libviewers_module.so': Now has 83 components total0m
90m2020-06-15 17:18:20.720 DEBUG engine/alice/application.cpp@348: Loaded 83 components: isaac::RealsenseCamera, isaac::alice::BufferAllocatorReport, isaac::alice::ChannelMonitor, isaac::alice::CheckJetsonPerformanceModel, isaac::alice::CheckOperatingSystem, isaac::alice::Config, isaac::alice::ConfigBridge, isaac::alice::ConfigLoader, isaac::alice::Failsafe, isaac::alice::FailsafeHeartbeat, isaac::alice::InteractiveMarkersBridge, isaac::alice::JsonToProto, isaac::alice::LifecycleReport, isaac::alice::MessageLedger, isaac::alice::MessagePassingReport, isaac::alice::NodeStatistics, isaac::alice::Pose, isaac::alice::Pose2Comparer, isaac::alice::PoseFromFile, isaac::alice::PoseInitializer, isaac::alice::PoseMessageInjector, isaac::alice::PoseToFile, isaac::alice::PoseToMessage, isaac::alice::PoseTree, isaac::alice::PoseTreeJsonBridge, isaac::alice::PoseTreeRelink, isaac::alice::ProtoToJson, isaac::alice::PyCodelet, isaac::alice::Random, isaac::alice::Recorder, isaac::alice::RecorderBridge, isaac::alice::Replay, isaac::alice::ReplayBridge, isaac::alice::Scheduling, isaac::alice::Sight, isaac::alice::SightChannelStatus, isaac::alice::Subgraph, isaac::alice::Subprocess, isaac::alice::TcpPublisher, isaac::alice::TcpSubscriber, isaac::alice::Throttle, isaac::alice::TimeOffset, isaac::alice::TimeSynchronizer, isaac::alice::UdpPublisher, isaac::alice::UdpSubscriber, isaac::map::Map, isaac::map::ObstacleAtlas, isaac::map::OccupancyGridMapLayer, isaac::map::PolygonMapLayer, isaac::map::WaypointMapLayer, isaac::navigation::DistanceMap, isaac::navigation::NavigationMap, isaac::navigation::RangeScanModelClassic, isaac::navigation::RangeScanModelFlatloc, isaac::rgbd_processing::DepthEdges, isaac::rgbd_processing::DepthImageFlattening, isaac::rgbd_processing::DepthImageToPointCloud, isaac::rgbd_processing::DepthNormals, isaac::rgbd_processing::DepthPoints, isaac::rgbd_processing::FreespaceFromDepth, isaac::sight::AliceSight, isaac::sight::SightWidget, isaac::sight::WebsightServer, isaac::viewers::BinaryMapViewer, isaac::viewers::ColorCameraViewer, isaac::viewers::DepthCameraViewer, isaac::viewers::Detections3Viewer, isaac::viewers::DetectionsViewer, isaac::viewers::FiducialsViewer, isaac::viewers::FlatscanViewer, isaac::viewers::GoalViewer, isaac::viewers::ImageKeypointViewer, isaac::viewers::LidarViewer, isaac::viewers::MosaicViewer, isaac::viewers::ObjectViewer, isaac::viewers::OccupancyMapViewer, isaac::viewers::PointCloudViewer, isaac::viewers::PoseTrailViewer, isaac::viewers::SegmentationCameraViewer, isaac::viewers::SegmentationViewer, isaac::viewers::SkeletonViewer, isaac::viewers::TensorViewer, isaac::viewers::TrajectoryListViewer, 0m
33m2020-06-15 17:18:20.723 WARN  engine/alice/application.cpp@164: The function Application::findComponentByName is deprecated. Please use `getNodeComponentOrNull` instead. Note that the new method requires a node name instead of a component name. (argument: 'websight/isaac.sight.AliceSight')0m
0m2020-06-15 17:18:20.723 INFO  engine/alice/application.cpp@255: Starting application 'realsense_camera' (instance UUID: 'e24992d0-af66-11ea-8bcf-c957460c567e') ...0m
90m2020-06-15 17:18:20.723 DEBUG engine/gems/scheduler/execution_groups.cpp@476: Launching 0 pre-start job(s)0m
90m2020-06-15 17:18:20.723 DEBUG engine/gems/scheduler/execution_groups.cpp@485: Replaying 0 pre-start event(s)0m
90m2020-06-15 17:18:20.723 DEBUG engine/gems/scheduler/execution_groups.cpp@476: Launching 0 pre-start job(s)0m
90m2020-06-15 17:18:20.723 DEBUG engine/gems/scheduler/execution_groups.cpp@485: Replaying 0 pre-start event(s)0m
0m2020-06-15 17:18:20.723 INFO  engine/alice/backend/asio_backend.cpp@33: Starting ASIO service0m
0m2020-06-15 17:18:20.727 INFO  packages/sight/WebsightServer.cpp@216: Sight webserver is loaded0m
0m2020-06-15 17:18:20.727 INFO  packages/sight/WebsightServer.cpp@217: Please open Chrome Browser and navigate to http://&lt;ip address&gt;:30000m
33m2020-06-15 17:18:20.727 WARN  engine/alice/backend/codelet_canister.cpp@225: Codelet 'websight/isaac.sight.AliceSight' was not added to scheduler because no tick method is specified.0m
33m2020-06-15 17:18:20.728 WARN  engine/alice/components/Codelet.cpp@53: Function deprecated. Set tick_period to the desired tick paramater0m
33m2020-06-15 17:18:20.728 WARN  engine/alice/backend/codelet_canister.cpp@225: Codelet '_check_operating_system/isaac.alice.CheckOperatingSystem' was not added to scheduler because no tick method is specified.0m
33m2020-06-15 17:18:20.728 WARN  engine/alice/components/Codelet.cpp@53: Function deprecated. Set tick_period to the desired tick paramater0m
33m2020-06-15 17:18:20.730 WARN  engine/alice/components/Codelet.cpp@53: Function deprecated. Set tick_period to the desired tick paramater0m
1;31m2020-06-15 17:18:20.741 ERROR engine/alice/components/Codelet.cpp@229: Component 'camera/realsense' of type 'isaac::RealsenseCamera' reported FAILURE:

    No device connected, please connect a RealSense device
0m
1;31m2020-06-15 17:18:20.741 ERROR engine/alice/backend/event_manager.cpp@42: Stopping node 'camera' because it reached status 'FAILURE'0m
33m2020-06-15 17:18:20.743 WARN  engine/alice/backend/codelet_canister.cpp@225: Codelet 'camera/realsense' was not added to scheduler because no tick method is specified.0m
0m2020-06-15 17:18:21.278 INFO  packages/sight/WebsightServer.cpp@113: Server connected / 10m
0m2020-06-15 17:18:30.723 INFO  engine/alice/backend/allocator_backend.cpp@57: Optimized memory CPU allocator.0m
0m2020-06-15 17:18:30.724 INFO  engine/alice/backend/allocator_backend.cpp@66: Optimized memory CUDA allocator.0m

</code></pre>
",6/16/2020 17:28,,536,0,0,1,,9459427.0,,3/7/2018 23:27,6.0,,,,,,,
2486,36758570,Robotics Square Grid Intersection Point,|algorithm|math|geometry|trigonometry|robotics|,"<p>I'm trying to determine the point at which my robot will intersect with a wall given its location in a map and an angle its pointing at in radians. So to sum the problem up, given a square grid of any size [1-infinity], a object within that grid, and the angle at which that object is facing (radians), find the point of intersection with the border of the grid. For instance you have a 10 x 10 grid, your object is at position (5,5), and it is facing at an angle of pi/8 radians (Northeast direction). If this object were to move in a straight line, where would it intersect with the wall? Is there a generalized solution that would work for any position and any angle? So far what I'm doing is calculating a point outside the grid on the same trajectory and looking at all the points until I find a wall, but I feel like there is probably a more elegant solution. Thanks for the help!</p>
",4/21/2016 2:33,36777656.0,324,2,1,1,,6233200.0,"Hanover, MD, USA",4/21/2016 2:21,42.0,36760730.0,"<p>Pseudocode for ray with starting points inside rectangle:<br>
Starting point <code>(X0, Y0)</code><br>
Ray angle <code>Theta</code>, <code>c = Cos(Theta), s = Sin(Theta);</code><br>
Rectangle coordinates: <code>bottom left (X1,Y1), top right (X2,Y2)</code>  </p>

<pre><code>if c &gt;= 0 then //up
  XX = X2
else
  XX = X1

if s &gt;= 0 then  //right
  YY = Y2
else
  YY = Y1

if c = 0 then //vertical ray
   return Intersection = (X0, YY)

if s = 0 then  //horizontal ray
   return Intersection = (XX, Y0)

tx = (XX - X0) / c   //parameter when vertical edge is met
ty = (YY - Y0) / s   //parameter when horizontal edge is met

if tx &lt;= ty then  //vertical first
    return Intersection = (XX, Y0 + tx * s)
else            //horizontal first
    return  Intersection = (X0 + ty * c, YY)
</code></pre>
",844416.0,0.0,0.0,61099581.0,"Does you grid consist of 100 cells and you need intersections with each grid line touched? Or just single rectangle, and you need intersection with it's perimeter?"
1284,8799363,Robotics Club Programming Portion,|microcontroller|robotics|labview|robot|,"<p>My school has entered into a Robotics Tournament that competes several schools against each other(this is my school's first year). The objective of the robot is to shoot a ball into a hoop. I am a member of the Programming team. Our job as the programmers is to program a robot and a computer to control the robot. The computer has 2 joy sticks attached to it, one for moving the entire robot(spinning the wheels and causing the robot to move) and one is for the ""throwing arm"". A signal is going to be sent from the computer to the robot using wifi. All of the programming MUST be done in LabView. </p>

<p>I have never heard of LabView before until i joined this club and i have my doubts about it. The reason why we must use LabView is because most of the kids on the programming team have no programming experience whatsoever. LabView has to be able to interface with the joy sticks and then send that information to the robot using wifi. The micro controller on the robot supports LabView.</p>

<p>Now to my question, is LabView dynamic enough to preform this task? Can LabView even support networking? Can LabView even interface with the joy sticks? I have read a lot of the documentation for LabView from this website:</p>

<ul>
<li><a href=""http://www.ni.com/gettingstarted/labviewbasics/environment.htm"" rel=""nofollow"">http://www.ni.com/gettingstarted/labviewbasics/environment.htm</a></li>
</ul>

<p>My concern is that LabView is not dynamic enough for what we are trying to use it for as a team and we are going to have to program the computer and the micro controller using C. There are only 2 people on the team who can program sufficiently in C so we would have to teach the rest of the members the basics of C.</p>

<p>All relevant answers are welcomed and appreciated.</p>
",1/10/2012 6:22,8803293.0,376,4,2,2,0.0,1072647.0,United States,11/30/2011 4:20,396.0,8803293.0,"<p>LabVIEW can totally do this. I am biased: I've written a textbook on it and am teaching classes:-); I also do this for a living. In comparision to C, well, C can do anything, but LabVIEW does hardware on a much higher level. Doesn't mean I don't like bending pointers for a bit; but it's nice to not care about low-level functions for a while.</p>

<p>Interfacing a joystick is pretty simple, it looks like this: <a href=""http://digital.ni.com/public.nsf/allkb/CA411647F224787B86256DD000669EFE"" rel=""nofollow"">http://digital.ni.com/public.nsf/allkb/CA411647F224787B86256DD000669EFE</a>
To interface Wifi, it depends on how the robot should receive the information. TCP/IP would go like this: <a href=""http://zone.ni.com/devzone/cda/tut/p/id/2710"" rel=""nofollow"">http://zone.ni.com/devzone/cda/tut/p/id/2710</a></p>
",619238.0,4.0,1.0,10983026.0,"My non-constructive advice: leave the sinking boat! LabView is a piece of [something]. LabView is not dynamic at all (how it is mostly used) but you will probably be able to do all the tasks required with it. School competition are a fun way to learn things but learning the wrong things is wrong. If the rules allow it, I would strongly urge you and your team members to use C or any other language. C is particularly apt for embedded systems. I might be biased because I have both used LabView and C..."
1589,14041109,how to recognise a zebra crossing from top view using opencv?,|opencv|computer-vision|robotics|,"<p>you can get all the details about the problem from this pdf document: www.shaastra.org/2013/media/events/70/Tab/422/Modern_Warfare_ps_v1.pdf</p>

<p>how to recognize a zebra crossing from top view using opencv ?? 
it is not a straight zebra crossing it has some twists and turns
i have some ideas,
1. parallel line detection but the available techniques only works to find straight parallel not the ones that has curve in it.
2. template matching to match a template of back to back white and black stripes but it becomes tedious since i cant find any pattern matching techniques with scaling and rotation.</p>

<p>in fact a idea on any single part of the problem will be so helpful!!
it is driving me crazy someone please help!!!!
any help is appreciated 
thanks in advance ....</p>

<p>NOTE:
 the zebra crossing i am talking about has the inside lines perpendicular to the border lines, just like a normal zebra crossing. only difference is that the path has curves like a river </p>
",12/26/2012 13:14,,2576,2,2,3,0.0,1929880.0,,12/26/2012 12:58,13.0,14041773.0,"<p>Here is a relevant paper:</p>

<p><a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2874980/"" rel=""nofollow"">Detecting and Locating Crosswalks using a Camera Phone</a></p>

<p>While this is not an opencv implementation it should be a reasonable place to start.  </p>
",684650.0,2.0,2.0,19398538.0,I'd put some research into recognizing a zebra from Hough transform.
3445,57065924,What is the data contained in ROS topic velodyne_msgs/VelodynePacket Message?,|ros|robotics|lidar|,"<p>I am doing a light weight program to monitor received beams for lidar. Preferably, I do not want to cache the entire UDP data packet or point cloud data due to the light weight nature.</p>

<p>The question is what is the data contained in ROS message velodyne_msgs/VelodynePacket. This message contains smaller data but I do not know if it is related. </p>

<p>By read the <a href=""http://wiki.ros.org/velodyne_msgs"" rel=""nofollow noreferrer"">Ros Wiki</a> on this topic but the link for velodynepackt did not provide useful info on the content. </p>
",7/16/2019 21:52,,706,1,0,0,,4318755.0,,12/3/2014 6:24,20.0,57068895.0,"<p>Check the message definition to see what fields a message contains and their types. Message files will usually either have field names that are self explanatory or will have comments (<code>## text</code>) describing the fields. You can look at the message definitions either online or locally. To look at them locally use <code>roscd</code> to get to the package directory <code>roscd &lt;package_name&gt;/msg</code> and then using <code>cat</code> to see the contents of the message file. In your case, this would be: </p>

<p><code>roscd velodyne_msgs/msg
cat VelodynePacket.msg
cat VelodyneScan.msg</code> </p>

<p>The relevant message files are available online from the page you linked to:
<a href=""http://docs.ros.org/api/velodyne_msgs/html/msg/VelodyneScan.html"" rel=""nofollow noreferrer"">http://docs.ros.org/api/velodyne_msgs/html/msg/VelodyneScan.html</a>
<a href=""http://docs.ros.org/api/velodyne_msgs/html/msg/VelodynePacket.html"" rel=""nofollow noreferrer"">http://docs.ros.org/api/velodyne_msgs/html/msg/VelodynePacket.html</a></p>

<p>In regards to your specific question about creating a lightweight application, you have a few options. </p>

<ol>
<li>Use the provided ROS message and subscribe to it. Most of the time if you don't have a ton of large data traveling around, you'll be okay and will be able to keep up with real time data. The majority of the time associated with ROS usually comes from the network transport, so if that's a problem, you'll need to not pass the data over ROS. </li>
<li>Put your code in a ROS Nodelet. This gives you the advantages of ROS data abstractions while eliminating the network data transfer that occurs between nodes. This is akin to using a pointer to the data field. </li>
<li>If you really don't want all the scan data, but still want to use ROS, you can write your own driver node. This will read from the LIDAR the data you want and discard the data you don't. You can do the raw data processing in that node (no ROS message required) or publish the data you care about and do the processing in another node. </li>
</ol>
",3954529.0,-1.0,2.0,,
3898,64417385,How to find the transformation between two IMU measurements?,|quaternions|robotics|imu|,"<p>I am looking at the Clubs Dataset <a href=""https://clubs.github.io/"" rel=""nofollow noreferrer"">https://clubs.github.io/</a> for some research into multiway registration of point clouds. I am initially trying to use the ICP registration method in a sequential order adding one point cloud at a time.</p>
<p>The dataset has the RGB parameters for the various poses of the object.</p>
<pre><code>1525694104,-0.0913515162993169,-0.16815189159691318,0.4504956847817425,0.4591556084159897,-0.7817619820248951,-0.3682132612946675,0.20601769355692517
1525694157,-0.22510740390250225,-0.32514596548025265,0.45221561140129063,0.2388698281592328,-0.8750788198918591,-0.4081451880445991,0.10293936130543749
1525694174,-0.4179094161019803,-0.39403349319958664,0.4522321523188167,-0.004021371419719342,0.9070013543525104,0.4210736433584828,0.005455788205342825
</code></pre>
<p>The columns are namely</p>
<blockquote>
<p>filename, translation across 3 axes, quaternion for rotation.</p>
</blockquote>
<p>I converted the images into point clouds and when I try to align the point clouds, I would need to have a good estimation of the transformation. Given the measurements at two points, how would I find out the transformation between those points. I would use that as my intiial estimate of my ICP registration algorithm.</p>
",10/18/2020 19:18,,131,0,0,1,,3961840.0,,8/20/2014 20:26,4.0,,,,,,,
1621,14437630,hardware emulation,|hardware|driver|robot|device-emulation|,"<p>I need to write controlling software for a very specialized and fairly complex industrial robot. I'm not allowed to access the actual robot before testing the final software, because it has to work all the time. It would be necessary to somehow emulate the robot, so I can develop my code. I have drivers for the robot, and some manuals but since it is a complex stuff, I would prefer not to build a simulator from scratch.</p>

<p>What I'm looking for is a software where I just load the device drivers and based on them, the hardware can be simulated, with all of its capabilities.</p>

<p>Does something like that exists out there or am I on a completely wrong track?</p>

<p>PS. No, I don't have very good programming skills, but I'm ready to learn :)</p>

<p>Thanks in advance!</p>
",1/21/2013 11:27,,247,1,2,2,0.0,1996854.0,,1/21/2013 11:17,3.0,15869712.0,"<p>Normally Robot manufacturers supply simulation / emulation software.
Eg. </p>

<ul>
<li>Nachi robotics haveAX/FD on desk,  </li>
<li>Adept robots have Adept Ace(<a href=""http://www.adept.com/products/software/pc/adept-ace/general"" rel=""nofollow"">http://www.adept.com/products/software/pc/adept-ace/general</a>) </li>
<li>Kuka haveKUKA SIM</li>
<li>.... and so on</li>
</ul>

<p>In these programs you can create robot programs, simulate robot movement and interface with all Inputs and outputs. You can also import programs already running on the robot and alter/debug them offline</p>

<p>I believe that creating a simulation/emulation of the entire robot would be a ginormous amount of work, especially if you don't have a lot of experience with programming.</p>
",1944249.0,1.0,0.0,20101767.0,"Can you get some data traces, that is, what's being sent to the robot and its responses? That could help as you could build your simulator and test it to some degree with that data."
822,4063416,What to study to get into robotics?,|robotics|,"<p>What should someone study at university level if he/she wants to get into robotics and build robotics? So far 'Mechatronics' seems to be the field I'm looking for? I looked at a few plain 'robotics' courses but they seem to be only about the electrical and computer work, and don't include any details on building the mechanical components of robots?</p>
",10/31/2010 14:11,9386568.0,82302,5,2,32,0.0,49153.0,http://ali.actor,12/26/2008 4:39,34744.0,4063436.0,"<p>Mechanical and electrical engineering and computer science.</p>

<p>Mechanical engineering will inform choices about servos, linkages, gears, and all other mechanical components.</p>

<p>Control theory is the junction of mechanical and electrical engineering.  You'll need that.</p>

<p>So much of control is digital these days, so EE and computer science will be a part of it.</p>

<p>It's a big field.  Good luck.</p>
",37213.0,1.0,0.0,4365470.0,"You'd probably get better answers if you provide your current background in electronics, automation and similar stuff."
3917,64689816,Line following and obstacle avoidance with ePuck [Webots],|python|python-3.x|robotics|webots|,"<p>So I'm trying to program a very simple collision avoidance behavior by following the line using ground and distance sensors of E-puck robot. The robot program was written in Python and the simulation is running on <a href=""https://www.cyberbotics.com/#cyberbotics"" rel=""nofollow noreferrer"">Webots</a>. The robot should go forwards following the line until an obstacle is detected by the front distance sensors, and then to turn towards the obstacle-free direction and then get back to the line. Something like <a href=""https://www.youtube.com/watch?v=jM_hqbmrdQ8"" rel=""nofollow noreferrer"">that</a>. So I'm trying to reproduce the same behaviour but when the robot aproaches to an obstacle it stucks and doesn't try to avoid it. When I remove either line following part of code or obstacle avoidance part the appropriate result is working but simultaneously they don't functioning. I've seen some codes written in C but I'm not familiar with C programming language. Please don't send me to see the tutorials, since I already followed them. My code:</p>
<pre><code>&quot;&quot;&quot;line_following_behavior controller.&quot;&quot;&quot;


from controller import Robot, DistanceSensor, Motor
import numpy as np

#-------------------------------------------------------
# Initialize variables

TIME_STEP = 64
MAX_SPEED = 6.28

speed = 1 * MAX_SPEED

# create the Robot instance.
robot = Robot()

# get the time step of the current world.
timestep = int(robot.getBasicTimeStep())   # [ms]

# states
states = ['forward', 'turn_right', 'turn_left']
current_state = states[0]

# counter: used to maintain an active state for a number of cycles
counter = 0
counter_max = 5

#-------------------------------------------------------
# Initialize devices

# distance sensors
ps = []
psNames = ['ps0', 'ps1', 'ps2', 'ps3', 'ps4', 'ps5', 'ps6', 'ps7']
for i in range(8):
    ps.append(robot.getDistanceSensor(psNames[i]))
    ps[i].enable(timestep)

# ground sensors
gs = []
gsNames = ['gs0', 'gs1', 'gs2']
for i in range(3):
    gs.append(robot.getDistanceSensor(gsNames[i]))
    gs[i].enable(timestep)


# motors    
leftMotor = robot.getMotor('left wheel motor')
rightMotor = robot.getMotor('right wheel motor')
leftMotor.setPosition(float('inf'))
rightMotor.setPosition(float('inf'))
leftMotor.setVelocity(0.0)
rightMotor.setVelocity(0.0)


#-------------------------------------------------------
# Main loop:
# - perform simulation steps until Webots is stopping the controller
while robot.step(timestep) != -1:
    # Update sensor readings
    psValues = []
    for i in range(8):
        psValues.append(ps[i].getValue())

    gsValues = []
    for i in range(3):
        gsValues.append(gs[i].getValue())

    # detect obstacles
    right_obstacle = psValues[0] &gt; 80.0 or psValues[1] &gt; 80.0 or psValues[2] &gt; 80.0
    left_obstacle = psValues[5] &gt; 80.0 or psValues[6] &gt; 80.0 or psValues[7] &gt; 80.0

    # initialize motor speeds at 50% of MAX_SPEED.
    leftSpeed  = speed
    rightSpeed = speed
    # modify speeds according to obstacles
    if left_obstacle:
        # turn right
        leftSpeed  = speed
        rightSpeed = -speed
    elif right_obstacle:
        # turn left
        leftSpeed  = -speed
        rightSpeed = speed
    
    

    # Process sensor data
    line_right = gsValues[0] &gt; 600
    line_left = gsValues[2] &gt; 600

    # Implement the line-following state machine
    if current_state == 'forward':
        # Action for the current state
        leftSpeed = speed
        rightSpeed = speed
        # update current state if necessary
        if line_right and not line_left:
            current_state = 'turn_right'
            counter = 0
        elif line_left and not line_right:
            current_state = 'turn_left'
            counter = 0
            
    if current_state == 'turn_right':
        # Action for the current state
        leftSpeed = 0.8 * speed
        rightSpeed = 0.4 * speed
        # update current state if necessary
        if counter == counter_max:
            current_state = 'forward'

    if current_state == 'turn_left':
        # Action for the current state
        leftSpeed = 0.4 * speed
        rightSpeed = 0.8 * speed
        # update current state if necessary
        if counter == counter_max:
            current_state = 'forward'        

    # increment counter
    counter += 1
    
    #print('Counter: '+ str(counter), gsValues[0], gsValues[1], gsValues[2])
    print('Counter: '+ str(counter) + '. Current state: ' + current_state)

    # Update reference velocities for the motors
    leftMotor.setVelocity(leftSpeed)
    rightMotor.setVelocity(rightSpeed)
</code></pre>
",11/5/2020 1:09,,2287,0,0,0,,11059311.0,,2/13/2019 23:42,21.0,,,,,,,
3036,49209624,generating occupancy grid maps from open source maps,|mapping|ros|robotics|,"<p>I want to know is it possible to write a program that  generates a 2d occupancy grid map from an open source map such as ""Openstreetmap"" in order to use it with robot localization ..</p>

<p>Will the information that can be extracted from such maps will be enough to know if this is a building so it's an occupied cell but this is a street so it's a free cell?</p>

<p>The program should take a map in XML file for example and automatically output the OGM.</p>
",3/10/2018 13:29,,451,0,3,0,,9351257.0,,2/12/2018 17:59,1.0,,,,,,85442451.0,"No, it will most likely not be enough for robot localization. There are a lot of things changing all the time in the real world, and they will not be visible in the map. The robot's localization is likely to fail if you use outdated data from the map."
3021,48999577,How to convert an image to RGB data values in c++,|c++|colors|robotics|,"<p>I am creating a robot along with my team (of which is called The Rusty Huskies) for a <em>FIRST</em> competition. We need to know how to convert an image to different values of rgb so that our robot can detect which switch we are looking at (the switch is where we would be placing our blocks to gain points).</p>

<p>So it would turn out to be something like this:</p>

<pre><code>#include &lt;color_reader.h&gt;
#include &lt;string&gt;

class Robot: public frc::IterativeRobot {

std::string color = """";

colorDetector colorDet;

colorDet.readImg(""image.png"");

r = colorDet.r;
g = colorDet.g;
b = colorDet.b;

if (r &gt;= 150) {

color = ""red"";

} else {

color = ""blue"";

}

};

START_ROBOT_CLASS(Robot)
</code></pre>

<p>Thanks for any help in advance!</p>
",2/27/2018 1:04,,3074,1,1,-1,0.0,9416091.0,,2/27/2018 0:39,1.0,48999674.0,"<p><a href=""https://stackoverflow.com/questions/1536159/getting-rgb-values-for-each-pixel-from-a-raw-image-in-c"">Getting RGB values for each pixel from a raw image in C</a></p>

<p>In this post the same question is discussed, depends with camera manufacturer.</p>

<p>You have to search for a library to work with your camera, writing a code might be too difficult.</p>
",2244977.0,1.0,0.0,85004523.0,Install OpenCV in your project. It has many real-time image processing functionalities for such robotic projects
1101,5507790,Best 3D library to model robotic motion,|python|opengl|robotics|modeling|,"<p>A short while I asked for suggestions on choosing a Python-compatible 3D graphics library for robotic motion modelling (using inverse kinematics in Python). After doing a bit of research and redefining my objectives I hope I can ask once again for a bit of assistance.</p>

<p>At the time I thought Blender was the best option - but now I'm having doubts. One key objective I have is the ability integrate the model into a custom GUI (wxPython). Seems like this might be rather difficult (and I'm unsure of the performance requirements). </p>

<p>I think I'm now leaning more towards OpenGL (PyOpenGL + wxglcanvas), but I'm still struggling to to determine if it's the right tool for the job. I'm more of a CAD person, so I have trouble envisioning how to draw complex objects in the API and create motion. I read I could design the object in, say Blender, then import it into OpenGL somehow, but I'm unsure of the process? And how difficult is manipulating motion of objects? For example, if I create a joint between two links, and I move one link, would the other link move dynamically according to the first, or would I need to program each link's movement independently?</p>

<p>Have I missed any obvious tools? I'm not looking for complete robotic modelling packages, I would like to start from scratch so I can incorporate it into my own program. For for learning more than anything. So far I've already looked into vPython, Pyglet, Panda3D, Ogre, and several professional CAD packages. </p>

<p>Thanks</p>
",3/31/2011 23:54,,3114,2,0,3,0.0,,,,,5508425.0,"<p>There is a similar project going on that implements a <a href=""http://code.google.com/p/robotics-toolbox-python/"" rel=""nofollow"">robotic toolbox</a> for matlab and python, it has ""Rudimentary 3D graphics"", but you can always interface it with blender with a well knit <a href=""http://en.wikibooks.org/wiki/Blender_3D%3a_Noob_to_Pro/Advanced_Tutorials/Python_Scripting/Introduction"" rel=""nofollow"">script</a>, it will be less work than reinventing the wheel</p>
",681215.0,2.0,1.0,,
3366,55429137,How to control a 4 DOF robotic arm using Ardunio?,|arduino|arduino-uno|robotics|,"<p>I recently bought a 4 DOF <em>(Degree Of Freedom)</em> robotic arm kit. I successfully assembled it and now I want to program the arduino to control it.
I know how to make the servos work using arduino but could not figure out how to move the hand to specific positions.</p>

<p>I tried manually creating a two dimensonal array with rotational values for each motor in degrees. This works but it is very hard to get the values and create the array. Currently I adjusted the values by trial and error.</p>

<p><strong>The array I created manually :</strong></p>

<pre><code>short first[] = { 180 , 80 , 0 , 90 };
short pos[][4] = 
{
  { 180 , 80 , 00 , 85 },
  { 180 , 85 , 00 , 80 },
  { 180 , 90 , 00 , 75 },
  { 180 , 95 , 00 , 75 },
  { 180 , 100 , 0 , 70 },
  { 180 , 110 , 0 , 70 },
  { 180 , 115 , 0 , 70 },
  { 180 , 120 , 0 , 65 },
  { 180 , 125 , 0 , 65 },
  { 180 , 130 , 0 , 65 },
  { 180 , 135 , 0 , 65 },
  { 180 , 140 , 0 , 65 },
  { 180 , 145 , 0 , 65 },
  { 180 , 150 , 0 , 65 },
  { 180 , 150 , 0 , 70 },
  { 180 , 150 , 0 , 75 },
  { 180 , 150 , 0 , 80 },  
  { 180 , 150 , 0 , 90 },
  { 180 , 145 , 0 , 90 },
  { 180 , 140 , 0 , 90 },
  { 180 , 135 , 0 , 90 },
  { 180 , 130 , 0 , 90 },
  { 180 , 125 , 0 , 90 },
  { 180 , 120 , 0 , 90 },
  { 180 , 115 , 0 , 90 },
  { 180 , 110 , 0 , 90 },
  { 170 , 110 , 0 , 90 },
  { 160 , 110 , 0 , 90 },
  { 150 , 110 , 0 , 90 }, 
  { 140 , 110 , 0 , 90 }, 
  { 130 , 110 , 0 , 90 },
  { 130 , 115 , 0 , 90 }, 
  { 120 , 120 , 0 , 90 },
  { 120 , 125 , 0 , 90 },
  { 120 , 130 , 0 , 90 },
  { 120 , 135 , 0 , 90 },
  { 120 , 137 , 0 , 90 },
  { 120 , 139 , 0 , 90 },
  { 120 , 140 , 0 , 85 },
  { 120 , 140 , 0 , 80 },
  { 120 , 140 , 0 , 75 },
  { 120 , 140 , 0 , 70 },   

};
</code></pre>

<p><strong>The complete code that I wrote</strong> :</p>

<pre><code>/*
 * claws - 90 close 75 open
 * elbow - 0 to 100
 * sholder - 30 to 180
*/

Servo Servos[4];

void setup()
{
  Servos[0].attach(3);
  Servos[1].attach(5);
  Servos[2].attach(9);
  Servos[3].attach(11);

  reset();
  run();
  Servos[0].detach();
  Servos[1].detach();
  Servos[2].detach();
  Servos[3].detach();
}

void run()
{
  for(int i=0; i&lt;sizeof(pos) / sizeof(short) /4 ; i++)
  {
    for(int j=3; j&gt;=0; j--)
    {
      Servos[j].write(pos[i][j]);
      delay(15);
    }
    delay(15);
  }
  for(int i=-1+ sizeof(pos) / sizeof(short) /4;i&gt;=0 ; i--)
  {
    for(int j=3; j&gt;=0; j--)
    {
      Servos[j].write(pos[i][j]);
      delay(15);
    }
    delay(15);
  }
  delay(3000);
}

void reset()
{
  for(int i=3; i&gt;=0; i--)Servos[i].write(first[i]);
}


void loop(){}
</code></pre>

<p>I want some function to calculate the values of the array for any given coordinate or something like that.(That is the moves of each servo to position the end of the arm at that point)</p>

<p><strong>Photo of the Arm</strong> : </p>

<p><strong><a href=""https://i.stack.imgur.com/2ZPL8.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2ZPL8.jpg"" alt=""Robotic Arm""></a></strong> 
<a href=""https://i.stack.imgur.com/yObAw.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yObAw.jpg"" alt=""Robotic Arm""></a></p>

<p><strong>Here is the product page of the actual arm</strong> : </p>

<p><a href=""https://www.amazon.in/gp/product/B07LDNY9J3/ref=ppx_yo_dt_b_asin_title_o03_s00?ie=UTF8&amp;psc=1"" rel=""nofollow noreferrer"">https://www.amazon.in/gp/product/B07LDNY9J3/ref=ppx_yo_dt_b_asin_title_o03_s00?ie=UTF8&amp;psc=1</a></p>
",3/30/2019 7:17,57345583.0,3582,1,4,0,,10182024.0,"Kerala, India",8/5/2018 6:28,164.0,57345583.0,"<p>I finally solved the problem ! I tried figuring out the inverse kinematics for the arms but I found out it is very hard and due to the uncertainty in the hardware it doesn't work good either. The actual solution was to use sensors. I put distance sensors (Ultra-sonic) on the arm so that I can measure distance between arm parts in real time. Since I know the length of each segment of the arm (I don't have to worry about uncertainty here.) and also the distance between them I can do simple trigonometry to calculate the coordinates of the tip of the arm. This means that I can simply use a feedback loop to position the arm with ineradicable accuracy and overcome the limitations of the Hardware.</p>

<p>I do understand that this is not the exact answer for the asked question but I found out from my experience that this method is the most suitable for the situation (When compared to the attempt according to the question).</p>
",10182024.0,1.0,0.0,97643335.0,"You need to find the inverse kinematics equations for your robot configuration. Probably, as it is a kit, someone else has calculated them and you can find them online. If that's not the case, you need to obtain the Denavit-Hartemberg parameters for the configuration, and then obtain the matrices for each DOF."
3222,52974661,Multiple View Stereo software recommendation,|computer-vision|robotics|,"<p>I have a collection of camera images taken by 4 calibrated cameras mounted on a mobile robot moving in a static outdoor environment. In addition, I have information from a farely accurate OxTS RT3000 Inertial Navigation System (IMU + GPS).</p>

<p>I would like to combine these images to form a 3d model (point cloud) of the static environment. I know there are many Structure from Motion applications, but I would like to find some software/library that is able to make use of the odometry and calibration, at least as an initialization, and to produce a dense point cloud. (All of this is for offline recordings.)</p>

<p>Any suggestions?</p>
",10/24/2018 17:17,53089201.0,124,1,0,-1,,7869068.0,,4/14/2017 20:16,18.0,53089201.0,"<p>Agisoft Photoscan does what you want. From their manual:</p>

<blockquote>
  <p>PhotoScan supports import of external and internal camera orientation parameters. Thus, if precise camera
  data is available for the project, it is possible to load them into PhotoScan along with the photos, to be
  used as initial information for 3D reconstruction job.</p>
</blockquote>

<p>Taken from page 21 of <a href=""http://www.agisoft.com/pdf/photoscan-pro_1_4_en.pdf"" rel=""nofollow noreferrer"">http://www.agisoft.com/pdf/photoscan-pro_1_4_en.pdf</a></p>

<p>You may have to do some wrangling of data to get it into a supported format, but it's certainly possible. You'll probably want to use the local coordinate output of the OxTS IMU to go from lat/lon/alt to XYZ and save yourself the conversion. Be careful also to correct for the extrinsic parameters - the rotations and translations between the IMU navigation frame and the cameras.</p>
",6261432.0,0.0,2.0,,
3998,66947260,How to control this robot with a PS4 DualShock controller (Python)?,|python|loops|object|robotics|,"<p>I am trying to control a robot with a ps4 controller connected via Bluetooth to raspberry pi, but have a problem with the while loop I need to make it move. The robot cycles leg positions in a pattern that makes it walk. I made a successful program that uses the keyboard where holding w, a, s, or d will move the robot in the usual directions and stop the robot once released. However, the ps4 controller uses functions for input in a way that confuses me.</p>
<p>Here is the keyboard code:</p>
<pre><code>from move import move, steady, steady_X
import sys, tty, termios, time
import Adafruit_PCA9685

def getch():
    fd = sys.stdin.fileno()
    old_settings = termios.tcgetattr(fd)
    try:
        tty.setraw(sys.stdin.fileno())
        ch = sys.stdin.read(1)
    finally:
        termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)
    return ch

pwm =  Adafruit_PCA9685.PCA9685()

if __name__ == '__main__':
    step = 1
    move_stu = 1
    try:
        while 1:
            char = getch()
            if char == 'x':
                pwm.set_all_pwm(0,300)
                time.sleep(1)
                break
    
            if char == 'w':
                move(step, 35, 'forward')
                step += 1
                if step &gt; 4:
                    step = 1
                time.sleep(0.08)

            if char == 's':
                move(step, 35, 'backward')
                step += 1
                if step &gt; 4:
                    step = 1
                time.sleep(0.08) 

            if char == 'a':
                move(step, 35, 'left')
                step += 1
                if step &gt; 4:
                    step = 1
                time.sleep(0.08)

            if char == 'd':
                move(step, 35, 'right')
                step += 1
                if step &gt; 4:
                    step = 1
                time.sleep(0.08)

            if char == 'e':
                pwm.set_all_pwm(0,300)
                time.sleep(0.1)
                steady_X()

            if char == ' ':
                steady()

    except KeyboardInterrupt:
        pwm.set_all_pwm(0,300)
        time.sleep(1)
</code></pre>
<p>where 35 is the speed of the robot.</p>
<p>The controller code is:</p>
<pre><code>from pyPS4Controller.controller import Controller
from move import move, steady, steady_X
import sys, tty, termios, time
import Adafruit_PCA9685



class MyController(Controller):


    def __init__(self, **kwargs):
        Controller.__init__(self, **kwargs)
        self.step = 1
        self.pwm = Adafruit_PCA9685.PCA9685()
        self.command = False

    def on_x_press(self):
        self.command = True
        while self.command == True:
            move(self.step, 35, 'forward')
            self.step +=1
            if self.step &gt; 4:
                self.step = 1
            time.sleep(0.08)

    def on_x_release(self):
        self.command = False



controller = MyController(interface=&quot;/dev/input/js0&quot;, connecting_using_ds4drv = False, event_definition = None)
controller.listen()
</code></pre>
<p>Where once controller.listen() is called it starts taking inputs from the controller.
Right now, once x is pressed the robot will move forward and not stop when it is released. So my question is how can I make it stop moving when x is released and the on_x_release(self) function is called?</p>
",4/5/2021 1:19,,617,1,0,0,,15382824.0,,3/12/2021 13:07,2.0,67334609.0,"<p>I might not have a direct fix to your problem because I do not have the hardware to test it out, but you may consider writing a <code>stop()</code> function where you explicitly set all motors to the 0 position <em>(if you use a H-bridge controller, this means setting both of the channels to off)</em> . Then on calling the function <code>on_x_release(self)</code>, you will set the self.command flag to False as you normally do, but also, call the <code>stop()</code> function to explicitly stop the motors from moving.</p>
<pre><code>from pyPS4Controller.controller import Controller
from move import move, steady, steady_X
import sys, tty, termios, time
import Adafruit_PCA9685

class MyController(Controller):


    def __init__(self, **kwargs):
        Controller.__init__(self, **kwargs)
        self.step = 1
        self.pwm = Adafruit_PCA9685.PCA9685()
        self.command = False

    def on_x_press(self):
        self.command = True
        while self.command == True:
            move(self.step, 35, 'forward')
            self.step +=1
            if self.step &gt; 4:
                self.step = 1
            time.sleep(0.08)

    def on_x_release(self):
        self.command = False
        stop() # (or modify the move function to accept a 'stop' string)


controller = MyController(interface=&quot;/dev/input/js0&quot;, connecting_using_ds4drv = False, 
event_definition = None)
controller.listen()
</code></pre>
<p>On a separate note, I think this issue might be because you do not have a &quot;default&quot; condition where the robot does nothing. Usually the way I would go about with such a problem is to define the general behaviour of the robot as doing nothing, and define the x key as an interrupt. So, whenever the key is pressed, an event occurs, and the equivalent eventHandler is called <em>(think of ISR if you know microcontrollers well)</em> . Designing it in this way will probably give you more control.</p>
",8955618.0,0.0,0.0,,
3623,59476046,Webots - BoundingObject of Robot becomes null after world reload (how to prevent that?),|robotics|webots|,"<p>I've a Robot node with children = [SolidCylinderJoint]. SolidCylinderJoint is a proto that I created, which defines a DEF node as a field, i.e.,</p>
<pre><code>field SFNode geometry DEF BODY Cylinder {
                height 0.1
                radius 0.05
            }
</code></pre>
<p>Now I USE the BODY DEF node as the boundingObject of the robot, like this:</p>
<p><a href=""https://i.stack.imgur.com/9K0G3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9K0G3.png"" alt=""enter image description here"" /></a></p>
<p>Now, this works great, but as soon as I hit &quot;Reload World&quot; or restart webots, the boundingObject becomes NULL again. I think this is happening because the robot node is loaded before the Proto, and at the time it's trying to set the boundingObject to BODY, it doesn't find that definition and hence defaults to NULL.</p>
<p>World file: <a href=""https://pastecode.xyz/view/fab1533d"" rel=""nofollow noreferrer"">https://pastecode.xyz/view/fab1533d</a></p>
<p>Proto file: <a href=""https://pastecode.xyz/view/f558d13c"" rel=""nofollow noreferrer"">https://pastecode.xyz/view/f558d13c</a></p>
",12/25/2019 7:08,59576668.0,151,1,0,1,,1210650.0,"Seattle, WA, USA",2/15/2012 6:49,483.0,59576668.0,"<p>First, there is an issue in your PROTO, you are not allowed to make an IS in the default argument of the fields (i.e. baseColor IS baseColor):</p>

<pre><code>field SFVec3f baseColor 0.985946 0 0.0481575
field SFNode appearance PBRAppearance { baseColor IS baseColor metalness 0.3 }
</code></pre>

<p>About the issue with the DEF-USE, this is indeed a bug, it seems default argument of the PROTO are created after the root node and therefore not found at the creation of the root node.
I have reported this here and hopefully it will be fixed in the next version of Webots:
<a href=""https://github.com/cyberbotics/webots/issues/1231"" rel=""nofollow noreferrer"">https://github.com/cyberbotics/webots/issues/1231</a></p>
",8427891.0,1.0,0.0,,
3160,51205858,Finding the translation between 2d and 3d coordinate,|python|opencv|math|robotics|coordinate-systems|,"<p>I have a task to find the object in an image.</p>

<p><img src=""https://i.stack.imgur.com/T3gGG.png"" alt=""object1""></p>

<p>and translate the 2d coordinate(x,y) from my camera to 3d coordinate for my robotic arm. Now I can find the 2d coordinate with my opencv python code, and 3d coordinate by my teaching method from my robotic program but in different origin point. However, the method that I use to convert 2d to 3d coordinate is still wrong. Since the origin of the robotic arm and camera is not the same point. So I would like to ask that what formula/code should I use to convert 2d coordinate(x,y) to 3d coordinate(x,y,z) if the origin is not the same.</p>
",7/6/2018 8:14,,864,2,2,0,0.0,10040383.0,,7/6/2018 3:45,2.0,51206531.0,"<p>Assuming that:</p>

<ul>
<li>You are able to find 3D coordinates centered on the camera</li>
<li>You know the coordinates of the camera ""origin"" in relation to the arm</li>
<li>You know how many degree, the camera's coordinate system is rotated in respect to a given axis in the arm's coordinate system.</li>
<li>You need to obtain arm-centered coordinates.</li>
</ul>

<p>You just need to apply a <em>rigid transformation</em> known as <em>roto-translation</em>.
To simplify the following example code we'll also assume that the Z axes of both coordinate systems are parallel to each other, perpendicular to the ""table"", and have the same direction. So, the only rotation possible is around the Z axis.</p>

<pre><code>x, y, z = get_3d(x, y)

x -= camera_origin_x_in_arm_coord_system
y -= camera_origin_y_in_arm_coord_system
z -= camera_origin_z_in_arm_coord_system

arm_x = x*cos(theta) - y*sin(theta)
arm_y = x*sin(theta) + y*cos(theta)
arm_z = z
</code></pre>

<p>For more complex rotations please refer to the answers to <a href=""https://stackoverflow.com/questions/6802577/rotation-of-3d-vector"">this question</a>.</p>
",1833711.0,0.0,2.0,89392819.0,"In my understanding you are able to find 3D coordinates centered on the camera, but you need to obtain arm-centered coordinates? Am I right?"
2463,35970213,Angle to a circle tangent line,|matlab|math|geometry|robotics|,"<p>I cannot upload pics so I will try to explain my problem the best. I want to simulate the detection of a moving object by a unicycle type robot. The robot is modelled with position (x,y) and direction theta as the three states. The obstacle is represented as a circle of radius r1. I want to find the angles alpha_1 and alpha_2 from the robot's local coordinate frame to the circle, as shown here:</p>

<p><img src=""https://i.stack.imgur.com/FdSY4.jpg"" alt=""""></p>

<p>So what I am doing is trying to find the angle from the robot to the line joining the robot and the circle's centre (this angle is called aux_t in my code), then find the angle between the tangent and the same line (called phi_c). Finally I would find the angles I want by adding and substracting phi_c from aux_t. The diagram I am thinking of is shown:</p>

<p><img src=""https://i.stack.imgur.com/b0GUg.jpg"" alt=""""></p>

<p>The problem is that I am getting trouble with my code when I try to find the alpha angles: It starts calculating the angles correctly (though in negative values, not sure if this is causingmy trouble) but as both the car and the cicle get closer, phi_c becomes larger than aux_t and one of the alphas suddenly change its sign. For example I am getting this:</p>

<p><strong>aux_t</strong>//////<strong>phi_c</strong>//////<strong>alpha_1</strong>//////<strong>alpha_2</strong><br>
-0.81//////+0.52//////-1.33//////-0.29</p>

<p>-0.74//////+0.61//////-1.35//////-0.12</p>

<p>-0.69//////+0.67//////-1.37//////-0.02</p>

<p>-0.64//////+0.74//////-1.38//////+0.1</p>

<p>So basically, the alpha_2 gets wrong form here. I know I am doing something wrong but I'm not sure what, I don't know how to limit the angles from 0 to pi. Is there a better way to find the alpha angles?
Here is the section of my code:</p>
",3/13/2016 12:25,35972839.0,357,2,1,2,,5239521.0,,8/18/2015 13:32,3.0,35972839.0,"<p>As far as your math goes, the only change I would make would be to <em>subtract</em> <code>(pi/2 - theta)</code> from the target's angle rather than add it. This will give you angles in the more typical orientation (counter-clockwise being positive).</p>

<p>I'm not completely sure why you think that <code>alpha_2</code> is wrong in the data that you posted in your answer. What is happening is that your robot is getting very close to the target and the <code>alpha_2</code> tangent line actually moves to the other side or the line pointing in the direction your robot is looking. I have created a similar situation here where the labels on the tangent lines are the angle relative to the robot (yellow line) and all angles are forced to be between 0 and 2*pi.</p>

<p><a href=""https://i.stack.imgur.com/mUosO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mUosO.png"" alt=""enter image description here""></a></p>

<p>To address you question about forcing an angle to be within a particular range. To do this you will want to use the modulus (<a href=""http://www.mathworks.com/help/matlab/ref/mod.html"" rel=""nofollow noreferrer""><code>mod</code></a> in MATLAB). In these examples, I have used <code>mod(theta, 2*pi)</code> because technically if your robot is facing <em>away</em> from the target the angles can be greater than pi.</p>

<p>As a test I have performed a simple simulation that moves the robot around and shows the angles of the tangent lines relative to the robot's direction (again, between 0 and 2pi)</p>

<p><a href=""https://i.stack.imgur.com/xlYTA.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xlYTA.gif"" alt=""enter image description here""></a></p>

<p>If you really want your angles to be between 0 and pi, you could instead use <code>mod(theta, pi)</code> instead.</p>

<pre><code>alpha_1 = mod(alpha_1, pi);
alpha_2 = mod(alpha_2, pi);
</code></pre>
",670206.0,4.0,1.0,59596013.0,"this maybe because `atan2` returns a value of `aux_t` between -pi and pi, so it changes sign as soon as it crosses that half-line."
3372,55608358,"How to fix Mujoco CmakeLists build error ""/usr/bin/ld: cannot find -lglfw""?",|c++|cmake|shared-libraries|simulator|robotics|,"<p>I'm trying to simulate physics of a robot with Mujoco in C++. Because the project is a part of a bigger workspace, I need to use cmake to build the executable. However, I cannot seem to properly link all the dependant libraries, so I cannot get rid of the error:</p>

<pre><code>~: /usr/bin/ld: cannot find -lglfw
</code></pre>

<p>I did a bit of research on the web on how to properly set up Mujoco in CmakeLists, and found some examples <a href=""http://www.mujoco.org/forum/index.php?threads/c-makefile-cmakelists-txt.3840/"" rel=""nofollow noreferrer"">here</a>, <a href=""http://www.mujoco.org/forum/index.php?threads/cmake-request.3361/"" rel=""nofollow noreferrer"">here</a>, and <a href=""https://github.com/atabakd/MuJoCo-Tutorials"" rel=""nofollow noreferrer"">here</a>.</p>

<p>I replicated the CmakeLists files from the examples above, but the error still persisted. Here are the relevant snippets from my files. I defined an environment variable <code>MUJOCO_PATH</code> to point to the Mujoco folder on my machine. Concretely <code>$HOME/.mujoco/mujoco200</code>.</p>

<p>CmakeLists.txt</p>

<pre><code>######################################################
# define the include directory of all ${CATKIN_PKGS} #
######################################################
include_directories(
    ${PROJECT_SOURCE_DIR}/include
    ${catkin_INCLUDE_DIRS}
    ${Eigen_INCLUDE_DIRS}
    $ENV{MUJOCO_PATH}/include
)

########################################################
# manage the creation of the libraries and executables #
########################################################
set(USE_GL 1)

link_directories($ENV{MUJOCO_PATH}/bin)

#Finding main mujoco library
if(${USE_GL})
    file(GLOB LIB_MUJOCO $ENV{MUJOCO_PATH}/bin/libmujoco[0-9][0-9][0-9].so)
else()
    file(GLOB LIB_MUJOCO $ENV{MUJOCO_PATH}/bin/libmujoco[0-9][0-9][0-9]nogl.so)
endif()
#Showing mujoco library found
message(STATUS ""MuJoCo lib: "" ${LIB_MUJOCO})

add_subdirectory(src)
</code></pre>

<p>src/CmakeLists.txt</p>

<pre><code>set(BIN_NAME mujoco_finger_test)

add_executable(${BIN_NAME} ${BIN_NAME}.cpp)
target_link_libraries(${BIN_NAME} ${LIB_MUJOCO})

# Standard libraries for GL
target_link_libraries(${BIN_NAME} GL GLU glut )

# Additional libraries from mujoco package
target_link_libraries(${BIN_NAME} libglew.so libglfw.so.3 libglewegl.so libglewosmesa.so)
</code></pre>

<p>Does anyone have an idea why this could be the case? Am I missing something from these examples?</p>

<p>Thanks!</p>
",4/10/2019 8:47,55608420.0,9020,1,0,1,,5576434.0,,11/18/2015 11:56,63.0,55608420.0,"<p>You should find the GL/GLW package instead ot this: <code>target_link_libraries(${BIN_NAME} libglew.so libglfw.so.3 libglewegl.so libglewosmesa.so)</code>. This doesn't ensure that these libraries are available and can be found, whereas the <code>FIND_PACKAGE(GLEW)</code>.</p>

<p>See <a href=""https://stackoverflow.com/questions/27472813/linking-glew-with-cmake"">Linking GLEW with CMake</a> for more information on the subject.</p>
",2266772.0,3.0,0.0,,
4541,75700380,Welding IIWA arm to a Planar Joint,|python|robotics|drake|,"<p>Me and my friends are trying to obtain a mobile IIWA arm using PyDrake library. We are able to manually define a planar joint using two Prismatic joints and one Revolute joint. First, we were playing with a basic box to move around. However, when adding the IIWA arm, our program started to give the following error message:</p>
<pre><code>Traceback (most recent call last):
  File &quot;planar_joint.py&quot;, line 107, in &lt;module&gt;
    simulator.AdvanceTo(12.0)
RuntimeError: Error control wants to select step smaller than minimum allowed (1e-14)
</code></pre>
<p>I read that might be caused by singularity of IIWA but I believe we did not force IIWA to obtain a such position. Another thing I have read was the cause of this problem might be getting an infinite value in the physics engine. It is more likely but I do not know how to fix the problem. You can find the code below.</p>
<p>Main Code:</p>
<pre><code>meshcat = StartMeshcat()

meshcat.Flush()
builder = DiagramBuilder()
plant, scene_graph = AddMultibodyPlantSceneGraph(builder, time_step=0.0005)

parser = Parser(plant)

scene_models = parser.AddModels(&quot;cargobot-models/scene_without_robot.dmd.yaml&quot;)

iiwa = plant.GetModelInstanceByName(&quot;iiwa&quot;)

false_body = plant.AddRigidBody(
        &quot;false_body&quot;, iiwa,
        SpatialInertia(0, [0, 0, 0], UnitInertia(0, 0, 0)))
false_body2 = plant.AddRigidBody(
        &quot;false_body2&quot;, iiwa,
        SpatialInertia(0, [0, 0, 0], UnitInertia(0, 0, 0)))

mobile_base_y = plant.AddJoint(PrismaticJoint(
        &quot;mobile_base_y&quot;, plant.GetFrameByName(&quot;world&quot;), plant.GetFrameByName(&quot;false_body&quot;), 
        [0, 1, 0], -3, 3))
plant.AddJointActuator(&quot;mobile_base_y_actuator&quot;, mobile_base_y)

mobile_base_x = plant.AddJoint(PrismaticJoint(
        &quot;mobile_base_x&quot;, plant.GetFrameByName(&quot;false_body&quot;), plant.GetFrameByName(&quot;false_body2&quot;), 
        [1, 0, 0], -3, 3))
plant.AddJointActuator(&quot;mobile_base_x_actuator&quot;, mobile_base_x)

mobile_base_theta = plant.AddJoint(RevoluteJoint(
        &quot;mobile_base_theta&quot;, plant.GetFrameByName(&quot;false_body2&quot;), plant.GetFrameByName(&quot;iiwa_link_0&quot;), 
        [0, 0, 1],  -np.pi, np.pi))
plant.AddJointActuator(&quot;mobile_base_theta_actuator&quot;, mobile_base_theta)

plant.Finalize()

visualizer = MeshcatVisualizer.AddToBuilder(
    builder, scene_graph, meshcat)

plant_context = plant.CreateDefaultContext()

kp=[100] * plant.num_positions()
ki=[1] * plant.num_positions()
kd=[20] * plant.num_positions()

planar_controller = builder.AddSystem(
    InverseDynamicsController(plant, kp, ki, kd, False))
planar_controller.set_name(&quot;planar_controller&quot;)
builder.Connect(plant.get_state_output_port(),
                planar_controller.get_input_port_estimated_state())
builder.Connect(planar_controller.get_output_port_control(),
                plant.get_actuation_input_port())

diagram = builder.Build()
context = diagram.CreateDefaultContext()

states = [1, 0, 0, -1.57, 0.1, 0, -1.2, 0, 1.6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
planar_controller.GetInputPort('desired_state').FixValue(
    planar_controller.GetMyMutableContextFromRoot(context), states)

simulator = Simulator(diagram, context)
visualizer.StartRecording()
simulator.AdvanceTo(12.0)
visualizer.StopRecording()
visualizer.PublishRecording()
</code></pre>
<p>Yaml file for the scene:</p>
<pre><code>directives:          
    - add_model:
        name: cargo-space
        file: file:///usr/cargobot/cargobot-project/trajopt/cargobot-models/cargo-space.sdf

    - add_weld:
        parent: world
        child: cargo-space::cargo-space
        X_PC:
            rotation: !Rpy { deg: [0, 90, 0 ]}
            translation: [-1.5, 0, 0.4]

    - add_frame: 
        name: table_top_center
        X_PF:
            base_frame: world
            rotation: !Rpy { deg: [0, 0, 0]}
            translation: [0, 0, -0.5]

    - add_model:
        name: table_top
        file: file:///usr/cargobot/cargobot-project/trajopt/cargobot-models/table_top.sdf

    - add_weld: 
        parent: table_top_center
        child: table_top_link

    - add_model:
        name: iiwa
        file: package://drake/manipulation/models/iiwa_description/iiwa7/iiwa7_no_collision.sdf
        default_joint_positions:
            iiwa_joint_1: [-1.57]
            iiwa_joint_2: [0.1]
            iiwa_joint_3: [0]
            iiwa_joint_4: [-1.2]
            iiwa_joint_5: [0]
            iiwa_joint_6: [ 1.6]
            iiwa_joint_7: [0]

    - add_model:
        name: wsg
        file: package://drake/manipulation/models/wsg_50_description/sdf/schunk_wsg_50_with_tip.sdf
    - add_weld:
        parent: iiwa::iiwa_link_7
        child: wsg::body
        X_PC:
            translation: [0, 0, 0.09]
            rotation: !Rpy { deg: [90, 0, 90]}
</code></pre>
<p>We tried to set the robot 1 unit to the left.</p>
",3/10/2023 19:41,,100,0,3,1,,20705465.0,,12/6/2022 17:42,4.0,,,,,,133602815.0,"Adding 2 prismatic joints and 1 revolute joint before the iiwa_link_1 to the given iiwa7 sdf worked! I still do not understand the upper solution did not work, though."
269,1085351,An algorithm for a drawing and painting robot - any tips?,|image-processing|artificial-intelligence|computer-vision|robotics|,"<p>Algorithm for a drawing and painting robot -</p>

<p>Hello </p>

<p>I want to write a piece of software which analyses an image, and then produces an image which captures what a human eye perceives in the original image, using a minimum of bezier path objects of varying of colour and opacity. </p>

<p>Unlike the recent twitter super compression contest <em>(<a href=""https://stackoverflow.com/questions/891643/twitter-image-encoding-challenge"">see: stackoverflow.com/questions/891643/twitter-image-encoding-challenge</a>)</em>, my goal is not to create a replica which is faithful to the image, but instead to replicate the human experience of looking at the image.</p>

<p>As an example, if the original image shows a red balloon in the top left corner, and the reproduction has something that looks like a red balloon in the top left corner then I will have achieved my goal, even if the balloon in the reproduction is not quite in the same position and not quite the same size or colour.</p>

<p>When I say ""as perceived by a human"", I mean this in a very limited sense.  i am not attempting to analyse the meaning of an image, I don't need to know what an image is of, i am only interested in the key visual features a human eye would notice, to the extent that this can be automated by an algorithm which has no capacity to conceptualise what it is actually observing.</p>

<p>Why this unusual criteria of human perception over photographic accuracy?</p>

<p>This software would be used to drive a drawing and painting robot, which will be collaborating with a human artist <em>(<a href=""http://video.google.com/videosearch?q=mr%20squiggle"" rel=""nofollow noreferrer"">see: video.google.com/videosearch?q=mr%20squiggle</a>)</em>.</p>

<p>Rather than treating marks made by the human which are not photographically perfect as necessarily being mistakes, The algorithm should seek to incorporate what is already on the canvas into the final image.</p>

<p>So relative brightness, hue, saturation, size and position are much more important than being photographically identical to the original. The maintaining the topology of the features, block of colour, gradients, convex and concave curve will be more important the exact size shape and colour of those features</p>

<p>Still with me?</p>

<p>My problem is that I suffering a little from the ""when you have a hammer everything looks like a nail"" syndrome. To me it seems the way to do this is using a genetic algorithm with something like the comparison of wavelet transforms <em>(see: <a href=""http://grail.cs.washington.edu/projects/query/"" rel=""nofollow noreferrer"">grail.cs.washington.edu/projects/query/</a>)</em> used by retrievr <em>(see: <a href=""http://labs.systemone.at/retrievr/"" rel=""nofollow noreferrer"">labs.systemone.at/retrievr/</a>)</em> to select fit solutions.</p>

<p>But the main reason I see this as the answer, is that these are these are the techniques I know, there are probably much more elegant solutions using techniques I don't now anything about.</p>

<p>It would be especially interesting to take into account the ways the human vision system analyses an image, so perhaps special attention needs to be paid to straight lines, and angles, high contrast borders and large blocks of similar colours.</p>

<p>Do you have any suggestions for things I should read on vision, image algorithms, genetic algorithms or similar projects?</p>

<p>Thank you</p>

<p>Mat</p>

<p><i>PS. Some of the spelling above may appear wrong to you and your spellcheck. It's just international spelling variations which may differ from the standard in your country: e.g. Australian standard: colour vs American standard: color </i></p>
",7/6/2009 3:56,1126601.0,1972,5,1,9,0.0,133507.0,,7/6/2009 3:36,190.0,1085373.0,"<p>That's quite a big task. You might be interested in image vectorizing (don't know what it's called officially), which is used to take in rasterized images (such as pictures you take with a camera) and outputs a set of bezier lines (i think) that approximate the image you put in. Since good algorithms often output very high quality (read: complex) line sets you'd also be interested in simplification algorithms which can help enormously. </p>
",117069.0,1.0,1.0,56553786.0,"For the benefit of new users I want to point out that this question would not be allowed to exist if posted today. It is very broad, asks for general recommendations, and doesn't have a definite answer. It likely still exists for historical purposes so don't use this as a template for a good question"
2995,48271579,How to do sensor fusion?,|sensors|robotics|,"<p>Let suppose that I have two measures from two difference sensors of the same variable. I'd like to know if there's a way to do an information fusion and obtain a unique measure that describes the best way possible the whole system (Both sensors). </p>

<p>I know Bar-Shalom - Campo sensor fusion model, but I'd like to know if there are any model that doesn't adopte the classical Gaussian assumption, so that the sensor fusion can deal with bad data/gross erros.</p>

<p>Thank you.</p>
",1/15/2018 22:29,48337666.0,248,2,2,0,,,,,,48337666.0,"<p>For sensor fusion, you can go for Kalman Filter. There are few tutorials and research papers available for extended kalman filter, used for sensor fusion. </p>
",9238248.0,0.0,0.0,83526941.0,"FYI, as you did this twice. Plural form of `sensor` is `sensors` not `sensores`"
475,2625390,2D Inverse Kinematics Implementation,|animation|rotation|robotics|inverse-kinematics|,"<p>I am trying to implement Inverse Kinematics on a 2D arm(made up of three sticks with joints). I am able to rotate the lowest arm to the desired position. Now, I have some questions:</p>

<ol>
<li><p>How can I make the upper arm move alongwith the third so the end point of the arm reaches the desired point. Do I need to use the rotation matrices for both and if yes can someone give me some example or an help and is there any other possibl;e way to do this without rotation matrices???</p></li>
<li><p>The lowest arm only moves in one direction. I tried google it, they are saying that cross product of two vectors give the direction for the arm but this is for 3D. I am using 2D and cross product of two 2D vectors give a scalar. So, how can I determine its direction???</p></li>
</ol>

<p>Plz guys any help would be appreciated....</p>

<p>Thanks in advance
Vikram</p>
",4/12/2010 20:56,,7755,4,0,10,0.0,314916.0,,4/12/2010 20:52,69.0,2625610.0,"<p>I'll give it a shot, but since my Robotics are two decades in the past, take it with a grain of salt.</p>

<p>The way I learned it, every joint was described by its own rotation matrix, defined relative to its current position and orientation. The coordinate of the whole arm's endpoint was then calculated by combining the rotation matrices together. </p>

<p>This achieved exactly the effect you are looking for: you could move only one joint (change its orientation), and all the other joints followed automatically.</p>

<p>You won't have much chance in getting around matrices here - in fact, if you use homogeneous coordinates, all joint calculations (rotations as well as translations) can be modeled with matrix multiplications. The advantage is that the full arm position can then be described with a single matrix (plus the arm's origin).</p>

<p>With this transformation matrix, you can tackle the inverse kinematic problem: since the transformation matrix' elements will depend on the angles of the joints, you can treat the whole calculation 'endpoint = startpoint x transformation' as a system of equations, and with startpoint and endpoint known, you can solve this system to determine the unknown angles. The difficulty herein lies that the equation may not be solvable, or that there are multiple solutions.</p>

<p>I don't quite understand your second question, though - what are you looking for?</p>
",258146.0,7.0,0.0,,
3385,55997039,Algorithms for a line follower robot (with camera) capable following very sharp turns and junction,|python|opencv|computer-vision|robotics|opencv3.1|,"<p>I want to write code (python,opencv) for a line follower robot equipped with a camera and Raspberry Pi. I want to make to robot go has fast as possible</p>

<ol>
<li>The course has few very sharp turns like this: <a href=""https://i.stack.imgur.com/g7G8C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g7G8C.png"" alt=""https://imgur.com/a/h07WlAD/""></a> I'm assuming that using ROI (region of interest) will not work well when the robot in near the turn (it will also capture/""see"" the other line) - for example as shown below. What is the best approach here?</li>
</ol>

<p><a href=""https://i.stack.imgur.com/walJK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/walJK.png"" alt=""https://imgur.com/a/b9K66Rp""></a></p>

<ol start=""2"">
<li><p>In the course there is a junction as shown in below image,
<a href=""https://i.stack.imgur.com/tZgNA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tZgNA.png"" alt=""enter image description here""></a></p>

<p>How to ""understand"" that this is a junction? and if the robot is
coming for the bottom of the image, how to make the algorithm
continue driving straight and not get confused by the horizontal
line?</p></li>
</ol>
",5/5/2019 22:42,,2045,1,3,1,,11455950.0,,5/5/2019 16:25,8.0,62512400.0,"<p>I can recommend this awesome video on YouTube:</p>
<p><a href=""https://www.youtube.com/watch?v=tpwokAPiqfs&amp;t=868s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=tpwokAPiqfs&amp;t=868s</a></p>
<p>It contains several episodes and teachs a lot of useful stuff. Good luck!!!</p>
",13749389.0,0.0,0.0,98641189.0,"I'm experimenting, I wrote a code that slices the input image from the camera (horizontal). On each slice I run threshold-> findcontours-> find the centeroid of the contour. I keep track on the line using a sliding window, because on the right to of the line (in some places) there are signaling lines and I need to filter them out. When I get close to a sharp  turn, the sliding doesn't work well"
4481,75006535,How to calculate the angle between Quaternions,|python|ros|sensors|robotics|imu|,"<p>I have two IMU sensors giving me Euler angles. I convert them to Quaternion by following</p>
<pre><code>quaternion_angle_1=tf.transformations.quaternion_from_euler(roll_1,pitch_1,yaw_1)
quaternion_angle_2=tf.transformations.quaternion_from_euler(roll_2,pitch_2,yaw_2)
</code></pre>
<p>Now I want to caculate the angle between these measurements from the IMU sensors. How can I do that?</p>
",1/4/2023 13:57,,436,2,1,1,,14447735.0,"Peshawar, Pakistan",10/14/2020 8:46,41.0,75009966.0,"<p>Sounds like you want the relative rotation between two quaternions. You could simple calculate the angle between the euler angles, then convert that to a quaternion. But if you need to do it strictly with quaternions <code>tf</code> allows you to do this directly:</p>
<pre><code>relative_quat = quaternion_angle_1 * quaternion_angle_2.inverse()
</code></pre>
",11245187.0,0.0,0.0,132369553.0,Does this answer your question? [How to obtain the angle between two quaternions?](https://stackoverflow.com/questions/57063595/how-to-obtain-the-angle-between-two-quaternions)
4740,77738665,Calculating IMU random walk using allan variance,|ros|robotics|calibration|imu|,"<p>I am trying to calculate accel and gyro noise and random walk for Intel realsense using <a href=""https://github.com/ori-drs/allan_variance_ros"" rel=""nofollow noreferrer"">allan variance</a>. I quickly generated imu csv in the format specified <a href=""https://github.com/ethz-asl/kalibr/wiki/bag-format"" rel=""nofollow noreferrer"">here</a> using custom CPP script for just 3 seconds. I wanted to check if it will work or not before recording 20 hours of IMU data.</p>
<p>I created ros bag from csv by running:</p>
<pre><code>rosrun kalibr kalibr_bagcreater --folder ./data/imu_data --output-bag ./data/newbag.bag
</code></pre>
<p>Checking the bag created:</p>
<pre><code>root@e9408da6622e:/data# rosbag info /data/data/newbag.bag  
path:        /data/data/newbag.bag 
version:     2.0 
duration:    2.4s 
start:       Dec 30 2023 18:41:25.42 (1703961685.42) 
end:         Dec 30 2023 18:41:27.84 (1703961687.84) 
size:        312.7 KB 
messages:    830 
compression: none [1/1 chunks] 
types:       sensor_msgs/Imu [6a62c6daae103f4ff57a132d6f95cec2] 
topics:      /imu0   830 msgs    : sensor_msgs/Imu 
</code></pre>
<p>Then I converted it to allan variance compliant bag by running:</p>
<pre><code>rosrun allan_variance_ros cookbag.py --input /data/data/imu_data/newbag.bag --output /data/data/imu_data/cooked_bag.bag 
</code></pre>
<p>Checking the bag created:</p>
<pre><code>root@e9408da6622e:/catkin_ws# rosbag info /data/data/imu_data/cooked_bag.bag  
path:         /data/data/imu_data/cooked_bag.bag 
version:      2.0 
duration:     2.4s 
start:        Dec 30 2023 18:41:25.42 (1703961685.42) 
end:          Dec 30 2023 18:41:27.84 (1703961687.84) 
size:         41.7 KB 
messages:     830 
compression:  lz4 [1/1 chunks; 8.47%] 
uncompressed: 296.1 KB @ 122.5 KB/s 
compressed:    25.1 KB @  10.4 KB/s (8.47%) 
types:        sensor_msgs/Imu [6a62c6daae103f4ff57a132d6f95cec2] 
topics:       /imu0   830 msgs    : sensor_msgs/Imu
</code></pre>
<p>I created allan variance config file imu_confog.yaml:</p>
<pre><code>imu_topic: &quot;/imu0&quot;
imu_rate: 346
measure_rate: 346 # since bag contains 830 messages / 2.4 seconds = 345.8333 approx 346
sequence_time: 3 # since bag contains 2.4 seconds worth of data
</code></pre>
<p>(I also tried with both rates 400, no luck.)</p>
<p>Here is how my data folder looks like:</p>
<p><img src=""https://github.com/ori-drs/allan_variance_ros/assets/52235655/dbddd258-5c01-41b7-87d4-9e03aa431e3e"" alt=""image"" /></p>
<p>Then I run allan variance:</p>
<pre><code>root@e9408da6622e:/catkin_ws# rosrun allan_variance_ros allan_variance /data/data/imu_data /data/data/allan_variance/imu_config.yaml
</code></pre>
<p>And finally run Analyse.py:</p>
<pre><code>root@e9408da6622e:/catkin_ws# rosrun allan_variance_ros analysis.py --data /data/data/imu_data/allan_variance.csv --config /data/data/allan_variance/imu_config.yaml
/catkin_ws/src/allan_variance_ros/scripts/analysis.py:23: RuntimeWarning: divide by zero encountered in log
  logy = np.log(y)
Traceback (most recent call last):
  File &quot;/catkin_ws/src/allan_variance_ros/scripts/analysis.py&quot;, line 101, in &lt;module&gt;
    accel_wn_intercept_x, xfit_wn = get_intercept(period[0:white_noise_break_point], acceleration[0:white_noise_break_point,0], -0.5, 1.0)
  File &quot;/catkin_ws/src/allan_variance_ros/scripts/analysis.py&quot;, line 24, in get_intercept
    coeffs, _ = curve_fit(line_func, logx, logy, bounds=([m, -np.inf], [m + 0.001, np.inf]))
  File &quot;/usr/lib/python3/dist-packages/scipy/optimize/minpack.py&quot;, line 708, in curve_fit
    ydata = np.asarray_chkfinite(ydata, float)
  File &quot;/usr/lib/python3/dist-packages/numpy/lib/function_base.py&quot;, line 495, in asarray_chkfinite
    raise ValueError(
ValueError: array must not contain infs or NaNs
</code></pre>
<p>But as you can see it is giving error <code>array must not contain infs or NaNs</code>. Turns out there are some NaNs in allan_variance.csv file generated. What I am doing wrong here?</p>
<p>I have uploaded all bags and csv file on drive <a href=""https://drive.google.com/drive/folders/15M2P3NHtgvJvRPZMNqd5uMvz1R0GN-0S?usp=sharing"" rel=""nofollow noreferrer"">here</a>.</p>
",12/31/2023 8:56,,41,0,0,0,,6357916.0,,5/19/2016 19:14,480.0,,,,,,,
4114,69425729,Why does my program makes my robot turn the power off?,|python|raspberry-pi|controls|robotics|pid-controller|,"<p>I'm trying to put together a programmed robot that can navigate the room by reading instructions off signs (such as bathroom-right). I'm using the AlphaBot2 kit and an RPI 3B+.</p>
<p>the image processing part works well, but for some reason, the MOTION CONTROL doesn't work.
I wrote a simple PID controller that &quot;feeds&quot; the motor, but as soon as motors start turning, the robot turns off.</p>
<pre><code>iPWM = 20 # initial motor power in duty cycle
MAX_PWM = 100 
dt = 0.001 # time step

#PID PARAMETERS#
KP = 0.2
KD = 0.01
KI = 0.00005

targets = ['BATHROOM', 'RESTAURANT', 'BALCONY']

class PID(object):
    def __init__(self,target):
        
        self.target = target
        self.kp = KP
        self.ki = KI
        self.kd = KD 
        self.setpoint = 320
        self.error = 0
        self.integral_error = 0
        self.error_last = 0
        self.derivative_error = 0
        self.output = 0
        
    def compute(self, pos):
        self.error = self.setpoint - pos
        #self.integral_error += self.error * TIME_STEP
        self.derivative_error = (self.error - self.error_last) / dt
        self.error_last = self.error
        self.output = self.kp*self.error + self.ki*self.integral_error + self.kd*self.derivative_error
        if(abs(self.output)&gt;= MAX_PWM-iPWM and (((self.error&gt;=0) and (self.integral_error&gt;=0))or((self.error&lt;0) and (self.integral_error&lt;0)))):
            #no integration
            self.integral_error = self.integral_error
        else:
            #rectangular integration
            self.integral_error += self.error * dt
        
        if self.output&gt;= MAX_PWM-iPWM:
            self.output = MAX_PWM-iPWM
    
            
        elif self.output &lt;= -MAX_PWM:
            self.output = iPWM - MAX_PWM
        return self.output
        



class MOTORS(object):
    
    def __init__(self,ain1=12,ain2=13,ena=6,bin1=20,bin2=21,enb=26):
        self.AIN1 = ain1
        self.AIN2 = ain2
        self.BIN1 = bin1
        self.BIN2 = bin2
        self.ENA = ena
        self.ENB = enb
        self.PA  = iPWM
        self.PB  = iPWM

        GPIO.setmode(GPIO.BCM)
        GPIO.setwarnings(False)
        GPIO.setup(self.AIN1,GPIO.OUT)
        GPIO.setup(self.AIN2,GPIO.OUT)
        GPIO.setup(self.BIN1,GPIO.OUT)
        GPIO.setup(self.BIN2,GPIO.OUT)
        GPIO.setup(self.ENA,GPIO.OUT)
        GPIO.setup(self.ENB,GPIO.OUT)
        self.PWMA = GPIO.PWM(self.ENA,500)
        self.PWMB = GPIO.PWM(self.ENB,500)
        self.PWMA.start(self.PA)
        self.PWMB.start(self.PB)
        self.stop()

    def forward(self):
        self.PWMA.ChangeDutyCycle(self.PA)
        self.PWMB.ChangeDutyCycle(self.PB)
        GPIO.output(self.AIN1,GPIO.LOW)
        GPIO.output(self.AIN2,GPIO.HIGH)
        GPIO.output(self.BIN1,GPIO.LOW)
        GPIO.output(self.BIN2,GPIO.HIGH)
    
    def updatePWM(self,value):
        if value&lt;0:
            self.PA = iPWM+abs(value)
            self.PB = iPWM
            self.PWMA.ChangeDutyCycle(self.PA)
            self.PWMB.ChangeDutyCycle(self.PB)
        if value&gt;0:
            self.PA = iPWM
            self.PB = iPWM+value
            self.PWMB.ChangeDutyCycle(self.PB)
            self.PWMA.ChangeDutyCycle(self.PA)
        if value ==0:
            self.PA = iPWM
            self.PB = iPWM
            self.PWMB.ChangeDutyCycle(self.PB)
            self.PWMA.ChangeDutyCycle(self.PA)
            
        GPIO.output(self.AIN1,GPIO.LOW)
        GPIO.output(self.AIN2,GPIO.HIGH)
        GPIO.output(self.BIN1,GPIO.LOW)
        GPIO.output(self.BIN2,GPIO.HIGH)
        
        
    
    def stop(self):
        self.PWMA.ChangeDutyCycle(0)
        self.PWMB.ChangeDutyCycle(0)
        GPIO.output(self.AIN1,GPIO.LOW)
        GPIO.output(self.AIN2,GPIO.LOW)
        GPIO.output(self.BIN1,GPIO.LOW)
        GPIO.output(self.BIN2,GPIO.LOW)
</code></pre>
<p>then I have the loop over the camera captures, where i identify the nearest sign and calculate his width and x coordiante of its center:</p>
<pre><code>cx = int(x+w//2)

    if d&lt;= 60:
        mot.stop()
        GPIO.cleanup()
dutyCycle = pid.compute(cx)
pwm = mot.updatePWM(dutyCycle)


</code></pre>
",10/3/2021 14:22,69425825.0,87,1,2,0,,4912015.0,,5/18/2015 12:22,21.0,69425825.0,"<p>It is probably not the software. Your power supply is not sufficient or stable enough to power your motors and the Raspberry Pi.  It is a very common problem. Either:</p>
<ul>
<li>Use separate power supplies which is recommended</li>
<li>Or Increase your main power supply and use some short of stabilization of power</li>
</ul>
<p>What power supply and power configuration are you using?</p>
",14649310.0,1.0,5.0,122710213.0,is it possible that if ill turn off the VNC viewer and won't show any image will sort it out?
3675,60927654,How to control Turtlebot3 in Gazebo using a web interface?,|html|ros|robot|gazebo-simu|,"<p>If I have a simulated Turtlebot3 robot in Gazebo, how could I link it and control its movement using a self-made HTML/Bootstrap web interface (website?) I have tried many tutorials but none of them have worked (may be because they are all from a few years ago). Would appreciate any recent links or tutorials!</p>
",3/30/2020 9:33,,1019,2,0,0,,13151812.0,,3/30/2020 9:29,8.0,60936765.0,"<p>This is not something I have done before, but with a quick search I found some useful information.</p>

<p>You need to use <a href=""http://wiki.ros.org/rosbridge_suite?distro=melodic"" rel=""nofollow noreferrer""><code>rosbridge_suite</code></a> and in specific <a href=""http://wiki.ros.org/rosbridge_server"" rel=""nofollow noreferrer""><code>rosbridge_server</code></a>. The latter provides a low-latency bidirectional communication layer between a web browser and servers. This allows a website to talk to ROS using the rosbridge protocol.</p>

<p>Therefore, you need to have this suite installed and then what you can do is to use it to publish a <code>Twist</code> message from the website (based on the website UI controls) to Turtlebot's command topic.</p>

<p>Don't think of Gazebo in this equation. Gazebo is the simulator and is using under-the-hood ROS topics and services to simulate the robot. What you really need to focus on is how to make your website talk with ROS and publish a Twist message to the appropriate ROS topic.</p>

<p>I also found a JavaScript library from ROS called <a href=""http://wiki.ros.org/roslibjs"" rel=""nofollow noreferrer""><code>roslibjs</code></a> that implements the <code>rosbridge</code> protocol specification. You can, therefore, use JavaScript to communicate with ROS and publish robot velocities to the TurtleBot.</p>

<p>An example excerpt from <a href=""http://wiki.ros.org/roslibjs/Tutorials/BasicRosFunctionality"" rel=""nofollow noreferrer"">this</a> tutorial (not tested):</p>

<pre><code>&lt;script type=""text/javascript"" type=""text/javascript""&gt;
  var cmdVel = new ROSLIB.Topic({
    ros : ros,
    name : '/cmd_vel',
    messageType : 'geometry_msgs/Twist'
  });

  var twist = new ROSLIB.Message({
    linear : {
      x : 0.1,
      y : 0.2,
      z : 0.3
    },
    angular : {
      x : -0.1,
      y : -0.2,
      z : -0.3
    }
  });

  cmdVel.publish(twist);
&lt;/script&gt;
</code></pre>

<p>As you can see above the JavaScript code creates an instance of the Twist message with the linear and angular robot velocities and then publishes this message to ROS's <code>/cmd_vel</code> topic. What you need to do is to integrate this into your website, make the velocities in this code to be dynamic based on the website UI controls and start the <code>rosbridge</code> server.</p>
",4946896.0,0.0,0.0,,
2561,39203597,How to order one dimensional matrices base on values,|matlab|matrix|geometry|coordinate-systems|robotics|,"<p>I want to determine a point in space by geometry and I have math computations that gives me several theta values. After evaluating the theta values, I could get N 1 x 3 dimension matrix where N is the number of theta evaluated. Since I have my targeted point, I only need to decide which of the matrices is closest to the target with adequate focus on the three coordinates (x,y,z).
Take a view of the analysis in the figure below:</p>

<p><a href=""https://i.stack.imgur.com/elRC7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/elRC7.png"" alt=""Fig 1""></a>
<br><sup><em>Fig 1: Determining Closest Point with all points having minimal error</em></sup></p>

<p>It can easily be seen that the third matrix is closest using <code>sum(abs(Matrix[x,y,z])).</code> However, if the method is applied on another figure given below, obviously, the result is wrong. </p>

<p><a href=""https://i.stack.imgur.com/SVfq5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SVfq5.png"" alt=""Fig 2""></a>
<br><sup><em>Fig 2: One Point has closest values with 2-axes of the reference point</em></sup></p>

<p>Looking at point B, it is closer to the reference point on y-,z- axes but just that it strayed greatly on x-axis.</p>

<p>So how can I evaluate the matrices and select the closest one to point of reference and adequate emphasis will be on error differences in all coordinates (x,y,z)?</p>
",8/29/2016 10:05,39809301.0,61,1,1,1,,6766411.0,,8/28/2016 7:34,13.0,39809301.0,"<p>If your results is in terms of (x,y,z), why don't evaluate the euclidean distance of each matrix you have obtained from the reference point?</p>

<p>Sort of matlab code:</p>

<pre><code>Ref_point = [48.98, 20.56, -1.44];
Curr_point = [x,y,z];
Xd = (x-Ref_point(1))^2 ;
Yd = (y-Ref_point(2))^2 ;
Zd = (z-Ref_point(3))^2 ;
distance = sqrt(Xd + Yd + Zd);
%find the minimum distance
</code></pre>
",6564973.0,0.0,0.0,65765849.0,"I don't think an ordering is really what you need.  I think what you're really asking about is ""spatial indexing"" (https://en.wikipedia.org/wiki/Spatial_database#Spatial_index).  For example: https://en.wikipedia.org/wiki/R-tree."
1149,6140332,Real time live streaming with an iPhone for robotics,|objective-c|sockets|video-streaming|vlc|robotics|,"<p>For a research purpose, I developed and app to control a wheeled mobile robot using the gyro and the accelerometer of an iPhone. The robot has a IP address, and I control it by sending messages through a socket. Since the robot has to be controlled from anywhere in the world, I mounted a camera on top of it. I tried to stream the video from the camera using the http live streaming protocol and vlc, but the latency is too high (15-30sec) to properly control it.</p>

<p>Now, vlc has the possibility to stream over udp or http, but the point is how do I decode the stream on the iPhone? How should I treat the data coming into the socket in order to visualize them as a continuous live video? </p>
",5/26/2011 14:34,,644,0,2,1,,771498.0,,5/26/2011 14:34,11.0,,,,,,7303231.0,I'm not sure I understood your question. You are already streaming via http and you want udp? Is that it?
1677,15250748,Maze/Grid continued - Finding an optimal solution - memory/learning issue (JAVA),|java|game-physics|robot|maze|,"<p>In relation to a previous questions found here: <a href=""https://stackoverflow.com/questions/15096066/connecting-a-maze-grids-walls-so-all-are-interconnected"">Connecting a maze/grid&#39;s walls so all are interconnected</a></p>

<p>Currently my robot reads the grid location in front to the left, directly in front and in front to the right like so:</p>

<pre><code>[00][01][02]
[10][11][12]
[20][^^][22]
</code></pre>

<p>[21] facing north would read [10][11][12] which may contain an object/trail its looking for.</p>

<p>There is a trail within my grid that I've made my robot able to follow around mapping the sensor inputs onto actions like so:</p>

<pre><code>Sensor Actions:
000 =
001 = 
010 =
011 =
100 =
101 =
110 =
111 =
</code></pre>

<p>These can equal turnleft(), turnright(), goforward() or do nothing.</p>

<p>Currently it can find 14/15 of the trail pieces but cannot find the last piece which has a gap from the trail like so:</p>

<pre><code>#KEY: x = empty, R = robot start position, T = trail

[x][x][x][x][x][x][x][x]
[x][x][x][x][x][x][T][x]
[x][x][x][x][x][x][x][x] &lt;- Here is the gap!
[R][T][T][T][T][x][T][x]
[x][x][x][x][T][x][T][x]
[x][x][x][x][T][x][T][x]
[x][x][x][x][T][x][T][x]
[x][x][x][x][T][T][T][x]
</code></pre>

<p>The problem I have is when it reaches a gap in the trial it can't deal with it and ends up spinning pointlessly until it runs out of lifes. </p>

<p>I know I need to add in some form of memory or learning but I'm not sure what or how to implement this!</p>

<p>Any suggestions?</p>
",3/6/2013 15:01,,267,1,0,0,,1082213.0,Southwest England,12/5/2011 19:44,175.0,15252750.0,"<p>Hard to know for sure without your code, but it sounds like your robot is just lacking a way to move onto non-trail spaces. Assuming you've got something basically like this (pseudocode):</p>

<pre><code>if (isTrailAhead) {
    moveAhead();
} else {
    turn();
}
</code></pre>

<p>Obviously this won't let you bridge gaps, because you'll keep spinning in circles without a trail to follow. There are a lot of ways you might go about dealing with this problem; the first two to come to my mind are:</p>

<ol>
<li><p>Store a counter and increment it each time you look for a trail space and don't see one. If the counter gets above a certain number (say 4), just move straight ahead regardless of what's there (assuming that's possible). Reset the counter to 0 every time you move ahead.</p>

<pre><code>if (isTrailAhead) {
    moveAhead();
} else {
    if (turnCounter &gt; 4) {
        moveAhead();
    else {
        turn();
    }
}
</code></pre>

<p>Of course in this case you need an instance variable named turnCounter, and your moveAhead() and turn() functions should increment or reset this counter as appropriate.</p></li>
<li><p>In the pseudo-else-clause above (when you don't see a trail), generate a random number between 0 and 10. If it's greater than some predetermined number, move ahead if possible.</p>

<pre><code>if (isTrailAhead) {
    moveAhead();
} else {
    if (new Random().nextInt(10) &gt;= 8) {
        moveAhead();
    else {
        turn();
    }
}
</code></pre></li>
</ol>

<p>Basically, all you need is a way to break out of loops. Either of these should do the trick for you.</p>
",2069350.0,0.0,0.0,,
2010,24986253,Touch sensor not working,|c|robotics|nxt|,"<p>I am attempting to create a very simple program in RobotC. In this program the robot will move forward until the touch sensor is hit.</p>

<pre><code>#pragma config(Sensor, S2,     touchSensor,    sensorTouch)

void setMotors(int a, int b){
    motor[motorA] = a;
    motor[motorB] = b;
}

task main(){
    wait1Msec(100);//Wait for sensor to init

    setMotors(50, 50);

    while(sensorValue(touchSensor) == 0){
        //Do Nothing
    }

    setMotors(0, 0);
}
</code></pre>

<p>This code should make the robot move forward until the touch sensor is triggered.
Whenever I try and do anything with the touch sensor it does not work. When I output the value to the debug log it shows 180 when pressed and 1024 when released. I have verified that it is working normally by viewing the value on the brick itself.</p>

<p>Robot C Version: 4.0</p>
",7/27/2014 22:26,24987679.0,192,1,10,1,,1478191.0,Earth,6/24/2012 14:26,483.0,24987679.0,"<p>Apparently, your touch sensor is <a href=""http://www.robotc.net/wiki/NXT_Functions_Sensors#SensorRaw"" rel=""nofollow"">stuck in SensorRaw mode</a>. It is unclear - from the documentation I could find - how this could be fixed in code, but a work-around would be to explicitly put the sensor into raw mode (in case the situation changes in the future), and then compute the boolean value with a function like this:</p>

<pre><code>bool sensorIsOn(short sensorRawValue)
{
    bool isOn = false;
    if(sensorRawValue &gt; 512)
    {
        isOn = true;
    }
    return isOn;
}
</code></pre>
",1896761.0,2.0,0.0,38844972.0,"Awesome, that's what I'd expect given those values. So, change the while statement to `while(sensorValue(touchSensor) > 500)` and let me know what happens."
2786,44097582,rigid body transformation with only two points,|python|matrix|3d|gps|robotics|,"<p>I have a tractor with two GPS antennas which are mounted at the front of the tractor on either side.</p>

<p><a href=""https://i.stack.imgur.com/dCNT2.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dCNT2.jpg"" alt=""view of the front of the tractor""></a></p>

<p>I get the position (latitude and longitude) of the right antenna (left in the photo) and the north, east, down projection to the second antenna. I have been attempting to plant straight rows simply using the midpoint of the line between the two antennas. I'm getting a lot more roll than I expected and my rows are far from straight.</p>

<p>I want to know the position of a point on the ground. The point of interest is the center point between the two antennas, projected perpendicular to the line between the antennas. I imagine it as the point at the base of a ""T"" where the ends of the top bar of the ""T"" are each antenna and the ""T"" is allowed to rotate freely such that it points toward the ground. Incidentally, the antennas are 1 m apart and 2.4 m high when operating.  A little bit of roll makes a significant difference.</p>

<p>Here's generally what I think I want:</p>

<ol>
<li>Choose an origin point.  (It's common with farm equipment to use the
midpoint of the rear axle.) </li>
<li>Measure to determine the coordinates of the antennas.</li>
<li>Choose the coordinates of the point of interest.</li>
<li>Calculate the UTM coordinates for antenna A from the receiver output.</li>
<li>Calculate the UTM coordinates of antenna B from the receiver projection.</li>
<li>Plug all of this information into a magical equation.</li>
<li>Get the UTM coordinates of the point of interest.</li>
</ol>

<p>6 is the tricky one. I get roll information from the two antennas but I assume that there is zero pitch for now. This simplifies the problem enough that I was able to solve it using simple trig. This morning I'm thinking about how to do it using unit vectors.</p>

<p>I would much prefer to have a general way to do this. Then I would be able to add pitch information (from an IMU/INS) and also determine the position of my hitch, each row of the planter, etc.</p>

<p>From reading other questions here, I think I want to do a rigid body transformation with a constraint (no pitch). Am I close? I strongly prefer a Python solution.</p>
",5/21/2017 13:41,,150,0,1,1,,7362612.0,"Rensselaer, IN, United States",1/1/2017 14:21,35.0,,,,,,75425224.0,This might help: http://nghiaho.com/?page_id=671
3172,51385737,ROS Human-Robot mapping (Baxter),|ros|virtual-reality|robotics|,"<p>I'm having some difficulties understanding the concept of teleoperation in ROS so hoping someone can clear some things up.</p>

<p>I am trying to control a Baxter robot (in simulation) using a HTC Vive device. I have a node (publisher) which successfully extracts PoseStamped data (containing pose data in reference to the lighthouse base stations) from the controllers and publishes this on separate topics for right and left controllers.</p>

<p>So now I wish to create the subscribers which receive the pose data from controllers and converts it to a pose for the robot. What I'm confused about is the mapping... after reading documentation regarding Baxter and robotics transformation, I don't really understand how to map human poses to Baxter.</p>

<p>I know I need to use IK services which essentially calculate the co-ordinates required to achieve a pose (given the desired location of the end effector). But it isn't as simple as just plugging in the PoseStamped data from the node publishing controller data to the ik_service right?</p>

<p>Like a human and robot anatomy is quite different so I'm not sure if I'm missing a vital step in this.</p>

<p>Seeing other people's example codes of trying to do the same thing, I see that some people have created a 'base'/'human' pose which hard codes co-ordinates for the limbs to mimic a human. Is this essentially what I need?</p>

<p>Sorry if my question is quite broad but I've been having trouble finding an explanation that I understand... Any insight is very much appreciated!</p>
",7/17/2018 15:57,,223,1,1,1,,9051647.0,,12/4/2017 15:14,16.0,51389138.0,"<p>You might find my former student's work on <a href=""https://www.youtube.com/watch?v=zZtOlOuqEdw&amp;index=24&amp;list=LLMmUzvvz7M1JUoho1NzrRiA"" rel=""nofollow noreferrer"">motion mapping using a kinect sensor with a pr2</a> informative.  It shows two methods:</p>

<ol>
<li><p>Direct joint angle mapping  (eg if the human has the arm in a right angle then the robot should also have the arm in a right angle).</p></li>
<li><p>An IK method that controls the robot's end effector based on the human's hand position.</p></li>
</ol>

<blockquote>
  <p>I know I need to use IK services which essentially calculate the
  co-ordinates required to achieve a pose (given the desired location of
  the end effector). But it isn't as simple as just plugging in the
  PoseStamped data from the node publishing controller data to the
  ik_service right?</p>
</blockquote>

<p>Yes, indeed, this is a fairly involved process!  In both cases, we took advantage of the kinects api to access the human's joint angle values and the position of the hand.  You can read about how Microsoft research implemented the human skeleton tracking algorithm here:</p>

<p><a href=""https://www.microsoft.com/en-us/research/publication/real-time-human-pose-recognition-in-parts-from-a-single-depth-image/?from=http%3A%2F%2Fresearch.microsoft.com%2Fapps%2Fpubs%2F%3Fid%3D145347"" rel=""nofollow noreferrer"">https://www.microsoft.com/en-us/research/publication/real-time-human-pose-recognition-in-parts-from-a-single-depth-image/?from=http%3A%2F%2Fresearch.microsoft.com%2Fapps%2Fpubs%2F%3Fid%3D145347</a></p>

<p>I am not familiar with the Vive device. You should see if it offers a similar api for accessing skeleton tracking information since reverse engineering Microsoft's algorithm will be challenging.  </p>
",1556335.0,1.0,4.0,89754780.0,"If you can render a common reference frame for both the data coming from your VR setup and the real robot, then whatever poses you get from VR can be sent for control on the robot via an appropriate transformation. This assumes you have a task space controller for the robot. Then you can totally avoid any configuration mapping between humans and Baxter because it all operates in a common operational (task) space."
3393,56360142,What does it actually mean by Task Planning?,|robotics|,"<p>I have just started studying robotics. This is my first question to get started with robotics. Pardon me if this question is too naive.</p>

<p>What does it actually mean by <a href=""https://ieeexplore.ieee.org/document/56654"" rel=""nofollow noreferrer"">Task Planning</a> in case of robotics?</p>

<p>How is it different from conventional ""robot control""?</p>
",5/29/2019 12:00,56373815.0,40,1,0,0,,159072.0,,6/2/2009 18:25,6588.0,56373815.0,"<p><strong>Robot Motion control</strong> is generally dealt with low-level navigation like localization (what is the current location of the robot), path planning (chose a path from a to b by avoiding obstacles).</p>

<p><strong>Tasking planning</strong> is generally dealing with high-level planning. For example, task for a robot with a manipulator is to clean the desk (go to desk, find objects on desk and choose the order to pickup items, move the picked items to dusbin).</p>
",1595504.0,1.0,0.0,,
1232,7017136,Would Arduino be the most viable language to support the construction of a human-like robot?,|arduino|robotics|,"<p>If you wanted to build a human-like robot, what language would be most suitable?</p>

<p>Arduino?</p>

<p>Or would you need to write mostly low-level microcontroller-level code at the 1s and 0s level?</p>
",8/10/2011 20:12,,212,1,2,-1,0.0,888682.0,,8/10/2011 20:12,3.0,7017462.0,"<p>There's probably a few people typing the same response as I type...</p>

<p>The term Arduino can be a bit confusing for beginners. There is the Arduino microcontroller, which stores the executable code and contains inputs and outputs, and the Arduino IDE, which compiles code and uploads it to the Arduino microcontroller. As mentioned in <a href=""https://stackoverflow.com/questions/5931572/how-is-programming-an-arduino-different-than-standard-c/5931887#5931887"">How is programming an Arduino different than standard C?</a>, the programming language used to program the Arduino microcontroller is C or C++.</p>

<p>If a human-like robot is defined as a basic robot with two legs that move in a predefined routine, an Arduino should suffice.</p>

<p>To get started building a robot using an Arduino, <a href=""https://rads.stackoverflow.com/amzn/click/com/1430232404"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">Beginning Arduino</a> and <a href=""https://rads.stackoverflow.com/amzn/click/com/1430231831"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">Arduino Robotics</a> are two recent books that have been getting good reviews. I recommend starting with one of these books. If you have questions specific to microcontrollers or electronics, <a href=""https://electronics.stackexchange.com/"">https://electronics.stackexchange.com/</a> would be a good resource.</p>
",72035.0,3.0,0.0,8380246.0,"Arduino isn't a language, it's a prototyping platform that uses Atmel AVR microcontrollers. With some additional information, this might be a good question to ask on [Electronics Engineering StackExchange](http://electronics.stackexchange.com/)."
