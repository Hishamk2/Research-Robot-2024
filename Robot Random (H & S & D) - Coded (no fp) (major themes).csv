,Unnamed: 0,questionId,questionTitle,Tags,questionBody,questionCreationDate,AcceptedAnswerId,questionViewCount,AnswerCount,CommentCount,questionScore,questionFavoriteCount,questionUserId,questionUserLocation,questionUserCreationDate,questionUserViews,answerId,answerBody,answerUserId,answerScore,answerCommentCount,commentId,commentText,code
0,207,742369,Pathfinding Algorithm for Robot,|c|algorithm|artificial-intelligence|robotics|,"<p>I have a robot that uses an optical mouse as a position track. Basically, as the robot moves it is able to track change in X and Y directions using the mouse. The mouse also tracks which direction you are moving - ie negative X or positive X. These values are summed into separate X and Y registers. <br></p>

<p>Now, the robot rotates in place and moves forward only. So the movement of the robot is ideally in straight lines (although the mouse tracking can pickup deviations if you veer off) at particular angles. A particular set of movements of the robot would be like:
<br>
A: Rotate 45 degrees, move 3 inches <br>
B: Rotate 90 degrees, move 10 inches <br>
C: Rotate -110 degrees, move 5 inches <br>
D: Rotate 10 degrees, move 1 inch <br>
But each time the mouse X and mouse Y registers give the real distances you moved in each direction.
<br><br>
Now, if I want to repeat the movement set going from A to D only, how can I do this using the information I have already gathered. I know I can basically sum all the angles and distances I feed into it already, but this would prove to be inaccurate if there were large errors in each movement orders. How can I use the raw information from my mouse? A friend provided an idea that I could continuously sine and cosine the mouse values and calculate the final vector but I'm not really sure how this would work.</p>

<p>The problem is that the mouse only gives relative readings so rotating or moving backwards, you are potentially erasing information. So yeah, what I am wondering is how you can implement the algorithm so it can continually track changes to give you a shortest path if you moved in zigzags to get there originally.</p>
",4/12/2009 20:29,742476,2722,3,5,2,0,90046,"Montreal, Canada",4/12/2009 20:22,132,742458,"<p>Not an answer to your question, but perhaps a cautionary tale...
I did exactly this kind of robot as a school project a year back. It was an utter failure, though I learnt quite a bit while doing it.
As for using the mouse for tracking how far you have driven: It did not work well for us at all, or any of the other groups. Probably because the camera in the mouse was out of focus due to the fact that we needed to have the mouse a few mm above the floor. The following year no group doing the same project used this methid. They instead put markings on the weels and used a simple ir-sensor to calculate how many revolutions the wheels made.</p>
",13565,0,3,554134,"The problem is that the mouse only gives relative readings so rotating or moving backwards, you are potentially erasing information. So yeah, what I am wondering is how you can implement the algorithm so it can continually track changes to give you a shortest path if you moved in zigzags on the way.",Moving
1,235,786997,Multi threading using NXT,|multithreading|robotics|lego-mindstorms|,"<p>At my robotics club we are trying to get the multi-threading capability to work on LEGO Mindstorms NXT, but it seems that the threads are interfering with each other and causing the program to stop entirely.</p>

<p>Does anyone know how to correctly implement multi-threading on the NXT visual programming environment.</p>
",4/24/2009 18:15,,3038,2,0,4,0,60723,,1/30/2009 17:23,110,787028,"<p><strong>Does anyone know how to correctly implement multi-threading on the NXT visual programming environment?</strong></p>

<p>Yes, lots of people know how to implement multi-threading on the NXT visual programming environment.</p>

<p>Unless you are more specific, though, you aren't going to find out what's wrong with your particular problem.  There are lots of gotchas, though.</p>

<p>For instance, if you have two threads inside a loop, the loop will not loop until BOTH threads complete.</p>

<p>There are many others - too many to enumerate here - so start debugging your program piece by piece, and post examples of your 'code' (screenshots?) and then describe what it's doing, and what you want it to do.</p>
",2915,1,0,,,Timing
2,266,1050911,Is it possible to use anonymous functions in C++ .NET?,|.net|c++|function|anonymous|robotics|,"<p><a href=""http://en.wikipedia.org/wiki/Anonymous_function"" rel=""nofollow noreferrer"">Wikipedia</a> seems to say that C++0x will support anonymous functions. Boost also seem to support it. However I'm using .NET so if I could stick with it it would be awesome.</p>

<p>Basically I just want to write some quick code for objects. I have a robot which can have about 85 - 90 states. Most of the states are just ""integer values passed to the robot microcontroller"". So I tell the robot to go to state 35 for example.</p>

<p>However some states require additionnal manipulations such as user input so I'd like to keep it simple and write just a few lines of code for the differences. I've considered using derived classes but it involves a lot of code just to modify a few lines.</p>
",6/26/2009 19:48,,959,3,0,1,0,6367,"Montreal, Canada",9/14/2008 21:53,1610,1065052,"<p>Anonymous functions, alternatively called <a href=""http://msdn.microsoft.com/en-us/library/bb397687.aspx"" rel=""nofollow noreferrer"">Lambda Expressions</a> or Delegates, are a language feature of C# and not part of the .NET framework. I don't think Microsoft has added anonymous functions to managed-C++, and I've found some <a href=""http://www.codeproject.com/KB/interviews/whidbey_cpp.aspx?fid=28262&amp;df=90&amp;mpp=25&amp;noise=3&amp;sort=Position&amp;view=Quick&amp;select=1215662#xx1215662xx"" rel=""nofollow noreferrer"">comments</a> which seem to agree with me.</p>

<p>Not to worry, though. As you mentioned, <a href=""http://www.boost.org/doc/libs/1_39_0/doc/html/lambda.html"" rel=""nofollow noreferrer"">Boost.Lambda</a> is a nifty library that you can use. What is nice is that it is implemented as templates completely in headers. So, all you have to do is include the headers. Any standards-conforming C++ compiler should support it. I understand your desire to stick with what you already have, but the effort it takes to download and use these headers should really be minimal.</p>

<p>If you really don't want to use Boost, then you can try using C#, but I recommend that you just try the Boost Lambda library. It is probably easier than you think.</p>
",35881,-1,2,,,Specifications
3,425,1895242,failsafe for networked robot,|php|robotics|,"<p>I have a robot that I'm controlling via a browser. A page with buttons to go forward, reverse, etc is written in PHP hosted on an onboard computer. The PHP is just sending ASCII characters over a serial connection to a microcontroller. Anyway, I need to implement a failsafe so that when the person driving it gets disconnected, the robot will stop. The only thing I can think to do is to ping the person on the web page or something, but I'm sure there is a better way than that. The robot is connected either via an ad hoc network or a regular wireless network that is connected to the internet. Obviously if I go with the ping method then there will have to be a delay between the actual time disconnected and when it realizes it's been disconnected. I'd like this delay to be a small as possible, whatever the method used. I'd appreciate any ideas on how to do this.</p>
",12/13/2009 1:00,1895355,124,3,0,0,,90903,"Atlanta, GA",4/15/2009 2:09,184,1895258,"<p>Pinging a web client is somewhat unreliable, for you have to take into account, that the client ip might change.</p>

<p>On the other hand, you could emulate a ""dead-man-button"" via Ajax. Let the webpage send a defined command every now and then (maybe every 5 to 10 seconds). If the robot doesn't receive the message for some time, it can stop. The Ajax script could run in the background so the controlling user won't even notice anything.</p>

<p>This would of course mean, that your robot needs to have a counter which is incremented every second and reset when the message is received. The moment the timer variable is too high, FULL STOP</p>
",25253,2,0,,,Connections
4,475,2625390,2D Inverse Kinematics Implementation,|animation|rotation|robotics|inverse-kinematics|,"<p>I am trying to implement Inverse Kinematics on a 2D arm(made up of three sticks with joints). I am able to rotate the lowest arm to the desired position. Now, I have some questions:</p>

<ol>
<li><p>How can I make the upper arm move alongwith the third so the end point of the arm reaches the desired point. Do I need to use the rotation matrices for both and if yes can someone give me some example or an help and is there any other possibl;e way to do this without rotation matrices???</p></li>
<li><p>The lowest arm only moves in one direction. I tried google it, they are saying that cross product of two vectors give the direction for the arm but this is for 3D. I am using 2D and cross product of two 2D vectors give a scalar. So, how can I determine its direction???</p></li>
</ol>

<p>Plz guys any help would be appreciated....</p>

<p>Thanks in advance
Vikram</p>
",4/12/2010 20:56,,7755,4,0,10,0,314916,,4/12/2010 20:52,69,2625610,"<p>I'll give it a shot, but since my Robotics are two decades in the past, take it with a grain of salt.</p>

<p>The way I learned it, every joint was described by its own rotation matrix, defined relative to its current position and orientation. The coordinate of the whole arm's endpoint was then calculated by combining the rotation matrices together. </p>

<p>This achieved exactly the effect you are looking for: you could move only one joint (change its orientation), and all the other joints followed automatically.</p>

<p>You won't have much chance in getting around matrices here - in fact, if you use homogeneous coordinates, all joint calculations (rotations as well as translations) can be modeled with matrix multiplications. The advantage is that the full arm position can then be described with a single matrix (plus the arm's origin).</p>

<p>With this transformation matrix, you can tackle the inverse kinematic problem: since the transformation matrix' elements will depend on the angles of the joints, you can treat the whole calculation 'endpoint = startpoint x transformation' as a system of equations, and with startpoint and endpoint known, you can solve this system to determine the unknown angles. The difficulty herein lies that the equation may not be solvable, or that there are multiple solutions.</p>

<p>I don't quite understand your second question, though - what are you looking for?</p>
",258146,7,0,,,Actuator
5,607,3069144,Finding distance travelled by robot using Optical Flow,|localization|opencv|computer-vision|robotics|opticalflow|,"<p>I'm working on a project right now in which we are developing an autonomous robot. I have to basically find out the distance travelled by the robot between any 2 intervals. I'm using OpenCV, and using the <em>Optical Flow functions</em> of OpenCV, I'm able to find out the velocity/distance of each pixel in 2 different images. Using this information, I want to be able to find out the distance travelled by the robot in the interval between those 2 images.</p>

<p>I thought of a way in which we could develop an input output mapping between the distance travelled by pixels and the distance travelled by the bot (using some tests). In this way, using neural networks, we would be able to find the relationship. However, the optical flow would depend on the distance of the camera from the pixel, which would cause problems.</p>

<p>Is there any way to solve this problem?</p>
",6/18/2010 11:19,,3513,4,0,1,0,280454,India,2/24/2010 15:38,573,3070004,"<p>I hope you do end up accepting answers you received in the past. Anyway, I had posted the solution to this problem on SO (in OpenCV) a while back, so here it is:</p>

<p><a href=""https://stackoverflow.com/questions/2135116/how-can-i-determine-distance-from-an-object-in-a-video/2152097#2152097"">How can I determine distance from an object in a video?</a></p>
",71131,3,2,,,Incoming
6,631,3146232,"Robot, stereo vision, driving around and AForge",|robotics|aforge|,"<p>I am thinking about building a small robot that will drive around, judge distance to objects and avoid obstacles (hopefully it will) - nothing complex, just a home fun project.</p>

<p>I am planning on using two webcams to decide distances and track objects, and AForge.Net as the framework. I have never done anything AI and robotics related, so I have no idea where to start.</p>

<p>What sort of components would I need (wheels and engine, etc., that can be programmatically manipulated, programmable controller of some sort?) and where do I get all this stuff?</p>

<p>Overall, where do I start with this to make it happen? I am looking for a set of tools that would solve the basic problems and allow me to concentrate on programming of my robot.</p>
",6/30/2010 3:16,3189452,735,5,0,4,0,178980,Australia,9/25/2009 9:56,1175,3146405,"<p>Since you have no prior experience, you could try <a href=""http://en.wikipedia.org/wiki/Lego_Mindstorms"" rel=""nofollow noreferrer"">Lego Mindstorms</a> programmable robotics.</p>

<p>Personally, I think this is a complex project, though fascinating ;-)</p>
",108130,1,0,,,Specifications
7,780,3636814,Deciding on the covariance for a Kalman Filter matrixes,|math|probability|robotics|,"<p>I am beginning to explore using probability in my robotics applications. My goal is to progress to full SLAM, but I am starting with a more simple Kalman Filter to work my way up.</p>

<p>I am using Extended Kalman Filter, with state as [X,Y,Theta]. I use control input [Distance, Vector], and I have an array of 76 laser ranges [Distance,Theta] as my measurement input.</p>

<p>I am having trouble knowing how to decide on the covariance to use in my Gaussian function. Because my measurements are uncertain (The laser is about 1cm accurate at &lt; 1meter, but can be up to 5cm accurate at ranges higher) I do not know how to create the 'function' to estimate the probability of this. I know this function is supposed to 'linearize' to be used, but I'm not sure how to go about this.</p>

<p>I am reasonably confident on how to decide on the function for my state Gaussian, I am happy to use a plain old mean=0,variance=1 on this.. This should work no? I would appreciate some help from people understanding Kalman Filters, because I think I may be missing something.</p>
",9/3/2010 14:39,3636999,551,2,0,5,0,282090,,1/10/2010 12:34,138,3636999,"<p><a href=""http://www.roboticsproceedings.org/rss01/p38.pdf"" rel=""nofollow noreferrer"">This</a> paper could be a good starting point for you, but you might just choose to manually tweak the values. That's probably good enough for your application.</p>
",436667,4,1,,,Coordinates
8,822,4063416,What to study to get into robotics?,|robotics|,"<p>What should someone study at university level if he/she wants to get into robotics and build robotics? So far 'Mechatronics' seems to be the field I'm looking for? I looked at a few plain 'robotics' courses but they seem to be only about the electrical and computer work, and don't include any details on building the mechanical components of robots?</p>
",10/31/2010 14:11,9386568,82302,5,2,32,0,49153,http://ali.actor,12/26/2008 4:39,34744,4063436,"<p>Mechanical and electrical engineering and computer science.</p>

<p>Mechanical engineering will inform choices about servos, linkages, gears, and all other mechanical components.</p>

<p>Control theory is the junction of mechanical and electrical engineering.  You'll need that.</p>

<p>So much of control is digital these days, so EE and computer science will be a part of it.</p>

<p>It's a big field.  Good luck.</p>
",37213,1,0,4365470,"You'd probably get better answers if you provide your current background in electronics, automation and similar stuff.",Other
9,1011,5207515,Robot camera + motion detection,|camera|detection|robotics|motion|,"<p>I have a project in which we (me and my student) will develop a system for robot.
In this robot we have a camera that capture.</p>
<p>My question is how to detect motions and movements?
Is there a solution? Which technics and tools to use?
Which language to use (possible for Java for example)?</p>
",3/5/2011 23:30,5207561,1163,4,1,0,0,604156,"Paris, France",2/5/2011 6:51,1331,5207561,"<p>Consider using OpenCV:</p>

<p><a href=""http://opencv.org"" rel=""nofollow"">http://opencv.org</a></p>

<p>It has a lot of useful vision algorithms built in, and supports, C, C++ and Python, as well as GPU functionality.</p>
",213489,3,0,19870840,"[Moving camera_motion_detection][1] thread may be useful as it deals with the similar problem.





  [1]: http://stackoverflow.com/questions/12986265/detecting-motion-on-opencv-c-moving-camera",Incoming
10,1105,5540608,Tegra based robotics platform,|android|serial-port|arm|robotics|i2c|,"<p>I am looking into the possibility of developing a Tegra based robotics platform running Android. To do this I need to be able to preform serial, I2C, and possibly PWM communications, does the Tegra platform have allow this? And does Android support access to this kind of hardware level communication?</p>
",4/4/2011 15:18,,220,2,0,0,,127896,,6/23/2009 22:33,170,6579235,"<p>For communicating with hardware from the Android, I suggest looking at the <a href=""http://developer.android.com/guide/topics/usb/adk.html"" rel=""nofollow"">Android Open Accessory Development Kit</a>. This will allow you to communicate with external devices (e.g. a <a href=""http://shop.moderndevice.com/products/freeduino-usb-host-board"" rel=""nofollow"">Freeduino</a>) over the phone's USB port. Besides allowing you to get the project working without opening the phone, this is generally the path of least resistance.</p>
",111426,0,0,,,Remote
11,1131,5611290,Is it a good idea to use a single-board-computer in a UAV robot?,|robotics|,"<p>I'm not sure it's good or bad, the robot should have computer vision for SLAM. What's your idea?</p>
",4/10/2011 11:00,5611298,442,2,0,0,0,689779,"Toronto, ON, Canada",4/3/2011 12:10,822,5611298,"<p>Yes, that's how we did it when I was in school (albeit nine years ago). You want to focus on algorithms, not learning to program an unfamiliar platform.</p>

<p>Assuming the ""A"" stands for <em>aerial</em>, don't invest in anything you don't want crashing at high speed. And mind the vibrations.</p>
",153285,0,1,,,Moving
12,1135,5765591,General Advice - Robotics / AI,|artificial-intelligence|robotics|,"<p>This is not for asking any doubts or queries. I know this is a technical forum and hence would be the best platform for me to get advice on.</p>

<p>I am a Master's student in computer engineering and hold interest in Robotics. I am confused as to where should I start if from. I have 2 courses one is on controlling of robots and other is based on introduction to AI. I don't want to take both the courses together. I am confused as to do I need to go for controlling of robots first or AI first?</p>

<p>Also, if you know any good forums/blogs on AI then please share... Would be a lot helpful.</p>

<p>Thanks. </p>
",4/23/2011 16:58,5802255,422,2,2,0,,667851,,3/20/2011 0:59,170,5765709,"<p>Well, I would take the AI class first, because I would want to know more about the logic before going to the control part.<br>
As far as forums go, you could check out the <a href=""http://www.ai-forum.org/"" rel=""nofollow"">AI Forum</a>, and the <a href=""http://ai-depot.com/"" rel=""nofollow"">AI Depot</a> (the AI Depot is not exactly a forum, but it has some good resources and articles).  </p>

<p>You can also check out these Area51 StackExchange site proposals:<br>
<a href=""http://area51.stackexchange.com/proposals/2149/cognitive-science"">Cognitive Science</a><br>
<a href=""http://area51.stackexchange.com/proposals/29987/robotics-research"">Robotics Research</a><br>
<a href=""http://area51.stackexchange.com/proposals/26434/machine-learning"">Machine Learning</a>  </p>
",545616,2,1,6604513,"Probably I'm not the best person to give this kind of advice (that's why I leave just a comment), but I'd go for the control class before. You should know what robots can/can't do and how do they work before trying to control them",Other
13,1149,6140332,Real time live streaming with an iPhone for robotics,|objective-c|sockets|video-streaming|vlc|robotics|,"<p>For a research purpose, I developed and app to control a wheeled mobile robot using the gyro and the accelerometer of an iPhone. The robot has a IP address, and I control it by sending messages through a socket. Since the robot has to be controlled from anywhere in the world, I mounted a camera on top of it. I tried to stream the video from the camera using the http live streaming protocol and vlc, but the latency is too high (15-30sec) to properly control it.</p>

<p>Now, vlc has the possibility to stream over udp or http, but the point is how do I decode the stream on the iPhone? How should I treat the data coming into the socket in order to visualize them as a continuous live video? </p>
",5/26/2011 14:34,,644,0,2,1,,771498,,5/26/2011 14:34,11,,,,,,7303231,I'm not sure I understood your question. You are already streaming via http and you want udp? Is that it?,Connections
14,1216,6620778,Determining the duration of a frequency and the magnitude,|algorithm|controls|robotics|,"<p>I am working with a system in which I am getting data from a sensor (gyro) at 1KHz.</p>

<p>What I am trying to do is determine when the system is vibrating so that I can turn down the PID gains on the output.<br>
What I currently have is a high pass filter on the incoming values.  I then have set the alpha value to 1/64, which I believe should be filtering for about a 10KHz frequency.  I then take this value and then integrate if it is individual above a threshold.  When my integrated value passes another threshold, I then assume that the system is vibrating.  I also reset the integrated value every half second to ensure that it does simply grow towards the threshold.<br>
What I am trying to do with this system is make sure that it is really vibrating and not seeing a jolt.  I have tried to do this with a upper limit to how much will be added to the integrated value, but this is not really appearing to work.</p>

<p>What I am looking for is any better way to go about detecting that the system is vibrating, and not being effected by a jolt, my primary issue is that that I do not miss detect a jolt for a vibration because then that will cause the values on the PID to be lowered unnecessarily.</p>
",7/8/2011 6:37,6620873,137,2,0,2,0,144600,California,7/24/2009 16:05,288,6620873,"<p>FFT.  It will separate out the ""jolts"" from the vibrations, because jolts will register across all frequencies and vibrations will spike around a particular frequency.</p>
",,1,1,,,Incoming
15,1218,6641055,Obstacle avoidance with stereo vision,|opencv|computer-vision|robotics|,"<p>I'm working on a stereo-camera based obstacle avoidance system for a mobile robot. It'll be used indoors, so I'm working off the assumption that the ground plane is flat. We also get to design our own environment, so I can avoid specific types of obstacle that generate false positives or negatives.</p>

<p>I've already found plenty of resources for calibrating the cameras and getting the images lined up, as well as information on generating a disparity map/depth map. What I'm struggling with is techniques for detecting obstacles from this. A technique that instead worked by detecting the ground plane would be just as useful.</p>

<p>I'm working with openCV, and using the book Learning OpenCV as a reference.</p>

<p>Thanks, all</p>
",7/10/2011 12:20,6643158,6051,1,0,8,0,482404,"Worcester, MA",10/21/2010 0:14,89,6643158,"<p>From the literature I've read, there are three main approaches:</p>

<ol>
<li><p><strong>Ground plane approaches</strong> determine the ground plane from the stereo data and assume that all points that are not on the plane are obstacles. If you assume that the ground is the dominant plane in the image, then you may be able to find it simply a plane to the reconstructed point cloud using a robust model-fitting algorithm (such as RANSAC).</p></li>
<li><p><strong>Disparity map approaches</strong> skip converting the stereo output to a point cloud. The most popular algorithms I've seen are called v-disparity and uv-disparity. Both look for the same attributes in the disparity map, but uv-disparity can detect some types of obstacles that v-disparity alone cannot.</p></li>
<li><p><strong>Point cloud approaches</strong> project the disparity map into a three-dimensional point cloud and process those points. One example is ""inverted cone algorithm"" that uses a minimum obstacle height, maximum obstacle height, and maximum ground inclination to detect obstacles on arbitrary, non-flat, terrain.</p></li>
</ol>

<p>Of these three approaches, detecting the ground-plane is the simplest and least reliable. If your environment has sparse obstacles and a textured ground, it should be sufficient. I don't have much experience with disparity-map approaches, but the results look very promising. Finally, the Manduchi algorithm works extremely well under the widest range of conditions, including on uneven terrain. Unfortunately, it is very difficult to implement and is extremely computationally expensive.</p>

<p><strong>References:</strong></p>

<ul>
<li><strong>v-Disparity</strong>: Labayrade, R. and Aubert, D. and Tarel, J.P. Real time obstacle detection in stereovision on non flat road geometry through v-disparity representation</li>
<li><strong>uv-Disparity</strong>: Hu, Z. and Uchimura, K. UV-disparity: an efficient algorithm for stereovision based scene analysis</li>
<li><strong>Inverted Cone Algorithm:</strong> Manduchi, R. and Castano, A. and Talukder, A. and Matthies, L. Obstacle detection and terrain classification for autonomous off-road navigation</li>
</ul>

<p>There are a few papers on ground-plane obstacle detection algorithms, but I don't know of a good one off the top of my head. If you just need a starting point, you can read about my implementation for a recent project in Section 4.2.3 and Section 4.3.4 of <a href=""http://mkoval.org/downloads/projects/igvc/igvc_design.pdf"" rel=""noreferrer"">this design report</a>. There was not enough space to discuss the full implementation, but it does address some of the problems you might encounter.</p>
",111426,12,0,,,Moving
16,1219,6645215,ROS on Android Phone,|android|robotics|,"<p>Currently I'm trying to run ROS node on Android Phone to remotely control a robotic. I found that ROS node on Android Phone could publish message to a Topic, but could not subscribe to any topic. Is anyone know the reason?</p>

<p>Thanks,
Liu.</p>
",7/11/2011 2:27,,845,1,1,2,,838134,,7/11/2011 2:27,7,6655346,"<p>You should search <a href=""http://answers.ros.org/questions/"" rel=""nofollow"">ROS Answers</a> for your question or others similar that might help you solve your problem. If you don't find a solution, feel free to post the question there. Since ROS Answers is a ROS specific forum, you'll likely get much better support there than here on StackOverflow.</p>

<p>If you do post there, please include as much detail as you can to help with debugging the problem. Any error messages, how you have your network configured, etc would be helpful (and if possible, include links to your source code).</p>
",130251,4,0,7852557,Are you getting any errors or is it silently failing?,Other
17,1232,7017136,Would Arduino be the most viable language to support the construction of a human-like robot?,|arduino|robotics|,"<p>If you wanted to build a human-like robot, what language would be most suitable?</p>

<p>Arduino?</p>

<p>Or would you need to write mostly low-level microcontroller-level code at the 1s and 0s level?</p>
",8/10/2011 20:12,,212,1,2,-1,0,888682,,8/10/2011 20:12,3,7017462,"<p>There's probably a few people typing the same response as I type...</p>

<p>The term Arduino can be a bit confusing for beginners. There is the Arduino microcontroller, which stores the executable code and contains inputs and outputs, and the Arduino IDE, which compiles code and uploads it to the Arduino microcontroller. As mentioned in <a href=""https://stackoverflow.com/questions/5931572/how-is-programming-an-arduino-different-than-standard-c/5931887#5931887"">How is programming an Arduino different than standard C?</a>, the programming language used to program the Arduino microcontroller is C or C++.</p>

<p>If a human-like robot is defined as a basic robot with two legs that move in a predefined routine, an Arduino should suffice.</p>

<p>To get started building a robot using an Arduino, <a href=""https://rads.stackoverflow.com/amzn/click/com/1430232404"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">Beginning Arduino</a> and <a href=""https://rads.stackoverflow.com/amzn/click/com/1430231831"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">Arduino Robotics</a> are two recent books that have been getting good reviews. I recommend starting with one of these books. If you have questions specific to microcontrollers or electronics, <a href=""https://electronics.stackexchange.com/"">https://electronics.stackexchange.com/</a> would be a good resource.</p>
",72035,3,0,8380246,"Arduino isn't a language, it's a prototyping platform that uses Atmel AVR microcontrollers. With some additional information, this might be a good question to ask on [Electronics Engineering StackExchange](http://electronics.stackexchange.com/).",Specifications
18,1234,7045073,Problematic Parallel Distributive Processing using Python Libraries or any language,|python|arduino|robotics|pdp|,"<p>I've been sitting on this idea of working on a ""networked intelligence"" to look into some interesting ideas on the nature of intelligence and computers. I've decided to go about doing this by designing small robotic agents that will utilize PDP across some medium, (i.e. wifi/IR or something, to be decided), to enable them to gather large quantities of data independently and then be able to process and find trends in data efficiently by utilizing them together as a ""supercomputer"" (I always think it's odd using that term, but it's apt, one is utilizing multiple independent processing units in unison). I'm aware that Python has some PDP libraries available, and I was hoping to program the robots onto little Arduinos, and I've got a strong idea of how to do every component of the system except for actually implementing the PDP architecture across the system. </p>

<hr>

<p><strong>TL;DR</strong>? I want to make a bunch of little robots that can essentially connect together to form a small supercomputer and share and amalgamate information across all the agents. Is it feasible to create a PDP program that will freely relinquish parts of its processing power and then add in new ones.</p>

<p>I'm a pretty strong programmer, so if it's a matter of complexity and time, I'm willing to apply myself, but if it's an issue of having to strip apart some BIOS software and writing in Assembly, then I'd rather not. I'm not as familiar with PDP ideas as I would like to, and if you have any recommended reading to get me started, much appreciated.</p>

<p><em>Another note</em>, the languages or platform is completely up for changes, I'd just like to see concrete evidence that one is better than the other.</p>
",8/12/2011 19:02,,200,1,0,2,0,892338,,8/12/2011 19:02,5,7046312,"<p>Interesting idea, remins me on <a href=""http://en.wikipedia.org/wiki/Wireless_sensor_network"" rel=""nofollow"">sensor networks</a>.</p>

<p>You may find that an ardunio is a little underpowered for what you want. Perhaps it would be more efficient and easier to send the data back to a PC for processing.</p>

<p>If you want to continue with the ardunio idea, you could implement <a href=""http://en.wikipedia.org/wiki/MapReduce"" rel=""nofollow"">MapReduce</a> which is a fairly simple construct that allows you to write distributed programs very easily.</p>

<p>I have a write up on the <a href=""http://stephenholiday.com/articles/2011/introduction-to-mapreduce/"" rel=""nofollow"">basics of MapReduce</a>.</p>

<p>There is the famous <a href=""http://hadoop.apache.org/"" rel=""nofollow"">Haddop implementation</a> as well as <a href=""http://discoproject.org/"" rel=""nofollow"">Disco</a> (python/erlang) and a very simple shell implementation called <a href=""https://github.com/erikfrey/bashreduce"" rel=""nofollow"">BashReduce</a> that Last.fm created.</p>
",429688,0,0,,,Specifications
19,1284,8799363,Robotics Club Programming Portion,|microcontroller|robotics|labview|robot|,"<p>My school has entered into a Robotics Tournament that competes several schools against each other(this is my school's first year). The objective of the robot is to shoot a ball into a hoop. I am a member of the Programming team. Our job as the programmers is to program a robot and a computer to control the robot. The computer has 2 joy sticks attached to it, one for moving the entire robot(spinning the wheels and causing the robot to move) and one is for the ""throwing arm"". A signal is going to be sent from the computer to the robot using wifi. All of the programming MUST be done in LabView. </p>

<p>I have never heard of LabView before until i joined this club and i have my doubts about it. The reason why we must use LabView is because most of the kids on the programming team have no programming experience whatsoever. LabView has to be able to interface with the joy sticks and then send that information to the robot using wifi. The micro controller on the robot supports LabView.</p>

<p>Now to my question, is LabView dynamic enough to preform this task? Can LabView even support networking? Can LabView even interface with the joy sticks? I have read a lot of the documentation for LabView from this website:</p>

<ul>
<li><a href=""http://www.ni.com/gettingstarted/labviewbasics/environment.htm"" rel=""nofollow"">http://www.ni.com/gettingstarted/labviewbasics/environment.htm</a></li>
</ul>

<p>My concern is that LabView is not dynamic enough for what we are trying to use it for as a team and we are going to have to program the computer and the micro controller using C. There are only 2 people on the team who can program sufficiently in C so we would have to teach the rest of the members the basics of C.</p>

<p>All relevant answers are welcomed and appreciated.</p>
",1/10/2012 6:22,8803293,376,4,2,2,0,1072647,United States,11/30/2011 4:20,396,8803293,"<p>LabVIEW can totally do this. I am biased: I've written a textbook on it and am teaching classes:-); I also do this for a living. In comparision to C, well, C can do anything, but LabVIEW does hardware on a much higher level. Doesn't mean I don't like bending pointers for a bit; but it's nice to not care about low-level functions for a while.</p>

<p>Interfacing a joystick is pretty simple, it looks like this: <a href=""http://digital.ni.com/public.nsf/allkb/CA411647F224787B86256DD000669EFE"" rel=""nofollow"">http://digital.ni.com/public.nsf/allkb/CA411647F224787B86256DD000669EFE</a>
To interface Wifi, it depends on how the robot should receive the information. TCP/IP would go like this: <a href=""http://zone.ni.com/devzone/cda/tut/p/id/2710"" rel=""nofollow"">http://zone.ni.com/devzone/cda/tut/p/id/2710</a></p>
",619238,4,1,10983026,"My non-constructive advice: leave the sinking boat! LabView is a piece of [something]. LabView is not dynamic at all (how it is mostly used) but you will probably be able to do all the tasks required with it. School competition are a fun way to learn things but learning the wrong things is wrong. If the rules allow it, I would strongly urge you and your team members to use C or any other language. C is particularly apt for embedded systems. I might be biased because I have both used LabView and C...",Specifications
20,1292,8897938,I am designing a Java Robot?,|java|robotics|,"<p>I am using a robot programmed with Java with a distance and touch sensor (but no gps or compass) to navigate a 1.0 by 2.5 metre obstacle course. The robot only knows its position by dead reckoning (like the number if turns of its wheels). When it turns it can measure the number of degrees from its last path travelled. After it finds the obstacles it needs to produce a map of where they are most likely to be. I want to extend a JPanel Class and override its paintComponent() method and then use the methods of the Graphics class to draw on the JPanel. I know that there are many drawxxxx methods for drawing. But I was wondering how I could actually achieve this, like the actual code that is necessary to produce this?!</p>
",1/17/2012 16:13,,219,1,2,2,,1154353,,1/17/2012 16:10,67,8897979,"<p>There's a great lesson on that on the official Java Swing tutorial page:</p>

<p><a href=""http://docs.oracle.com/javase/tutorial/uiswing/painting/step2.html"" rel=""nofollow"">http://docs.oracle.com/javase/tutorial/uiswing/painting/step2.html</a></p>
",939023,0,0,11127026,[What have you tried?](http://mattgemmell.com/2008/12/08/what-have-you-tried/),Moving
21,1299,9058716,How to get a direction for a robot from a method in a sensor class?,|java|sensors|robotics|,"<p>I am making a program using the LRV(Least recently visited) Algorithm. Basically, I design the algorithm for a robot to traverse through a grid (which is a 2D char array). The robot whilst traversing the grid checks whether each cell is either <code>EMPTY</code> (defined by 'O'), <code>OCCUPIED</code> ( defined by 'S' ) or <code>BLOCKED</code> (defined by 'X'). The cells can only be occupied by an object known as Sensor (this has its own class). <code>BLOCKED</code> cells cannot be traversed on. Each time the robot must move, it receives a direction from the sensor. So in the beginning the robot would be placed on the grid and it would drop a sensor and get a direction from it, or get a direction from a pre-existing sensor.</p>

<p>Now that I've explained my program, my specific question is,
I have a class Sensor that has a <code>getVisitingDirection</code> method that returns INT.
I have a counter for each direction (North, South, East and West of type INT)
Here is the class.</p>

<pre><code>package ITI1121A;
public class Sensor {

private int cColumns;
private int cRows;
private int North;
private int South;
private int West;
private int East;

public Sensor(int sX, int sY) { 

cColumns = sX;
cRows = sY;
South= -1;
North = -1;
West = -1;
East = -1;

}
/* ADD YOUR CODE HERE */
public int getX ()
{return cColumns;}
public int getY ()
{return cRows;}

public int getVisitingDirection(GridMap g1)
  boolean temp;
{
  if(cRows==0){
  //top row
    if(cColumns==0){
    temp=g1.isCellBlocked(cColumns+1,cRows);
    if (temp=false){
    return West++;
    }

    }

  }

}

public void increaseCounter(int direction)
{}

}
</code></pre>

<p>Now where I am stuck is at getVisitingDirection, I've tried to make if statements to check the top left edge of the grid ( coordinates 0,0) and yeah that's about it. 
I want the method to give a direction to the robot and then increase the counter of that direction.
Having real difficulty even getting the concept here.
Any help will be highly appreciated!
Thanks
Varun</p>
",1/30/2012 2:35,9061046,1918,1,5,5,0,1168160,"Toronto, ON, Canada",1/24/2012 22:55,93,9061046,"<p>I've put a function in pseudo-code that should set you in the right path.</p>

<pre><code>// lets assume binary code where 0000 represents top, right, bottom, left 
// (0011 would mean can go bottom or left)
public int getVisitingDirection()
{
    String tmpBinary = ""b""; // to initialize the field

    // check if can go up
    tmpBinary += (cCollums&gt;0&amp;&amp;checkIfUpIsBlocked()) ""1"" : ""0"";
    // TODO check if can go right (compare with size + 1)
    // check if can go bottom (compare with size +1 )
    // check if can go left (check if &gt; 0)

    // when this runs tmpBinary will be in the form of a binary representation
    // this will be passed to the robot that can then chooses where to go
    // 1111 would mean that all squares are clear to go
    // 1101 would mean top, right and left
    // etc...

}

private boolean checkIfUpIsBlocked()
{
    // TODO must check if the cell is blocked
    return true;
}
</code></pre>

<p>Do notice that you have to create the checkIfUpIsBlocked + methods.</p>

<p><strong>On top of that seams pretty good.</strong><br>
You may want to change the int fields by enums as they are easier to read and less prone to human errors.</p>

<p><strong>How to return directions with an int number?</strong><br>
You can use the binary logic and return a single int to represent multiple directions.  </p>

<pre><code>0000 (int 0)  =&gt; no possible direction
0001 (int 1)  =&gt; left direction
0010 (int 2)  =&gt; bottom direction
0011 (int 3)  =&gt; left and bottom
0100 (int 4)  =&gt; right direction
(...)
1111 (int 15) =&gt; all directions possible
</code></pre>
",67945,3,3,11368410,"placed at (3,3), check if empty, if yes then place 'S'..then get direction from 'S' for next move, so on and so forth. everytime robot detects sensor in a cell it asks for a direction",Moving
22,1312,9262830,What machine learning algorithm is appropriate for shooting basketballs?,|algorithm|machine-learning|computer-vision|robotics|,"<p>We are making a robot that shoots basketballs into hoops. </p>

<p>From an image and our knowledge of the camera's angle and the target's dimensions (the targets are coated with retroreflective tape), we know how far away we are, X and Y (distance being Z, more or less)</p>

<p>This is fed into the machine learning algorithm, which should spit out</p>

<ol>
<li>Speed to be sent to the canon</li>
<li>Horizontal tilt </li>
<li>Vertical tilt</li>
</ol>

<p>What kind of machine learning algorithm is this, and how would you train it?</p>
",2/13/2012 15:12,,344,2,4,1,,731881,"Boston, MA, USA",4/29/2011 23:10,118,9263009,"<p>I would recommend a reinforcement learning approach. It'll be slow ; so maybe you could initialize the solution with your own estimate (basic physics) and refine it with reinforcement learning.</p>
",71131,1,1,11673860,"If you know your current position, target position, why do you need machine learning at all? You can just calculate speed and tilt",Other
23,1327,9533311,Referencing objects through a tcp/ip connection - for robotics,|c#|tcp|robotics|,"<p>I've begun to design a framework for a robot I'm going to build. The control-software is - for reasons of later portability and also for the challenge herein - an attempt at mimicking the human system (of course drastically simplified). Thus the framework has a nervous system, consisting of a brain and spinal cord, the latter through which the brain controls the sensors and 'limbs' of the peripheral nervous system - i.e. the robots camera, microphones, motorcontrols and so on.</p>

<p>My challenge is in that I can't figure out how to send commands from the brain - via the spinal cord - to the sensor objects, initialized by the peripheral nervous system controller. In my implementation, the spinal cord is a tcp/ip server, accepting connections from the sensors and sending them up to the brain. How would something like this be accomplished? The brain has an awareness of the sensory objects, but it shouldn't be able to instantiate them - this is only for the peripheral nervous system. So how to call functions on those sensor objects, from the brain via the spinal cord, to the peripheral nervous system and finally to the sensor in question? </p>
",3/2/2012 12:29,9533516,238,3,0,0,,1208500,,2/14/2012 7:04,340,9533500,"<p>In Java you could do something like this with RMI. In C# please take a look at CORBA.</p>

<p>Doing things like this with a tcp/ip-server (I guess you want to open a socket and then parse command through it) is pain in the neck. If you want to do this else, you have to define Commands which are sended, received, parsed and the right method is called.</p>

<p>If you are able to set up sth. like a IIS you could probably write a Webservice in C#. The Actors would consume the Methods of the Webservice and ""remotely"" call them.</p>
",619619,0,1,,,Incoming
24,1332,9614729,Issue regarding practical approach on machine learning/computer vision fields,|c++|matlab|machine-learning|computer-vision|robotics|,"<p>I am really passionate about the machine learning,data mining and computer vision fields and I was thinking at taking things a little bit further.</p>

<p>I was thinking at buying a LEGO Mindstorms NXT 2.0 robot for trying to experiment machine learning/computer vision and robotics algorithms in order to try to understand better several existing concepts.</p>

<p>Would you encourage me into doing so? Do you recommend any other alternative for a practical approach in understanding these fields which is acceptably expensive like(nearly 200 - 250 pounds) ? Are there any mini robots which I can buy and experiment stuff with?</p>
",3/8/2012 8:28,9616077,466,2,1,4,0,442124,Romania,9/8/2010 6:42,607,9616077,"<p>If your interests are machine learning, data mining and computer vision then I'd say a Lego mindstorms is not the best option for you. Not unless you are also interested in robotics/electronics.</p>

<ul>
<li>Do do interesting machine learning you only need a computer and a problem to solve. Think <a href=""http://aichallenge.org/"">ai-contest</a> or <a href=""http://mlcomp.org/"">mlcomp</a> or similar.</li>
<li>Do do interesting data mining you need a computer, a lot of data and a question to answer. If you have an internet connection the amount of data you can get at is only limited by your bandwidth. Think <a href=""http://www.netflixprize.com/"">netflix prize</a>, try your hand at collecting and interpreting data from wherever. If you are learning, <a href=""http://www.autonlab.org/tutorials/"">this is a nice place to start</a>.</li>
<li>As for computer vision: All you need is a computer and images. Depending on the type of problem you find interesting you could do some processing of <a href=""http://www.opentopia.com/hiddencam.php"">random webcam images</a>, take all you holiday photo's and try to detect <a href=""https://en.wikipedia.org/wiki/Face_detection"">where all your travel companions are</a> in them. If you have a webcam your options are endless.</li>
</ul>

<p>Lego mindstorms allows you to combine machine learning and computer vision. I'm not sure where the datamining would come in, and you will spend (waste?) time on the robotics/electronics side of things, which you don't list as one of your passions. </p>
",7531,8,0,12199822,The question might be too subjective but I would definitely encourage you to do so. Those LEGO Mindstorms seem a bit expensive but I've seen many awesome applications for them.,Specifications
25,1353,10075285,vehicle's obstacle avoidance sensor,|c|microcontroller|sensors|robotics|,"<p>I'm new in robotics and I would like to make a vehicle in order to play with my 3 years-old son. I have a PIC 16F917 microcontroller and a Half H bridge L293DNE. What I've tried so far in order to make the vehicle move is s C code guided by the following link: <a href=""http://www.google.gr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CCQQFjAA&amp;url=http%3A%2F%2Fwww.societyofrobots.com%2Fmember_tutorials%2Ffiles%2FLukas%2520PIC%2520Tutorial.doc&amp;ei=8tWCT4GeNZD6sgaQmYzRBA&amp;usg=AFQjCNFw6ZoQSyoKwRj3uPaLmchgBzGY4Q&amp;sig2=tLQ7VN9IJen-TXCznu4jLA"" rel=""nofollow"">http://www.google.gr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CCQQFjAA&amp;url=http%3A%2F%2Fwww.societyofrobots.com%2Fmember_tutorials%2Ffiles%2FLukas%2520PIC%2520Tutorial.doc&amp;ei=8tWCT4GeNZD6sgaQmYzRBA&amp;usg=AFQjCNFw6ZoQSyoKwRj3uPaLmchgBzGY4Q&amp;sig2=tLQ7VN9IJen-TXCznu4jLA</a>. So I was able to make the robot move forward, backward and turn left and right. What I want to do now is to find suitable sensors for obstacle avoidance. Since I have no previous experience could someone recommend the appropriate  compatible with the microcontroller sensors? If so, I would like to know in which pins of microcontroller I should connect the sensors and moreover the suitable programming order in C , for the sensors to interact with the microcontroller.</p>

<p>Thanks in advance!!!</p>
",4/9/2012 15:09,,849,1,3,2,0,582964,,1/20/2011 12:46,18,12331464,"<p>The sparkfun link in a previous comment is a good place to get sensors.  </p>

<p>Ultrasonic sensors are good for rough estimations of distance.  They have a wide beam, and the results are noisy.  Sharp IR sensors have a narrower beam and are a little more accurate.  And for very close range, (and very cheap), you can simply use an IR emitter and detector.  Typically, the IR is pulsed so that you can measure the difference between ambient IR lighting and when the pulse is on.  (Search for ""infrared proximity detector with a 555 timer"", or something along those lines).</p>
",159595,1,0,16551937,Please checkout and contribute to the new robotics stackexchange forum: http://area51.stackexchange.com/proposals/40020/robotics,Incoming
26,1359,10420966,Manipulator/camera calibration issue (linear algebra oriented),|python|linear-algebra|robotics|calibration|,"<p>I'm working on a research project involving a microscope (with a camera connected to the view port; the video feed is streamed to an application we're developing) and a manipulator arm. The microscope and manipulator arm are both controlled by a Luigs &amp; Neumann control box (very obsolete - the computer interfaces with it with a serial cable and its response time is slowwww.) The microscope can be moved in 3 dimensions; X, Y, and Z, whose axes are at right angles to one another. When the box is queried, it will return decimal values for the position of each axis of each device. Each device can be sent a command to move to a specific position, with sub-micrometer precision.</p>

<p>The manipulator arm, however, is adjustable in all 3 dimensions, and thus there is no guarantee that any of its axes are aligned at right angles. We need to be able to look at the video stream from the camera, and then click on a point on the screen where we want the tip of the manipulator arm to move to. Thus, the two coordinate systems have to be calibrated.</p>

<p>Right now, we have achieved calibration by moving the microscope/camera's position to the tip of the manipulator arm, setting that as the synchronization point between the two coordinate systems, and moving the manipulator arm +250um in the X direction, moving the microscope to the tip of the manipulator arm at this new position, and then using the differences between these values to define a 3d vector that corresponds to the distance and direction moved by the manipulator, per unit in the microscope coordinate system. This is repeated for each axis of the manipulator arm. </p>

<p>Once this data is obtained, in order to move the manipulator arm to a specific location in the microscope coordinate system, a system of equations can be solved by the program which determines how much it needs to move the manipulator in each axis to move it to the center point of the screen. This works pretty reliably so far.</p>

<p>The issue we're running into here is that due to the slow response time of the equipment, it can take 5-10 minutes to complete the calibration process, which is complicated by the fact that the tip of the manipulator arm must be changed occasionally during an experiment, requiring the calibration process to be repeated. Our research is rather time sensitive and this creates a major bottleneck in the process.</p>

<p>My linear algebra is a little patchy, but it seems like if we measure the units traveled by the tip of the manipulator arm per unit in the microscope coordinate system and have this just hard coded into the program (for now), it might be possible to move all 3 axes of the manipulator a specific amount at once, and then to derive the vectors for each axis from this information. I'm not really sure how to go about doing this (or if it's even possible to do this), and any advice would be greatly appreciated. If there's any additional information you need, or if you need clarification on anything please let me know.</p>
",5/2/2012 20:14,,321,1,3,0,,1370967,,5/2/2012 20:00,120,10421975,"<p>You really need four data points to characterize three independent axes of movement.</p>

<p>Can you can add some other constraints, ie are the manipulator axes orthogonal to <em>each other</em>, even if not fixed relative to the stage's axes? Do you know the manipulator's alignment <em>roughly</em>, even if not exactly?</p>

<p>What takes the most time - moving the stage to re-center? Can you move the manipulator and stage at the same time? How wide is the microscope's field of view? How much distance-distortion is there near the edges of the view - does it actually have to be re-centered each time to be accurate? Maybe we could come up with a reverse-screen-distortion mapping instead?</p>
",33258,0,4,13448482,"Are you sure that the manipulator arm is controlled in x, y and z? Usually robot arms are controlled by either absolute or relative angle of each of the joints. Or is the control box already converting (x,y,z) to joint angles?",Actuator
27,1363,10460809,What is * in AT command?,|protocols|robotics|at-command|,"<p>I came accros this kind of line in a proprietary use of the AT commands:</p>

<blockquote>
  <p>AT*REF=1,290717696&lt;LF&gt;</p>
</blockquote>

<p>It is a proprietary command as it is used in a protocol to control a robot.</p>

<p>According to what I read on <a href=""http://en.wikipedia.org/wiki/Hayes_command_set#Description"" rel=""nofollow"">Wikipedia</a> and other sources, AT command extensions should use ""\"" or ""%"". There is no mention of ""*"".</p>

<p>So what does <code>*</code> define?</p>
",5/5/2012 10:06,,141,1,2,1,,757293,France,5/17/2011 11:52,283,19373686,"<p>There are more than two different characters in use by various manufacturers when implementing the first character of a proprietary AT command.  Some that I've seen:</p>

<p>! @ # $ % ^ * _</p>

<p>The manufacturer of your device may have chosen '*' for commands that have similar functionality, or they may have chosen to implement ALL of their proprietary AT commands with '*' as the first character.</p>

<p>There are many AT command references available online in PDF format from many different manufacturers.  Perhaps the manufacturer of your device makes this information available as well.</p>
",2592733,0,0,13514626,"My question is not about the command, but about the choice of * in the command.",Remote
28,1407,11581346,Voice and face recognition libraries for .Net Gadgeteer,|voice-recognition|robotics|face-recognition|.net-micro-framework|.net-gadgeteer|,"<p>Recently, I've been looking into robotics and I've become interested in trying to make a robot of my own. It woulde be a simple one but I want it to be capable of face and speech recognition at least.</p>

<p>I've found a set called EZ-Robots that comes with face/speech recognition libraries already, but thought that .NET gadgeteer would be better (more simple, scalable...). So far, I haven't tried any programming yet, I still need to decide which set to buy (which way to go).</p>

<p>I am an experienced programmer, but absolute beginner in harware stuff.</p>

<p>My vision is that I focus mostly on software and build only basic, simple hardware. I want to interact with robot so he must recognize me (or at least tell human from other objects). I would like to speak to it, give it command etc. I live in Czech Repubic and there isn't much software available for voice recognition in my language. If I could record a command and he would recognize it, that's enough.</p>

<p>It is more of a hobby matter :).</p>

<p>For any advice, I would be grateful.</p>
",7/20/2012 14:26,,472,0,0,1,,840405,Czech Republic and Slovakia,7/12/2011 9:27,201,,,,,,,,Specifications
29,1449,11794299,Distance and Angle robot control,|algorithm|robotics|,"<p>I am sorry if it is not the right Stack Exchange website i should ask this question on, but it seems to me that this is closely related to software programming more than electrican engineering ! If it should be elsewhere, please tell me.</p>

<p>Let's assume we want to control a 2 motorized wheels robot to go from point A to point B. Let's assume i want to control my robot by providing it the distance and angle. </p>

<p>I could first do the control on my angle, and then on the distance, so the robot first turns around its center (by providing positive order to one wheel, negative to the other one), and then travel until the point B is reached (by providing two positive oders to the wheels). </p>

<p>However, if i want the robot to do it in one move, i want the two different control values to be controlled at the same time. This way the robot does have a nice curve until it reaches the destination. Doing this, i provide two positive orders to the wheels, and i add on one side a positive order, and a negative one to the other side.</p>

<p>With this last solution, i have some troubles to understand how to reach the right direction.</p>

<p>If, for example, i want to go 1meter backward. I will set my order as point B : (1m,180°). With the first solution, no problem, the angle is done first, and when it is done, the 1m are done.
With the second solution, i move WHILE i turn, therefore, the curve is way bigger than the ordered 1m, and it stops after 1m, not at the point B.</p>

<p>How should i address this concern ? Do you have any advice, or maybe did i not understand well this technique ? I tried to simulate a control system with a little XNA game to try out solutions even without any robot, so feel free to give any advices you may think interesting !</p>

<p>Al_th</p>
",8/3/2012 10:53,11794398,2190,3,0,2,0,1503324,France,7/5/2012 8:09,188,11794398,"<p>I'm not fully qualified to answer this myself, but there is some great material about this on Udacity: <a href=""http://www.udacity.com/view#Course/cs373/CourseRev/apr2012/Unit/510040/Nugget/515042"" rel=""nofollow"">http://www.udacity.com/view#Course/cs373/CourseRev/apr2012/Unit/510040/Nugget/515042</a>. Sebastian Thrun (who runs Google's autonomous car project) will explain this much better than I ever could.</p>

<p>Edit: the example in the video assumes a bicycle model (two wheels only), so it's not directly applicable to your case. However, you might get some valueable info that you can utilize.</p>
",177628,1,1,,,Moving
30,1457,12042285,Problems making Carmen Robotics toolkit in Fedora,|makefile|fedora|robotics|carmen|,"<p>I'm running into issues installing the Carmen Robotics toolkit in Fedora;</p>

<h2>Making:</h2>

<p>When I type <code>make</code>, I get the following error message</p>

<pre><code>---- Copying global/carmen-std.ini to carmen.ini 

   ***********
   E X P O R T
   ***********

---- Copying ipc.h to [path]/carmen-0.7.4-beta/include/carmen
</code></pre>

<p>... many similar lines  </p>

<pre><code>---- Copying param_interface.h to [path]/carmen-0.7.4-beta/include/carmen
Makefile:7: *** missing separator.  Stop.
make: *** [export] Error 255
</code></pre>

<p>I've googled around and saw that this can be caused by spaces instead of tabs at the beginnings of lines. There is no such issue anywhere near line 7 of the makefile.
Doing make -d gives a lot of output, which ends with:</p>

<pre><code>Updating goal targets.... Considering target file `export'.  
File `export' does not exist.  
Finished prerequisites of target file `export'.  
Must remake target `export'.  
Invoking recipe from ../Makefile.rules:285 to update target `export'.  
Putting child 0x174b8e0 (export) PID 5816 on the chain.  
Live child 0x174b8e0 (export) PID 5816  
Reaping winning child 0x174b8e0 PID 5816  
Live child 0x174b8e0 (export) PID 5819  
Reaping winning child 0x174b8e0 PID 5819  
Removing child 0x174b8e0 PID 5819 from chain.  
Successfully remade target file `export'.  
GNU Make 3.82 Built for x86_64-redhat-linux-gnu Copyright (C) 2010  
Free Software Foundation, Inc. License GPLv3+:  
GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;  
This is free software: you are free to change and redistribute it.  
There is NO WARRANTY, to the extent permitted by law.  
Reading makefiles...  
Reading makefile `Makefile'...   
Reading makefile `../Makefile.conf' (search path) (no ~ expansion)...  
Reading makefile `../Makefile.vars' (search path) (no ~ expansion)...  
Makefile:7: *** missing separator.  Stop.  
Reaping losing child 0xda4940 PID 5794   
make: *** [export] Error 255  
Removing child 0xda4940 PID 5794 from chain.  
</code></pre>

<p>I've heard that getting to Carmen to compile can be a terrible experience, but I didn't expect that it would give me <em>this</em> much trouble, especially since I'd done it successfully on another computer in the past.</p>

<p><em><strong>I can't even make clean</em></strong></p>

<p>Does anyone have sage wisdom to offer on this topic?</p>
",8/20/2012 17:33,12044070,195,1,1,0,,1530111,"Medford, MA",7/16/2012 21:27,90,12044070,"<p>I downgraded from Make 3.82 to 3.81 and this issue went away.</p>
",1530111,0,0,16078848,"Solved one problem
> Searching for linux kernel headers... not found
The problem here was that 
a) --headers didn't seem to be working.
I edited the config file to point to /usr/src/kernels/$DISTRO/include/",Error
31,1469,12462461,"Is it worth to learn Ada instead of another languages [c++, c#]?",|programming-languages|ada|robotics|,"<p>If Im going to make robots, which language do you recommend me? 
In our university we can choose between several languages. Most of the students are choosing Ada just because our teacher uses it. </p>

<p>After some research I found out that ada is old. What are your thoughts? 
Is is worth to learn it? </p>
",9/17/2012 15:27,12469567,25584,8,6,21,0,,,,,12462526,"<p>It depends on what you want to do with this language. Lisp is even older than Ada and is widely used within artificial intelligence. Learn the language you want to, learn the language which is easy for you to learn and easy for you to understand it's concepts. Then you can go further and learn more languages.</p>
",1616951,2,3,135693634,"C++ for embedded system, and C# for Windows OS application, for Ada...here is an opinion (https://www.reddit.com/r/ProgrammingLanguages/comments/eyeonv/whatever_happened_to_the_language_ada/fggvgc1/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)",Specifications
32,1524,12994888,Path Finding - Autonomous Rover,|infinite-loop|dijkstra|robot|,"<p>I am working on a navigation task for an autonomous rover. Right now, the rover can calculate the shortest path between the current position and a final destination given certain obstacles. I am using dijkstra's algorithm to find the shortest path and it's working well. </p>

<p>The rover has a fixed range with which it can identify that there is an obstacle infront of it or not. The problem I am facing is that the rover gets stuck in an infinite loop of same path (from point A to B, then point B to A) when an the final destination happens to be on a region that cannot be reached or seen by the vision of the rover. </p>

<p>My question is how should I detect that I am stuck in this loop and I can't reach the destination now that I should get a new final destination or just quit. </p>
",10/21/2012 4:06,,180,1,1,0,,1473690,,6/22/2012 1:47,33,13815930,"<p>It depends on your application, if the destination cannot be reached you can detect this checking that all the reachable places in your map are visited and the destination is not one of them. Otherwise if you just missed some possible path, once you detect that you are stuck you can try to change your rover behavior. For example trying to go close to walls with sensors (if you have one) pointing to it.</p>
",1058082,0,0,17668059,The problem with the infinite loop is not clear (to me). Would you mind explaining it a bit more (preferable with an example).,Moving
33,1571,14002701,how to reprogram an old computer rom and use it as rom memory for another task?,|microcontroller|robotics|rom|,"<p>I've ripped open an old Pentium desktop. The main board is a Zida 5svx. I got to know from the manual (which i downloaded from the internet) the location of the ROM chip on the board, and took it out. It was mentioned in the manual that the chip was a Flash EEPROM.</p>

<p>Now, what I am interested in is this: Is there a way to erase the ROM and flash it with, say a C program to blink an LED (i know this might put you into a fit of laughter, but read on all the same), or control a motor?</p>

<p>I also want to know if I can construct a mega-sized micro-controller with the left-over Pentium, some MBs of RAM, and this ROM.</p>

<p>Any suggestions?</p>

<p>P.S: I know that such a uC will require appropriate power supply setup and things.</p>
",12/22/2012 12:08,,1293,1,5,2,,1592082,,8/11/2012 10:24,109,35949961,"<p>The key is in getting and deeply studying the manufacturer's datasheets for each device you remove and wish to reuse.  I am supposing that since you asked the question that you did that you are not a professional electrical engineer - that's OK, but you will need to do hours, days, or weeks of study to truly understand the datasheets well enough to successfully reuse your motherboard chips because they are written for professional engineers with years of experience, and unfortunately they were not written to be understood by hobbyists.  If you can succeed in acquiring and thoroughly understanding all of the datasheets (and the related user's guides as well for the more complex chips) then you have made it to the point where you might be able to start a custom design based on your recovered parts, on paper at least.  In order to test your design and insure that each part of it is working will require at least an oscilloscope and volt meter - and the knowledge of how to use them.  An understanding of basic electronics is essential, you will not succeed without it.  Very good soldering/rework/assembly skills will be required as well if you hope to have your design truly work - you can do everything else right and it can still fail if your skills in this area are lacking.  There is simply not enough time for me to advise you on everything you will need to know - but if you are motivated, dedicated, and you don't give up when setbacks and roadblocks occur (and trust me, they occur all too frequently for even the best engineers and best designs) - meaning that you are not easily frustrated when things don't work - then you have a chance at success.  I wish you all the best, and try to have fun while doing it (important in case fun is all you ever get out of your project).  :)</p>
",6045061,0,0,19397251,Thanks dwelch. I think that ought to open some doors...,Actuator
34,1589,14041109,how to recognise a zebra crossing from top view using opencv?,|opencv|computer-vision|robotics|,"<p>you can get all the details about the problem from this pdf document: www.shaastra.org/2013/media/events/70/Tab/422/Modern_Warfare_ps_v1.pdf</p>

<p>how to recognize a zebra crossing from top view using opencv ?? 
it is not a straight zebra crossing it has some twists and turns
i have some ideas,
1. parallel line detection but the available techniques only works to find straight parallel not the ones that has curve in it.
2. template matching to match a template of back to back white and black stripes but it becomes tedious since i cant find any pattern matching techniques with scaling and rotation.</p>

<p>in fact a idea on any single part of the problem will be so helpful!!
it is driving me crazy someone please help!!!!
any help is appreciated 
thanks in advance ....</p>

<p>NOTE:
 the zebra crossing i am talking about has the inside lines perpendicular to the border lines, just like a normal zebra crossing. only difference is that the path has curves like a river </p>
",12/26/2012 13:14,,2576,2,2,3,0,1929880,,12/26/2012 12:58,13,14041773,"<p>Here is a relevant paper:</p>

<p><a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2874980/"" rel=""nofollow"">Detecting and Locating Crosswalks using a Camera Phone</a></p>

<p>While this is not an opencv implementation it should be a reasonable place to start.  </p>
",684650,2,2,19398538,I'd put some research into recognizing a zebra from Hough transform.,Incoming
35,1594,14219623,frisbee trajectory,|c++|robotics|,"<p>This is my first post.  I'm the lead programmer on a FIRST robotics team, and this year's competition is about throwing Frisbees.  I was wondering if there was some sort of ""grand unified equation"" for the trajectory that takes into account the air resistance, initial velocity, initial height, initial angle, etc.  Basically, I would like to acquire data from an ultrasonic rangefinder, the encoders that determine the speed of our motors, the angle of our launcher, the rotational force (should be pretty constant.  We'll determine this on our own) and the gravitational constant, and plug it into an equation in real time as we're lining up shots to verify/guesstimate whether or not we'll be close.  If anyone has ever heard of such a thing, or knows where to find it, I would really appreciate it!  (FYI, I have already done some research, and all I can find are a bunch of small equations for each aspect, such as rotation and whatnot.  It'll ultimately be programmed in C++). Thanks!</p>
",1/8/2013 16:30,14219916,607,1,5,1,0,1958743,,1/8/2013 16:17,11,14219916,"<p>I'm a mechanical engineer who writes software for a living. Before moving to tech startups, I worked for Lockheed Martin writing simulation software for rockets. I've got some chops in this area.</p>

<p>My professional instinct is that there is no such thing as a ""grand unified equation"". In fact, this is a hard enough problem that there might not be very good theoretical models for this even if they are correct: for instance, one of your equations will have to be the lift generated from the frisbee, which will depend on its cross-section, speed, angle of attack, and assumptions about the properties of the air. Unless you're going to put your frisbee in a wind tunnel, this equation will be, at best, an approximation.</p>

<p>It gets worse in the real world: will you be launching the frisbee where there is wind? Then you can kiss your models goodbye, because as casual frisbee players know, the wind is a huge disturbance. Your models can be fine, but the real world can be brutal to them.</p>

<p>The way this complexity is handled in the real world is that almost all systems have feedback: a pilot can correct for the wind, or a rocket's computer removes disturbances from differences in air density. Unless you put a microcontroller with control surfaces on the frisbee, you're just not going to get far with your open loop prediction - which I'm sure is the trap they're setting for you by making it a frisbee competition.</p>

<p>There is a solid engineering way to approach the problem. Give Newton a boot and do they physics equations yourselves.</p>

<p>This is the empirical modeling process: launch a frisbee across a matrix of pitch and roll angles, launch speeds, frisbee spin speeds, etc... and backfit a model to your results. This can be as easy as linear interpolation of the results of your table, so that any combination of input variables can generate a prediction.</p>

<p>It's not guess and check, because you populate your tables ahead of time, so can make some sort of prediction about the results. You will get much better information faster than trying the idealized models, though you will have to keep going to fetch your frisbee :)</p>
",53139,6,0,19853022,"We'll be competing in an enclosed stadium, so unless the cheering fans somehow knock our Frisbee off path, almost everything is constant.  I guess we'll have to make data tables  :/  The only actual variable we will be manipulating is the angle of the launcher.  Because of this, the Frisbee should only be angled in one axis.  Our launcher is bizarrely consistent, so I guess making data tables won't be terrible, just really time consuming.  Hopefully we'll be able to generate a parabolic equation that can get us to within a few feet of our target.  Thanks!",Moving
36,1630,14514435,How to set an int to 1 if dependent on a button and in a while loop?,|c|robot|,"<p>I'm programming a robot, and unfortunately in its autonomous mode I'm having some issues.
I need to set an integer to 1 when a button is pressed, but in order for the program to recognize the button, it must be in a while loop. As you can imagine, the program ends up in an infinite loop and the integer values end up somewhere near 4,000.</p>

<pre><code> task autonomous()
   {
    while(true)
        {
    if(SensorValue[positionSelectButton] == 1)
        {
            positionSelect = positionSelect + 1;
            wait1Msec(0350);
        }
        }
   }
</code></pre>

<p>I've managed to get the value by using a wait, but I do NOT want to do this. Is there any other way I can approach this?</p>
",1/25/2013 2:32,14515103,128,4,2,1,,394842,New York,7/17/2010 20:22,304,14514458,"<p>Try remembering the current position of the button, and only take action when its state changes from off to on.</p>

<p>Depending on the hardware, you might also get a signal as though it flipped back and forth several times in a millisecond.  If that's an issue, you might want to also store the timestamp of the last time the button was activated, and then ignore repeat events during a short window after that.</p>
",459640,0,1,20235194,how about `positionSelect = 1,Incoming
37,1648,14818109,How to properly set projection and modelview matrices in OpenGL using camera parameters,|python|opengl|opencv|computer-vision|robotics|,"<p>I have a set of points (3D) taken from a range scanner. Sample data can be found here: <a href=""http://pastebin.com/RBfQLm56"" rel=""nofollow"">http://pastebin.com/RBfQLm56</a></p>

<p>I also have the following parameters for the scanner: </p>

<pre><code>camera matrix
[3871.88184, 0, 950.736938;
  0, 3871.88184, 976.1383059999999;
  0, 0, 1]



distortion coeffs
[0.020208003; -1.41251862; -0.00355229038; -0.00438868301; 6.55825615]



camera to reference point (transform)

[0.0225656671, 0.0194614234, 0.9995559233, 1.2656986283;

  -0.9994773883, -0.0227084301, 0.0230060289, 0.5798922567;

  0.0231460759, -0.99955269, 0.0189388219, -0.2110195758;

  0, 0, 0, 1]
</code></pre>

<p>I am trying to render these points properly using opengl but the rendering does not look right. What is the correct way to set openGL projection and modelview matrix? This is what I currently do - </p>

<pre><code>znear = 0.00001
zfar =  100
K = array([[3871.88184, 0, 950.736938],[0, 3871.88184, 976.1383059999999],[0, 0, 1]])
Rt =array([[0.0225656671, 0.0194614234, 0.9995559233, 1.2656986283],[-0.9994773883, -0.0227084301, 0.0230060289, 0.5798922567],[0.0231460759, -0.99955269, 0.0189388219, -0.2110195758]])
ren.set_projection(K,zfar,znear)
ren.set_projection_from_camera(Rt)
</code></pre>

<p>The function being used are: </p>

<pre><code>def set_projection(self,K,zfar,znear):
    glMatrixMode(GL_PROJECTION);
    glLoadIdentity();
    f_x = K[0,0]
    f_y = K[1,1]
    c_x = K[0,2]
    c_y = K[1,2]
    fovY = 1/(float(f_x)/height * 2);
    aspectRatio = (float(width)/height) * (float(f_y)/f_x);
    near = zfar
    far = znear
    frustum_height = near * fovY;
    frustum_width = frustum_height * aspectRatio;
    offset_x = (width/2 - c_x)/width * frustum_width * 2;
    offset_y = (height/2 - c_y)/height * frustum_height * 2;
    glFrustum(-frustum_width - offset_x, frustum_width - offset_x, -frustum_height - offset_y, frustum_height - offset_y, near, far);


def set_modelview_from_camera(self,Rt):
    glMatrixMode(GL_MODELVIEW)
    glLoadIdentity()
    Rx = array([[1,0,0],[0,0,-1],[0,1,0]])
    R = Rt[:,:3]
    U,S,V = linalg.svd(R)
    R = dot(U,V)
    R[0,:]=-R[0,:]
    t=Rt[:,3]
    M=eye(4)
    M[:3,:3]=dot(R,Rx)
    M[:3,3]=t
    M=M.T
    m=M.flatten()
    glLoadMatrixf(m)
</code></pre>

<p>Then I just render points (pasting snippet): </p>

<pre><code>def renderLIDAR(self,filename):
    glClear(GL_COLOR_BUFFER_BIT|GL_DEPTH_BUFFER_BIT)
    glPushMatrix();

    glEnable(GL_DEPTH_TEST)
    glClear(GL_DEPTH_BUFFER_BIT)
    glPointSize(1.0)
    f = open(filename,'r')
    f.readline() #Contains number of particles
    for line in f:
        line = line.split(' ')
        glBegin(GL_POINTS)
        glColor3f (0.0,1.0,0.0); 
        x = float(line[0])
        y = float(line[1])
        z = float(line[2])
        glVertex3f(x,y,z)
        #print x,y,z
        glEnd()

    glPopMatrix();
</code></pre>
",2/11/2013 18:04,14818668,1697,1,0,3,0,1166892,,1/24/2012 11:40,11,14818668,"<p>The matrices you get back, most notably the last one in your question are what in OpenGL is the composition of projection and modelview, also called Modelviewprojection, i.e.</p>

<p>MVP = P · M</p>

<p>As long as you're not interested in performing illumination calculations, you can use just that in a vertex shader, i.e.</p>

<pre><code>#version 330

uniform mat4 MVP;
in vec3 position;

void main()
{
    gl_Position = MVP * vec4(position, 1);
}
</code></pre>

<p>BTW, OpenGL and probably the library you're using as well, are using column major order, i.e. the order of the elements in memory is</p>

<pre><code>0 4 8 c
1 5 9 d
2 6 a e
3 7 b f
</code></pre>

<p>so what's written in source code must be thought as ""transposed"" (of course it is not). Since the matrix you wrote follows the same scheme you can just put it into the uniform as it is. The only question that remains are the boundaries of the NDC space used by the range scanner. But that could be taken care of with an additional matrix applied. OpenGL uses the range [-1, 1]^3 so the worst thing that can happen is, that if it's in the other popular NDC range [0, 1]^3, you'll see your geometry just squeezed into the upper left hand corner of your window, and maybe turned ""inside out"" if the Z axis goes into the other direction. Just try it, I'd say it already matches OpenGL.</p>

<p>Anyway, if you want to use it with illumination, you have to decompose it into a projection and a modelview part. Easier said than done, but a good starting point is to orthonormalize the upper left 3×3 submatrix, which yields the rotational part of the modelview 'M'. You then have to find a matrix P, that, when left multiplied with M yields the original matrix. That's an overdetermined set of linear equations, so a Gauss-Jordan scheme can do it. And if I'm not entirely mistaken, what you already got in form of that camera matrix is either the decomposed M or P (I'd go for M).</p>

<p>Once you got that you may want to get the translational part (the 4th column) into the modelview matrix as well.</p>
",524368,3,4,,,Incoming
38,1677,15250748,Maze/Grid continued - Finding an optimal solution - memory/learning issue (JAVA),|java|game-physics|robot|maze|,"<p>In relation to a previous questions found here: <a href=""https://stackoverflow.com/questions/15096066/connecting-a-maze-grids-walls-so-all-are-interconnected"">Connecting a maze/grid&#39;s walls so all are interconnected</a></p>

<p>Currently my robot reads the grid location in front to the left, directly in front and in front to the right like so:</p>

<pre><code>[00][01][02]
[10][11][12]
[20][^^][22]
</code></pre>

<p>[21] facing north would read [10][11][12] which may contain an object/trail its looking for.</p>

<p>There is a trail within my grid that I've made my robot able to follow around mapping the sensor inputs onto actions like so:</p>

<pre><code>Sensor Actions:
000 =
001 = 
010 =
011 =
100 =
101 =
110 =
111 =
</code></pre>

<p>These can equal turnleft(), turnright(), goforward() or do nothing.</p>

<p>Currently it can find 14/15 of the trail pieces but cannot find the last piece which has a gap from the trail like so:</p>

<pre><code>#KEY: x = empty, R = robot start position, T = trail

[x][x][x][x][x][x][x][x]
[x][x][x][x][x][x][T][x]
[x][x][x][x][x][x][x][x] &lt;- Here is the gap!
[R][T][T][T][T][x][T][x]
[x][x][x][x][T][x][T][x]
[x][x][x][x][T][x][T][x]
[x][x][x][x][T][x][T][x]
[x][x][x][x][T][T][T][x]
</code></pre>

<p>The problem I have is when it reaches a gap in the trial it can't deal with it and ends up spinning pointlessly until it runs out of lifes. </p>

<p>I know I need to add in some form of memory or learning but I'm not sure what or how to implement this!</p>

<p>Any suggestions?</p>
",3/6/2013 15:01,,267,1,0,0,,1082213,Southwest England,12/5/2011 19:44,175,15252750,"<p>Hard to know for sure without your code, but it sounds like your robot is just lacking a way to move onto non-trail spaces. Assuming you've got something basically like this (pseudocode):</p>

<pre><code>if (isTrailAhead) {
    moveAhead();
} else {
    turn();
}
</code></pre>

<p>Obviously this won't let you bridge gaps, because you'll keep spinning in circles without a trail to follow. There are a lot of ways you might go about dealing with this problem; the first two to come to my mind are:</p>

<ol>
<li><p>Store a counter and increment it each time you look for a trail space and don't see one. If the counter gets above a certain number (say 4), just move straight ahead regardless of what's there (assuming that's possible). Reset the counter to 0 every time you move ahead.</p>

<pre><code>if (isTrailAhead) {
    moveAhead();
} else {
    if (turnCounter &gt; 4) {
        moveAhead();
    else {
        turn();
    }
}
</code></pre>

<p>Of course in this case you need an instance variable named turnCounter, and your moveAhead() and turn() functions should increment or reset this counter as appropriate.</p></li>
<li><p>In the pseudo-else-clause above (when you don't see a trail), generate a random number between 0 and 10. If it's greater than some predetermined number, move ahead if possible.</p>

<pre><code>if (isTrailAhead) {
    moveAhead();
} else {
    if (new Random().nextInt(10) &gt;= 8) {
        moveAhead();
    else {
        turn();
    }
}
</code></pre></li>
</ol>

<p>Basically, all you need is a way to break out of loops. Either of these should do the trick for you.</p>
",2069350,0,0,,,Moving
39,1687,15548139,Embedded Systems Bit Count,|arm|avr|robotics|,"<h2>I do apologize if this is a duplicate even though I did search around here for a similar question, I only found one.</h2>

<p>So my programming team in my Engineering class currently use a 32-bit 72MHz ARM Cortex-M3 microprocessor. We're all seniors in high school, and we're struggling to use the libraries and whatnot, mostly due to poor docs from the manufacturer of the Bioloid Premium we're using. However we are about to purchase an 8-bit 16MHz AVR microcontroller because it has a wider range of support online and an easier-to-use library + more documentation. My question here is, would the decreased bit-count as well as the lower processor speed really matter to us? We're not going to be doing a lot of process-intensive programming, but more like a basic robotics class.
So, main differences between an 8-bit 16MHz AVR microprocessor and a 32-bit 72MHz ARM Cortex-M3 microprocessor?
Also, (if it holds any relevancy):</p>

<ol>
<li>We're using a Bioloid Premium by Robotis w/ CM530 (ARM), about to switch to CM510 (AVR).</li>
<li>We'll be using Embedded C instead of Robotis' RoboPlus IDE as our instruction set.</li>
</ol>

<p><em>I have googled around, found out what a bit-count was, and more about it's impact on processor speed, but not a lot of documents about it give a clear and concise answer and that's why I came here, because it's for clear and concise answers. (So please don't tell me to Google it when I've spent the past twenty minutes doing so.)</em></p>
",3/21/2013 12:52,15639663,314,2,2,1,,1928611,,12/25/2012 18:36,16,15564169,"<blockquote>
  <p>We're using a Bioloid Premium by Robotis w/ CM530 (ARM), about to
  switch to CM510 (AVR). We'll be using Embedded C instead of Robotis'
  RoboPlus IDE as our instruction set.</p>
</blockquote>

<p>I looked around at the products you refer to, and your question seems to be missing the issues you should really be concerned with.</p>

<p>The Bioloid Premium kit looks pretty sweet, with all the parts put together and configured for you already. Much of robotics courses are usually concerned with designing the hardware. You are not going to be doing any of that. So your tasks really come down to programming the hardware you are given.</p>

<p>That said, there is a world of difference between the RoboPlus IDE, which seems similar to the Lego Mindstorms drag and drop interface, and writing code in C using AVR Studio!</p>

<p>I have used AVR Studio before, but there was a major change in versions recently. You might need to modify the example programs to work in the latest version, and you will probably need some help with that.</p>

<p>It looks like they supply you with enough example code to use the periperpherals, but I don't see right away how to write a main() function to do something like follow a plan. Perhaps, there are some examples online.</p>

<p>But to answer your question, you are probably not going to run into any limitations in terms of processor capacity. They switched to a cheaper and more powerful processor to write the newer version of their control software, but the old hardware will be great, too. Working in C, you will become familiar with how to actually use an MCU, and that knowledge will transfer to other chips. The AVR family is a great one to start with. It has lots of features and is pretty sensible in how it works, with lots of documentation and third-party support. Definitely download the <a href=""http://www.atmel.com/devices/atmega2561.aspx?tab=documents"" rel=""nofollow"">datasheet</a> from Atmel for the chip you are using, although it is a dense and difficult read. You will only need to read parts of it. Also, check out the <a href=""http://www.avrfreaks.net/"" rel=""nofollow"">AVR Freaks</a> forums.</p>

<p>This sounds like a fantastic high school course. Have fun with it! </p>
",66363,2,0,22042439,"The answer I think really depends on what ""basic robotics"" is meant: for some basic robotics are circuits that blink leds, for others basic robotics are autonomous learning robots that sweep your house, do your laundry and chase your cat. These different robots require different processing power, which will affect the MCU choices.",Specifications
40,1709,16250256,"Object distance to camera, using opencv",|c++|opencv|robotics|,"<p>I want to estimate the distance of an object to my camera. This must be using Opencv.
I read that I have to use 2 cameras in stead of one and I found some code with Matlab, but I don't have any experience in it.
Any help will be appreciated.</p>
",4/27/2013 9:04,16250458,7990,3,3,3,0,1807373,"Madinah, Saudi Arabia",11/7/2012 20:49,793,16250305,"<ol>
<li><p><a href=""https://stackoverflow.com/questions/4588485/is-it-possible-to-measure-distance-to-object-with-camera"">Is it possible to measure distance to object with camera?</a></p></li>
<li><p>see this <a href=""https://photo.stackexchange.com/a/12437"">answer:</a></p>

<pre><code>distance to object (mm) = focal length (mm) * real height of the object (mm) * image height (pixels)
                          ---------------------------------------------------------------------------
                          object height (pixels) * sensor height (mm)
</code></pre></li>
</ol>
",1031417,1,0,23250440,I know object dimension only. What do you mean by sensor? I have camera,Incoming
41,1732,16664330,Balancing robot PID tuning,|c++|arduino|robotics|pid-controller|,"<p>I'm trying to build a two-wheeled balancing robot for fun. I have all of the hardware built and put together, and I think I have it coded as well. I'm using an IMU with gyro and accelerometers to find my tilt angle with a complimentary filter for smoothing the signal. The input signal from the IMU seems pretty smooth, as in less than 0.7 variance + or - the actual tilt angle.</p>

<p>My IMU sampling rate is 50&nbsp;Hz and I do a <a href=""http://en.wikipedia.org/wiki/PID_controller"" rel=""nofollow noreferrer"">PID</a> calculation at 50&nbsp;Hz too, which I think should be fast enough.</p>

<p>Basically, I'm using the PID library found at <em><a href=""http://playground.arduino.cc//Code/PIDLibrary"" rel=""nofollow noreferrer"">PID Library </a></em>.</p>

<p>When I set the P value to something low then the wheels go in the right direction.</p>

<p>When I set the P value to something large then I get an output like the graph.</p>

<p><img src=""https://i.stack.imgur.com/mjJ5J.jpg"" alt=""Enter image description here""></p>
",5/21/2013 7:22,,3763,4,5,3,0,1268168,"Menlo Park, CA, USA",3/14/2012 5:48,245,16712118,"<p>If you haven't already, I'd suggest you do a search on the terms <a href=""https://www.google.com/search?q=arduino%20PID"" rel=""nofollow"">Arduino PID</a> (obvious suggestion but lots of people have been down this road). I remember when that PID library was being written, the author posted quite a bit with tutorials, etc. (<a href=""http://brettbeauregard.com/blog/2011/04/improving-the-beginners-pid-introduction/"" rel=""nofollow"">example</a>). Also I came across this <a href=""http://playground.arduino.cc/Code/PIDAutotuneLibrary"" rel=""nofollow"">PIDAutotuneLibrary</a>.</p>

<p>I wrote my own PID routines but also had a heck of a time tuning and never got it quite right.</p>
",840992,0,0,23992677,"Steven, I find your project fascinating! I assume IMU is Inertial Measurement Unit? @JoachimPileborg above is correct, lots of code and hard to deduce logic without full knowledge about your hardware. I see you are starting with PID = 0,0,0. My suspicion these values should be different. When P is high you say the wheels go in the wrong direction, and that sounds like your software logic is faulty. I suggest you describe the project on a web site if possible then post a link here. I for one am interested. Good luck.",Actuator
42,1772,17103286,Any alternatives to libhand?,|opengl|computer-vision|robotics|ogre|libhand|,"<p><strong><a href=""http://www.libhand.org/"" rel=""nofollow"">LibHand</a></strong> seems to be tightly integrated with <strong>OGRE</strong> for rendering. And extending any OGRE program beyond the basics is an uphill task.</p>

<p>Are there any alternatives to LibHand? I'm hoping to use it with simple OpenGL programs.</p>
",6/14/2013 7:29,,447,0,2,1,,1630,"Santa Clara, CA, USA",8/17/2008 17:03,7263,,,,,,24756268,"As an experienced SO user, you should know that we're not a replacement for Google.",Specifications
43,1781,17724865,Hardware to Software incorporation/interaction,|robotics|hardware-interface|,"<p>I have taken interest in basic hardware interaction with software.</p>

<p>What's a good language to start learning to control hardware? Can Java do the job?</p>
",7/18/2013 13:30,17737491,188,2,2,0,,2128576,Philippines,3/3/2013 10:00,367,17733390,"<p>This depends on the platform. If you have a good java API for your device, it works well enough. In general though C or C++ are the languages of choice when it comes to hardware. The reason for that is that they are able to directly access arbitrary memory addresses through the pointer construct. This is in most cases the way to interact with hardware. This is not directly possible in java. </p>
",672634,2,1,33082173,"@darlinton I was pretty much talkinh about anything basic. Like, sending electricity to a port on command. So far Arduino was the best answer.",Specifications
44,1819,18718014,Arduino: void loop() gives unexpected result,|c++|arduino|robotics|,"<p>I am developing a simple robot (with 4 wheeled base) which moves forward and detects an object  through distance sensor, picks it with an arm. Object is a box with some sort of handle from upward direction, hence arm is placed below the, so called, handle of object and lifts it upward. The task I want to achieve that the robot should move forward and place the object in a container (it is also a physical object) placed at some specified location. </p>

<p>The robot moves with 4 DC motors present in wheels of its base and arm is controlled by a separate DC motor.</p>

<p>What I want is that: </p>

<p>The robot should move forward until it detects the object. <br>
When object is detected then it should stop the the wheels and start the arm to lift the object upward. <br>
Then it should move forward until it detects the second object i.e. the container. <br>
When second object (container) is detected then the wheels should be stopped and arm should be activated to put the object in the container.</p>

<p>For this I have written the following code</p>

<pre><code>#define mp1 3
#define mp2 4
#define m2p1 5
#define m2p2 6

#define echoPin 7 // Echo Pin
#define trigPin 8 // Trigger Pin
#define LEDPin 13 // Onboard LED

#define armPin1 9  // Pin 1 of arm
#define armPin2 10  // Pin 2 of arm

int maximumRange = 200; // Maximum range needed
int minimumRange = 18; // Minimum range needed
long duration, distance; // Duration used to calculate distance

int first = 0;

void setup()
{
  Serial.begin(9600);

 //Setting the pins of motors
 pinMode(mp1, OUTPUT);
 pinMode(mp2, OUTPUT);
 pinMode(m2p1, OUTPUT);
 pinMode(m2p2, OUTPUT);

 //Setting the pins of distance sensor
 pinMode(trigPin, OUTPUT);
 pinMode(echoPin, INPUT);

 pinMode(LEDPin, OUTPUT); // Use LED indicator (if required)

 pinMode(armPin1, OUTPUT);
 pinMode(armPin2, OUTPUT);
}// end setup method

long calculateDistance(){
  //Code of distance sensor
  digitalWrite(trigPin, LOW); 
  delayMicroseconds(2); 

  digitalWrite(trigPin, HIGH);
  delayMicroseconds(20); 

  digitalWrite(trigPin, LOW);
  duration = pulseIn(echoPin, HIGH);

 //Calculate the distance (in cm) based on the speed of sound.
 return duration/58.2;
}

void loop()
{
  distance = calculateDistance();

  while(distance &gt; minimumRange) {
    forward();
    distance = calculateDistance();
  }

  while(distance &lt;= minimumRange &amp;&amp; first == 0){
    stopMotors();

    pickObject();

    distance = calculateDistance();
  }

  while(distance &gt; minimumRange) {
    forward();
    distance = calculateDistance();
  }

  while(distance &lt;= minimumRange &amp;&amp; first == 1){
    stopMotors();

    putObject();

    distance = calculateDistance();
  }// end second while copied 

  first = 1;

}// end loop function

void pickObject(){
  digitalWrite(armPin1, LOW);
  digitalWrite(armPin2, HIGH);
}

void putObject(){
  digitalWrite(armPin1, HIGH);
  digitalWrite(armPin2, LOW);
}

void stopMotors(){
  digitalWrite(mp1, LOW);
  digitalWrite(mp2, LOW);
  digitalWrite(m2p1, LOW);
  digitalWrite(m2p2, LOW);
}

void forward(){
  digitalWrite(mp1, LOW);
  digitalWrite(mp2, HIGH);
  digitalWrite(m2p1, HIGH);
  digitalWrite(m2p2, LOW);
}
</code></pre>

<p>But the code does not work as I want it to do. It actually moves the arm in downward direction. It may be because I have not understood the flow of <code>loop()</code> function. </p>

<p>Can anyone tell what is the problem in my code and what should be my code to get the desired result?</p>
",9/10/2013 11:40,18719619,1028,4,2,0,,534984,,12/8/2010 12:43,180,18719274,"<p>Here is your problem:</p>

<p>The loop function runs constantly, every time it ends it starts right back up.  You set first to 1, but you never re set it to 0, so every loop it goes through first is 1.</p>

<pre><code>while(distance &lt;= minimumRange &amp;&amp; first == 1){
    stopMotors();

    putObject();

    distance = calculateDistance();
  }// end second while copied 

  first = 1;
</code></pre>
",642230,1,0,27582414,"@TheTerribleSwiftTomato : It does stop when it senses any object, but then moves the arm in downward direction every time.",Actuator
45,1894,21059100,UAV simulation in MATLAB for target tracking,|matlab|simulink|robotics|,"<p>My objective is to track a moving target on ground by an UAV and this simulation is to be done in MATLAB. I have written code for developing Urban environment plot and also put a moving point in the plot(for the target).Now for the UAV I need to implement certain guidance laws(is what I understand from resources on the Internet).So how should I proceed with it?Will I be able to write code for that also?Or should I develop a simulink model and integrate my previous code as well?
Thanks a lot in advance.</p>
",1/11/2014 6:03,,1069,1,0,0,,2417324,,5/24/2013 11:31,34,21074683,"<p>It should be possible to implement a simulation either with Matlab or Simulink. Which is more fit will depend on the approach you decide to take. Maybe you should decide what motion planning algorithm to implement before jumping into implementation. Wikipedia has a list of common algorithms:
<a href=""http://en.wikipedia.org/wiki/Motion_planning#Algorithms"" rel=""nofollow"">http://en.wikipedia.org/wiki/Motion_planning#Algorithms</a></p>
",3142795,0,0,,,Incoming
46,1901,21249131,Monte-Carlo localization for mobile-robot,|robotics|montecarlo|,"<p>I'm implementing Monte-Carlo localization for my robot that is given a map of the enviroment and its starting location and orientation. My approach is as follows:</p>

<ol>
<li>Uniformly create 500 particles around the given position</li>
<li>Then at each step:
<ul>
<li>motion update all the particles with odometry (my current approach is newX=oldX+ odometryX(1+standardGaussianRandom), etc.)</li>
<li>assign weight to each particle using sonar data (formula is for each sensor probability*=gaussianPDF(realReading) where gaussian has the mean predictedReading)</li>
<li>return the particle with biggest probability as the location at this step</li>
<li>then 9/10 of new particles are resampled from the old ones according to weights and 1/10 is uniformly sampled around the predicted position</li>
</ul></li>
</ol>

<p>Now, I wrote a simulator for the robot's enviroment and here is how this localization behaves: <a href=""http://www.youtube.com/watch?v=q7q3cqktwZI"" rel=""nofollow"">http://www.youtube.com/watch?v=q7q3cqktwZI</a></p>

<p>I'm very afraid that for a longer period of time the robot may get lost. If add particles to a wider area, the robot gets lost even easier.</p>

<p>I expect a better performance. Any advice?</p>
",1/21/2014 4:22,,859,1,2,0,,1993650,"Cambridge, USA",1/19/2013 21:46,92,21394245,"<p>The biggest mistake is that you assume the particle with the highest weight to be your posterior state. This disagrees with the main idea of the particle filter.</p>

<p>The set of particles you updated with the odometry readings is your proposal distribution. By just taking the particle with the highest weight into account you completely ignore this distribution. It would be the same if you just randomly spread particles in the whole state space and then take the one particle that explains the sonar data the best. You only rely on the sonar reading and as sonar data is very noisy your estimate is very bad. A better approach is to assign a weight to each particle, normalize the weights, multiply each particle state by its weight and sum them up to obtain your posterior state. </p>

<p>For your resample step i would recommend to remove the random samples around the predicted state as they corrupt your proposal distribution. It is legit to generate random samples in order to recover from failures, but those are supposed to be spread over the whole state space and explicitly not around your current prediction. </p>
",3127962,5,1,32053386,"Please don't ask the same question on [multiple stack exchange sites](http://robotics.stackexchange.com/q/2337/37). If you accidentally ask on the wrong site, it can be migrated to the correct one.",Moving
47,1914,21743600,How to measure distance at angle in image python,|python|mapping|robotics|particle-filter|,"<p>I'm working on a particle filter for an autonomous robot right now, and am having trouble producing expected distance measurements by which to filter the particles. I have an image that I'm using as a map. Each pixel represents a certain scaled area in the enviroment. Space the robot can occupy is white, walls are black, and areas that are exterior to the enviroment are grey.</p>

<p>If you are unfamiliar with what a particle filter is, my python code will create a predetermined number of random guesses as to where it might be (x,y,theta) in the white space. It will then measure the distance to the nearest wall with ultrasonic sensors at several angles. The script will compare these measurements with the measurements that would have been expected at each angle for each guessed location/orientation. Those that most closely match the actual measurements will survive while guesses that are less likely to be right will be eliminated.</p>

<p>My problem is finding the nearest wall AT a given angle. Say the sensor is measuring at 60°.  For each guess, I need to adjust the angle to account for the guessed robot orientation, and then measure the distance to the wall at that angle. It's easy enough find the nearest wall in the x direction:</p>

<pre><code>from PIL import Image
#from matplotlib._png import read_png
from matplotlib.pyplot import *
mapp = Image.open(""Map.png"")
pixels = mapp.load()
width = mapp.size[0]
height = mapp.size[1]
imshow(mapp)

pixelWidth = 5

for x in range(width):
    if mapp.getpixel((x, 100)) == (0,0,0,255): #Identify the first black pixel
        distance = x*pixelWidth self.x
</code></pre>

<p>The problem is that I can't tell the script to search one pixel at a time going at a 60°, or 23°, or whatever angle. Right now the best thing I can think of is to go in the x direction first, find a black pixel, and then use the tangent of the angle to determine how many pixels I need to move up or down, but there are obvious problems with this, mostly having to do with corners, and I can't imagine how many if statements it's going to take to work around it. Is there another solution?</p>
",2/13/2014 1:43,21746379,1597,1,0,2,,2202534,,3/23/2013 15:02,16,21746379,"<p>Okay, I think I found a good approximation of what I'm trying to do, though I'd still like to hear if anyone else has a better solution. By checking the tangent of the angle I've actually traveled so far between each pixel move, I can decide whether to move one pixel in the x-direction, or in the y-direction.</p>

<pre><code>for i in range(len(angles)):
    angle = self.orientation+angles[i]
       if angle &gt; 360:
           angle -= 360
        x = self.x
        y = self.y
        x1 = x
        y1 = y
        xtoy_ratio = tan(angle*math.pi/180)
        if angle &lt; 90:
            xadd = 1
            yadd = 1
        elif 90 &lt; angle &lt; 180:
            xadd = -1
            yadd = 1
        elif 180 &lt; angle &lt; 270:
            xadd = -1
            yadd = -1
        else:
            xadd = 1
            yadd = -1
        while mapp.getpixel(x,y) != (0,0,0,255):
            if (y-y1)/(x-x1) &lt; xtoy_ratio:
                y += yadd
            else:
                x += xadd
        distance = sqrt((y-y1)^2+(x-x1)^2)*pixel_width
</code></pre>

<p>The accuracy of this method of course depends a great deal on the actual length represented by each pixel. As long as pixel_width is small, accuracy will be pretty good, but if not, it will generally go pretty far before correcting itself.</p>

<p>As I said, I welcome other answers.</p>

<p>Thanks</p>
",2202534,0,0,,,Incoming
48,1925,22003790,NXT bluetooth pairing always failed,|bluetooth|robotics|nxt|lego-mindstorms|,"<p>Over 10 hours spend on this and really drive me crazy, any help is appreciate!</p>

<p>Tried with <strong>Android phone</strong> first:</p>

<p>Samsung NOTE2, Samsung Grand2, Samsung Glaxy S2 with android 2.2, android 4.2, androind 4.4 both with Original <strong>official</strong> NXT firmware, and latest <strong>LeJOS</strong>, If pairing from phone to NXT, the NXT would prompt window, just input the PIN same as in phone, but just <strong>quickly NXT returned to main menu</strong>, nothing more could see and still empty in Contracts list. otherwise, pairing from NXT to Phone, after Phone input the PIN, the NXT still say '<strong>line is busy</strong>' or 'unsuccessful' in LeJOS, while the Phone state the NXT is paired successfully.</p>

<p>Tried with PC to NXT, both in WinXP and Win7, I even bought a <strong>standalone</strong> Bluetooth USB Key, similar, once Input the PIN in PC same as the one in NXT set, the NXT quickly to say 'Line is busy', then the PC always take couple minutes to finish the Driver installation seems like mapping to some COM Port, then PC state paired successfully, but it never did in NXT.</p>

<p>I googled a lot, no useful information can get. Any idea?</p>
",2/25/2014 3:08,22446506,1672,1,0,-1,0,1602160,"Shanghai, China",8/16/2012 3:21,193,22446506,"<p>Finally I get it to work by replace the Bluetooth stack in my laptop, the experience proved the windows 7 stack can't suit, the one I used now is called bluesoleil, it also provide a management software, which I would say the UI really worried me at the beginning, it likes the very old days software, even more, it asking for license.
with this bluesoleil, I also get my arduino bluetooth sensor paired to my laptop.
I tried a generic stack provided by broadcom, but failed by it keep asking me to plug in the bluetooth adapter, and actually it's always there.
So I have to give up the paring between NXT and android phone, the andorid phone don't support replace the bluetooth stack except refresh another ROM.
Hope this information could help someone like me.</p>
",1602160,0,0,,,Remote
49,1935,22755882,Are interrupts the right thing to use for my robot?,|arduino|interrupt|robotics|motordriver|,"<p>So I have this old motorized wheelchair that I am trying to convert into a robot. I replaced the original motor driver with a sabertooth 2x12 and I’m using an Arduino micro to talk to it. The motors shaft goes all the way threw so I attached a magnet and a Hall Effect sensor to the back side to act as a rotary encoder. My current goal is to be able to tell the robot to move forward a certain amount of feet then stop. I wrote some code to do this linearly however this didn't work so well. Then I learned about interrupts and that sounded like exactly what I needed. So I tried my hand at that and things went wrong on several different levels.</p>

<p>LEVEL ONE: I have never seemed to be able to properly drive the motors it seems like any time I put the command to turn them on inside of a loop or if statement they decide to do what they want and move sporadically and unpredictably </p>

<p>LEVEL TWO: I feel like the interrupts are interrupting themselves and the thing I set in place to stop the wheels from moving forward because I can tell it to move 14 rotary encoder clicks forward and one wheel will continue moving way past 1000 clicks while the other stops</p>

<p>LEVEL THREE: couple times I guess I placed my interrupts wrong because when I uploaded the code windows would stop recognizing the Arduino and my driver would break until I uploaded the blink sketch after pressing the reset button which also reloaded and fixed my drivers. Then if I deleted one of my interrupts it would upload normally.</p>

<p>LEVEL FOUR: my Hall Effect sensors seem to not work right when the motors are on. They tend to jump from 1 to 200 clicks in a matter of seconds. This in turn floods my serial port and crashes the Arduino ide.</p>

<p>So as you can see there are several flaws somewhere in the system whether it’s hardware or software I don't know. Am I approaching this the right way or is there some Arduino secret I don’t know about that would make my life easier? If I am approaching this right could you take a look at my code below and see what I’m doing wrong.</p>

<pre><code> #include &lt;Servo.h&gt;//the motor driver uses this library

Servo LEFT, RIGHT;//left wheel right wheel

int RclickNum=0;//used for the rotory encoder
int LclickNum=0;//these are the number of ""clicks"" each wheel has moved

int D =115;//Drive
int R =70;//Reverse
int B =90;//Break

int Linterrupt = 1;//these are the interrupt numbers. 0 = pin 3 and 1 = pin 2
int Rinterrupt = 0;

int clickConvert = 7;// how many rotery encoder clicks equal a foot

void setup()
{
  Serial.begin(9600); //starting serial communication 
  LEFT.attach( 9, 1000, 2000);//attaching the motor controller that is acting like a servo
  RIGHT.attach(10, 1000, 2000);
  attachInterrupt(Linterrupt, LclickCounter, FALLING);//attaching the rotory encoders as interrupts that will 
  attachInterrupt(Rinterrupt, RclickCounter, FALLING);//trip when the encoder pins go from high to low


}
void loop()
{//This is for controling the robot using the standard wasd format
  int input= Serial.read();
  if(input == 'a')
    left(2);
  if(input == 'd')
    right(2);
  if(input == 'w')
    forward(2);
  if(input == 's')
    backward(2);
  if(input == 'e')
    STOP();
}

void forward(int feet)//this is called when w is sent threw the serial port and is where i am testing all of my code. 
{
  interrupts(); //turn on the interrupts
  while(RclickNum &lt; feet * clickConvert || LclickNum &lt; feet * clickConvert)// while either the left or right wheel hasnt made it to the desired distance
  {
    if(RclickNum &lt; feet * clickConvert)//check if the right wheel has gone the distance
      RIGHT.write(D); //make the right wheel move
    else
      RIGHT.write(B);//stop the right wheel

    if(LclickNum &lt; feet * clickConvert)
      LEFT.write(D);
    else
      LEFT.write(B);
  }
  noInterrupts();//stop the interrupts 
  resetCount();//set the click counters back to zero
}

//once i have the forward function working i will implament it through out the other functions
//----------------------------------------------------------------------

void backward(int feet)
{
  RIGHT.write(R);
  LEFT.write(R);
}

void left(int feet)
{
  RIGHT.write(D);
  LEFT.write(R);
}

void right(int feet)
{
  RIGHT.write(R);
  LEFT.write(D);
}

void STOP()
{
  resetCount();
  RIGHT.write(B);
  LEFT.write(B);
}

void LclickCounter()//this is called by the left encoder interrupt
{
  LclickNum++; 
  Serial.print(""L"");
  Serial.println(LclickNum); 
}

void RclickCounter()//this is called by the right encoder interrupt
{
  RclickNum++;
  M Serial.print(""R"");
  Serial.println(RclickNum);
}


void resetCount()
{
  RclickNum=0;
  LclickNum=0;
}
</code></pre>
",3/31/2014 7:33,22760663,929,1,1,0,,3106569,,12/16/2013 8:20,14,22760663,"<ol>
<li><p>don't use <code>interrupt()</code> and <code>nointerrupt()</code> (or <code>cli()</code> and <code>sei()</code>) as they will stop timer and serial interrupt, breaking a lot of things. Just set to 0 the counting variable OR use detachInterrupt and attachInterrupt.</p></li>
<li><p>variable used inside interrupt AND normal execution flow should be declared as <code>volatile</code>, or their value my be unsyncornized. So declare them like <code>volatile int RclickNum=0;</code></p></li>
<li><p>interrupt should be fast to execute, as by default other interrupt will NOT execute while inside an interrupt.</p></li>
<li><p>NEVER use Serial inside interrupt;  if Serial buffer is full, it will call Serial.flush(), that will wait for Serial interrupt of byte written, but because you are alreadi inside an interrupt will never happen...dead lock aka you code hangs forever!</p></li>
<li><p>because your ""moving"" function use quite a long time to execute, if multiple command arrive to the serial, thay will remain isnode the buffer until readed. So if in the terminal you write ""asd"" and then ""e"", you will see robot go left, backward, right, stop (yes, actually the stop function is not usefull as it does nothing because your ""moving"" function are ""blocking"", that mean they won't return until they ended, so the loop() code (and the read of ""e"") will not execute until the buffer of serial has been processed.</p></li>
</ol>
",1668605,1,0,34688720,"General advice: keep the interrupt handlers small and do not spawn other activities.  That means remove those Serial.println().  If you need something echo'd, do it in the loop().  Encoders always count.  Don't turn on and off the interrupts.  If I walk up and physically push the robot, the encoders should still count.  Similar, if it stops on a hill, you want it to hold its position.  As alternate to on/off: When you start a move, capture a snapshot of the current position and calculate the target.  When reach the target can stop.  Last: don't ride that thing without a helmet :)",Actuator
50,1954,23207910,Basic continuous 2D map exploration ( with obstacles ) algorithms,|algorithm|mapping|2d|robotics|,"<p>Here is the problem: There is a map which size is anywhere from 200*200 pixels to 1000*1000 pixels. Each pixel is a third of an inch in length/width.  </p>

<p>The map has walls coming from it (which can be of any size), and can be pre-processed by any means. However, when the problem starts, a robot (of pixel size 18*18) is placed in an unknown location, along with several obstacles and one goal, all of which are in an unknown location.</p>

<p>If the robot bumps into any wall/object/goal it immediately dies. As such it has a simple laser scanner that perfectly sees an 80*80 pixel square ahead of it,centered on the robot.</p>

<p>I have already solved the problem of localizing the robot and determining its position (within a small error) on the grid. I am having a bit of trouble with making a good algorithm for going across the entire map until the goal is found.</p>

<p>I have the idea of making the robot go to the lower right, and somehow sweeping left to right, avoiding obstacles, and walls, until the goal is found, however I am unsure of a good way to do that.</p>

<p>Are there any decent algorithms for such a thing, or are there better ones for what I am trying to do?</p>
",4/21/2014 23:48,,2368,2,0,1,0,3513412,,4/9/2014 2:49,2,23208064,"<p>The one and only algorithm that comes to my mind is a simple Lee algorithm. <a href=""http://www.oop.rwth-aachen.de/documents/oop-2007/sss-oop-2007.pdf"" rel=""nofollow"">Here's</a> a pretty decent tutorial about what it does and how it works.</p>

<p>Using this algorithm you should be able to find all the obstacles and eventually find the goal, also you will find the shortest path to the goal.</p>

<p>The only big difference is that you have to move an 80x80 object instead of a 1x1 object, but I will let you deal with the way you will implement this.</p>
",2253278,0,0,,,Moving
51,1977,24095107,Working of CCD algorithm for Inverse Kinematics,|algorithm|3d|vector-graphics|robotics|inverse-kinematics|,"<p>Lets say I've a robotic arm with joints at points A,B,C,D in a 3D space. Let D be the end effector(bottommost child) and A be the topmost parent. Let T be the target point anywhere in the space. The aim is to make the end effector reach the target with minimum rotations in top levels(parents).</p>

<p>What I initially thought:</p>

<p>1) Rotate the arm C by angle TCD.
2) Then rotate the arm B by new angle TBD.
3) Then rotate the arm A by new angle TAD.</p>

<p>But the end effector seems to point away from the target after step 2. What am I doing wrong and how can I fix it?</p>
",6/7/2014 7:57,,2239,2,9,1,0,1137624,"Chennai, India",1/8/2012 21:32,2514,24135139,"<p>Before I started use some more advanced approaches I did it like this (using simple <strong>CCD cyclic coordinate descent</strong>):</p>
<pre class=""lang-cpp prettyprint-override""><code>pe=desired_new_position;

for (i=0;i&lt;number_of_actuators;i++)
 {
 // choose better direction
                   p=direct_kinematics(); d =|p-pe|; // actual step
 actuator(i)--;  p=direct_kinematics(); d0=|p-pe|; // previous step
 actuator(i)+=2; p=direct_kinematics(); d1=|p-pe|; // next step
 actuator(i)--;  dir=0; d0=d;
      if ((d0&lt;d)&amp;&amp;(d0&lt;d1)) dir=-1;
 else if ((d1&lt;d)&amp;&amp;(d1&lt;d0)) dir=+1;
 else continue;

 for (;;)
  {
  actuator(i)+=dir; p=direct_kinematics(); d =|p-pe|;
  if (d&gt;d0) { actuator(i)-=dir; break; }
  if (actuator(i) on the edge limit) break;
  }

 }
</code></pre>
<p>[notes]</p>
<ol>
<li><p>you can modify it to inc/dec actuator position by some step instead of 1</p>
<p>stop if difference crossed zero then start again with smaller step until <code>step == 1</code> This will improve performance but for most application is <code>step=1</code> enough because new position is usually near the last one.</p>
</li>
<li><p>beware that this can get stuck in local min/max</p>
<p>if the output get stuck (effector position is unchanged) then randomize the actuators and try again. Occurrence of this depend on the kinematics complexity and on the kind of path you want to use</p>
</li>
<li><p>if the arms are driven more on the top  then on the bottom</p>
<p>then try reverse the i-for loop</p>
</li>
<li><p>if you have to control the effector normal</p>
<p>then you have to exclude its rotation axises from CCD and set it before CCD</p>
</li>
</ol>
",2521214,1,7,37164184,Do you calculate the actual angle? Or the angle in the arm's movement plane? The latter would be the correct approach.,Actuator
52,2003,24719506,Mindstorms NXT Programming opposite facing ultrasonic and light sensors,|robotics|lego-mindstorms|nxt|,"<p>I'm not usually one to ask for help, but after weeks of trying to get this to work, I'm reaching out for help as a ditch attempt.</p>

<p>I've been having trouble with programming my LEGO Sumo bot. Because of this, I started to learn the program more and more. I've still had this one problem though. How would I program my sumo bot to utilize opposite facing ultrasonic sensors? The closest I've gotten was having two normal 'spin seek destroy back up' loops running in parallel with another loop of the same idea only with opposite directions and the other sensor ports. </p>

<p>The problem with that is the robot seems to want to do each seek and destroy loop in a pattern. Front, back, front, back, and so on. This presents problems and negates the entire purpose of having both sensors. The other problem is when the back ultrasonic sensor is triggered first, the robot wants to spin in circles to seek and move the direction of the back ultrasonic at the same time. So it will jump backwards and turn back and forth in a stuttering motion.</p>

<p>My hopes is to have the robot spin and move toward an object that the ultrasonic sensor sees. Regardless of which ultrasonic sensor is triggered. After the target is seen the robot will move until the light sensor sees white, and move in the opposite direction. </p>

<p>I can provide more information if necessary.</p>

<p>I hope I'm okay in asking this here, it really is my last effort.</p>

<p>This is the full code:
<img src=""https://i.stack.imgur.com/sC1D8.png"" alt=""full code""></p>

<p>This is the code inside each loop:
<img src=""https://i.stack.imgur.com/XYSCr.png"" alt=""inside each loop""></p>
",7/13/2014 4:16,,3052,1,6,0,,3833554,,7/13/2014 3:57,10,30516761,"<p>Check both sensors in a loop and use the output of both sensors to decide which action to take.</p>
",2250588,0,0,38341231,"It sounds like you need to check both sensors in one ""spin seek destroy back up"" loop.",Incoming
53,2010,24986253,Touch sensor not working,|c|robotics|nxt|,"<p>I am attempting to create a very simple program in RobotC. In this program the robot will move forward until the touch sensor is hit.</p>

<pre><code>#pragma config(Sensor, S2,     touchSensor,    sensorTouch)

void setMotors(int a, int b){
    motor[motorA] = a;
    motor[motorB] = b;
}

task main(){
    wait1Msec(100);//Wait for sensor to init

    setMotors(50, 50);

    while(sensorValue(touchSensor) == 0){
        //Do Nothing
    }

    setMotors(0, 0);
}
</code></pre>

<p>This code should make the robot move forward until the touch sensor is triggered.
Whenever I try and do anything with the touch sensor it does not work. When I output the value to the debug log it shows 180 when pressed and 1024 when released. I have verified that it is working normally by viewing the value on the brick itself.</p>

<p>Robot C Version: 4.0</p>
",7/27/2014 22:26,24987679,192,1,10,1,,1478191,Earth,6/24/2012 14:26,483,24987679,"<p>Apparently, your touch sensor is <a href=""http://www.robotc.net/wiki/NXT_Functions_Sensors#SensorRaw"" rel=""nofollow"">stuck in SensorRaw mode</a>. It is unclear - from the documentation I could find - how this could be fixed in code, but a work-around would be to explicitly put the sensor into raw mode (in case the situation changes in the future), and then compute the boolean value with a function like this:</p>

<pre><code>bool sensorIsOn(short sensorRawValue)
{
    bool isOn = false;
    if(sensorRawValue &gt; 512)
    {
        isOn = true;
    }
    return isOn;
}
</code></pre>
",1896761,2,0,38844972,"Awesome, that's what I'd expect given those values. So, change the while statement to `while(sensorValue(touchSensor) > 500)` and let me know what happens.",Incoming
54,2102,26486572,Scan Matching Algorithm giving wrong values for translation but right value for rotation,|python|robotics|least-squares|slam-algorithm|,"<p>I've already posted it on robotics.stackexchange but I had no relevant answer.</p>

<p>I'm currently developing a SLAM software on a robot, and I tried the Scan Matching algorithm to solve the odometry problem.</p>

<p>I read this article :
<a href=""http://webdiis.unizar.es/~montesan/web/pdfs/minguez06TRO.pdf"" rel=""nofollow"">Metric-Based Iterative Closest Point Scan Matching
for Sensor Displacement Estimation</a></p>

<p>I found it really well explained, and I strictly followed the formulas given in the article to implement the algorithm.</p>

<p>You can see my implementation in python there :
<a href=""https://github.com/pierrolelivier/SLAMScanMatching/blob/master/ScanMatching.py"" rel=""nofollow"">ScanMatching.py</a></p>

<p>The problem I have is that, during my tests, the right rotation was found, but the translation was totally false. The values of translation are extremely high.</p>

<p>Do you have guys any idea of what can be the problem in my code ?</p>

<p>Otherwise, should I post my question on the Mathematics Stack Exchange ?</p>

<p>The ICP part should be correct, as I tested it many times, but the Least Square Minimization doesn't seem to give good results.</p>

<p>As you noticed, I used many bigfloat.BigFloat values, cause sometimes the max float was not big enough to contain some values.</p>
",10/21/2014 12:19,,520,1,0,1,,2648364,,8/3/2013 10:32,34,30529898,"<p>don't know if you already solved this issue.</p>

<p>I didn't read the full article, but I noticed it is rather old.</p>

<p>IMHO (I'm not the expert here), I would try bunching specific algorithms, like feature detection and description to get a point cloud, descriptor matcher to relate points, bundle adjustement to get the rototraslation matrix.</p>

<p>I myself am going to try sba (<a href=""http://users.ics.forth.gr/~lourakis/sba/"" rel=""nofollow"">http://users.ics.forth.gr/~lourakis/sba/</a>), or more specifically cvsba (<a href=""http://www.uco.es/investiga/grupos/ava/node/39/"" rel=""nofollow"">http://www.uco.es/investiga/grupos/ava/node/39/</a>) because I'm on opencv.</p>

<p>If you have enough cpu/gpu power, give a chance to AKAZE feature detector and descriptor.</p>
",1254714,0,0,,,Moving
55,2127,26935863,Robotics - Recursive function for fractal.,|python|recursion|robotics|myro|calico-project|,"<p>I'm currently working with Myro/Calico with Robotics.  I'm trying to run a recursive function of a fractal. I'm using Python.</p>

<p>I've been following pseudocode here.
<a href=""http://wiki.scratch.mit.edu/wiki/Recursion_and_Fractals"" rel=""nofollow"">Fractal</a></p>

<p>So far I've tried to implement the first step without recursion.  And it runs well</p>

<pre><code># 1 foot per 2 seconds.  x * 2 = feet desired.  
def fractal(x):
    waitTime = x*2
    turnRight(1, 0.825) #90 degree turn
    forward(1, x/3) #move length/3 steps
    turnLeft(1, 0.55)#60 degree turn
    forward(1, x/3) #move length/3 steps
    turnRight(1, 1.1) #120 degree turn
    forward(1, x/3) #move length/3 steps
    turnLeft(1, 0.55) #60 degree turn
    forward(1, x/3) #move length/3 steps
</code></pre>

<p>While this works, my goal is to go through this recursively, but make a smaller curve at each iteration.  I attempted to do it, but my robot isn't moving as desired. </p>

<p>Here's my attempt at recursion</p>

<pre><code>def fractal(x):
    waitTime = x*2
    if (x == 1):
        forward(x/3)
    else:
        (x-1)/3
    turnLeft(1,0.55) #60 degrees
    if (x == 1):
        forward(x/3)
    else:
        (x-1)/3
    turnRight(1, 1.1) #120 degree turn
    if (x == 1):
        forward(x/3)
    else:
        (x-1)/3
    turnLeft(1, 0.55)#60 degree turn
    if (x == 1):
        forward(x/3)
    else:
        (x-1)/3
</code></pre>

<p>My robot is only turning left and right, but it's not making the full shape.  The one without the recursion started the fractal.  I just need recursion to go through the whole fractal.  </p>
",11/14/2014 17:44,26936172,267,1,7,0,,3577397,,4/27/2014 2:15,141,26936172,"<p>I'm thinking this is what you want to do</p>

<pre><code>x = number of interations
l = lenth(wait time)
def fractal(x, l):
    if (x == 1):
        forward(l/3)
    else:
        fractal((x-1), l/3)
    turnLeft(1,0.55) #60 degrees
    if (x == 1):
        forward(l/3)
    else:
        fractal((x-1), l/3)
    turnRight(1, 1.1) #120 degree turn
    if (x == 1):
        forward(l/3)
    else:
        fractal((x-1), l/3)
    turnLeft(1, 0.55)#60 degree turn
    if (x == 1):
        forward(l/3)
    else:
        fractal((x-1), l/3)
</code></pre>
",2097159,1,1,42416511,"I'm not sure, but I think you need to have two variables to `fractal` instead of just one. You can't easily represent the fractal depth _and_ the width of the curve with just one variable. What if I want a five iteration Koch curve that's 100 pixels wide? What would the call to `fractal` look like, and how would it differ if I wanted a seven iteration curve that's 100 pixels wide?",Moving
56,2149,27716172,RobotC: Multithreading in TeleOp (controlling mode),|multithreading|robot|,"<p>I am making a program for a robot in a competition, and need to multithread.</p>

<p>When I make a second task (task two()) and try to start (startTask) it with a button press from a controller, it just executes the first statement of the task and only as long as the button is pressed, instead of the whole block. I've tried many things including putting a loop in the second task also, using a function instead of a task and sleeping for 200 milliseconds before, and after the startTask(two); function, but the same thing happens every time.</p>

<p>I can't post my program because I don't want other people to steal it, sorry.</p>

<p>What edits will make it run the whole block?
Any help would be appreciated.</p>
",12/31/2014 4:35,27750568,259,1,0,0,,3527863,,4/12/2014 22:27,31,27750568,"<p>Since this is Controller Mode, I'm assuming that you are setting the motors to stop when the corresponding button is not pressed. </p>

<pre><code>if([...])
[...]
else
{
   setMotorSpeed(motor10, 0);
}
</code></pre>

<p>This is the cause for the stopping of the motors when you release. All of the other methods that you tried had nothing to do with this, so they shouldn't have worked.</p>

<p>You need to put something like this:</p>

<pre><code>int  Motor10Speed;
[...]
if([...])
[...]
else
{
   setMotorSpeed(motor10, Motor10Speed);
}
</code></pre>

<p>This will control an individual motor. Repeat this for all other motors being used.</p>

<p>After that is done, make the function look something like this:</p>

<pre><code>task mini_function();

task main()
{
[...]
}

task mini_function()
{
Motor10Speed = 50;
sleep(1000);
Motor10Speed = 0;
}
</code></pre>

<p>Expand the above program so it matches your current function, while using the <code>MotorSpeed</code> variables as <code>setMotorSpeed</code> variables.</p>

<p>This should make you able to drive and run a function at the same time without them interrupting each other.</p>
",3527863,1,0,,,Timing
57,2170,28534527,Programming a robot to explore a grid,|java|performance|grid|robot|,"<p>In my project I'm simply trying to make a robot which explores as much of the grid as it can, without taking the same path twice. Also, it has a sensor to see if an object is lying in its way(the object can only be in a corridor). But I'm having trouble trying to make the robot avoid taking the same path.</p>

<p>I tried resolving this issue by creating a 2D array for storing an integer value for each square in the grid. A value of 0 means the robot has not been on that square in the grid yet, a value of 1 means that square is blocked in the grid and a value of 2 means that the robot has been on that square before. If the robot sees that the square ahead of its current heading has value 2, then it would keep rotating to find a square with a value of 0, but if no square of value 0 around the robot exists, then it begins to backtrack.</p>

<p>My problem can be explained more clearly with this example:</p>

<p><img src=""https://i.stack.imgur.com/zgHy7.png"" alt=""enter image description here""></p>

<p>The triangle represents the robot and its start position, the bottom left corner is assumed to be the position (0,0) in my grid. The green circles represent items blocking it's path. The red square is the target for the robot. the robot can only move onto white squares in the grid.</p>

<p>When I start my programme the robot, moves forward(east since that is its current heading) till it gets to the junction just before the green circles. It looks ahead and detects an object in the way, so it rotates 90 degree anticlockwise and checks for another blockage, which again occurs hence it rotates anticlockwise again. So now the robot is at position (0,2) heading west. It can only move west to avoid leaving the grid or hitting an object, so it arrives back at its initial position but still heading west. It'll now keep rotating 90 degrees clockwise till it can find a direction which will keep it on the grid, i.e till it's facing east again. So the grid now looks like:</p>

<p><img src=""https://i.stack.imgur.com/590Dv.png"" alt=""enter image description here""></p>

<p>But now I want to ignore going ahead and onto the same path by ignoring that direction and rotating 90 degrees anticlockwise again to face north, so my robot can move north into a new path. I could simply ignore the direction and just keep rotating to find a new path, but what if I'm surrounded by been before paths and I want my robot to backtrack to its last junction. How can I efficiently do this. Also how can I efficiently detect when I need to backtrack.</p>

<p>Thank you</p>
",2/16/2015 4:27,28534606,1513,1,0,0,,2169454,,3/14/2013 10:59,68,28534606,"<p>Solving the predicament in picture 2 could be as simple as checking for other <code>white</code> squares around the robot before you make a move. In picture 2 the robot would see that the square he is facing is ""greyed out"" and decide to check all other directions and eventually find that there is an empty white square to the North of him.</p>

<p>Edit : Didn't realize it was an actual robot.</p>

<p>Since the only way of learning what is in a cell is by turning to that cell and using the sensor, the robot will have to do some amount of turning no matter what you do. When it encounters a wall or green object, it will have to turn until it finds a new path to travel. You could optimize it by disregarding the walls of the enclosure. For example, when the robot arrives back in his starting position facing west, you already know there is a wall to the south because of its coordinate position is (0,-1), which is invalid. This allows you to figure out that the open tile is to the north because you have already visited the tile to the east, requiring only one turn.</p>

<p>Furthermore, when the robot eventually travels all the way to the north, tile (0,6) you know there is a wall to the north and to the west because of its position. You could then intelligently guess that the open slot has to be to the east because the western tile (-1,6) is not valid and (0,7) is not valid either.</p>

<p>Without changing the sensor to see 2 blocks or installing more sensors on the robot (ie one on every side), there is not much more optimization that can be done due to the limited availability of information.</p>
",4080860,1,4,,,Moving
58,2171,28616358,Python bus between processes,|python|robotics|,"<p>I am completely starting over with my robots brain. All is developed in Python.</p>
<p>I want to keep everything as modular as possible and allow the use of multiple CPU cores (Raspberry PI 2).</p>
<p>I thought of using multiple processes. One for serial communication, one for every sensor, one for every higher function.
All connected by a steering &quot;brain&quot; function.</p>
<p>I want to connect processes by message busses. E.g. Should each sensor dart it's own bus. Higher functions and the brain can then sign in to the bus. In optimal case I would like to send whole objects too.</p>
<ul>
<li>Is there a good framework to provide the busses?</li>
<li>is there maybe a better approach to the whole topic?</li>
</ul>
",2/19/2015 20:35,28616511,260,1,3,2,,3173818,,1/8/2014 15:04,21,28616511,"<p>Try <a href=""http://nanomsg.org/"" rel=""nofollow"">nanomsg</a> (follow-up project to ZeroMQ):</p>

<blockquote>
  <p>nanomsg is a socket library that provides several common communication
  patterns. It aims to make the networking layer fast, scalable, and
  easy to use. Implemented in C, it works on a wide range of operating
  systems with no further dependencies.</p>
  
  <p>The communication patterns, also called ""scalability protocols"", are
  basic blocks for building distributed systems. By combining them you
  can create a vast array of distributed applications. The following
  scalability protocols are currently available:</p>
  
  <ul>
  <li>PAIR - simple one-to-one communication</li>
  <li>BUS - simple many-to-many    communication</li>
  <li>REQREP - allows to build clusters of stateless services    to process user requests</li>
  <li>PUBSUB - distributes messages to large sets    of interested subscribers</li>
  <li>PIPELINE - aggregates messages from    multiple sources and load balances them among many destinations</li>
  <li>SURVEY - allows to query state of multiple applications in a single    go</li>
  </ul>
</blockquote>
",1099876,1,2,45535833,`multiprocessing` ... its a builtin library it has things called `Pipe`'s that do about what you want,Timing
59,2183,28951574,Implementing a stack to use backtracking for struct,|c|struct|backtracking|robot|,"<p>This is a program that enables a Robot to follow black lines through a ""maze"". The maze is made of 7x7 black lines with a entry line leading through it. There can be empty squares and lines leading nowhere. where the black lines meet there is a white square. </p>

<p>The idea is to save these squares in a struct and make the robot run through the maze and save the individual structs in a stack. </p>

<p>I want to do this so that the robot can find the way back through backtracking.</p>

<pre><code>void push(int value ){
if(top ==49){ //uncertain what to write here 
} else{
    top = top+1;
    a[top] = value;
}

int pop(){

if(top ==-1){
    return -234;
}else{

    int r = a[top];
    top = top-1;
    return r;
   }
 }
</code></pre>

<p>I'm uncertain how make this work together with this </p>

<pre><code>typedef struct proxy {
int edges[4];
int visited;
int exists;
int token;
} proxy;

int main() {

   /* This part is to make simulation work, idea is to start in the middle
      an 14x14, because that allows us to go in any direction. 
      the arrays are the values that the simulation gives back. 
      This seems to work even though it's not pretty. */

   static int x = 7; 
   static int y = 7;
   static int zaehler = 0;  //Zähler für Rundenbegrenzung
   int Ary[14][14];
   int hilfN, hilfO, hilfS, hilfW;
   int i,j;
   int N[8] = {16, 48, 80, 112, 144, 176, 208, 240};
   int O[8] = {128, 144, 160, 176, 192, 208, 224, 240};
   int S[8] = {32, 48, 96, 112, 160, 176, 224, 240};
   int W[8] = {64, 80, 96, 112, 192, 208, 224, 240};
   int Intersec = Robot_GetIntersections();
   int myRobot_Move(x,y){
   return Robot_Move(x-7,y-7);
}
for(i= 0; i &lt; 14; i++){

    for(j= 0; j &lt; 14; j++){
        Ary[i][j] = 0;
        }
}

myRobot_Move(x,y);

while (x &lt; 14 &amp;&amp; y &lt; 14) {
Intersec = Robot_GetIntersections();
Ary[x][y] = Intersec;

for (hilfN=0; hilfN&lt;8; hilfN++){
    if(N[hilfN] == Intersec){
        if(Ary[x][y+1] == 0){
            y++;
            myRobot_Move(x,y);
        }
    }
}
//Osten
for (hilfO=0; hilfO&lt;8; hilfO++){
        if(O[hilfO] == Intersec){
            if(Ary[x+1][y] == 0){
                x++;
                myRobot_Move(x,y);
            }
        }
    }
//Süden
for (i=0; i&lt;8; i++){
        if(S[i] == Intersec){
            if(Ary[x][y-1] == 0){
                y--;
                myRobot_Move(x,y);
            }
        }
    }
//Westen
for (i=0; i&lt;8; i++){
            if(W[i] == Intersec){
                if(Ary[x-1][y] == 0){
                x--;
                myRobot_Move(x,y);
            }
        }
    }

if (Ary[x][y+1] &amp;&amp; Ary[x+1][y] &amp;&amp; Ary[x][y-1] &amp;&amp; Ary[x-1][y] &gt; 0){
    break;
    printf(""End"");
   }
 }


return EXIT_SUCCESS;
}
</code></pre>
",3/9/2015 20:55,,76,1,0,0,,4651383,,3/9/2015 20:23,1,32453418,"<p>If you want your stack <code>a</code> to save <code>struct proxy</code>s instead of <code>int</code>s, change the presumed <code>int a[50];</code> to <code>proxy a[50];</code> and the functions <code>push()</code> and <code>pop()</code> as follows:</p>

<pre><code>void push(proxy value)
{
    if (top == 49) puts(""'a' stack overflow""), exit(1);
    else a[++top] = value;
}

proxy pop()
{
    if (top == -1) return (proxy){-234};
    else return a[top--];
}
</code></pre>
",2413201,0,0,,,Moving
60,2184,28994556,Can somebody help explain RobotC Syntax?,|c|robotics|,"<p>I have been programming for some time in Python, and JavaScript.  I have also programmed with the arduino language which is a mix between C and C++.  I was just introduced to RobotC.  The syntax used for RobotC is not like any language I have learned.  Can someone help explain these syntax differences so I can better understand it?</p>

<p><strong>Problem 1:</strong></p>

<p>When making a motor turn, you can use the following syntax:</p>

<pre><code>motor[motorA] = 50
</code></pre>

<p>What did that line just do?  In any other programming language that is how you would change a value in an array, but in RobotC it acts like a function call.  Is 'motor' an array, or an object?  And why do I need a function when controlling servos?</p>

<p><strong>Problem 2:</strong></p>

<p>When in the history of programming is this allowed?</p>

<pre><code>motor[leftMotor] = motor[rightMotor] = speed = 127;
</code></pre>

<p>And what which of the following would this code do?</p>

<pre><code>speed = 127;
motor[rightMotor] = speed;
motor[leftMotor] = motor[rightMotor];
</code></pre>

<p>or</p>

<pre><code>speed = 127;
motor[rightMotor] = 127;
motor[leftMotor] = 127;
</code></pre>
",3/11/2015 18:26,28994604,423,2,7,-1,,4206450,,11/2/2014 0:44,23,28994604,"<p>1) You are setting the value of the item in array <code>motor</code> at index <code>motorA</code> to be equal to <code>50</code>.</p>

<p>2) Multiple in-line assignments are evaluated from right to left, so this is the same as</p>

<pre><code>speed = 127;
motor[rightMotor] = speed;
motor[leftMotor] = motor[rightMotor];
</code></pre>
",3133680,5,0,46237974,"To understand Problem 1, you need to find the definition of `motor`. If that doesn't clear things up for you, then you'll need to add the definition of `motor` to the question. All three snippets in #2 do the same thing, and the first snippet has been allowed since C was originally developed, which would be around 1970.",Actuator
61,2199,29168032,Extended Kalman Filter - error in update step,|matlab|prediction|robotics|kalman-filter|atan2|,"<p>I have to implement EKF without actually having good mathematical understanding of it. (Great... not...) So far I have been doing well but since I tried to implement the prediction step things started going wrong. </p>

<ol>
<li>The agent that uses EKF (red) shoots off in a random direction</li>
<li>Eventually some variables (pose, Sigma, S, K) become NaNs and the simulation fails</li>
</ol>

<p><img src=""https://i.stack.imgur.com/7S9YC.png"" alt=""enter image description here""></p>

<p>I base my code on the code from Thrun's ""Probabilistic Robotics"" on page. 204. This is the part of the code that seems to be messing things up</p>

<pre><code>% Get variables
[x,y,th] = getPose(mu_bar);
numoffeatures = size(map,2);

for f = 1:numoffeatures
    j = c(f);
    [fx,fy] = getFeatures(map,f);

    q = (fx-x).^2 + (fy-y).^2;  

    z_hat = [sqrt(q);
             atan2(fy-y,fx-x)-th;
             j];

    H = [(-fx-x)/sqrt(q) (-fy-y)/sqrt(q) 0;
          (fy-y)/q       (-fx-x)/q      -1;
                0               0        0];

    S = H*Sigma_bar*H'+Q;
    K = Sigma_bar*H'/inv(S);

    mu_bar    = mu_bar+K*(z(:,j)-z_hat);
    Sigma_bar = (eye(3)-K*H)*Sigma_bar; 
end
</code></pre>

<p>I am totally clueless... Any ideas and hints will be appreciated. Thank you.  </p>

<p><strong>UPDATE</strong></p>

<p>The reason of the agent shooting off is the 'error' when computing the difference between two angles. Those are computed using atan2. Although I know what the problem is I still can't figure out how to fix it.</p>

<p>Let's imagine that after computing atan2 for two objects I have values resulting in a = 135 and b = 45. I computed the difference between them for both possibilities 90 degrees and 270 degrees but the agent still doesn't behave the way it is supposed to. I've never really encountered atan2 before. Is my understanding of calculating the difference between atan2 values wrong? Here is the illustration of my understanding:</p>

<p><img src=""https://i.stack.imgur.com/2yFez.jpg"" alt=""enter image description here""></p>
",3/20/2015 13:45,,755,1,9,0,,1160598,,1/20/2012 12:19,187,29175515,"<p><code>Q</code> is the process noise?
You cannot set the process noise as </p>

<pre><code>Q = randn*eye(3);
</code></pre>

<p>because you may have negative covariance, this doesn't make sense.</p>
",2827670,0,4,46560281,"q is the euclidean distance -> if q = Inf, then one of the parameters fx,fy,x,y is also Inf.  Which one?",Incoming
62,2219,29399433,Path planning algorithm for connected multi-robot system,|algorithm|robotics|motion-planning|,"<p>Say I have three robots connected as a triangle, how do I manage to avoid obstacles in working space?</p>

<p>I'm thinking about using A*, but I got problem that it works for one robots but when comes to 3 robots it will cause collision. </p>

<p>I'm really new to robotics, could anyone at least have some idea that which algorithm will be a better fit?</p>

<p>I don't need the actual code, just a feasible idea is fine.</p>

<p>Thanks in advance!!!</p>
",4/1/2015 19:43,,116,0,2,1,,2730755,,8/29/2013 20:26,143,,,,,,46975006,"What exactly do you mean by 'connected as a triangle'? Are they attached to inflexible rods and must move simultanously, or are they 'connected' in terms of communication?",Remote
63,2240,29615931,Building an Android app for ROS,|android|gradle|robotics|ros|,"<p>I am following <a href=""http://wiki.ros.org/ApplicationsPlatform/Clients/Android/Tutorials/Getting%20Started#Building_An_Android_App"" rel=""nofollow"">this tutorial</a> to build and android app for ROS</p>

<p>After I import this in Android studio having following problem, I am using Android studio in windows 7.</p>

<pre><code>Error:(66, 0) Gradle DSL method not found: 'android()'
Possible causes:The project 'android_apps' may be using a version of Gradle that does not contain the method.
Open Gradle wrapper fileThe build file may be missing a Gradle plugin.
Apply Gradle plugin
</code></pre>
",4/13/2015 22:09,,330,0,2,0,,474986,"Grenoble, France",10/13/2010 19:53,538,,,,,,47377663,Please include the specific code you are using from the tutorial so this question ages gracefully. It's possible that link might go offline or be moved in the future.,Error
64,2242,29952691,Error libraries Raspbian with GoPiGo,|python|opencv|raspberry-pi|raspbian|robotics|,"<p>I am working on a GoPiGo and i'm trying to make this robot to move when the camera detect a circle. </p>

<p>The mainly problem i have here is that when i try to use the gopigo library in order to use functions as fwd(),stop(), etc, if i do NOT use sudo in the command line, and just type ""python CircleDetector_MOVEMENT.py"" it detects the gopigo library, but doesn't detect the picamera.array:</p>

<pre><code>Traceback (most recent call last):
  File ""CircleDetector_MOVEMENT.py"", line 2, in &lt;module&gt;
    from picamera.array import PiRGBArray
ImportError: No module named picamera.array
</code></pre>

<p>which i import from PIRGBarray. And when i use sudo python myprogram.py, it does NOT detect the gopigo library, and the error is the next: </p>

<pre><code>Traceback (most recent call last):
   File ""CircleDetector_MOVEMENT.py"", line 8, in &lt;module&gt;
    from gopigo import *        #Has the basic functions for controlling the          GoPiGo Robot
ImportError: No module named gopigo
</code></pre>

<p>I guess it can be something related with permission, but i have no idea how to solve it.</p>

<p>So, if you please know what can be happening here, i'll be grateful. In their forum have told me that is an I2C issue, but i'm a noob in all this whole thing and i don't know how to solve it. </p>

<p>Any Help is appreciated.</p>

<p>P.S. Here you are my code, if it helps:</p>

<pre><code>#import everything i need to get working all modules.
from picamera.array import PiRGBArray
from picamera import PiCamera
import time
import cv2
import os
import numpy as np
from gopigo import *    #Has the basic functions for controlling the GoPiGo     Robot
import sys  #Used for closing the running program
os.system('sudo modprobe bcm2835-v4l2')

h=200
w=300

camera = PiCamera()
camera.resolution = (w, h)
camera.framerate = 5
rawCapture = PiRGBArray(camera, size=(w, h))
time.sleep(0.1)


for frame in camera.capture_continuous(rawCapture, format=""bgr"",     use_video_port=True):
imagen_RGB = frame.array
copia_RGB = imagen_RGB.copy()



     gris = cv2.cvtColor(imagen_RGB, cv2.COLOR_BGR2GRAY)
     gris = cv2.medianBlur(gris,9)



    img_circulos = None
    img_circulos = cv2.HoughCircles(gris, cv2.cv.CV_HOUGH_GRADIENT, 1, 20, param1=50, param2=50, minRadius=0, maxRadius=0)


    if img_circulos is not None:

        v = 1
        img_circulos = np.round(img_circulos[0, :]).astype(""int"")


        for (x, y, r) in img_circulos:

            cv2.circle(copia_RGB, (x, y), r, (0, 255, 0), 3)
            cv2.rectangle(copia_RGB, (x - 5, y - 5),(x + 5, y + 5), (0, 128, 255, -1))
    if v == 1
       fwd()

    cv2.imshow(""Imagen Combinada"", copia_RGB)

    key = cv2.waitKey(1) &amp; 0xFF
    rawCapture.truncate(0)
    if key == ord(""q""):
        break
</code></pre>
",4/29/2015 19:20,,726,1,0,3,,4837678,,4/27/2015 12:31,7,40457954,"<p>Are you using a virtual environment to run the OpenCV code? If yes then you might have to copy gopigo.py to your virtual environment and run <code>python &lt;filename&gt;</code> to have it working.</p>
",7093992,0,1,,,Error
65,2246,30382559,serial interfacing of 89s52 with Hyperterminal... getting garbage values,|serial-port|embedded|microcontroller|robotics|8051|,"<p>I need to transfer data serially from AT89s52 to Hyperterminal of PC.
For that I made a sample program to print ""Hello"" on the hyperterminal of my PC by programming the below given code inside 89s52 microcontroller and connecting it to my PC via serial port. Now when I am opening hyperterminal for the respective port, I should be seeing ""Hello"" printed on the screen multiple times, but what actually I am seeing is some garbage data getting printed.
This is the code I have used.</p>

<pre><code>#include &lt; AT89X52.H&gt;

#include &lt; STDLIB.H&gt; 

#include &lt; STDIO.H&gt; 

unsigned int i; 

void main (void) 

{ 

TMOD = 0x20; 

SCON = 0x50; 

TH1 = 0xFD; 

TL1 = 0xFD; 

TR1 = 1; 

TI = 1; 

P1 = 0; 

while (1) 

{ 

puts(""Hello\r""); 

P1 ^= 0x01; /* Toggle P1.0 each time we print */ 

for(i=0;i&lt;25000;i++); 

} 

} 
</code></pre>

<p>In the Hyper terminal I am not getting correct output i.e. Hello. Instead I am seeing some Garbage characters..
Can anybody help on this please..?</p>
",5/21/2015 19:20,,396,1,6,1,,4924902,,5/21/2015 13:22,2,30447360,"<p>Can you See P1 is toggling? I would rather send a single character first and observe what is sending by using an oscilloscope. You should see a digital signal corresponding to the ASCII value of the character being split-out from the TX pin of the micro. Also you can check the baud rate (exact value) by using the scope. If you are convinced that the correct value is sent at the right baud rate it is most likely that you got a bad connection or perhaps the baud rate should slightly be changed. </p>
",4816842,0,0,48878942,What's the crystal frequency?,Connections
66,2254,30577463,Get turn angle to position on coordinate map?,|python|math|geometry|robotics|,"<p>I'm trying to get a robot to turn to face another robot, based on their respective coordinates on a global coordinate map.</p>

<p>This is the code I wrote, but it doesn't seem to work at all:</p>

<pre><code>def calcAngleToCoords(self, curAngle, curPosition, targPosition):
    retVal = False

    if type(curPosition) is list and type(targPosition) is list:
        x_1, y_1 = curPosition
        x_2, y_2 = targPosition
        # Sets origin coordinate to zero
        x_2 = x_2 - x_1
        y_2 = y_2 - y_1

        radius = math.sqrt(y_2 ** 2 + x_2 ** 2) # Pythagorean Thereom, a^2 + b^2 = c^2 | Radius = c, y_2 = a, x_2 = b
        angle = curAngle * (math.pi / 180)

        x_1 = radius * math.cos(angle)
        y_1 = radius * math.sin(angle)

        turnArc = math.atan( (y_1 - y_2) / (x_2 - x_1) ) * (180 / math.pi)

        retVal = turnArc
        # TODO: Check to see if the angle is always clockwise.
    else:
        raise TypeError(""Invalid parameter types. Requires two lists."")

    return(retVal)
</code></pre>

<p>Can any one tell me a better way to do this or what I'm doing wrong? It's for a project I'm working on and the deadline is coming up really soon so any help would be appreciated!</p>
",6/1/2015 15:32,,600,1,1,1,,4962011,,6/1/2015 15:27,11,30579215,"<p>There is no need to calculate radius</p>

<pre><code>    x_2 = x_2 - x_1
    y_2 = y_2 - y_1
    angle = curAngle * (math.pi / 180)
    dx = math.cos(angle)
    dy = math.sin(angle)
    turnArc = math.atan2(x_2 * dy - y_2 * dx,  x_2 * dx + y_2 * dy ) * (180 / math.pi)
</code></pre>

<p>Note using of atan2 function, that returns angle between -pi and pi. It correctly determines rotation direction and finds the shortest turn. If you need to rotate in clockwise direction only, add 180 if turnArc is negative</p>
",844416,0,1,49226183,How do you know it doesn't work?,Coordinates
67,2278,31478749,Controlling two dc motors (on arduino) through c++ source code,|c++|eclipse|opencv|arduino|robotics|,"<p>I'm working on a face detection robot project.I'm using opencv software to detect faces. When the face is detected i want to get the x-y coordinates and send them to an arduino board. </p>

<p>The arduino has two dc motors connected. The first dc motor will spin a base (the base of a robot's head) according to y-coordinates (y-axis). </p>

<p>The second dc motor will handle the x-coordinates on the x-axis (i want to make the robot's eyes go up and down). </p>

<p>I work with the code on eclipse (kepler), my os is ubuntu 12.04 and i have an arduino uno.  My source code is written in c++ and opencv. The arduino is connected with my pc through USB port.  </p>

<p>My question is how can i take the x-y coordinates from my opencv source code in order to transfer them to arduino?</p>

<p>And how can i receive and handle the coordinates in arduino?</p>
",7/17/2015 14:57,31479691,600,1,0,0,,2304504,Greece,4/21/2013 14:06,99,31479691,"<p>You need some way to interface with USB serial ports from your C++ code. A quick Google search leads me to this C++ serial library for Ubuntu: <a href=""https://apps.ubuntu.com/cat/applications/precise/libserial-dev/"" rel=""nofollow"">libserial-dev</a>. </p>

<p>On the Arduino side, you certainly want to look at the <a href=""https://www.arduino.cc/en/reference/serial"" rel=""nofollow"">Arduino Serial interface</a> to receive the data you're sending.</p>

<p>To follow that up, look for a tutorial on basic usage. Start with a simple ""Hello World"" and then try to echo back and forth between your C++ code and your Arduino. Then, it's up to you to design your data transfer protocol.</p>
",1576412,1,0,,,Remote
68,2281,31547941,ROS - ROAR compile error on Ubuntu,|ubuntu|robotics|ros|,"<p>I am trying to install <code>ROAR</code> on Ubuntu 14.04 following this tutorial: <a href=""http://wiki.ros.org/roar"" rel=""nofollow"">http://wiki.ros.org/roar</a></p>

<p>However, I am stuck on <code>Compiling</code> step 3.2 <code>rosmake roar --rosdep-install</code><br/>
I get an error <code>error: sudo: rosmake: command not found</code></p>

<p>I downloaded <br/><code>svn co https://mediabox.grasp.upenn.edu/svn/penn-ros-pkgs/roar_stack/trunk ~/ros/roar_stack</code> into my home directory, not <code>/opt/ros</code><br/>
and set PATH to <code>export ROS_PACKAGE_PATH=~/ros:$/home/myfolder/ros/roar_stack</code></p>

<p>What am I doing wrong?</p>

<p><strong>update</strong> I installed a clean version of <code>Ubuntu 14.04</code> on VMware and installed <code>ROS Jade</code> and the same problem persists.</p>
",7/21/2015 19:28,,294,0,9,0,,1057045,,11/21/2011 2:22,531,,,,,,51057256,"I rebooted and now `rosmake -h` returns data, but i ran your steps then tried to run `rosmake` and now getting `command not found`",Error
69,2299,31898387,Void function with two arguments but not both need to be used,|java|android|null|arguments|robotics|,"<p>First Tech Challenge, A robotics organization now has an android platform instead of Lego. My team is transitioning to java but I have one simple question. If I have a function that has two arguments <code>power1</code> and <code>power2</code> and both can be set to a double but if the second one isn't specified, it shall run both motors with only the <code>power1</code> variable. Here is my code:</p>

<pre><code>package com.qualcomm.ftcrobotcontroller.opmodes;

public class BasicFunctions extends HardwareAccess {

    /*
    This file lists some basic funcions on the robot that we can use without typing large amounts of code.
    To use this code in the opmode add ""extends BasicFunctions"" to the end of the class line
     */
    public void setDriveMotorPower(double power1, double power2) {
        if (power2 = null) {
            motorLeft.setPower(power1);
            motorRight.setPower(power1);
        }

        motorLeft.setPower(power1);
        motorRight.setPower(power2);

    }

    public void init() {

    }

    @Override
    public void loop() {

    }
}
</code></pre>

<p><strong>Update:</strong> Here is a screenshot of Android Studio:</p>

<p><a href=""https://i.stack.imgur.com/wmqcY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wmqcY.png"" alt=""enter image description here""></a></p>
",8/8/2015 20:44,31898467,87,2,3,1,,5067982,,7/1/2015 4:35,21,31898463,"<p>If a function has 2 paramters, you have to send them both but there is another idea: If you don't wanna use <code>power2</code> for example you can pass it with some value that is impossible to happen like -1 or anything according to your situation then check <code>if(power2 == -1)</code> and that's better than checking for null.<br>
By the way, the method <code>.setPower()</code> for will be called 4 times because you just used <code>if</code> not <code>if else</code>. It should be like</p>

<pre><code>motorLeft.setPower(power1);
if (power2 == null) { // or -1 like I said before
        motorRight.setPower(power1);
} else {
    motorRight.setPower(power2);
}
</code></pre>
",4371389,0,3,51711929,And what is the question ?,Actuator
70,2349,32292996,interface with the computer in a motor vehicle using the .NET framework,|.net|interface|robotics|,"<p>Is there a way to communicate with the ECU or engine computer of a car to do things like read ODB/CAN codes similar to a handheld code scanner you would get at the local auto parts store? </p>

<p>Ideally a Microsoft .NET API or COM interface is what I am personally looking for, but I can see an API like that coming in other strange and legacy flavors.  </p>
",8/30/2015 3:53,,56,2,0,0,,589509,"Arkham, Essex County, Massachusetts.",1/25/2011 19:09,620,32293235,"<p>You can't just plug USB in to your car and expect it to work, there will be some kind of hardware between the two. That peice of hardware will come with a SDK to write software that uses it.</p>

<p>What you need to do is choose a piece of hardware to buy that comes with a SDK that works in .NET (in a fully managed class or, more likely, as a set of C interfaces you will need to P/Invoke to).</p>
",80274,0,0,,,Specifications
71,2361,32419077,General Advice to Creating a C++ Robot Template,|c++|oop|templates|robotics|,"<p>I'm working on a project that has evolved so many times, I've really started thinking about writing a generic class that controls other nested generic classes.</p>

<p>The issue is, I have so many ways I could go about it that I've become completely overwhelmed. I've reviewed vectors and template class examples, but I truly have no idea how to begin.</p>

<p>Let me explain,
I need to be able to use this template to create an object that I control directly.
The main, top level object, is the robot I'm working on documented here (Probably not helpful to read, included as reference):</p>

<pre><code>Github: MiddleMan5/Onyx-BeagleBone which pulls from: 
Github: MiddleMan5/Project-Onyx-Quadruped
(New member 2-link restriction)
</code></pre>

<p>Which has the C++ Assembly.h file I'm working on making generic:
<a href=""https://github.com/MiddleMan5/Onyx-BeagleBone/blob/master/include/Assembly.h"" rel=""nofollow"">""Assembly File""</a></p>

<p>Now I'm sorry if my syntax is a little rusty, I'm very new to C++ in a general sense. I have great difficulty with pointers, object vectors, etc.
I'm really just looking to be guided in a general direction of study, and less a ""Fix my code!"" answer.</p>

<p>Bear with me as I may mar C++ terms: 
I want to be able to create any type of robot configuration and manipulate EITHER it's individual assembly's End Effectors, or the entire chassis as a point particle physics object (if the robot is mobile). </p>

<p>This would allow me to use the same template to create a robot arm as well as my intended Quadruped. (Which is, in reality, a solid mobile mass that has a center and 4 ""Arms""</p>

<p>Each initialized Robot must contain only one Body that in-itself contains at least one Base, one Joint, and one End Effector.</p>

<p>The creation of Top-Level object should fail if any of these things should fail to be included before Body.assemble creates the now-active robot. (<code>Body.assemble</code> creates <code>Chassis Onyx</code>)</p>

<p>I can now move the entire structure by saying something like <code>Onyx.Leg(1).move(X,Y)</code> or <code>Onyx.rotate(X,45); //Move all legs to satisy final body rotation angle</code> </p>

<p>Documented in my Brainstorming file here: <a href=""https://github.com/MiddleMan5/Onyx-BeagleBone/blob/master/doc/Joints_Links_Bases.txt"" rel=""nofollow"">Please at least skim</a>.
This file contains diagrams and the ""definitions"" I'm using to prototype pseudo-code.</p>

<p>I only want one ""Robot"".h and ""Robot"".cpp to define actions, movements, sizes, shapes, etc. and one Main.cpp to provide entry into the code. (Maybe I'm asking a bit much) but logically to me, this seems do-able.</p>

<p>Includes:</p>

<pre><code>Main.cpp
        |_
        | Onyx.h
        |_Onyx.cpp //Body.move, body.rotate, body.raise
            |_Assembly.h To create the body as a combination of assemblies
</code></pre>

<p>Onyx as a quadruped looks like this:</p>

<pre><code>ONYX Complete Diagram Example:
        EE          EE
        ||    __    ||
         |   |  |   |                      
         0__0+__+0__0
             _||_
         __0+_  _+  &lt;-  0__
         0    \/          0  &lt;-\
         |                |  &lt;- - ""Leg"" and Body are two attached assemblies inside one complete containing Robot chassis          
        ||                || &lt;-/
        EE                EE
</code></pre>

<p>Should be controlled like:</p>

<pre><code>Onyx.EE1.moveTo(x,y);
</code></pre>

<p>or:</p>

<pre><code>Onyx.moveTo(x,y);
</code></pre>

<p>Where I could further define gaits and parameters that would allow the robot ro take steps to move it's entire chassis to location (X, Y)</p>

<p>and has definitions for joints, links, and bases as such:</p>

<pre><code>|Joint: Takes X and Y coordinates, rotates Link.            ----0----
|__ End Effector: A joint combined with an attached Link.   --------EE
|__ Base: A joint created that can be fixed statically through a link to another base joint
|   |__ Can be attached actively to a link and a joint 
|   |__ Can be used as a static reference to an assembly (Such as a leg) and attached to a joint on a different plane
|
|__ Primary Joint: the joint(s) that attach directly to the Body's base(s)

|Link: A length of physical mass between a joint.
|__Segment: two or more attached in-line links (multiple links combined at different angles can specify independent shapes or masses).
            Segments are useful for attaching spaced Bases. (Attach.Base(Segment1_2(End)); Segments can be attached either by their additive L1 or L2
</code></pre>

<p>I really am frustrated because the project has very quickly become way to complicated for me to even understand. This is why I was looking at creating a template. But it doesn't seem like accessing an object's object's method (Onyx.leg.move) is even possible in C++. Am I wrong?</p>

<p>If you actually read through to the end I want to give you a hand. Any advice you have on areas of study (Or advice to make my question clearer) would be GREATLY valued.</p>

<p>Thank You</p>

<p>Edit:
Okay, I went ahead and created a Class 'Assembly', which has methods to access a private subclass 'assemble' which attaches a ""Leg with links n"" to the assembly (god I need a different name) with subclass's Base, Link, and End_Effector. I guess I'm going to have to write individual methods and method handlers for each subclass of a subclass. This seems way to ugly and complicated with plenty of room to make errors.</p>

<p>The pseudo-flow as follows:</p>

<pre><code>Assembly Onyx //Creates Robot: Onyx
Onyx.assemble(4, 3, 2); //Adds 4 legs each with 3 joints which includes 2 axis
for(i; 0 ---&gt; 3)
Onyx.numleg_numL_length(i,1,114); //Defines L1 of Leg1 as 114mm
Onyx.numleg_numL_length(i,2,140); //Defines L2 of Leg1 as 114mm
</code></pre>

<p>and on and on and on and on.</p>

<p>Please tell me there's a simple trick that allows me to assign multiple variables to a named class instead of this infinite chain of methods functions and subclasses</p>
",9/6/2015 1:07,,222,0,3,0,,5304869,,9/6/2015 0:12,11,,,,,,52704546,"One quick reaction is that it’s possible to access an object’s members’ methods if they’re public, but you don’t want to do this. A basic approach is that the robot has certain parts, and the code to translate a command into low-level motions should be part of the `Robot`class methods.  The legs should be `private` data. So, the class itself should be implementing `Onyx.moveTo(x,y)` by turning it into a bunch of commands like `legs[0].move()`. If you want the controller to be able to move individual legs, too, there should be a public interface like `Onyx.moveLeg(whichLeg, how)`.",Other
72,2371,32683092,Trigger a relay with Java,|java|robotics|,"<p>I haven't yet started this project, but I am trying to figure out the best way a trigger a relay when a button is clicked on a UI in java. The relay will release a lock on a door when the button is clicked. I've looked at rasberri pi, but I am not familiar with that product. Can anyone suggest how I should go about doing this?</p>
",9/20/2015 19:00,,1263,1,5,0,,4513904,,1/31/2015 6:03,41,32683738,"<p>I've implemented this in several different ways and indeed done all sorts of device interface with Java for engineering experimentation and automation.</p>

<p>The real question here is do you have a specific relay device in mind as this is going to drive how you implement a Java interface? As examples, the two most recent Java-Relay applications I have implemented involved one of the following devices:</p>

<ul>
<li><strong>Case 1</strong>: Using Java to control low contact resistance measurement/signal SPDT(Single Pole, Double Throw) relays

<ul>
<li>Device:  <a href=""http://www.keysight.com/en/pd-1000001313:epsg:pro-pn-34970A/data-acquisition-data-logger-switch-unit?&amp;cc=US&amp;lc=eng"" rel=""nofollow"">Keysight (Agilent/HP) 34970A</a> and the <a href=""http://www.keysight.com/en/pd-1000000085%3Aepsg%3Apro-pn-34903A/20-channel-actuator-gp-switch-module-for-34970a-34972a?nid=-33237.536880682&amp;cc=US&amp;lc=eng"" rel=""nofollow"">20 Channel Switch Unit</a></li>
</ul></li>
<li><strong>Case 2</strong>: Using Java to control SPDT power relays

<ul>
<li>Device: <a href=""http://www.sainsmart.com/sainsmart-4-channel-5v-usb-relay-board-module-controller-for-automation-robotics.html"" rel=""nofollow"">SainSmart USB Relay Board</a></li>
</ul></li>
</ul>

<p>In Case 1 I used <a href=""http://rxtx.qbang.org/wiki/index.php/Main_Page"" rel=""nofollow"">RXTX</a> which is a Java serial port implementation to interface with the Agilent 34970 using a serial port.</p>

<p>In Case 2 I used the <a href=""http://www.ftdichip.com/Support/SoftwareExamples/CodeExamples/OtherPlatforms.htm"" rel=""nofollow"">JavaFTDI</a> package to interface directly with the FTDI chip onboard the relay board using BitBang mode. While I eventually go this option working, the combined lack of documentation from FTDI and SainSmart made me gouge my eyes out for days.</p>

<p>Certainly you could alternatively use a Raspberry Pi and its GPIOs to either control a separate relay purchased from <a href=""http://www.digikey.com/product-search/en/relays?k=relay"" rel=""nofollow"">Digi-Key</a> or in fact use the GPIOs themselves as a relay assuming very low voltage was used. I'm imagining two scenarios, one where the user is actually directly interfacing with the Pi or another where the Pi is on the network and through a user interface running on a separate PC the user is leveraging <a href=""https://en.wikipedia.org/wiki/Java_remote_method_invocation"" rel=""nofollow"">RMI</a> to cause the Pi to change GPIO states.</p>

<p>Really, what I would suggest is looking at what relay you want to use, and post a more specific question regarding how to interface to that relay assuming Java is your preferred language. Alternatively, you could ask what the lowest barrier to entry/learning curve/cost options there are for relays that can be controlled through Java. Without more detail, it's hard to recommend a path forward.</p>
",2665000,2,2,53211023,http://denkovi.com/usb-eight-channel-relay-board-for-automation,Remote
73,2405,33680363,Map representation for localization,|java|dictionary|localization|robotics|data-representation|,"<p>I would like to write in Java localization system for a robot. However I am stuck at the very beginning. I don't know how to represent the map. The map is not complicated and will never be bigger than few meters by few meters. It doesn't change when robot is moving. </p>

<p>The readings that I will have from sensors are angle (provided by compass) and pairs of integers (angle and distance). </p>
",11/12/2015 20:05,33689342,169,1,3,0,,5522101,,11/3/2015 21:42,2,33689342,"<p><a href=""http://probabilistic-robotics.org/"" rel=""nofollow"">Probabilistic Robotics</a> by Thrun, Burgard and Fox covers a number of different techniques for modelling maps suitable for robotics applications. These include:</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Occupancy_grid_mapping"" rel=""nofollow"">Occupancy Grids</a>. Occupancy grids are conceptually similar to an image - black pixels are obstacles, white pixels are passable. </li>
<li>Feature Based Maps. Feature based maps estimate position of each obstacle in a list.</li>
</ul>

<p>The suitability of each approach depends on how sparse / cluttered the environment is and what types of sensors are available for updating the map.</p>
",2246,1,0,55133802,What is in the map? Is it discrete or vector based? And why do you want to represent a map? Isn't what you have a list of readings instead of a map?,Moving
74,2412,34290860,Tracking position and velocity using a kalman filter,|opencv|localization|robotics|kalman-filter|,"<p>I am using a kalman filter (constant velocity model) to track postion and velocity of an object. I measure x,y of the object and track x,y,vx,vy . Which works but if a add gausian noise of +- 20 mm to the sensor readings x,y,vx,vy fluctuates even though the point is not moving just noise. For location that is good enough for my needs but velocity changes when the point is stationary and that is causing problems with my object speed calculations. Is there a way around this problem? also if switching to constant acceleration model improve on this? I am tracking a robot via a camera.</p>

<p>I am using opencv implementation and my kalman model is same as [1]</p>

<p>[1] <a href=""http://www.morethantechnical.com/2011/06/17/simple-kalman-filter-for-tracking-using-opencv-2-2-w-code/"" rel=""noreferrer"">http://www.morethantechnical.com/2011/06/17/simple-kalman-filter-for-tracking-using-opencv-2-2-w-code/</a></p>
",12/15/2015 13:45,,3764,1,0,6,0,89904,,4/12/2009 4:27,3143,34324218,"<p>The most important thing about designing a Kalman filter is not the data, it's the error estimates.  The matrices in that example seem to be chosen arbitrarily, but you should pick them using specific knowledge of your system.  In particular:</p>

<ul>
<li>Error covariance has units.  It's standard deviation squared.  So your position error is in ""length squared"" and velocity in ""length per time squared"".  The values in those matrices will be different depending on whether you work in m or mm.</li>
<li>You are implementing a ""constant velocity"" model, but the ""processNoiseCov"" from the example sets the same values for both position and velocity process noise.  This means that you might be wrong about your position due to being wrong about your velocity, <em>and you might be wrong because the object teleported around in a way that's independent of velocity</em>.  In a CV model you would expect that the position process noise would be very low (basically nonzero only for numerical reasons and to cover modelling error) and the true unknown motion of the system would be attributed to unknown velocity.  This problem also interferes with the KF's ability to infer velocity from position input, because the ""teleportation error"" of position is not attributed to velocity change.</li>
<li>If you're putting in +/-20mm of error (you should really put in Gaussian noise if you want to simulate ideal behavior) you have an approximate standard deviation of 11.5mm or a variance of (11.5mm)^2.  Not knowing what your units are (mm or m) I can't say what the numerical value of ""measurementNoiseCov"" should be, but it's not 0.1 (as in the example).</li>
</ul>

<p>And finally, even with all of that correct, keep in mind that the KF is ultimately a linear filter.  Whatever noise you put in will show up in the output, just scaled by some factor (the Kalman gain).</p>
",479989,3,0,,,Incoming
75,2445,35374892,Quadricopter + arduino+mpu6050(gyro+oscillo) +PID,|arduino|arduino-uno|robotics|gyroscope|pid-controller|,"<p>so I'm trying to creat a quadricopter with an arduino and gyroscope mp6050 and this with PID algorithme (using arduino PID,mpu library) so I make everything work separately but when it comes to use PID  I don't know how to do it which one will be the input or setput ... I'm confused on how I can use gyroscope information and brushless information and other information to make my quadri fly ... thank you </p>
",2/13/2016 1:13,35464003,1612,1,0,-2,,4366398,,12/16/2014 12:48,33,35464003,"<p>It sounds like you need a better understanding of what PID does. Here is a great article about real world implmentation of PID  <a href=""http://eas.uccs.edu/~cwang/ECE4330F12/PID-without-a-PhD.pdf"" rel=""nofollow noreferrer"">http://eas.uccs.edu/~cwang/ECE4330F12/PID-without-a-PhD.pdf</a> </p>

<p>It follows this code from AVR's website exactly (they make the ATMega32p microcontroller  chip on the UNO boards) <a href=""http://www.atmel.com/Images/doc2558.pdf"" rel=""nofollow noreferrer"">PDF explanation</a> and <a href=""http://www.atmel.com/images/AVR221.zip"" rel=""nofollow noreferrer"">Atmel Code in C</a> </p>

<p><a href=""https://i.stack.imgur.com/TJTJC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TJTJC.png"" alt=""enter image description here""></a><br>
A PID controller is a feedback control loop. </p>

<p><strong>The input</strong> (""Command"" on the diagram) is the desired position, in your case this is what you want the gyroscope to read.</p>

<p><strong>The ""Output""</strong> is a control signal. It will tell your brushless motors (the ""Plant"") what to do to achieve the desired position.</p>

<p><strong>The Feedback (input/output)</strong> We then use our ""Sensors"" to read the actual position. In your case this is the gyroscope data</p>

<p>Now the PID controller takes the <code>error = desired position - actual position</code> and uses the error to create the next command. The overall goal is to drive the <code>error</code> down to 0, in other words <code>desired position = actual position</code> The exact details of your PID coefficients are based on your specific setup and usually they must be tuned manually in order to achieve desirable results (see the links provided for more details). </p>

<p>Loooking at the <a href=""http://playground.arduino.cc/Code/PIDLibrary"" rel=""nofollow noreferrer"">Arduino PID Library Documentation</a> shows that they have easy methods to set <code>KP, KD, KI, input, output</code> THey even have an autotune parameter that can automatically find your PID constants. Hope that helps, Good luck.</p>
",2705382,1,0,,,Remote
76,2453,35722264,Kinect robotic arm detection,|image-processing|kinect|robotics|motion-detection|lbr-iiwa-rapi|,"<p>can i use Kinect sensor to detect the motion of a robotic arm (KUKA LBR iiwa 7R800) and calculate it's links angles in order to make it control another robotic arm.</p>
",3/1/2016 11:56,,378,1,3,0,,6002349,,3/1/2016 11:47,5,35724567,"<p>Of course this is possible but I don't think it is a good idea.</p>

<p>Suboptimal accuracy, lag due to processing of the 3d-data. I guess there are also cases where you cannot see all joints/links.</p>

<p>Kuka robots can output their joint angles directly. Use this data for synchronization or control both robots using the same external data.</p>

<p>Any measurement error might cause unwanted movements which in case of industrial robots can cause severe damage!</p>
",2858170,0,3,59123778,what's the motivation? what's the second robot type?,Incoming
77,2456,35733974,Single Core processor: Undesirable behaviour with Pthreads and OpenCV,|multithreading|opencv|posix|robotics|,"<p>I'm developing a surveillance robot in which I'm using a PcDuino2 board (ARM Cortex A8 Single-Core) as the unique processor in the project. Ubuntu is installed on the board as my OS of choice.</p>

<p>My software was written in this way: I'm using Posix threads, I've a threaded TCP server which receives commands from a HTML interface (Websocket), thus, this commands are validated as soon as the server receives them and some functions are called in order to move the robot (up, down, left, right) or start/stop a OpenCV thread that looks for a colored ball (just an example). </p>

<p>The fact is when the OpenCV thread isn't running, the robot's movement works as expected, but when OpenCV thread is running I can't move the robot with my interface (even receiving the commands correctly and calling the correct functions). </p>

<p>It seems like the OpenCV thread consumes all the processor resources.</p>

<p>Well, my source files are hosted in my github:</p>

<p><a href=""https://github.com/victorsantosdev/thesis-arm/tree/master/thr_tennisBall"" rel=""nofollow"">https://github.com/victorsantosdev/thesis-arm/tree/master/thr_tennisBall</a></p>

<p>The main files to take a look are server.cpp and tasks.cpp, so you'll get a better understanding of what I've written.</p>

<p>It's possible, using a sigle-core processor, to keep motors running in parallel with OpenCV thread processing?</p>
",3/1/2016 21:33,,105,0,5,0,,6004484,,3/1/2016 20:08,2,,,,,,59378675,"not sure what the ""right"" way is to implement it. maybe som event-loop like system?",Timing
78,2463,35970213,Angle to a circle tangent line,|matlab|math|geometry|robotics|,"<p>I cannot upload pics so I will try to explain my problem the best. I want to simulate the detection of a moving object by a unicycle type robot. The robot is modelled with position (x,y) and direction theta as the three states. The obstacle is represented as a circle of radius r1. I want to find the angles alpha_1 and alpha_2 from the robot's local coordinate frame to the circle, as shown here:</p>

<p><img src=""https://i.stack.imgur.com/FdSY4.jpg"" alt=""""></p>

<p>So what I am doing is trying to find the angle from the robot to the line joining the robot and the circle's centre (this angle is called aux_t in my code), then find the angle between the tangent and the same line (called phi_c). Finally I would find the angles I want by adding and substracting phi_c from aux_t. The diagram I am thinking of is shown:</p>

<p><img src=""https://i.stack.imgur.com/b0GUg.jpg"" alt=""""></p>

<p>The problem is that I am getting trouble with my code when I try to find the alpha angles: It starts calculating the angles correctly (though in negative values, not sure if this is causingmy trouble) but as both the car and the cicle get closer, phi_c becomes larger than aux_t and one of the alphas suddenly change its sign. For example I am getting this:</p>

<p><strong>aux_t</strong>//////<strong>phi_c</strong>//////<strong>alpha_1</strong>//////<strong>alpha_2</strong><br>
-0.81//////+0.52//////-1.33//////-0.29</p>

<p>-0.74//////+0.61//////-1.35//////-0.12</p>

<p>-0.69//////+0.67//////-1.37//////-0.02</p>

<p>-0.64//////+0.74//////-1.38//////+0.1</p>

<p>So basically, the alpha_2 gets wrong form here. I know I am doing something wrong but I'm not sure what, I don't know how to limit the angles from 0 to pi. Is there a better way to find the alpha angles?
Here is the section of my code:</p>
",3/13/2016 12:25,35972839,357,2,1,2,,5239521,,8/18/2015 13:32,3,35972839,"<p>As far as your math goes, the only change I would make would be to <em>subtract</em> <code>(pi/2 - theta)</code> from the target's angle rather than add it. This will give you angles in the more typical orientation (counter-clockwise being positive).</p>

<p>I'm not completely sure why you think that <code>alpha_2</code> is wrong in the data that you posted in your answer. What is happening is that your robot is getting very close to the target and the <code>alpha_2</code> tangent line actually moves to the other side or the line pointing in the direction your robot is looking. I have created a similar situation here where the labels on the tangent lines are the angle relative to the robot (yellow line) and all angles are forced to be between 0 and 2*pi.</p>

<p><a href=""https://i.stack.imgur.com/mUosO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mUosO.png"" alt=""enter image description here""></a></p>

<p>To address you question about forcing an angle to be within a particular range. To do this you will want to use the modulus (<a href=""http://www.mathworks.com/help/matlab/ref/mod.html"" rel=""nofollow noreferrer""><code>mod</code></a> in MATLAB). In these examples, I have used <code>mod(theta, 2*pi)</code> because technically if your robot is facing <em>away</em> from the target the angles can be greater than pi.</p>

<p>As a test I have performed a simple simulation that moves the robot around and shows the angles of the tangent lines relative to the robot's direction (again, between 0 and 2pi)</p>

<p><a href=""https://i.stack.imgur.com/xlYTA.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xlYTA.gif"" alt=""enter image description here""></a></p>

<p>If you really want your angles to be between 0 and pi, you could instead use <code>mod(theta, pi)</code> instead.</p>

<pre><code>alpha_1 = mod(alpha_1, pi);
alpha_2 = mod(alpha_2, pi);
</code></pre>
",670206,4,1,59596013,"this maybe because `atan2` returns a value of `aux_t` between -pi and pi, so it changes sign as soon as it crosses that half-line.",Moving
79,2466,36340961,Building robotlocomotion drake-distro,|java|installation|cygwin|robotics|,"<p>I have 64 bit win 8 and installed cygwin 64(and jdk 8).
when I use make command this pops up:
error message </p>

<p><a href=""https://i.stack.imgur.com/ysnuN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ysnuN.png"" alt=""enter image description here""></a>
the distro' link is <a href=""https://github.com/RobotLocomotion/drake"" rel=""nofollow noreferrer"">drake distro</a>
Thanks in advance</p>
",3/31/2016 18:26,,62,0,3,0,,5556374,,11/12/2015 20:33,67,,,,,,60310194,"It might help to be more specific about what this project is, and/or where it's from. Is it from here: https://github.com/lcm-proj/lcm ? If so, specific build issues with building from source might get quicker responses if you create an issue ticket on the project?",Error
80,2469,36403200,Same coordinate system between two objects C#,|c#|robotics|coordinate-systems|kinect-sdk|,"<p>I am writing a code using C#. I use a Kinect SDK V2 and a robot, for example my robot model is an IRB 1600. From the Kinect sensor I take a human point cloud when a human is detecting from the camera and from the robot I take one position (X-Y-Z position) that tells me where the robot is every time I ask it.</p>
<p>My problem is that the camera and robot have got different coordinate systems the sensor have the sender of the camera and the robot has his bases. I want to create a same coordinate system between them for distance calculation between human and robot. Are any methods to do that? tutorials?</p>
",4/4/2016 12:50,,87,1,0,1,,5436280,,10/12/2015 10:30,19,36409262,"<p>I think you just need to make a calibration...</p>

<p>Somewhere you need to say that your 3D coordinate system starts at (0,0,0) and then re-convert all the positions based on this coordinate system. Shouldn't be hard to implement.</p>

<p>Nevertheless, if you have both 3D positions of the two sensors, a simple calculation in 3D gives you the distance.</p>

<p>Distance=sqrt(X2-X1)^2+(Y2-Y1)^2+(Z2-Z1)^2)</p>
",2205242,0,0,,,Incoming
81,2470,36482498,How to register Point clouds using HandEye calibration,|robot|point-clouds|,"<p>I have built a fringe projection based 3D-Scanner with a projector and camera each. I intend to mount this system on an industrial robot and use it for automatic scanning of components. I know that HandEye calibration needs to be performed to find transformation between Camera co-ord system to Robot base co-ord system. Now, I want to use this information to register point clouds. </p>

<p>To make it more clear:</p>

<p>At position A, I capture PCD1, then I move to position B and capture PCD2. A and B are in Robot co-ord system but I can convert them into Sensor Co-ordinate system using the HandEye calibration data. So, the Sensor was at A’ and B’. Using this information, I want to roughly register PCD1 and PCD2. </p>

<p>Can any one suggest ways to achieve this or refer to any relevant publication?</p>
",4/7/2016 16:49,,248,1,0,0,,4362116,,12/15/2014 11:22,19,36483418,"<p>You should check this thesis:</p>

<p><a href=""http://mediatum.ub.tum.de/doc/800632/800632.pdf"" rel=""nofollow"">http://mediatum.ub.tum.de/doc/800632/800632.pdf</a></p>

<p>And this library:</p>

<p><a href=""http://pointclouds.org/"" rel=""nofollow"">http://pointclouds.org/</a></p>
",6076911,0,1,,,Incoming
82,2473,36625422,how robot tracking ball with fixed distance?,|c|tracking|robotics|,"<p>I am tracking ball with camera in my android phone and send x,y position,radius of ball (x,y position is a pixel in screen android phone ) to my stm32f board via bluetooth. I assemble my phone and stm32f1 kit in a mobile robot. Now i would like my robot move to ball with a fixed distance. </p>

<p>Ex: I set distance 10cm. When i move ball forward, my robot forward to ball and always keep 10cm from robot to ball</p>
",4/14/2016 13:59,36625617,122,1,0,-1,,5962385,,2/22/2016 10:25,133,36625617,"<p>Here is some pseudo code to get you started:</p>

<pre><code>while (TRUE) do
    get x, y position of ball
    get x, y position of self
    calculate distance between self and ball (hint: use Pythagoras)
    if (distance &lt; 10 cm)
        move away from ball
    else if (distance &lt; 10 cm)
        move towards ball
end
</code></pre>

<p>Now all you have to do is code this up in C.</p>
",253056,1,9,,,Moving
83,2486,36758570,Robotics Square Grid Intersection Point,|algorithm|math|geometry|trigonometry|robotics|,"<p>I'm trying to determine the point at which my robot will intersect with a wall given its location in a map and an angle its pointing at in radians. So to sum the problem up, given a square grid of any size [1-infinity], a object within that grid, and the angle at which that object is facing (radians), find the point of intersection with the border of the grid. For instance you have a 10 x 10 grid, your object is at position (5,5), and it is facing at an angle of pi/8 radians (Northeast direction). If this object were to move in a straight line, where would it intersect with the wall? Is there a generalized solution that would work for any position and any angle? So far what I'm doing is calculating a point outside the grid on the same trajectory and looking at all the points until I find a wall, but I feel like there is probably a more elegant solution. Thanks for the help!</p>
",4/21/2016 2:33,36777656,324,2,1,1,,6233200,"Hanover, MD, USA",4/21/2016 2:21,42,36760730,"<p>Pseudocode for ray with starting points inside rectangle:<br>
Starting point <code>(X0, Y0)</code><br>
Ray angle <code>Theta</code>, <code>c = Cos(Theta), s = Sin(Theta);</code><br>
Rectangle coordinates: <code>bottom left (X1,Y1), top right (X2,Y2)</code>  </p>

<pre><code>if c &gt;= 0 then //up
  XX = X2
else
  XX = X1

if s &gt;= 0 then  //right
  YY = Y2
else
  YY = Y1

if c = 0 then //vertical ray
   return Intersection = (X0, YY)

if s = 0 then  //horizontal ray
   return Intersection = (XX, Y0)

tx = (XX - X0) / c   //parameter when vertical edge is met
ty = (YY - Y0) / s   //parameter when horizontal edge is met

if tx &lt;= ty then  //vertical first
    return Intersection = (XX, Y0 + tx * s)
else            //horizontal first
    return  Intersection = (X0 + ty * c, YY)
</code></pre>
",844416,0,0,61099581,"Does you grid consist of 100 cells and you need intersections with each grid line touched? Or just single rectangle, and you need intersection with it's perimeter?",Moving
84,2506,36891435,Contour Detection in Opencv Python 2.7,|python-2.7|opencv|image-processing|automation|robotics|,"<p>I have been trying to identify the contours of red colored objects in Open Cv (python 2.7) and we have succeeded in identifying them. But, I want to detect the position of the red colored object (left or right) and I have not succeeded in doing so. If anyone could give me the code or steps to do so, I would be really thankful.</p>

<p>Our current code for identifying red colored objects is as follows:</p>

<pre><code>import numpy as np
import cv2
cap = cv2.VideoCapture(0)
while(1):
  _, frame = cap.read()
  hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
  lower_color = np.array([0, 50, 50])
  upper_color = np.array([60, 255, 255])
  mask = cv2.inRange(hsv, lower_color, upper_color)
  mask = cv2.erode(mask, None, iterations=2)
  mask = cv2.dilate(mask, None, iterations=2)
  res = cv2.bitwise_and(frame, frame, mask=mask)

  cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]
  cv2.drawContours(frame, cnts, 0, (127, 255, 0), 3)
 print cnts
 cv2.imshow('frame', frame)
 cv2.imshow('mask', mask)
 cv2.imshow('res', res)
 cv2.imshow('contours', frame)



 k = cv2.waitKey(5) &amp; 0xFF
 if k == 27:
    print ""release""

    break
 cap.release()
 cv2.destroyAllWindows()
</code></pre>
",4/27/2016 13:34,,1371,1,0,2,,6261639,,4/27/2016 13:08,7,36892486,"<p>Well, You are one step away of getting the position. You can create a <a href=""http://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=boundingrect#boundingrect"" rel=""nofollow""><code>boundingrect</code></a> around the contour(s) and then you can calculate it's center to obtain the coordinates of the object.</p>

<p>You can also try <a href=""http://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=minenclosingcircle#minenclosingcircle"" rel=""nofollow""><code>minEnclosingCircle</code></a> That will give you the center and the radius of it. This could be a little more direct to find the center :)</p>

<p><a href=""http://docs.opencv.org/2.4/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.html"" rel=""nofollow"">Here</a> you can find a small tutorial of both functions, but in c++.</p>

<p>in python would be something like this</p>

<pre><code>...  
cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]
cv2.drawContours(frame, cnts, 0, (127, 255, 0), 3)
(x,y),radius = cv2.minEnclosingCircle(cnts[0])
center = (int(x),int(y))
radius = int(radius)
cv2.circle(frame, center, radius, (255, 0, 0), 3)
...
</code></pre>

<p>in this case, center will be the position of your object. This code only takes in account the first contour in the array...</p>
",888688,1,2,,,Incoming
85,2514,37075632,"PID controller affect on a differential driving robot when the parameters (Kp, Ki, and Kd) are increased individually. [full Q written below]",|robotics|pid-controller|,"<p>Question: <strong>A PID controller has three parameters Kp, Ki and Kd which could affect the output performance. A differential driving robot is controlled by a PID controller. The heading information is sensed by a compass sensor. The moving forward speed is kept constant. The PID controller is able to control the heading information to follow a given direction. Explain the outcome on the differential driving robot performance when the three parameters are increased individually.</strong></p>

<p>This is a question that has come up in a past paper but most likely won't show up this year but it still worries me. It's the only question that has me thinking for quite some time. I'd love an answer in simple terms. Most stuff i've read on the internet don't make much sense to me as it goes heavy into the detail and off topic for my case.</p>

<p><strong>My take on this:</strong></p>

<p>I know that the <strong>proportional</strong> term, Kp, is entirely based on the error and that, let's say, double the error would mean doubling Kp (applying proportional force). This therefore implies that increasing Kp is a result of the robot heading in the wrong direction so Kp is increased to ensure the robot goes on the right direction or at least tries to reduce the error as time passes so an increase in Kp would affect the robot in such a way to adjust the heading of the robot so it stays on the right path.</p>

<p>The <strong>derivative</strong> term, Kd, is based on the rate of change of the error so an increase in Kd implies that the rate of change of error has increased over time so double the error would result in double the force. An increase by double the change in the robot's heading would take place if the robot's heading is doubled in error from the previous feedback result. Kd causes the robot to react faster as the error increases.</p>

<p>An increase in the <strong>integral</strong> term, Ki, means that the error is increased over time. The integral accounts for the sum of error over time. Even a small increase in the error would increase the integral so the robot would have to head in the right direction for an equal amount of time for the integral to balance to zero.</p>

<p><em>I would appreciate a much better answer and it would be great to be confident for a similar upcoming question in the finals.</em></p>

<p>Side note: i've posted this question on the Robotics part earlier but seeing that the questions there are hardly ever noticed, i've also posted it here. </p>
",5/6/2016 15:01,37122552,756,1,0,0,,5920532,,2/12/2016 21:09,67,37122552,"<p>I would highly recommend reading this article <a href=""http://eas.uccs.edu/~cwang/ECE4330F12/PID-without-a-PhD.pdf"" rel=""nofollow"">PID Without a PhD</a> it gives a great explanation along with some implementation details. The best part is the numerous graphs. They show you what changing the P, I, or D term does while holding the others constant.</p>

<p>And if you want real world Application  Atmel provides example code on their site (for 8 bit MCU) that perfectly mirrors the PID without a PhD article. It follows this code from AVR's website exactly (they make the ATMega32p microcontroller chip on the Arduino UNO boards) <a href=""http://www.atmel.com/Images/doc2558.pdf"" rel=""nofollow"">PDF explanation</a> and <a href=""http://www.atmel.com/images/AVR221.zip"" rel=""nofollow"">Atmel Code in C</a></p>

<p>But here is a general explanation the way I understand it.</p>

<p><strong>Proportional:</strong> This is a proportional relationship between the error and the target. Something like <code>Pk(target - actual)</code> Its simply a scaling factor on the error. It decides how quickly the system should react to an error (if it is of any help, you can think of it like amplifier slew rate). A large value will quickly try to fix errors, and a slow value will take longer. With Higher values though, we get into an overshoot condition and that's where the next terms come into play</p>

<p><strong>Integral:</strong> This is meant to account for errors in the past. In fact it is the sum of all past errors. This is often useful for things like a small dc/constant offset that a Proportional controller can't fix on its own. Imagine, you give a step input of 1, and after a while the output settles at .9 and its clear its not going anywhere. The integral portion will see this error is always ~.1 too small so it will add it back in, to hopefully bring control closer to 1. THis term usually helps to stabilize the response curve. Since it is taken over a long period of time, it should reduce noise and any fast changes (like those found in overshoot/ringing conditions). Because it's aggregate, it is a very sensitive measurement and is usually very small when compared to other terms. A lower value will make changes happen very slowly, and create a very smooth response(this can also cause ""wind-up"" see the article)</p>

<p><strong>Derivative:</strong> This is supposed to account for the ""future"". It uses the slope of the most recent samples. Remember this is the slope, it has nothing to do with the position error(<code>current-goal</code>), it is <code>previous measured position - current measured position</code>. This is most sensitive to noise and when it is too high often causes ringing. A higher value encourages change since we are ""<em>amplifying</em>"" the slope.</p>

<p>I hope that helps. Maybe someone else can offer another viewpoint, but that's typically how I think about it.</p>
",2705382,1,0,,,Remote
86,2541,37311326,Command received by controller through serial interface but robot arm doesn't move,|c#|serial-port|embedded|robotics|,"<pre><code>using System;
using System.Collections.Generic;
using System.ComponentModel;
using System.Data;
using System.Drawing;
using System.IO.Ports;
using System.Linq;
using System.Text;
using System.Threading.Tasks;
using System.Windows.Forms;


namespace SerialPort
{
    public partial class Form1 : Form
    {

    public Form1()
    {
        InitializeComponent();
        cmdClose.Enabled = false;
        foreach (String s in System.IO.Ports.SerialPort.GetPortNames()) 
        {
            txtPort.Items.Add(s);
        }
    }

    public System.IO.Ports.SerialPort sport;

    public void serialport_connect(String port, int baudrate , Parity parity, int databits, StopBits stopbits) 
    {
        DateTime dt = DateTime.Now;
        String dtn = dt.ToShortTimeString();

        sport = new System.IO.Ports.SerialPort(
        port, baudrate, parity, databits, stopbits);
        try
        {
            sport.Open();
            cmdClose.Enabled = true;
            cmdConnect.Enabled = false;
            txtReceive.AppendText(""["" + dtn + ""] "" + ""Connected\n"");
            sport.DataReceived += new SerialDataReceivedEventHandler(sport_DataReceived);
        }
        catch (Exception ex) { MessageBox.Show(ex.ToString(), ""Error""); }
    }

    private void sport_DataReceived(object sender, SerialDataReceivedEventArgs e)  
    {
        this.BeginInvoke(new Action(() =&gt;
        {
            DateTime dt = DateTime.Now;
            String dtn = dt.ToShortTimeString();       
            txtReceive.AppendText(""["" + dtn + ""] "" + ""Received: "" + sport.ReadExisting() + ""\n"");
        }));
    }

    private void cmdConnect_Click(object sender, EventArgs e)
    {
        String port = txtPort.Text;
        int baudrate = Convert.ToInt32(cmbbaudrate.Text);
        Parity parity = (Parity)Enum.Parse(typeof(Parity), cmbparity.Text);
        int databits = Convert.ToInt32(cmbdatabits.Text);
        StopBits stopbits = (StopBits)Enum.Parse(typeof(StopBits), cmbstopbits.Text);

        serialport_connect(port, baudrate, parity, databits, stopbits);

    }

    private void button1_Click(object sender, EventArgs e)
    {
        DateTime dt = DateTime.Now;
        String dtn = dt.ToShortTimeString();
        String data = txtDatatoSend.Text;
        sport.Write(data);
        txtReceive.AppendText(""["" + dtn + ""] "" + ""Sent: "" + data + ""\n"");
    }

    private void cmdClose_Click_1(object sender, EventArgs e)
    {
        DateTime dt = DateTime.Now;
        String dtn = dt.ToShortTimeString();

        if (sport.IsOpen) 
        {
            sport.Close();
            cmdClose.Enabled = false;
            cmdConnect.Enabled = true;
            txtReceive.AppendText(""["" + dtn + ""] "" + ""Disconnected\n"");
        }
    }
}
</code></pre>

<p>}</p>

<p><a href=""https://i.stack.imgur.com/TZEJg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TZEJg.png"" alt=""enter image description here""></a></p>

<p>After I send command through the serial interface as shown in picture, my robot doesn't move at all. However, if I close visual studio and open software called Roboteq, my robot will move following the command I sent previously without even loading the port in Roboteq. Any idea why that is? I think the controller received my command for sure, but somehow it doesn't execute, and maybe opening Roboteq makes it execute. Thanks in advance.</p>
",5/18/2016 22:51,,251,2,0,0,,6342881,,5/16/2016 21:45,15,37311522,"<p>Does your device require the command be terminated with a newline?</p>

<pre><code>    sport.WriteLine(data);
</code></pre>

<p><a href=""https://msdn.microsoft.com/en-us/library/system.io.ports.serialport.writeline(v=vs.110).aspx"" rel=""nofollow"">https://msdn.microsoft.com/en-us/library/system.io.ports.serialport.writeline(v=vs.110).aspx</a></p>
",3390788,0,3,,,Actuator
87,2545,37649397,Python pip install qibuild exception then I installed pip yet.,|python|python-2.7|pip|robotics|debian-based|,"<p>I'm new on this forum and I will ask you to be friendly with my first publication. Thank you!!</p>

<p>So, here is my problem. I installed pip and now I try to install qibuild with it. But I've got a problem.
This is the command that I run:</p>

<pre><code>pip install qibuild
</code></pre>

<p>And this the error that I have:</p>

<pre><code>Downloading/unpacking qibuild
Cleaning up...
Exception:
Traceback (most recent call last):
File ""/usr/lib/python2.7/dist-packages/pip/basecommand.py"", line 122, in  main
status = self.run(options, args)
File ""/usr/lib/python2.7/dist-packages/pip/commands/install.py"", line 290, in run
requirement_set.prepare_files(finder, force_root_egg_info=self.bundle,bundle=self.bundle)
File ""/usr/lib/python2.7/dist-packages/pip/req.py"", line 1178, in prepare_files
url = finder.find_requirement(req_to_install, upgrade=self.upgrade)
File ""/usr/lib/python2.7/dist-packages/pip/index.py"", line 194, in find_requirement
page = self._get_page(main_index_url, req)
File ""/usr/lib/python2.7/dist-packages/pip/index.py"", line 568, in _get_page
session=self.session,
File ""/usr/lib/python2.7/dist-packages/pip/index.py"", line 694, in get_page
req, link, ""connection error: %s"" % exc, url,
TypeError: __str__ returned non-string (type SysCallError)

Storing debug log for failure in /root/.pip/pip.log
</code></pre>

<p>I searched on the net and I didn't find anything like that. Someone have an idea of what I can do to fix this problem? </p>
",6/6/2016 4:05,,282,1,0,0,,6428577,,6/6/2016 3:46,0,39495954,"<p>Can you check the time on your system. Sometimes it so happens when your system time is inaccurate the pip installation fails. I have faced this issue once.</p>

<p>Change your system date to current date in accordance with your time zone ,
Hope this helps</p>
",6473089,0,0,,,Error
88,2554,38647114,ORB_SLAM installation on Ubuntu Xenial 16.04,|opencv|ubuntu|ros|robotics|slam|,"<p>Is it possible to install ORB_SLAM/ORB_SLAM2 on last version of Ubuntu (Xenial 16.04) without black magic? I know that the recommendation is to use Ubuntu 14.04 according to <a href=""https://github.com/raulmur/ORB_SLAM2"" rel=""nofollow"">https://github.com/raulmur/ORB_SLAM2</a>, but I currently have last version and I don't really want to change it or install 14 together with 16. I use OpenCV 2.4.8 and ROS/catkin build system and get the next error:</p>

<pre><code>/home/roman/ORB_SLAM2/src/Optimizer.cc:1244:1:   required from here
/usr/include/eigen3/Eigen/src/Core/util/StaticAssert.h:32:40: error: static assertion failed: YOU_MIXED_DIFFERENT_NUMERIC_TYPES__YOU_NEED_TO_USE_THE_CAST_METHOD_OF_MATRIXBASE_TO_CAST_NUMERIC_TYPES_EXPLICITLY
</code></pre>

<p>What's wrong with it? Thanks.</p>
",7/28/2016 21:34,,3011,2,0,2,,6117436,,3/26/2016 12:03,20,39684187,"<p>I had this same issue, this is what worked for me.</p>

<p>Install <strong><code>eigen</code></strong> form here <a href=""https://launchpad.net/ubuntu/trusty/amd64/libeigen3-dev/3.2.0-8"" rel=""nofollow"">https://launchpad.net/ubuntu/trusty/amd64/libeigen3-dev/3.2.0-8</a></p>

<p>Download the <code>.deb</code> file and install using </p>

<pre><code>sudo dpkg -i libeigen3-dev_3.2.0-8_all.deb
</code></pre>
",6876969,2,0,,,Other
89,2590,40584193,Why calculate jacobians in ekf-slam,|robotics|kalman-filter|slam|,"<p>I know it is a very basic question but I want to know why do we calculate the Jacobian matrices in EKF-SLAM, I have tried so hard to understand this, well it won't be that hard but I want to know it. I was wondering if anyone could help me on this.</p>
",11/14/2016 8:01,41658660,422,1,0,1,,6831268,,9/14/2016 13:58,100,41658660,"<p>The Kalman filter operates on linear systems.  The steps update two parts in parallel:  The state <code>x</code>, and the error covariances <code>P</code>.  In a linear system we predict the next <code>x</code> by <code>Fx</code>.  It turns out that you can compute the exact covariance of <code>Fx</code> as <code>FPF^T</code>.  In a non-linear system, we can update <code>x</code> as <code>f(x)</code>, but how do we update <code>P</code>?  There are two popular approaches:</p>

<ol>
<li>In the EKF, we choose a linear approximation of <code>f()</code> at <code>x</code>, and then use the usual method <code>FPF^T</code>.</li>
<li>In the UKF, we build an approximation of the distribution of <code>x</code> with covariance <code>P</code>.  The approximation is a set of points called <em>sigma points</em>.  Then we propagate those states through our real <code>f(sigma_point)</code> and we measure the resulting distribution's variance.</li>
</ol>

<p>You are concerned with the EKF (case 1).  What is a good linear approximation of a function?  If you zoom way in on a curve, it starts to look like a straight line, with a slope that's the derivative of the function at that point.  If that sounds strange, look at <a href=""https://en.wikipedia.org/wiki/Taylor_series"" rel=""nofollow noreferrer"" title=""the Wikipedia entry on Taylor series"">Taylor series</a>.  The multi-variate equivalent is called the Jacobian.  So we evaluate the Jacobian of <code>f()</code> at <code>x</code> to give us an <code>F</code>.  Now <code>Fx != f(x)</code>, but that's okay as long as the changes we're making to <code>x</code> are small (small enough that our approximated <code>F</code> wouldn't change much from before to after).</p>

<p>The main problem with the EKF approximation is that when we use the approximation to update the distributions after the measurement step, it tends to make the resulting covariance <code>P</code> too low.  It acts like corrections ""work"" in a linear way.  The actual update will depart slightly from the linear approximation and not be quite as good.  These small amounts of overconfidence build up as the KF iterates and have to be offset by adding some fictitious process noise to <code>Q</code>.</p>
",479989,3,0,,,Moving
90,2637,41645196,Robot Obstacle Recording/Avoiding,|java|while-loop|robotics|,"<p>I'm trying to make this robot go in random directions until it reaches an obstacle. Then it should record that obstacle (Obstacle = 1,2,3 etc) and switch direction. This should go on until the timer expires.</p>

<pre><code>public static void main(String args[]) throws Exception{

    Robot therobot = new Robot();

    int x = 10000;
    int obstacles = 0;
    Random rand = new Random();
    int r1 = rand.nextInt(255) + 1;
    int r2 = rand.nextInt(255) + 1;

    therobot.setWheelVelocities(100,100);
    long before = System.currentTimeMillis();

    while (System.currentTimeMillis() - before &lt; x){
        Thread.sleep(x);
        if( therobot.isObstacle() ==true || therobot.isTapped() == true)
        {
            r1 = rand.nextInt(255) - 255;
            r2 = rand.nextInt(255) - 255;
            obstacles = obstacles++;

            therobot.setWheelVelocities(r1, r2);
        }
    }
    System.out.println(obstacles);

    therobot.stopWheels();
    therobot.quit();
}
</code></pre>

<p>But this doesn't seem to work. It just goes until the timer expires but it won't stop or record anything.</p>

<p>What am I missing ? </p>
",1/13/2017 23:50,41645234,282,1,9,0,0,7133825,,11/8/2016 21:11,15,41645234,"<p>Having</p>

<pre><code>int x = 10000;
long before = System.currentTimeMillis();
while (System.currentTimeMillis() - before &lt; x){
    Thread.sleep(x);
    // Processing
}
</code></pre>

<p>The first iteration of while loop takes entire time duration of 10 seconds because of <code>Thread.sleep(10000)</code>.</p>

<p>Sleep amount should be significantly lesser than total duration.</p>
",2327745,0,4,70490511,"Yes, sorry, I didn't write it down here.",Moving
91,2680,42355760,Get centroid of point cloud data based on color using kinect v2 in ROS,|kinect|ros|point-cloud-library|robotics|kinect-v2|,"<p>I want to get the centroid of point cloud data based on color using kinect v2. Even after searching for a long time I was not able to find a package which can do this task. But since this is a general problem, I think there should be a existing package. </p>

<p>Please help. Thanks in advance!</p>
",2/20/2017 23:02,42371628,730,1,0,1,,5893603,United States,2/6/2016 23:46,48,42371628,"<p>If you are using PCL you can do </p>

<pre><code>pcl::PointXYZRGB centroid;
pcl::computeCentroid(*cloud, centroid);
</code></pre>

<p>Otherwise it is just the average of the points. For example:</p>

<pre><code>                pcl::PointXYZI centroid;

                float x = 0, y = 0, z = 0;
                for (int k = 0; k &lt; cloud-&gt;size(); k++)
                {
                   x += cloud-&gt;at(k).x;
                   y += cloud-&gt;at(k).y;
                   z += cloud-&gt;at(k).z;
                }
                centroid.x = x / (cloud-&gt;size() + 0.0);
                centroid.y = y / (cloud-&gt;size() + 0.0);
                centroid.z = z / (cloud-&gt;size() + 0.0);
</code></pre>
",6674213,2,0,,,Specifications
92,2687,42371935,How to make a stable feedback controller which can only generate intermittent impulses?,|signal-processing|robotics|robot|control-theory|feedback-loop|,"<p>I have a kuka iiwa (7 joint robot arm).  Attached to it is a circular aluminum platter with a steel ball on it.  The goal of the project (for giggles/ challenge) was to use the internal torque sensors of the robot in order to balance the ball in the middle of the platter.  Due to the fact that I was unable/ not allowed to use FRI (fast robot interface) whereby I can control the robot from C at about 3ms feedback loop, I can only update the robot position at about 4Hz... My quick and dirty solution consisted of the following:</p>

<p>Measure the torque on the final two axes of the arm and apply a mapping to generate the ball's position (filtration and hysteresis were well implemented to improve the data quality).  If the ball velocity was sufficiently stable, generate a motion that would cancel out that velocity (with an impulse ""go to angle and return to neutral position"" motion).  Overlaid on that was also a small proportional gain which would tend the ball towards the center of the platter.</p>

<p>My question:  What is the professional/ correct solution to this situation (where your controller can only hit the system with impulses rather than continuous feedback)?  </p>

<p>Here is a picture of the setup:
<a href=""https://i.stack.imgur.com/0j6Th.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0j6Th.jpg"" alt=""enter image description here""></a></p>
",2/21/2017 15:50,,75,1,0,1,0,7298298,"Milwaukee, WI, USA",12/14/2016 18:31,405,46008757,"<p>A slightly dampened negative feedback loop.
Where bal.posX dictates strength y.rot.arm ?</p>

<p><a href=""https://en.wikipedia.org/wiki/Control_system"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Control_system</a></p>

<p>I once coded something alike it, my colleague who's into PLC's called it that.
I did fuzzy logic optimization then..</p>
",613326,1,0,,,Connections
93,2730,42667789,How to exclude poses of a wheel based robot which place behind of the porevious pose,|robotics|kalman-filter|sensor-fusion|,"<p>I am currently working on coding sensor fusion of a wheel based robot pose from GPS, Lidar, Vision and Vehicle measure. Its model is basic kinematics using EKF and no discrimination against sensors i.e. data comes in based on time stamp.  </p>

<p>I have difficulty to fuse those sensors due to following issue;
Sometimes when the latest incoming data comes in from different sensor from a sensor gave previous state, the latest pose of the robot comes in behind previous pose. Therefore data fusion does not get so smooth and zigzag-ed as a result. </p>

<p>I would like discard data which plots behind/backwards of the previous data and take data which poses always forward/ahead of previous state even when sensor to provide the data changes between timestamp t and timestamp t+1. Since the data frame is global frame, it is impossible to rely on its x coordinate in minus to achieve this. </p>

<p>Please let me know if you had some idea on this. Thank you so much in advance.
Best,</p>
",3/8/2017 9:41,,49,1,0,0,,5558413,,11/13/2015 10:29,26,43466680,"<h2>Preliminary warning</h2>

<p>Let me slip here a warning before suggesting posible solutions to your problem: be careful with discarding data based on your current estimate, since you never know if last measure is ""pulling pose back"" or previous one was wrong and caused your estimate to move forward too much.</p>

<h2>Posible solutions</h2>

<p>In a Kalman-like filter, observations are assumed to provide independent, uncorrelated information about state vector variables. These observations are assumed to have a random error distributed as a zero mean gaussian variable. Real life is harder, though :-(</p>

<p>Sometimes, measures are affected by a ""bias"" (a fixed term, similar to the gaussian error having a non-zero mean). e.g. tropospheric perturbations are known to introduce a position error in GPS fixes that drifts slowly over time.
If you take several sensors observing the same variable, as GPS and Lidar for for position, but they have different biases, your estimation will be jumping back and forth. Scaling problems can have a similar effect.</p>

<p>I will assume this is the root of your problem. If not, please refine your question.</p>

<p>How can you mitigate this problem? I see several alternatives:</p>

<ul>
<li><strong>Introduce a bias/scale correction term in your state vector</strong> to compensate sensor bias/drift. This is a very common trick in EKFs for inertial sensor fusion (gyro/accelerometer), that can work nice when tuned properly.</li>
<li><strong>Apply some preprocessing to sensory inputs</strong> to correct known problems. It can be difficult to tune a filter for estimating state vector and sensor parameters at the same time.</li>
<li><strong>Change how observations are interpreted</strong>. For example, use difference between consecutive position observations so that you are creating a fake odometer sensor. This greatly reduces the drift problem.</li>
<li><strong>Post-process your output</strong>. Instead of discarding observations, integrate them and keep the ""jumping"" state vector internally, but smooth the output vector to eliminate the jumps. This is done in some UAV autopilots because such jumps affect the performance of PID controllers.</li>
</ul>

<p>Finally, the most obvious and simple approach: <strong>discard observations based on some statistical test</strong>. A chi-square test of the residual can be used to determine if an observation is too far from expected values and must be discarded. Be careful with this options, though: observation rejection schemes must be completed with a state vector reinitialization logic to resutl in a stable behavior.</p>

<p>Almost all these solutions require knowning the source of each observation, so you would no longer be able to treat them indistinctly.</p>
",1076211,0,0,,,Incoming
94,2750,43175819,Distance on robot,|python|robotics|,"<p>I created a <code>robot</code> that will run based on <code>python</code>. For its autonomous program I need it to run for a certain distance( say 10 feet). Currently I am using time to have it go the distance, but is there any way to implement distance in the code to make it more exact. Thank you.</p>

<p>This was code for an old robotics competition i did and i want to learn by improving it. I used these libraries:</p>

<pre><code>import sys
import wpilib
import logging
from time import time
</code></pre>

<p>This is the code:</p>

<pre><code>def autonomous_straight(self):
    '''Called when autonomous mode is enabled'''

    t0 = time()

    slow_forward = 0.25

    t_forward_time = 6.5
    t_spin_time = 11
    t_shoot_time = 11.5


    while self.isAutonomous() and self.isEnabled():
        t = time() - t0

        if t &lt; t_forward_time:

            self.motor_left.set(slow_forward)
            self.motor_right.set(-slow_forward)
            self.motor_gobbler.set(1.0)

        elif t &lt; t_spin_time:
            self.motor_left.set(2 * slow_forward)
            self.motor_right.set(2 * slow_forward)
            self.motor_shooter.set(-1.0)

        elif t &lt; t_shoot_time:
            self.motor_mooover.set(.5)

        else:
            self.full_stop()
            self.motor_gear.set(-1.0)

        wpilib.Timer.delay(0.01)

    self.full_stop()
</code></pre>
",4/3/2017 2:48,,508,1,5,0,,7748503,,3/21/2017 23:47,9,49038551,"<p>It looks like you're trying to drive a distance based on time.  While this may work over short distances at known speeds, it's generally much better to drive based on sensor feedback.  What you're going to want to take a look at are <code>encoders</code>, more specifically Rotary encoders.  Encoders are simply counters that keep track of 'ticks'.  Each 'tick' represents a percent rotation of the drive shaft.</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=d%20%3D%20cir%2Fres%20*%20num"" alt=""formula""></p>

<p>where <code>d</code> is distance traveled, <code>cir</code> is the wheel circumference <code>res</code> is the encoder 'resolution' (number of ticks per rotation), and <code>num</code> is the current tick count (read off the encoder)</p>

<pre><code># Example Implementation
import wpilib
import math

# Some dummy speed controllers
leftSpeed = wpilib.Spark(0)
rightSpeed = wpilib.Spark(1)

# Create a new encoder linked to DIO pins 0 &amp; 1
encoder = wpilib.Encoder(0, 1)

# Transform ticks into distance traveled
# Assuming 6"" wheels, 512 resolution
def travelDist (ticks):
  return ((math.pi * 6) / 512) * ticks

# Define auto, takes travel distance in inches
def drive_for(dist = 10):
  while travelDist(encoder.get()) &lt; dist:
    rightSpeed.set(1)
    leftSpeed.set(-1)  # negative because one controller must be inverted
</code></pre>

<p>This simple implementation will allow you to call <code>drive_for(dist)</code> to travel a desired distance with a fair degree of accuracy.  It does however, have quite a few problems.  Here we attempt to set a <em>linear</em> speed, notice there is no acceleration control.  This will cause error to build up over longer distances, the solution to this is PID control.  wpilib has constructs to simplify the math. Simply take the difference between your setpoint and current travel distance and plug it into your PID controller as an error.  The PID controller will spit out a new value to set your motor controllers to.  The PID controller (with some tuning) can account for acceleration, inertia, and overshoot.</p>

<p>docs on PID:
<a href=""http://robotpy.readthedocs.io/projects/wpilib/en/latest/_modules/wpilib/pidcontroller.html"" rel=""nofollow noreferrer"">http://robotpy.readthedocs.io/projects/wpilib/en/latest/_modules/wpilib/pidcontroller.html</a>?</p>
",9421876,1,0,73425414,Which parameters (wrt the robot's movement) do you have control over?,Actuator
95,2761,43414357,Why is my nested while loop not working?,|while-loop|nested-loops|robotics|robotc|,"<p>I'm currently programming in RobotC, for a Vex 2.0 Cortex. I'm using encoders to make my robot go straight.</p>

<p>This is my code:</p>

<pre><code>void goforwards(int time)
{
    int Tcount = 0;
    int speed1 = 40;
    int speed2 = 40;
    int difference = 10;


    motor[LM] = speed1;
    motor[RM] = speed2;
    while (Tcount &lt; time)
    {
        nMotorEncoder[RM] = 0;
        nMotorEncoder[LM] = 0;

        while(nMotorEncoder[RM]&lt;1000)
        {
            int REncoder = -nMotorEncoder[RM];
            int LEncoder = -nMotorEncoder[LM];

            if (LEncoder &gt; REncoder)
            {
                motor[LM] = speed1 - difference;
                motor[RM] = speed2 + difference;
                if (motor[RM]&lt;= 0)
                {
                    motor[RM] = 40;
                    motor[LM] = 40;
                }
            }
            if (LEncoder &lt; REncoder)
            {
                motor[LM] = speed1 + difference;
                motor[RM] = speed2 - difference;
                if (motor[RM]&lt;= 0)
                {
                    motor[RM] = 40;
                    motor[LM] = 40;
                }
                Tcount ++;
            }
        }
    }
}


task main()
{

    goforwards(10);
}
</code></pre>

<p>For reference, these are my Pragma settings:</p>

<pre><code>#pragma config(I2C_Usage, I2C1, i2cSensors)
#pragma config(Sensor, dgtl2,  ,               sensorDigitalIn)
#pragma config(Sensor, dgtl7,  ,               sensorDigitalOut)
#pragma config(Sensor, I2C_1,  ,               sensorQuadEncoderOnI2CPort,    , AutoAssign )
#pragma config(Sensor, I2C_2,  ,               sensorQuadEncoderOnI2CPort,    , AutoAssign )
#pragma config(Motor,  port1,           RM,            tmotorVex393_HBridge, openLoop, reversed, encoderPort, I2C_2)
#pragma config(Motor,  port10,          LM,            tmotorVex393_HBridge, openLoop, encoderPort, I2C_1)
</code></pre>

<p>When I excecute the code, the Robot's encoder values are very close, but the robot stops moving when they reach 1000. I thought the code I wrote should return the values of the encoders back to 0 after they reach 1 thousand, and thereby the code should re-iterate in the shell loop 10 times (in this case). What have I done wrong?</p>
",4/14/2017 15:13,43414410,206,2,1,0,0,6131978,,3/29/2016 23:37,5,43414410,"<p>You are updating <code>Tcount</code> at the wrong place. Do it Just at the end of the outer loop.</p>

<p>What you have written now makes <code>Tcount</code> increase everytime it moves ahead. By the time it reaches 1000 steps, <code>Tcount</code> is already 1000.</p>

<p>Your <code>times</code> is 10. So `Tcount >= time and it wont enter the outer while loop again.</p>
",2858773,1,0,73887612,RobotC is **not C**!,Moving
96,2786,44097582,rigid body transformation with only two points,|python|matrix|3d|gps|robotics|,"<p>I have a tractor with two GPS antennas which are mounted at the front of the tractor on either side.</p>

<p><a href=""https://i.stack.imgur.com/dCNT2.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dCNT2.jpg"" alt=""view of the front of the tractor""></a></p>

<p>I get the position (latitude and longitude) of the right antenna (left in the photo) and the north, east, down projection to the second antenna. I have been attempting to plant straight rows simply using the midpoint of the line between the two antennas. I'm getting a lot more roll than I expected and my rows are far from straight.</p>

<p>I want to know the position of a point on the ground. The point of interest is the center point between the two antennas, projected perpendicular to the line between the antennas. I imagine it as the point at the base of a ""T"" where the ends of the top bar of the ""T"" are each antenna and the ""T"" is allowed to rotate freely such that it points toward the ground. Incidentally, the antennas are 1 m apart and 2.4 m high when operating.  A little bit of roll makes a significant difference.</p>

<p>Here's generally what I think I want:</p>

<ol>
<li>Choose an origin point.  (It's common with farm equipment to use the
midpoint of the rear axle.) </li>
<li>Measure to determine the coordinates of the antennas.</li>
<li>Choose the coordinates of the point of interest.</li>
<li>Calculate the UTM coordinates for antenna A from the receiver output.</li>
<li>Calculate the UTM coordinates of antenna B from the receiver projection.</li>
<li>Plug all of this information into a magical equation.</li>
<li>Get the UTM coordinates of the point of interest.</li>
</ol>

<p>6 is the tricky one. I get roll information from the two antennas but I assume that there is zero pitch for now. This simplifies the problem enough that I was able to solve it using simple trig. This morning I'm thinking about how to do it using unit vectors.</p>

<p>I would much prefer to have a general way to do this. Then I would be able to add pitch information (from an IMU/INS) and also determine the position of my hitch, each row of the planter, etc.</p>

<p>From reading other questions here, I think I want to do a rigid body transformation with a constraint (no pitch). Am I close? I strongly prefer a Python solution.</p>
",5/21/2017 13:41,,150,0,1,1,,7362612,"Rensselaer, IN, United States",1/1/2017 14:21,35,,,,,,75425224,This might help: http://nghiaho.com/?page_id=671,Coordinates
97,2823,45093135,Controller for robotic arm in computer vision system,|c++|c|controller|robotics|,"<p>I'm working on a project of building a computer vision system. I have an embedded computer (Matrox 4Sight GPm) for running a C++ (OpenCV) program which I tested it with my laptop's built-in camera and it works.</p>

<p>And the idea is that when the certain conditions are met, the vision system will output a signal to trigger a robotic arm to perform a task.</p>

<p>Since I'm learning things from scratch, I wonder do I need an extra controller for controlling the arm?</p>

<p>If yes, what do I need to add in my computer vision coding part and how's the controller's code will look like (in C or C++) so that the vision can communicate with the controller to control the robotic arm?</p>

<p>If no (the embedded computer can control the robotic arm), what code do I need to add to make it happen?</p>

<p>I know this is a vague question,  but any direction for me to look into will be greatly appreciated! Thank you.</p>
",7/14/2017 1:19,45096834,218,1,2,0,,5957244,,2/21/2016 0:20,30,45096834,"<p>I went through the datasheet of <strong>Matrox 4Sight GPm</strong> and it seems a very powerful platform.
It has one <strong>RS-232</strong> and one <strong>RS-485</strong> port for serial communication, and also consists of one <strong>FPGA</strong> with <strong>Digital I/O's</strong> that can take input commands from <strong>Intel HM76 PCH</strong> processor.</p>

<p>In my opinion, an extra microcontroller should not be needed to drive a robotic arm. If robotic arm consists of simple D.C. motors, you need to figure out how to control the <strong>Digital I/O's</strong> of FPGA and interface a simple motor driver IC such as <strong>L293D</strong> or <strong>L298</strong> to that <strong>Digital I/O's</strong>.</p>

<p>If the robotic arm consists of servo motors then, in that case, you definitely need a microcontroller which has <strong>PWM</strong> on it. You need to program the <strong>RS-232</strong> of <strong>Matrox 4Sight GPm</strong> to send some custom commands to the microcontroller on the <strong>UART</strong> and you can write a simple program for microcontroller to drive the servo motors of that arm using <strong>PWM</strong> depending on the command received over the <strong>RS-232</strong> serial channel from <strong>Matrox 4Sight GPm</strong>.</p>

<p>I hope I have cleared some of your doubts.</p>
",2805824,1,1,77163420,"Most of the controllers are coded in C, that's why I also include C. And ppl who are familiar with C and controller might have some suggestions.",Remote
98,2888,45996174,Control a robot with REST API with JS and Mongodb,|mysql|rest|raspberry-pi|robotics|,"<p>I tried to write a control connection between my robot and PC or Android. 
Ok. So right now it looks like so:
There is an local apache2 server on my raspberry pi and my robot checks every time my MySQL database which is not efficient especially because MySQL is slow</p>

<p>I need a new solution to improve speed of communication beetween my robot and PC or Android app.</p>

<p>Is it efficient if I setup REST server on my raspberry pi with MongoDB in JS? MongoDB should be much faster than my current MySQL solution. And If I had REST service I can use it to communicate android app or webapp or my pc to my robot.</p>

<p>Could you give me any tip if that helps with my case? Or the current solution client-server with MySQL will give me the same effect. Or there is any other better solution to ensure control connection between raspberry pi robot and different clients.</p>
",9/1/2017 8:32,,137,0,5,0,,3904216,,8/3/2014 15:59,392,,,,,,78972636,Is the webserver necessary? What about  querying the database directly from the controller? Is there a way to do that?,Connections
99,2899,46326949,Is it possible to change the OMPL source code for the V-Rep plug-in?,|robotics|motion-planning|,"<p>I develop motion planning algorithms using ompl and I'm wondering if I can somehow change the V-Rep ompl plug-in so it runs my own ompl planning algorithms (like replace RRT-Connect, FMT,... etc with my own algorithms)?</p>

<p>How should I have do this?</p>
",9/20/2017 16:09,,149,1,0,0,,8642761,,9/20/2017 15:49,1,47566014,"<p>Here is how to add a new planner in OMPL. OMPL is installed from source.</p>

<ol>
<li>Add the source code of the planner to <code>ompl/src/ompl/geometric/planners</code> just like all the other planners.</li>
<li>Regenerate CMake related files and make. In <code>ompl/build/Release/</code>, do 
<code>cmake . ../..</code> then <code>make -j4</code></li>
<li>Include the header file of your custom planner wherever needed.</li>
</ol>
",3504538,0,0,,,Moving
100,2901,46455274,RobotC Sonar Sensor Array,|arrays|while-loop|rotation|robot|robotc|,"<p>I am using a sonar sensor to create an obstacle avoiding robot. The robot needs to be able to check 180 degrees in front of it, so I have made a ""head"" the sensor is mounted on that is attached to a motor with an axle that runs through a potentiometer. I have found the potentiometer values for the 5 sets of 45 degree intervals with the total 180 degrees required and documented. The sonar sensor must be able to scan a distance and then move 45 degrees, repeating the process until it reaches 180 degrees (to the right) only once it reaches that point of rotation, the scan distances are put into an array to be used by an avoidance task to be developed at a later time.
However, the sonar sensor stores values for certain angles before the head has actually reached that specified angle. 
<a href=""https://i.stack.imgur.com/CjFeQ.jpg"" rel=""nofollow noreferrer"">Obstacle Avoidance Robot (Head rotation system in the middle)</a></p>

<pre><code>  #pragma config(Sensor, in1,    headrot,        sensorPotentiometer)
#pragma config(Sensor, dgtl1,  fsonar,         sensorSONAR_inch)
#pragma config(Sensor, dgtl3,  bsonar,         sensorSONAR_inch)
#pragma config(Sensor, dgtl5,  steerrot,       sensorQuadEncoder)
#pragma config(Sensor, dgtl7,  wheelrot,       sensorQuadEncoder)
#pragma config(Motor,  port2,           head,          tmotorVex393_MC29, openLoop)
#pragma config(Motor,  port4,           drivem,        tmotorVex393_MC29, openLoop)
#pragma config(Motor,  port5,           steer,         tmotorVex393_MC29, openLoop)

int headrotationspeed = 25;
int frontscandistance[5]; //Array that hold each of the 5 angles at 45 degree intervals
int headangle =0;
task avoidance()
{
}
task frontscanner()
{
    //0 Degrees = 3500
    //45 Degrees = 2800
    //90 Degrees = 1900
    //135 Degrees = 1200
    //180 Degrees = 530
    while(true)
    {
        switch(headangle)
        {
        case 0:
            while(SensorValue[headrot]&lt;3500 &amp;&amp; SensorValue[headrot]&gt;3400)
            {
                motor[head]=-headrotationspeed;
            }
                motor[head]=0;
            frontscandistance[0] = SensorValue[fsonar];
            headangle+=45;
            break;

        case 45:
            while(SensorValue[headrot]&lt;2800 &amp;&amp; SensorValue[headrot]&lt;2700)
            {
                motor[head]=headrotationspeed;
            }
                motor[head]=0;
            frontscandistance[1] = SensorValue[fsonar];
            headangle+=45;
            break;

        case 90:
            while(SensorValue[headrot]&lt;1900 &amp;&amp; SensorValue[headrot]&lt;1800)
            {
                motor[head]=headrotationspeed;
            }
                motor[head]=0;
            frontscandistance[2] = SensorValue[fsonar];
            headangle+=45;
            break;

            case 135:
            while(SensorValue[headrot]&lt;1200 &amp;&amp; SensorValue[headrot]&lt;1100)
            {
                motor[head]=headrotationspeed;
            }
                motor[head]=0;
            frontscandistance[3] = SensorValue[fsonar];
            headangle+=45;
            break;
            case 180:
            while(SensorValue[headrot]&lt;550 &amp;&amp; SensorValue[headrot]&lt;440)
            {
                motor[head]=headrotationspeed;
            }
            motor[head]=0;
            frontscandistance[4] = SensorValue[fsonar];
            headangle=0;
            break;
        }
    }
}

    task main()
    {

        startTask(frontscanner);
        }
</code></pre>

<p>The sonar does not scan once the head has hit each 45 degree interval respectively even though the programming seems correct. What is causing the array to store values before the while loops finish positioning the head to the proper angle?</p>
",9/27/2017 18:54,50846432,335,1,0,0,,7099908,,11/1/2016 13:44,5,50846432,"<p>To start with, your indentation is off. That isn't good practice, as it can lead to placing blocks of code inside loops unintentionally.</p>

<p>To answer your question, after the first case is executed, all the other while loops are checking if the value is smaller than both your upper AND lower limits. The first case doesn't do this. </p>

<p>The following is your code, but with the changes mentioned above implemented.</p>

<pre><code>#pragma config(Sensor, in1,    headrot,        sensorPotentiometer)
#pragma config(Sensor, dgtl1,  fsonar,         sensorSONAR_inch)
#pragma config(Sensor, dgtl3,  bsonar,         sensorSONAR_inch)
#pragma config(Sensor, dgtl5,  steerrot,       sensorQuadEncoder)
#pragma config(Sensor, dgtl7,  wheelrot,       sensorQuadEncoder)
#pragma config(Motor,  port2,           head,          tmotorVex393_MC29, openLoop)
#pragma config(Motor,  port4,           drivem,        tmotorVex393_MC29, openLoop)
#pragma config(Motor,  port5,           steer,         tmotorVex393_MC29, openLoop)

int headrotationspeed = 25;
int frontscandistance[5]; //Array that hold each of the 5 angles at 45 degree intervals
int headangle =0;
task avoidance()
{
}

task frontscanner()
{
    //0 Degrees = 3500
    //45 Degrees = 2800
    //90 Degrees = 1900
    //135 Degrees = 1200
    //180 Degrees = 530
    while(true)
    {
        switch(headangle)
        {
        case 0:
            while(SensorValue[headrot]&gt;3500 || SensorValue[headrot]&lt;3400)
            {
                motor[head]=-headrotationspeed;
            }
            motor[head]=0;
            frontscandistance[0] = SensorValue[fsonar];
            headangle+=45;
            break;

        case 45:
            while(SensorValue[headrot]&gt;2800 || SensorValue[headrot]&lt;2700)
            {
                motor[head]=headrotationspeed;
            }
            motor[head]=0;
        frontscandistance[1] = SensorValue[fsonar];
        headangle+=45;
        break;

        case 90:
            while(SensorValue[headrot]&gt;1900 || SensorValue[headrot]&lt;1800)
            {
                motor[head]=headrotationspeed;
            }
            motor[head]=0;
            frontscandistance[2] = SensorValue[fsonar];
            headangle+=45;
            break;

        case 135:
            while(SensorValue[headrot]&gt;1200 || SensorValue[headrot]&lt;1100)
            {
                motor[head]=headrotationspeed;
            }
            motor[head]=0;
            frontscandistance[3] = SensorValue[fsonar];
            headangle+=45;
            break;

        case 180:
            while(SensorValue[headrot]&gt;550 || SensorValue[headrot]&lt;440)
            {
                motor[head]=headrotationspeed;
            }
            motor[head]=0;
            frontscandistance[4] = SensorValue[fsonar];
            headangle=0;
            break;
        }
    }
}

task main()
{
    startTask(frontscanner);
}
</code></pre>

<p>Also, I would like to point out that your code is written to turn the robot's head only when the measurement is IN the desired range. If any of the cases execute while the head is outside the desired range, the head won't move. If I understand correctly, you want the opposite. </p>

<p>The code below contains the modification necessary to not only turn the head while it is OUT of the desired range, but also automatically turns the head in the desired direction. </p>

<pre><code>#pragma config(Sensor, in1,    headrot,        sensorPotentiometer)
#pragma config(Sensor, dgtl1,  fsonar,         sensorSONAR_inch)
#pragma config(Sensor, dgtl3,  bsonar,         sensorSONAR_inch)
#pragma config(Sensor, dgtl5,  steerrot,       sensorQuadEncoder)
#pragma config(Sensor, dgtl7,  wheelrot,       sensorQuadEncoder)
#pragma config(Motor,  port2,           head,          tmotorVex393_MC29, openLoop)
#pragma config(Motor,  port4,           drivem,        tmotorVex393_MC29, openLoop)
#pragma config(Motor,  port5,           steer,         tmotorVex393_MC29, openLoop)

int headrotationspeed = 25;
int frontscandistance[5]; //Array that hold each of the 5 angles at 45 degree intervals
int headangle =0;
int reverse = 1 //Set this to -1 if the motor is turning in the wrong direction.

task avoidance()
{
}

task frontscanner()
{
    //0 Degrees = 3500
    //45 Degrees = 2800
    //90 Degrees = 1900
    //135 Degrees = 1200
    //180 Degrees = 530
    while(true)
    {
        switch(headangle)
        {
        case 0:
            while(SensorValue[headrot]&lt;3500 &amp;&amp; SensorValue[headrot]&gt;3400)
            {
                motor[head]=headrotationspeed*reverse*(3500-SensorValue[headrot]/abs(3500-SensorValue[headrot]));
            }
            motor[head]=0;
            frontscandistance[0] = SensorValue[fsonar];
            headangle+=45;
            break;

        case 45:
            while(SensorValue[headrot]&lt;2800 &amp;&amp; SensorValue[headrot]&gt;2700)
            {
                motor[head]=headrotationspeed*reverse*(2800-SensorValue[headrot]/abs(3500-SensorValue[headrot]));
            }
            motor[head]=0;
        frontscandistance[1] = SensorValue[fsonar];
        headangle+=45;
        break;

        case 90:
            while(SensorValue[headrot]&lt;1900 &amp;&amp; SensorValue[headrot]&gt;1800)
            {
                motor[head]=headrotationspeed*reverse*(1900-SensorValue[headrot]/abs(3500-SensorValue[headrot]));
            }
            motor[head]=0;
            frontscandistance[2] = SensorValue[fsonar];
            headangle+=45;
            break;

        case 135:
            while(SensorValue[headrot]&lt;1200 &amp;&amp; SensorValue[headrot]&gt;1100)
            {
                motor[head]=headrotationspeed*reverse*(1200-SensorValue[headrot]/abs(3500-SensorValue[headrot]));
            }
            motor[head]=0;
            frontscandistance[3] = SensorValue[fsonar];
            headangle+=45;
            break;

        case 180:
            while(SensorValue[headrot]&lt;550 &amp;&amp; SensorValue[headrot]&gt;440)
            {
                motor[head]=headrotationspeed*reverse*(550-SensorValue[headrot]/abs(3500-SensorValue[headrot]));
            }
            motor[head]=0;
            frontscandistance[4] = SensorValue[fsonar];
            headangle=0;
            break;
        }
    }
}

task main()
{
    startTask(frontscanner);
}
</code></pre>

<p>Also, I would highly recommend the VEX forums (found here: <a href=""https://www.vexforum.com/"" rel=""nofollow noreferrer"">https://www.vexforum.com/</a>) for future questions involving vex. It is more specialized towards your needs. My username is Tark1492. Feel free to message me directly if you ever get stuck on a specific question about RobotC.</p>
",9937692,0,0,,,Incoming
101,2938,47102736,Forward Kinematics for Baxter,|python|robotics|kinematics|,"<p>I've put together this Forward Kinematics function for Baxter arm robot based on its <a href=""http://sdk.rethinkrobotics.com/wiki/Hardware_Specifications#Joint_Names"" rel=""noreferrer"">hardware specs</a> and the following joints axis:<a href=""https://i.stack.imgur.com/JRWHX.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/JRWHX.png"" alt=""baxter zero configuration""></a>
The joint positions for the following forward kinematics are not matching the corresponding Cartesian coordinates, what am I doing wrong here?</p>

<pre><code>def FK_function_2(joints):
    def yaw(theta): #(rotation around z)
        y = np.array([[np.cos(theta), -np.sin(theta), 0],
                      [np.sin(theta), np.cos(theta), 0],
                      [0, 0, 1] ])
        return y

    R01 = yaw(joints[0]).dot(np.array([[-1,     0,   0],
                                       [0,      0,   1],
                                       [0,      1,   0]]))
    R12 = yaw(joints[1]).dot(np.array([[0,      0,   -1],
                                       [-1,     0,   0],
                                       [0,      1,   0]]))
    R23 = yaw(joints[2]).dot(np.array([[-1,     0,   0],
                                       [0,      0,   1],
                                       [0,      1,   0]]))
    R34 = yaw(joints[3]).dot(np.array([[-1,     0,   0],
                                       [0,      0,   1],
                                       [0,      1,   0]]))
    R45 = yaw(joints[4]).dot(np.array([[-1,     0,   0],
                                       [0,      0,   1],
                                       [0,      1,   0]]))
    R56 = yaw(joints[5]).dot(np.array([[-1,     0,   0],
                                       [0,      0,   1],
                                       [0,      1,   0]]))
    R67 = yaw(joints[6]).dot(np.array([[1,      0,   0],
                                       [0,      1,   0],
                                       [0,      0,   1]]))

    d = np.array([0.27035, 0, 0.36435, 0, 0.37429, 0, 0.229525])
    a = np.array([0.069, 0, 0.069, 0, 0.010, 0, 0])

    l1 = np.array([a[0]*np.cos(joints[0]), a[0]*np.sin(joints[0]), d[0]]);
    l2 = np.array([a[1]*np.cos(joints[1]), a[1]*np.sin(joints[1]), d[1]]); 
    l3 = np.array([a[2]*np.cos(joints[2]), a[2]*np.sin(joints[2]), d[2]]); 
    l4 = np.array([a[3]*np.cos(joints[3]), a[3]*np.sin(joints[3]), d[3]]); 
    l5 = np.array([a[4]*np.cos(joints[4]), a[4]*np.sin(joints[4]), d[4]]);
    l6 = np.array([a[5]*np.cos(joints[5]), a[5]*np.sin(joints[5]), d[5]]);
    l7 = np.array([a[6]*np.cos(joints[6]), a[6]*np.sin(joints[6]), d[6]]);

    unit = np.array([0, 0, 0, 1])
    H0 = np.concatenate((np.concatenate((R01, l1.reshape(3, 1)), axis=1), unit.reshape(1,4)), axis=0)
    H1 = np.concatenate((np.concatenate((R12, l2.reshape(3, 1)), axis=1), unit.reshape(1,4)), axis=0)
    H2 = np.concatenate((np.concatenate((R23, l3.reshape(3, 1)), axis=1), unit.reshape(1,4)), axis=0)
    H3 = np.concatenate((np.concatenate((R34, l4.reshape(3, 1)), axis=1), unit.reshape(1,4)), axis=0)
    H4 = np.concatenate((np.concatenate((R45, l5.reshape(3, 1)), axis=1), unit.reshape(1,4)), axis=0)
    H5 = np.concatenate((np.concatenate((R56, l6.reshape(3, 1)), axis=1), unit.reshape(1,4)), axis=0)
    H6 = np.concatenate((np.concatenate((R67, l7.reshape(3, 1)), axis=1), unit.reshape(1,4)), axis=0)


    T = H0.dot(H1).dot(H2).dot(H3).dot(H4).dot(H5).dot(H6)

    return T[0:3, 3]
</code></pre>
",11/3/2017 18:57,,1780,1,2,7,,4588128,,2/20/2015 13:13,140,47298942,"<p>Ok, so I have been looking at this and checked your code. The code is good and works with your defined kinematic chain with transformations from the base to the end of the robotic arm. </p>

<p>(H0 * H1 * H2 * H3 * H4 * H5 * H6)  is the correct kinematic chain where each represents a transformation from one joint to the next in the chain starting at the base of the arm.</p>

<p>The problem is your transformations are wrong. Your representation of H0 through H6 is not right and it is the numbers in these matrices that cause your transformations to not match the real transformations that take place. You need to from up the correct transformations from the base all the way to the end of the arm. Other than that, your approach is correct. </p>

<p>It looks like you are using normal DH parameters for your transformation matrices. Your values for a and d (and alpha which isn't shown in your code) are off and causing the transformations to be expressed incorrectly. The DH parameters are seen in <a href=""https://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters</a>.</p>

<p>I found an exact guide for Baxter's forward kinematics to help after going through the modified DH table to set up the transformations. I would look at the modified DH parameters at the end of the wiki article above since the guide uses that. </p>

<p><strong>Baxter Forward Kinematic Guide:</strong> <a href=""https://www.ohio.edu/mechanical-faculty/williams/html/pdf/BaxterKinematics.pdf"" rel=""nofollow noreferrer"">https://www.ohio.edu/mechanical-faculty/williams/html/pdf/BaxterKinematics.pdf</a></p>

<p>In this paper, the author, Robert Williams, sets up the DH parameters for the Baxter robotic arm and gets values different than what you have (I know you are using the normal DH parameters, but I would look at using the modified ones). His table is:</p>

<p><a href=""https://i.stack.imgur.com/HfMA7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HfMA7.png"" alt=""See paper link above from Robert Williams""></a></p>

<p>With lengths of: </p>

<p><a href=""https://i.stack.imgur.com/l9J26.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l9J26.png"" alt=""See paper link above from Robert Williams""></a></p>

<p>And using the modified DH matrix:</p>

<p><a href=""https://i.stack.imgur.com/8wWqm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8wWqm.png"" alt=""See paper link above from Robert Williams""></a></p>

<p>Now you can calculate matrices H0 through H6 and if you want you can also add the end effector geometry if you have that for an additional H7. Once you multiple them all together you should get the proper forward kinematic transformation (see the paper for additional resource). Both the left and right arms have the same kinematics. </p>

<p>When you multiply it all together then you get the expressions for the coordinates of x7, y7, and z7 from the base of the arm that are functions of the rotations of the joints and the geometry of the robot arm. See the paper on page 17 for the expressions for x7,y7, and z7. Also see page 14 for the individual transformations. </p>

<p>Also don't forget to express the angles in radians since your code uses regular trig functions. </p>

<p><strong>One last update:</strong>
I just remembered that it is easier for me to think of the intermediate translation and rotational steps one-by-one (instead of jumping straight to the DH matrix). The two approaches will be equivalent, but I like to think of each individual step that it takes to get from one rotation frame to the next. </p>

<p>For this you can use these building blocks.</p>

<p><strong>Pure Translation:</strong></p>

<pre><code>[1   0   0   u;
0    1   0   v;
0    0   1   w;
0    0   0    1]
</code></pre>

<p>Where u is the distance from the previous frame to the new frame measured from the previous x frame axis.</p>

<p>Where v is the distance from the previous frame to the new frame measured from the previous y frame axis.</p>

<p>Where w is the distance from the previous frame to the new frame measured from the previous z frame axis.</p>

<p><strong>Rotation about the z-axis by arbitrary theta:</strong>
<strong>This represent the robot joint rotation to an arbitrary theta.</strong></p>

<pre><code>[cos(theta)    -sin(theta)        0 0;
sin(theta)     cos(theta)         0 0;
0                   0             1 0;
0                   0             0 1]
</code></pre>

<p><strong>Combination of rotations around intermediate frames to get to final frame position: (these angles will usually be in increments of pi/2 or pi to be able to get to the final orientation)</strong>
Can use a rotation about the intermediate x axis, y axis, or z axis shown below.</p>

<p><strong>(Rotation about x axis by alpha)</strong></p>

<pre><code>R_x(alpha) =         [1             0           0              0;
                      0         cos(alpha)  -sin(alpha)        0;
                      0         sin(alpha)  cos(alpha)         0;
                      0            0            0              1];
</code></pre>

<p><strong>(Rotation about y axis by beta)</strong></p>

<pre><code>R_y(beta) =   [  cos(beta)     0      sin(beta)    0;
                     0         1          0        0;
                 -sin(beta)    0      cos(beta)    0;
                     0         0          0        1];
</code></pre>

<p><strong>(Rotation about z axis by gamma):</strong></p>

<pre><code>[cos(gamma)  -sin(gamma)     0      0;
sin(gamma)    cos(gamma)     0      0;
       0          0          1      0;
       0          0          0      1]
</code></pre>

<p>So with these building blocks you can build the sequence of steps to go from one frame to another (essentially any H matrix can be decomposed into these steps). The chain would be something like:</p>

<p>[H](transformation from previous frame to the next frame) = [Pure Translation from previous joint to new joint expressed in the previous joint's frame] * [Rotation about previous frame's z-axis by theta (for the joint) (since the joint has many positions, theta is left as symbolic)] * [All other intermediate rotations to arrive at the new joint frame orientation expressed as rotations about intermediate axis frames]</p>

<p>That is essentially what the DH parameters helps you do, but I like to think of the individual steps to get from one frame to the next instead of jumping there with the DH parameters. </p>

<p>With the corrected H0 through H6 transformations, your approach is correct. Just change the definitions of H0 through H6 in your code. </p>
",7117084,1,0,81410343,"Maybe you could comment your code a little bit. It is not that straight forward to understand what you intend with each step. I figured e.g. out that RXX is the robots joints rotation, but then you do not give us your coordinates for joints...",Actuator
102,2950,47165908,Running python script in the background - Raspbian,|python|background-process|raspberry-pi3|robotics|,"<p>I have recently constructed a small robot car using my raspberry pi 3 with raspbian stretch. I am currently at the point where I can drive the car around using an xbox360 controller plugged into the pi USB port. My issue is that I do not like having to ssh into the pi and run the script I have written in order to use the robot (robot.py).</p>

<p>What I would like to do is have a script running in the background which works something like this:</p>

<pre><code>#while true
    #if joystick is detected
        #robotoffflag=true
        #if startup button is pressed on the joystick &amp;&amp; robotoffflag
            #robotoffflag=false
            #run robot.py to control the robot
            #robot.py already has controls for shutting down the robot
            #which disables the motors and sets robotoffflag to true
</code></pre>

<p>This way, if my pi is powered up, I can at any point just press a button on my controller and start using the robot. Then when I am done, press another button on the controller to disable the robot.</p>

<p>My question is, am I going about this in the right way? Having a script running in the background which is basically an infinite loop with conditionals inside seems silly to me. Would it be better to have something more event driven?</p>
",11/7/2017 19:22,,555,1,0,0,,8088724,,5/30/2017 21:21,13,47166379,"<p>Here is something I might do, thought a bit of a hack.  Put the python code in to a loop, adding a very short sleep at the end of each loop to limit cpu usage.  Then run the python script inside a session of tmux, detached of course.  The python script will then run until the tmux session is killed, even if your ssh session exits.  tmux is just an example utility, by the way.  There are others.
Of course you could get fancy and implement the loop within a signal-and-wait mechanism, which would wait for a signal from your controller, then launch that from an operating system daemon.  Depends how deep you want to go ...</p>
",2363348,0,0,,,Remote
103,2983,48189649,ROS Python Script is not executable when catkin build is done,|python|ros|robotics|catkin|,"<p>I'm new to ROS.</p>

<p>I have developed a ROS python project. : <a href=""https://github.com/ildoonet/ros-video-recorder"" rel=""nofollow noreferrer"">https://github.com/ildoonet/ros-video-recorder</a></p>

<p>After cloning the repo into my ros workspace, It is not executed since scripts don't have a permission to run.</p>

<p>It is working fine if I add a permission for execution to the script files.</p>

<p>So.. I have to run 'chmod +x src/{repo_name}/scripts/{script_name}' on every scripts to run this script.</p>

<p>As I have experienced, there are ROS projects that is python based and also is able to be executed right after I download the git. (no need to add a permission)</p>

<p>How can I make my repo to do that? Do I have to add some commands in CMakelists or package.xml?</p>
",1/10/2018 14:30,,1291,1,0,1,,5107689,,7/12/2015 9:59,17,48257864,"<p>Turns out that I can change permissions on script files and commit them on github. Their permission will hold in other machines. </p>
",5107689,1,0,,,Error
104,2989,48227848,Using for / list comprehension for creating a tuple from any amount of other tuples,|python|tuples|list-comprehension|robotics|,"<p>With the following code I'm looking at how to create the <code>TRACKS[0]</code> and <code>ARM[0]</code> tuples (or even a whole set, e.g. <code>ARM</code>), as they are very similar - I think something like a list comprehension would work (as I'm picturing a for each loop).</p>

<pre><code># MOTORS: all, m1, m2, m3, m4, m5 (+, -)
MOTORS = (
  (
    (0b01010101, 0b00000001, 0b00000000),
    (0b10101010, 0b00000010, 0b00000000)
  ),
  (
    (2**0, 0, 0),
    (2**1, 0, 0)
  ),
  (
    (2**2, 0, 0),
    (2**3, 0, 0)
  ),
  (
    (2**4, 0, 0),
    (2**5, 0, 0)
  ),
  (
    (2**6, 0, 0),
    (2**7, 0, 0)
  ),
  (
    (0, 2**0, 0),
    (0, 2**1, 0)
  )
)

LED = (0,0,1)

# TRACKS: both, left, right (forward, reverse) 
TRACKS = (
    (
      (MOTORS[4][0][0] | MOTORS[5][0][0], MOTORS[4][0][1] | MOTORS[5][0][1], MOTORS[4][0][2] | MOTORS[5][0][2]),
      (MOTORS[4][1][0] | MOTORS[5][1][0], MOTORS[4][1][1] | MOTORS[5][1][1], MOTORS[4][1][2] | MOTORS[5][1][2])
    ),
    MOTORS[4],
    MOTORS[5]
  )

# ARM: all, elbow, wrist, grip (forward/open, reverse/close)  
ARM = (
  (
      (MOTORS[1][0][0] | MOTORS[2][0][0] | MOTORS[3][0][0], MOTORS[1][0][1] | MOTORS[2][0][1] | MOTORS[3][0][1], MOTORS[1][0][2] | MOTORS[2][0][2] | MOTORS[3][0][2]),
      (MOTORS[1][1][0] | MOTORS[2][1][0] | MOTORS[3][1][0], MOTORS[1][1][1] | MOTORS[2][1][1] | MOTORS[3][1][1], MOTORS[1][1][2] | MOTORS[2][1][2] | MOTORS[3][1][2])
    ),
    MOTORS[1],
    MOTORS[2],
    MOTORS[3]
  )

def motormsk (motor_id, motor_config):
  return (motor_config[motor_id][0][0] | motor_config[motor_id][1][0], motor_config[motor_id][0][1] | motor_config[motor_id][1][1], motor_config[motor_id][0][2] | motor_config[motor_id][1][2])
</code></pre>

<p>The <code>motormsk</code> function does a logical <code>OR</code> to create a mask of the values passed in and I thought that it could be used recursively to generate the mask, the function would need to take any number of tuples.</p>

<p>This configuration is used to interface with a USB motor control interface (as in the OWI-535 Robotic Arm Edge), that I'm adding virtual system config (<code>ARM</code> and <code>TRACKS</code>) to allow me to change them around / re-purpose them easily. </p>

<p>USAGE: sending <code>MOTORS[0][0]</code> starts all motors forward, <code>TRACKS[0][1]</code> starts the tracks in reverse, <code>TRACKS[1][0]</code> starts the left track forward and <code>motormsk(3, ARM)</code> stops the grip motor, etc.</p>

<p>There is a repl.it here: <a href=""https://repl.it/@zimchaa/robo-config"" rel=""nofollow noreferrer"">https://repl.it/@zimchaa/robo-config</a> - Thanks.</p>

<p>EDIT: With a suggestion to reword and a clarification of the question I've had a go at the problem for 2 elements:</p>

<pre><code>def motorcmb (motor_id_1, motor_dir_1, motor_id_2, motor_dir_2, motor_config):
  return (motor_config[motor_id_1][motor_dir_1][0] | motor_config[motor_id_2][motor_dir_2][0], motor_config[motor_id_1][motor_dir_1][1] | motor_config[motor_id_2][motor_dir_2][1], motor_config[motor_id_1][motor_dir_1][2] | motor_config[motor_id_2][motor_dir_2][2])
</code></pre>

<p>This produces: <code>motorcmb(1, 0, 2, 1, TRACKS)</code></p>

<p><code>=&gt; (64, 2, 0)</code></p>

<p>I'd still like to see what's possible / best practices for arbitrary numbers of elements.</p>
",1/12/2018 14:03,48229206,106,1,6,1,,9209207,UK,1/12/2018 13:25,3,48229206,"<p>I suggest using <code>itertools.chain()</code> to reduce a variable number of tuples to a single sequence, and then</p>

<pre><code>from operator import __or__
from functools import reduce 
x = reduce(__or__, myiterable)
</code></pre>

<p>to <code>or</code> them all together. I don't really understand the way your tuples are nested so I'm not going to try and go into specifics.</p>
",2084384,0,1,83439916,"@BoarGules - yes, essentially - although `motormsk()` does something a bit more specific - a general replacement could take an arbitrary number of motor tuples, the direction (i.e. +, -) and `OR` the components.",Programming
105,2995,48271579,How to do sensor fusion?,|sensors|robotics|,"<p>Let suppose that I have two measures from two difference sensors of the same variable. I'd like to know if there's a way to do an information fusion and obtain a unique measure that describes the best way possible the whole system (Both sensors). </p>

<p>I know Bar-Shalom - Campo sensor fusion model, but I'd like to know if there are any model that doesn't adopte the classical Gaussian assumption, so that the sensor fusion can deal with bad data/gross erros.</p>

<p>Thank you.</p>
",1/15/2018 22:29,48337666,248,2,2,0,,,,,,48337666,"<p>For sensor fusion, you can go for Kalman Filter. There are few tutorials and research papers available for extended kalman filter, used for sensor fusion. </p>
",9238248,0,0,83526941,"FYI, as you did this twice. Plural form of `sensor` is `sensors` not `sensores`",Incoming
106,3004,48455762,How to code EEPROM with potentiometer in Arduino,|c++|arduino|robotics|eeprom|servo|,"<p>I have A code I got from a site. I've been trying to add EEPROM in that code but I can't get it right. I tried adding EEPROM.write and EEPROm.read I also add another servo</p>

<p>Can someone help me with it, <strong>Adding EEPROM in the code?</strong> Thank you in advance :) </p>

<p>here's the code (Simple Robotic arm with play and record function):</p>

<pre><code>#include &lt;Servo.h&gt;
#include &lt;EEPROM.h&gt;

const int PinButton1 = 2;  // pin 2   
const int PinButton2 = 3;  // pin 3
int mode = 1;     // case 1 program robot arm. case 2 replay positions 
int bounce = 0;
volatile int buttonPress = 0;
unsigned long lastButtonPressTime = 0;
volatile unsigned long bounceTime = 0;
Servo Arm0Servo;      //servo objects
Servo Arm1Servo;      // Ihave renumbered servos 26 1 2017
Servo Arm2Servo;
Servo Arm3Servo;
Servo Arm4Servo;
int pos1,pos2,pos3,pos4,pos0;
int PosArm[5] ;       // array 
int ArmPos[100][5];   // array to hold arm positions up to 100
int PosCount = 0;     //  to count number of positions increased when button     pressed
int PosCountMax = 0;  //  number of positions recorded when double button push initiates replay mode
int PosReached = 0;   // used to check that arm has moved from one recorded position to next position
int addr = 0;
boolean recorded = false;

void setup() {  
     for(int i = 0; i &lt;100 ; i++ ){
     for(int p = 0; p &lt;5 ; p++ ){
         ArmPos[i][p] = -1;
     }
 }
pinMode(PinButton1 , OUTPUT);
digitalWrite(PinButton1 ,LOW);
pinMode(PinButton2, INPUT_PULLUP);
//  I have made a small change here due to problem using some Arduinos
//attachInterrupt( digitalPinToInterrupt(PinButton2),ButtonPress , LOW );
// digitalPinToInterrupt(PinButton2) may not be defined!
attachInterrupt( 1,ButtonPress , LOW );   // interupt to capture button presses

// attach servos to relervent pins on arduino nano
Arm0Servo.attach(12); // grip    90 to 180 open    limits of servo movement
Arm1Servo.attach(11); // elbow      to 130 up
Arm2Servo.attach(10); // shoulder   10 to 50 down
Arm3Servo.attach(9);  // turn    0  to 180 right 
Arm4Servo.attach(8);  // turn    0  to 180 right
}

void loop() {
  switch(mode){
    case 1 :  //  program robot arm. 1 press to remember position. 2 presses to progress next case 2 replay mode
              // analogRead(pin) that reads poteniometers on training arm
    PosArm[0] = map(analogRead(0),480,1024,180,10); // map (480,1024 value from potentiometer to 180,90 value sent to servo)
    EEPROM.write(addr, PosArm[0]);
    addr++;
    PosArm[1] = map(analogRead(1),480,1024,180,10);
    EEPROM.write(addr, PosArm[1]);
    addr++;
    PosArm[2] = map(analogRead(2),480,1024,180,10);
    EEPROM.write(addr, PosArm[2]);
    addr++;
    PosArm[3] = map(analogRead(3),480,1024,180,10);
    EEPROM.write(addr, PosArm[3]);
    addr++;
    PosArm[4] = map(analogRead(4),480,1024,180,10);
    EEPROM.write(addr, PosArm[4]);
             addr++;
    MoveArm();                          // call method  
    if(buttonPress == 1){               // flag set by interupt when button is pressed          
         buttonPress = 0;               // reset flag 
         if( millis() &gt; (lastButtonPressTime + 1000)){    // only one button press in one secound
             // record  position of arm PosArm to array[100][] of armpositions        
             ArmPos[PosCount][0] = PosArm[0];

             ArmPos[PosCount][1] = PosArm[1];

             ArmPos[PosCount][2] = PosArm[2];

             ArmPos[PosCount][3] = PosArm[3];

             ArmPos[PosCount][4] = PosArm[4];


             if (addr == 512) {
               EEPROM.write(addr, 255);
               break;
             }

             if( PosCount &lt; 100) {      // stop recording if over 100 positions recorded (memory limitations)
                 PosCount++;         
             }
         }else{                         //  more than one button press
             mode = 2;                  // go to next phase
             PosCountMax  = PosCount;   // set number of arm positions recorded
             PosCount = 0;              // reset count ready to read  arm position array from begining 
         }
    lastButtonPressTime = millis();              
    }
  break;
case 2 :        // read arm position array
    PosReached = 0;
    for(int i = 0; i &lt;5 ; i++ ){   //adjust servo positions   
        // we move the servos in small steps and the delay(20)  makes arm motion smooth and slow
        if( PosArm[i] &gt; ArmPos[PosCount][i]) PosArm[i] = PosArm[i] - 1;  // if actual position is greater than requird position reduce position by 1
        if( PosArm[i] &lt; ArmPos[PosCount][i]) PosArm[i] = PosArm[i] + 1;  // if actual position is less than required position value increase position valuue by 1
        if( PosArm[i] == ArmPos[PosCount][i]) PosReached++;              // check if servo  has reached required position/angle
    }

    if(PosReached == 5) PosCount++;        // if all 4 servos have reached position  then increase array index (PosCount)to next position in array
    if(PosCount == PosCountMax) PosCount = 0;   //  if end of array reached reset index to 0 and repeat.
    Play();   // physically move arm to updated position, this is broken into small steps
    delay(5);   // pause between moves so over all motion is slowed down
  break;
default :
  break;
  }    
}

void MoveArm() {   // write arm position data to servos
  for (int i = 0 ; i &lt; 5 ; i++) {
  if (PosArm[i] &lt; 5) PosArm[i] = 5;  // limit servo movement to prevent hitting end stops
  if (PosArm[i] &gt; 175) PosArm[i] = 175;  // servo.write limited 5 - 175  
  }
  Arm0Servo.write(PosArm[0]);
  Arm1Servo.write(PosArm[1]);
  Arm2Servo.write(PosArm[2]);
  Arm3Servo.write(PosArm[3]);
  Arm4Servo.write(PosArm[4]);
  Play();
  delay(5);
}
void Play() {   // write arm position data to servos
  for (int i = 0 ; i &lt; 5 ; i++) {
  if (PosArm[i] &lt; 5) PosArm[i] = 5;  // limit servo movement to prevent hitting end stops
  if (PosArm[i] &gt; 175) PosArm[i] = 175;  // servo.write limited 5 - 175  
  }

  if (addr == 0) {
  PosArm[0] = EEPROM.read(addr);
  pos0 = EEPROM.read(addr+5);
  addr++;

  PosArm[1] = EEPROM.read(addr);
  pos1 = EEPROM.read(addr+5);
  addr++;

  PosArm[2] = EEPROM.read(addr);
  pos2 = EEPROM.read(addr+5);
  addr++;

  PosArm[3] = EEPROM.read(addr);
  pos3 = EEPROM.read(addr+5);
  addr++;

  PosArm[4] = EEPROM.read(addr);
  pos4 = EEPROM.read(addr+5);
  addr++;

  }

  }
   void ButtonPress(){   //  interupt to capture button press
   if(micros() &gt; (bounceTime + 3000)){ // debounce timer 
    bounceTime = micros();          // ingnore interupts due to button bounce
    buttonPress = 1;     // flag for button pressed
}
</code></pre>

<p>}</p>
",1/26/2018 4:21,,702,1,6,-1,,7245083,,12/3/2016 12:46,23,48460095,"<p>The code that you posted will never attempt to read data from the EEPROM due to the following problem:</p>

<p>When your microcontroller is started, the variable <code>mode</code> is set to 1. If you look carefully at the <code>loop</code> function you can see that your code will enter the <code>case 1</code> branch at least once where it basically writes data to the EEPROM and - more importantly - will set <code>addr</code> to a value other than 0, due to the <code>addr++</code> instructions (unless you run the code so long that you encounter an integer overflow).</p>

<p>The only place where you attempt to read from the EEPROM is in the <code>Play</code> function. Due to the <code>if (addr == 0)</code> condition in this function you will never execute the <code>EEPROM.read</code> instructions as <code>addr</code> will not be 0 as explained above.</p>
",,1,1,83906103,done editing it.,Actuator
107,3021,48999577,How to convert an image to RGB data values in c++,|c++|colors|robotics|,"<p>I am creating a robot along with my team (of which is called The Rusty Huskies) for a <em>FIRST</em> competition. We need to know how to convert an image to different values of rgb so that our robot can detect which switch we are looking at (the switch is where we would be placing our blocks to gain points).</p>

<p>So it would turn out to be something like this:</p>

<pre><code>#include &lt;color_reader.h&gt;
#include &lt;string&gt;

class Robot: public frc::IterativeRobot {

std::string color = """";

colorDetector colorDet;

colorDet.readImg(""image.png"");

r = colorDet.r;
g = colorDet.g;
b = colorDet.b;

if (r &gt;= 150) {

color = ""red"";

} else {

color = ""blue"";

}

};

START_ROBOT_CLASS(Robot)
</code></pre>

<p>Thanks for any help in advance!</p>
",2/27/2018 1:04,,3074,1,1,-1,0,9416091,,2/27/2018 0:39,1,48999674,"<p><a href=""https://stackoverflow.com/questions/1536159/getting-rgb-values-for-each-pixel-from-a-raw-image-in-c"">Getting RGB values for each pixel from a raw image in C</a></p>

<p>In this post the same question is discussed, depends with camera manufacturer.</p>

<p>You have to search for a library to work with your camera, writing a code might be too difficult.</p>
",2244977,1,0,85004523,Install OpenCV in your project. It has many real-time image processing functionalities for such robotic projects,Incoming
108,3035,49205761,How to run two turtle bots simultaneously in ROS Kinetic?,|ros|robotics|,"<p>I am trying to run two turtle bots from two different terminals on ROS. But as soon as I run the second command it closes the first turtlebot and runs the other with same name. I think I am looking for a way to change the turtlebot node names while running it. </p>
",3/10/2018 5:40,,412,1,1,0,,4928387,"Surat, Gujarat, India",5/22/2015 9:29,25,49215805,"<p>you can change  name of node in a launch file
for that create a launch file 
and in launch file</p>

<pre><code>&lt;launch&gt;
&lt;node pkg=""yourpackage"" type=""yourdefaultnodename""name=""yourcustomnodename""/&gt;
&lt;/launch&gt;
</code></pre>

<p>you can use remap in your launch file too</p>
",8772302,0,0,85419734,"Please share some more information what exactly you tried. Add some launch commands, your hardware setup, ... With the information give by you it's hard to help. Maybe this page helps: http://wiki.ros.org/Nodes",Remote
109,3036,49209624,generating occupancy grid maps from open source maps,|mapping|ros|robotics|,"<p>I want to know is it possible to write a program that  generates a 2d occupancy grid map from an open source map such as ""Openstreetmap"" in order to use it with robot localization ..</p>

<p>Will the information that can be extracted from such maps will be enough to know if this is a building so it's an occupied cell but this is a street so it's a free cell?</p>

<p>The program should take a map in XML file for example and automatically output the OGM.</p>
",3/10/2018 13:29,,451,0,3,0,,9351257,,2/12/2018 17:59,1,,,,,,85442451,"No, it will most likely not be enough for robot localization. There are a lot of things changing all the time in the real world, and they will not be visible in the map. The robot's localization is likely to fail if you use outdated data from the map.",Moving
110,3045,49348325,Local minima in a Moore neighborhood,|algorithm|artificial-intelligence|robotics|motion-planning|,"<p>While trying to use a wavefront algorithm, I came across a scenario that I haven’t been able to easily solve.  I populate an array with a wavefront algorithm and using a Moore neighborhood, how do I decide which way to travel given that several of the neighbors are the exact same.</p>

<p><a href=""https://i.stack.imgur.com/ownV1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ownV1.png"" alt=""Moore Neighborhood""></a></p>

<p>My local neighborhood has the values of 4,4,5,5,5,4,5,6.  The local minima is 4, but I have 3 of them in this case.  How do I make a decision based if there are several local minima to move to.  Right now I am making a random decision if there are equal values, but I feel there’s a better way to decide.</p>

<p><a href=""https://i.stack.imgur.com/0ZbQj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0ZbQj.png"" alt=""Paths to destination""></a></p>

<p>All paths to the destination have the same number of steps (5 steps), but each path taken is different.  I am also assuming that the local minima is chosen for the pathway.</p>

<p>My question is, given an equal number of steps (in wavefront) is there a way other than randomization or order of the comparators are written in to make a decision on which way to move from one block to another?</p>

<p>Just a notation, 99 indicates a barrier.</p>

<p>The problem with just pure local minima or comparator order is a failure case pictured below.</p>

<p><a href=""https://i.stack.imgur.com/qzgx5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qzgx5.png"" alt=""Failure""></a></p>

<p>I’m not looking for an Astar solution to this question, but I feel in this case, I would be faced with the exact same decision problems as all paths are equal.</p>

<p>Thanks</p>

<p>EDIT:
I had incorrectly generated the numbers, embarrassing.</p>

<p>Here's a correct generation of the wavefront</p>

<p><a href=""https://i.stack.imgur.com/iZCuv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iZCuv.png"" alt=""corrected Wavefront""></a></p>

<p>When correctly generated, it shows there is only 1 path</p>

<p><a href=""https://i.stack.imgur.com/GpD6l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GpD6l.png"" alt=""Only 1 path showing in Moore""></a></p>

<p>Even when the barrier is a little more complex</p>

<p><a href=""https://i.stack.imgur.com/w2DAf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w2DAf.png"" alt=""Still Shortest pathway""></a></p>

<p>Note: in all the images (corrected) the pathway is generated using Van Neumann neighborhood, while the movement is via Moore neighborhood.</p>

<p>However, if using Van Neumann neighborhood generation and Van Neumann navigation, the problem pops up again.</p>

<p><a href=""https://i.stack.imgur.com/IjHt5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IjHt5.png"" alt=""enter image description here""></a> </p>

<p><a href=""https://i.stack.imgur.com/HY4Ea.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HY4Ea.png"" alt=""enter image description here""></a></p>
",3/18/2018 13:20,,161,0,2,0,,1413678,"Syracuse, NY",5/23/2012 21:48,36,,,,,,86349070,"You are absolutely correct on my pics, I calculated the wavefront by hand and placed on a spreadsheet to view the array. My calculation is off. I have since actually created a function to calculate automatically for me. After the numbers are correctly generated, the problem goes away.  The choice however resurfaces again using Van Neumann generation and navigation.",Moving
111,3056,49581278,Origin of a ROS map representation,|datagrid|maps|ros|robotics|,"<p>I am working with ROS and its map_server node.</p>

<p>I dont understand what the origin metadata info of a map means (conceptually). According to the official documetation:</p>

<blockquote>
  <p>origin : The 2-D pose of the lower-left pixel in the map, as (x, y,
  yaw), with yaw as counterclockwise rotation (yaw=0 means no rotation).
  Many parts of the system currently ignore yaw.</p>
</blockquote>

<p>It is not the initial pose of the robot? But It establish some area of interest of the occupance grid?</p>

<p>Why this value is so important for the Navigation Stack?</p>

<p>Can you give me a simple example of the same map with different origins?</p>
",3/30/2018 20:51,,4146,2,0,2,,8384050,,7/28/2017 21:04,9,51194449,"<p>Origin is generally the position of the robot at the beginning of the program. So yes the initial pose of the robot. When I've used it, it could be used as the original position of the robot. Generally, when using origin you create a deep copy of the current position.</p>

<pre><code>def initPose(self):
    origin = copy.deepcopy(self._current)

    q = [origin.orientation.x,
        origin.orientation.y,
        origin.orientation.z,
        origin.orientation.w]  # quaternion nonsense

    (roll, pitch, yaw) = euler_from_quaternion(q)
    return (self._current.position.x, self._current.position.y, yaw)

    # self._odom_list.waitForTransform('YOUR_STRING_HERE', 'YOUR_STRING_HERE', rospy.Time(0), rospy.Duration(1.0))
</code></pre>

<p>But I've also used origin to be the origin of a function. </p>

<pre><code>def navToPose(self, goal):
# self._odom_list.waitForTransform('map', 'base_footprint', rospy.Time(0), rospy.Duration(1.0))
# transGoal = self._odom_list.transformPose('base_footprint', goal) # transform the nav goal from the global coordinate system to the robot's coordinate system
    origin = copy.deepcopy(self._current)

    q = [origin.orientation.x,
         origin.orientation.y,
         origin.orientation.z,
         origin.orientation.w]  # quaternion nonsense

    (roll, pitch, yaw) = euler_from_quaternion(q)
     qc = [self._current.orientation.x,
           self._current.orientation.y,
           self._current.orientation.z,
           self._current.orientation.w]
    (rollc, pitchc, yawc) = euler_from_quaternion(qc)
    x = goal.pose.position.x
    y = goal.pose.position.y
    cx = origin.position.x
    cy = self._current.position.y

    print('current', cx, cy)
    print(x, y)
    theta = math.atan2(y-cy, x-cx)
    print ('angle is ', theta)
    self.rotate(theta)
    distance = (((x - cx) ** 2) + ((y - cy) ** 2)) ** .5
    print ('distance is ', distance)
    self.driveStraight(0.5, distance)
</code></pre>

<p>So generally, I've used it more as another variable. </p>

<p>Depending on how the Occupancy grid is done. Sometime the origin will refer to where it started on the grid. Allowing the program to know if it still on the map. This can create issues shown here: <a href=""https://answers.ros.org/question/285602/static-map-corner-at-origin-for-navigation-stack/"" rel=""nofollow noreferrer"">https://answers.ros.org/question/285602/static-map-corner-at-origin-for-navigation-stack/</a> (at least from what I've experienced)</p>

<p>For more information on the nav stack go: <a href=""http://wiki.ros.org/navigation"" rel=""nofollow noreferrer"">http://wiki.ros.org/navigation</a>
and here: <a href=""https://www.dis.uniroma1.it/~nardi/Didattica/CAI/matdid/robot-programming-ROS-introduction-to-navigation.pdf"" rel=""nofollow noreferrer"">https://www.dis.uniroma1.it/~nardi/Didattica/CAI/matdid/robot-programming-ROS-introduction-to-navigation.pdf</a></p>
",10026869,0,0,,,Moving
112,3059,49614877,How to move a delta ASDA-B2 motor to position x and y?,|java|robotics|servo|,"<p>I want to write a method which gets two number, <code>x</code> and <code>y</code> and move the servo to the correct position. <code>x</code> and <code>y</code> are the position of an object in an image which I extracted before. I have no idea how to move the servo in java. I skimmed the <a href=""http://www.acontrol.com.pl/uploads/pdf/instrukcje/ASDA-B2-user-guide.pdf"" rel=""nofollow noreferrer"">manual</a> and it seems there is nothing related to my issue. The servo should point to the position of the object. Mapping the position <code>x</code> and <code>y</code> to the suitable number for servo movement can be done after finding the way to move the servo.In addition, the servo is placed in the position N and M/2 of an NxM image and should point to the position of the object.
<strong>I use a PC or Laptop to communicate with servo (maybe USB or COM port)</strong></p>

<p>I use Java 9 and maven.</p>

<p>here is the simplified version of my problem:</p>



<pre><code>public void servoInitialization(){
     //initializeation
}
public void servoMove (int x, int y){

    //computation to map the `x` and `y` to correct number for moving the  servo
    //move the servo to the position
}
</code></pre>

<p><a href=""https://i.stack.imgur.com/zIbvH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zIbvH.png"" alt=""enter image description here""></a></p>
",4/2/2018 16:23,,156,0,6,1,0,7030791,USA,10/17/2016 11:37,503,,,,,,94966876,"Not sure this will help you though. Please check the protocol being used for your ASDA-B2 is this one. If it is, you can start playing around with it by sending bits to RS232 port(by using the protocol specification). https://www.manualslib.com/manual/1286287/Delta-Asd-B2-0121-B.html?page=253#manual For Cartesian to polar coordinate conversion, [![enter image description here](https://i.stack.imgur.com/u7dYz.jpg)](https://i.stack.imgur.com/u7dYz.jpg)",Remote
113,3071,49668825,Align Point Clouds with PCL on only one rotation axis,|computer-vision|point-cloud-library|robotics|point-clouds|,"<p>I'm trying to align two point clouds acquired from a robot with PCL. I compute correspondences and then align them with <code>estimateRigidTransformation</code>:</p>

<pre><code>pcl::registration::TransformationEstimationSVD&lt;PointT, PointT&gt; transformation;
transformation.estimateRigidTransformation(*source_keypoints, *target_keypoints, *correspondences, correspondence_transformation);
</code></pre>

<p>Then I do an ICP refinement:</p>

<pre><code>Eigen::Matrix4f transform; 
pcl::IterativeClosestPoint&lt;PointT, PointT&gt; icp; 
icp.setMaximumIterations(max_iterations);

TransformationEstimationLM::Ptr transformEst(new TransformationEstimationLM); 
icp.setTransformationEstimation(transformEst); 
icp.setInputSource(source); 
icp.setInputTarget(target); 
icp.align(*result);
</code></pre>

<p>This works well if the scene is not too cluttered but also often fails and then gives a completely wrong transformation (randomly rotated). Since the point clouds are already aligned with the ground plane and the robot only moves on the ground plane the clouds actually only differ in translation and the y (up) rotation. Is is possible to set this in PCL?</p>
",4/5/2018 9:28,,1734,1,0,1,0,4848533,,4/29/2015 23:17,2,50121742,"<p><a href=""http://docs.pointclouds.org/trunk/classpcl_1_1registration_1_1_warp_point_rigid3_d.html"" rel=""nofollow noreferrer"">WarpPointRigid3D</a> will allow you to do a 1D rotation and a 2D translation.  Here's a <a href=""http://www.pcl-users.org/Restricting-Transform-Dimensions-in-Cloud-Registration-ICP-td4020375.html"" rel=""nofollow noreferrer"">relevant mailing list</a> post.</p>
",730138,0,0,,,Coordinates
114,3145,50849484,Translate degrees into 3-motor movement,|python|math|robotics|,"<p>I have a robot with three motors (red), each with an omni-directional (blue) (<a href=""https://www.vexrobotics.com/media/catalog/product/cache/1/image/9df78eab33525d08d6e5fb8d27136e95/2/1/217-2584.jpg"" rel=""nofollow noreferrer"">see example of omniwheel here</a>). If I want the robot to move forward, I can activate both motor 1 and 3, and the wheel at 2 will spin freely, perpendicular to the direction its motor can spin.</p>

<p>How could I write a function that takes degrees as an input and outputs 3 values ranging from 0-1 (no motor speed, full motor speed), so the robot faces the same direction while moving towards the true bearing specified in degrees?</p>

<p>E.g. 45 degrees is inputted and the robot moves North-East, relative to North in the diagram, while maintaining a constant rotation.</p>

<p>Thanks. </p>

<p><a href=""https://i.stack.imgur.com/QD865.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QD865.png"" alt=""Diagram - blue=wheels, red=motors""></a></p>
",6/14/2018 4:10,50849964,472,3,1,2,0,7675033,Australia,3/7/2017 22:12,112,50849922,"<p>Some thoughts: </p>

<p>Motor rotation velocities V1..V3 should be in ranges -1..1, where -1 is full clockwise rotation, 1 is full CCW rotatation.</p>

<p>Motors create moment of rotation. To exclude rotation of robot, sum of moments should be zero. For equal legs</p>

<pre><code>  V1 + V2 + V3 = 0
</code></pre>

<p>When moments are compensated, every motor causes force along OX or OY axis corresponding to projection of it's velocity onto axis. To provide moving in direction Fi with speed S:</p>

<pre><code>- V1 * Sqrt(3)/2 + V2  - V3 * Sqrt(3)/2 = S * Cos (Fi)   //OX axis
 -V1 / 2 + V3 / 2  = S * Sin (Fi)                         //OY axis   
</code></pre>

<p>Checking for moving up </p>

<pre><code>   Fi  = Pi/2
   V1, V2, V3 = -1, 0, 1
   V1 + V2 + V3 = 0  //moment is OK
   Sqrt(3)/2 - Sqrt(3)/2 = 0  //zero speed or OX
   1/2 + 1/2 = S  //S speed for OY
</code></pre>

<p>In general: solve system of three linear equations, get V1, V2, V3</p>

<pre><code>  V1 + V2 + V3 = 0
- V1 * Sqrt(3)/2 + V2  - V3 * Sqrt(3)/2 = S * Cos (Fi)  
 -V1 / 2 + V3 / 2  = S * Sin (Fi)        
</code></pre>

<p>Solving for 45 degrees gives</p>

<pre><code>   &gt; solve({v1+v2+v3=0,-0.87*v1+v2-0.87*v3=0.71,-0.5*v1+0.5*v3=0.71},{v1,v2,v3});
   {v1 = -.90, v2 = .38, v3 = .52}
</code></pre>

<p>Some transformations to get closed-form solution:</p>

<pre><code>  V2 = -V1 - V3
- V1 * Sqrt(3)/2 -V1 - V3  - V3 * Sqrt(3)/2 = S * Cos (Fi)  
  V1 + V3 =  - 2 * S * Cos (Fi) / (2 + Sqrt(3))
  V3 - V1 =  2 * S * Sin(Fi)
  and finally
  V3 = S * (Sin(Fi) - Cos (Fi) / (2 + Sqrt(3)))
  V1 = - S * (Sin(Fi) + Cos (Fi) / (2 + Sqrt(3))) 
  V2 = -V1 - V3
</code></pre>

<p>If after calculations some velocity has absolute value V that exceeds 1.0, divide all velocities by this value to ensure they are in range    </p>

<p>(of course, real-life dynamic is more complex)</p>
",844416,2,0,88701978,"Isn't it just that the speed of the outer edge of a wheel due to rotation (along the motor axis) should be equal to the magnitude of the component of the travel velocity in the direction perpendicular to the motor axis? Cool setup though, I learned that omni wheels exist.",Actuator
115,3156,51187676,What's the difference between ROS2 and DDS?,|robotics|data-distribution-service|ros2|,"<p>ROS2 is a distributed architecture using publisher/subscriber messaging between nodes.</p>

<p>ROS2 has taken a different approach in its messaging layer and now employs the industry standard called Data Distributed Services (DDS).</p>

<p>But, DDS is a middleware for communication, also support publisher/subscriber.</p>

<p>So, we can use DDS directly, why use ROS2?</p>
",7/5/2018 9:20,55204587,9577,4,0,14,0,9920709,"Beijing, 北京市中国",6/10/2018 9:45,90,51188588,"<p>Indeed, ROS2 is based on DDS for the communication. (<a href=""https://github.com/ros2/ros2/wiki/DDS-and-ROS-middleware-implementations"" rel=""noreferrer"">https://github.com/ros2/ros2/wiki/DDS-and-ROS-middleware-implementations</a>)</p>

<p>ROS2 is used because it adds an abstraction making DDS easier to use. DDS needs lot of setup and configuration (partitions, topic name, discovery mode, message creation,...) which is done in the RMW package of ROS2. This package is also responsible to handle error when a message is published/received (taken).</p>

<p>You can use DDS directly (if you configure properly your publisher and subscriber you can also communicate with ROS2 publisher and subscriber) however you will have to create the message (.idl), call the generator to get the corresponding structure and sources files, create a domain, assign a topic, configure the datawriters/datareader,.. (have a look at some examples <a href=""https://github.com/rticommunity/rticonnextdds-examples/tree/master/examples/listeners/c"" rel=""noreferrer"">https://github.com/rticommunity/rticonnextdds-examples/tree/master/examples/listeners/c</a>)</p>

<p>So ROS2 is making your life easier. Plus, there are lots of packages that can be used above the messages.</p>
",6949178,6,0,,,Other
116,3160,51205858,Finding the translation between 2d and 3d coordinate,|python|opencv|math|robotics|coordinate-systems|,"<p>I have a task to find the object in an image.</p>

<p><img src=""https://i.stack.imgur.com/T3gGG.png"" alt=""object1""></p>

<p>and translate the 2d coordinate(x,y) from my camera to 3d coordinate for my robotic arm. Now I can find the 2d coordinate with my opencv python code, and 3d coordinate by my teaching method from my robotic program but in different origin point. However, the method that I use to convert 2d to 3d coordinate is still wrong. Since the origin of the robotic arm and camera is not the same point. So I would like to ask that what formula/code should I use to convert 2d coordinate(x,y) to 3d coordinate(x,y,z) if the origin is not the same.</p>
",7/6/2018 8:14,,864,2,2,0,0,10040383,,7/6/2018 3:45,2,51206531,"<p>Assuming that:</p>

<ul>
<li>You are able to find 3D coordinates centered on the camera</li>
<li>You know the coordinates of the camera ""origin"" in relation to the arm</li>
<li>You know how many degree, the camera's coordinate system is rotated in respect to a given axis in the arm's coordinate system.</li>
<li>You need to obtain arm-centered coordinates.</li>
</ul>

<p>You just need to apply a <em>rigid transformation</em> known as <em>roto-translation</em>.
To simplify the following example code we'll also assume that the Z axes of both coordinate systems are parallel to each other, perpendicular to the ""table"", and have the same direction. So, the only rotation possible is around the Z axis.</p>

<pre><code>x, y, z = get_3d(x, y)

x -= camera_origin_x_in_arm_coord_system
y -= camera_origin_y_in_arm_coord_system
z -= camera_origin_z_in_arm_coord_system

arm_x = x*cos(theta) - y*sin(theta)
arm_y = x*sin(theta) + y*cos(theta)
arm_z = z
</code></pre>

<p>For more complex rotations please refer to the answers to <a href=""https://stackoverflow.com/questions/6802577/rotation-of-3d-vector"">this question</a>.</p>
",1833711,0,2,89392819,"In my understanding you are able to find 3D coordinates centered on the camera, but you need to obtain arm-centered coordinates? Am I right?",Incoming
117,3177,51480521,ValueError: Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph,|python|tensorflow|deep-learning|ros|robot|,"<p>I want to use tensorflow model in ROS.and this code can work,however,it is very slow because of everytime it restore the model</p>

<pre><code>class image_converter:
def __init__(self):
    self.demo_pub = rospy.Publisher(
        ""/demo/pic"", Image, queue_size=1)
    # todo pub distance and angle data
    self.bridge = CvBridge()
    self.image_sub = rospy.Subscriber(
        ""/camera/live_view"", Image, self.callback)
def callback(self, data):
    self.callback_once(data)
def callback_once(self, data):
    # imgmsg_to_cv2
    try:
        cv_image = self.bridge.imgmsg_to_cv2(data, ""bgr8"")
    except CvBridgeError as e:
        print(e)
    img1 = cv_image
    data,_,img=read_img(img1)
        #print(data)
    with tf.Session() as sess:
        saver = tf.train.import_meta_graph('./model4/model.ckpt.meta')
        saver.restore(sess,tf.train.latest_checkpoint('./model4/'))
        graph = tf.get_default_graph()
        x = graph.get_tensor_by_name(""x:0"")
        feed_dict = {x:data}
        logits = graph.get_tensor_by_name('logits_eval:0')
        classification_result = sess.run(logits,feed_dict)
        index=(tf.argmax(classification_result,1).eval())
</code></pre>

<p>so i change this code like this :</p>

<pre><code>class image_converter:
def __init__(self):
    self.sess=tf.Session()
    saver = tf.train.import_meta_graph('./model4/model.ckpt.meta')
    saver.restore(self.sess,tf.train.latest_checkpoint('./model4/'))
    self.graph = tf.get_default_graph()
    self.x = self.graph.get_tensor_by_name(""x:0"")
    self.demo_pub = rospy.Publisher(
        ""/demo/pic"", Image, queue_size=1)
    # todo pub distance and angle data
    self.bridge = CvBridge()
    self.image_sub = rospy.Subscriber(
        ""/camera/live_view"", Image, self.callback)
def callback(self, data):
    try:
        cv_image = self.bridge.imgmsg_to_cv2(data, ""bgr8"")
    except CvBridgeError as e:
        print(e)
    data,_,img=read_img(cv_image)
    feed_dict = {self.x:data}
    logits = self.graph.get_tensor_by_name('logits_eval:0')
    classification_result = self.sess.run(logits,feed_dict)
    print(classification_result)
    index=(tf.argmax(classification_result,1).eval(session=self.sess))
</code></pre>

<p>there is an error occoured :in the last line 
<strong>ValueError: Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph.</strong></p>

<p>so,what should i do to solve this error.thank you very much.</p>
",7/23/2018 13:53,,774,0,0,2,0,8211166,,6/25/2017 6:24,117,,,,,,,,Other
118,3191,51995191,Using Flexible Collision Library (FCL),|c++11|collision-detection|robotics|,"<p>I am trying to implement self-Collision checks on a manipulator arm using the <em>Flexible Collision Library</em> and its difficult to find examples or tutorials. </p>

<p>The documentation available at for is not verbose enough with examples for integration and I just couldn't wrap my head around it. </p>

<p>Where can I find any suitable resources? </p>
",8/23/2018 22:38,,1717,0,2,0,,4796617,"San Francisco, CA, USA",4/16/2015 12:46,48,,,,,,94969703,"Maybe you can start from reading the code snippets from https://github.com/flexible-collision-library/fcl#interfaces and the related interfaces, which is well documented in the header files.",Moving
119,3222,52974661,Multiple View Stereo software recommendation,|computer-vision|robotics|,"<p>I have a collection of camera images taken by 4 calibrated cameras mounted on a mobile robot moving in a static outdoor environment. In addition, I have information from a farely accurate OxTS RT3000 Inertial Navigation System (IMU + GPS).</p>

<p>I would like to combine these images to form a 3d model (point cloud) of the static environment. I know there are many Structure from Motion applications, but I would like to find some software/library that is able to make use of the odometry and calibration, at least as an initialization, and to produce a dense point cloud. (All of this is for offline recordings.)</p>

<p>Any suggestions?</p>
",10/24/2018 17:17,53089201,124,1,0,-1,,7869068,,4/14/2017 20:16,18,53089201,"<p>Agisoft Photoscan does what you want. From their manual:</p>

<blockquote>
  <p>PhotoScan supports import of external and internal camera orientation parameters. Thus, if precise camera
  data is available for the project, it is possible to load them into PhotoScan along with the photos, to be
  used as initial information for 3D reconstruction job.</p>
</blockquote>

<p>Taken from page 21 of <a href=""http://www.agisoft.com/pdf/photoscan-pro_1_4_en.pdf"" rel=""nofollow noreferrer"">http://www.agisoft.com/pdf/photoscan-pro_1_4_en.pdf</a></p>

<p>You may have to do some wrangling of data to get it into a supported format, but it's certainly possible. You'll probably want to use the local coordinate output of the OxTS IMU to go from lat/lon/alt to XYZ and save yourself the conversion. Be careful also to correct for the extrinsic parameters - the rotations and translations between the IMU navigation frame and the cameras.</p>
",6261432,0,2,,,Incoming
120,3223,53070070,Create a moving marker. Robot Operating System (ROS),|openstreetmap|ros|path-finding|robotics|planning|,"<p>I'm trying to plan routes in ROS using OSM data and show the results in Rviz, using Python. Until now, my result is the following one:</p>

<p>Correct path computed by the algorithm shown in Rviz:
<a href=""https://i.stack.imgur.com/dxwVy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dxwVy.png"" alt=""Correct path computed by the algorithm shown in Rviz""></a></p>

<p>Now, what I need is to create a marker or something that follows this highlighted path (simulating a car moving on it). </p>

<p>The idea of my project is to simulate a car moving in the correct path and if the car goes to other street while moving because any reason (I will probably intentionally indicate by code that the car deviates), the algorithm is executed again to replan the route from the place the car is to the same final position.</p>

<p>Is there any way to do that simulation of the car moving in rviz? </p>

<p>I am very grateful for your contributions!</p>
",10/30/2018 17:46,,551,1,0,0,,5604964,Spain,11/25/2015 15:53,43,53084491,"<p>You can create your own models using <a href=""http://wiki.ros.org/robot_model/Tutorials"" rel=""nofollow noreferrer"">Robot_model</a> Package and the set it's <code>base_frame</code> to the frame thats moving alongside your highlighted path. </p>

<p>also you can use any existing models(It may not look like the thing you want)
and if you don't know your moving frame you can use Axe inside Rviz to represent the location of the frame at any moment</p>
",7350738,1,0,,,Moving
121,3241,53493976,Loop arcsin possible answers,|python|python-3.x|python-2.7|robotics|,"<p>I am currently taking a robotic course in my study . There is a lab that we require to find inverse kinematics for our robots and in order to do that, there will be many arcsin and arccos needed to find the all the theta(s)</p>

<p>For example,</p>

<pre><code>theta1 = arcsin(2/3)    &lt;--- This will give two answers
theta2 = arcsin(theta1/2)     &lt;--- This will give 2 answers with 2 different theta1
theta3 = arcsin(theta1/theta2) &lt;--- This too
</code></pre>

<p>So my question is , how do I loop to find all the multiple solutions for those dependent equations ?</p>
",11/27/2018 6:30,,124,0,5,0,,5957146,"Ottawa, ON, Canada",2/20/2016 23:32,26,,,,,,93857654,"Since sin is a cyclic function, there is actually an infinite amount of solutions, x = x0 + 2k*pi for any integer k. arcsin just returns the solution with 0 <= x < 2*pi.",Actuator
122,3287,54620356,How to correctly use ode45 function in MATLAB for differential drive robot?,|matlab|ode|robotics|,"<p>I am trying to determine the pose (x,y,theta) of a differential drive robot using ode45. The code I have below solves for the x position, but I am having an issue with the initial condition. I set it to 0 since at time 0 the robot is assumed to be at the origin, but I get an error. How do I set the initial condition for ode45 such that I get the expected output?</p>

<p>I tried to make the initial conditions vector of the same length as dxdt by setting initial conditions as a 41x1 zeros matrix, but I didn't understand the output and I don't believe I did it correctly.</p>

<pre><code>% Given conditions for a differential drive robot:
B = 20; % (cm) distance along axle between centers of two wheels
r = 10; % (cm) diameter of both wheels
w_l = 5*sin(3*t); % (rad/s) angular rate of left wheel
w_r = 5*sin(3*t); % (rad/s) angular rate of right wheel
v_l = r*w_l; % (cm/s) velocity of left wheel
v_r = r*w_r; % (cm/s) velocity of right wheel
v = (v_r+v_l)/B; % (cm/s) velocity of robot
theta = 90; % constant orientation of robot since trajectory is straight

% Solve differential equation for x:
dxdt = v*cos(theta); % diff equaition for x
tspan = [0 20]; % time period to integrate over
x0 = 0; % initial condition since robot begins at origin
[t,x] = ode45(@(t,y) dxdt, tspan, x0);
</code></pre>

<p>I want to solve the differential equation <code>dxdt</code> for <code>0</code> to <code>20</code> seconds with an initial condition of <code>0</code>. I expect the output to give me a vector of time from <code>0</code> to <code>20</code> and an array of for <code>x</code>. The problem I believe lies with the initial condition. MATLAB gives me an error in the live editor telling me, "" <code>@(t,y)dxdt returns a vector of length 69, but the length of initial conditions vector is 1. The vector returned by @(t,y)dxdt and the initial conditions vector must have the same number of elements.</code>""</p>
",2/10/2019 19:52,,801,1,0,0,,9832088,"Valley Stream, NY, USA",5/23/2018 3:59,9,54628271,"<p>The issue is not the initial condition. You need to define <code>dxdt</code> as a function i.e.</p>

<pre><code>% Define differential equation
function derivative = dxdt(t,y)

% Given conditions for a differential drive robot:
B = 20; % (cm) distance along axle between centers of two wheels
r = 10; % (cm) diameter of both wheels
w_l = 5*sin(3*t); % (rad/s) angular rate of left wheel
w_r = 5*sin(3*t); % (rad/s) angular rate of right wheel
v_l = r*w_l; % (cm/s) velocity of left wheel
v_r = r*w_r; % (cm/s) velocity of right wheel
v = (v_r+v_l)/B; % (cm/s) velocity of robot
theta = 90; % constant orientation of robot since trajectory is straight

derivative = v*cos(theta); % diff equation for x

end
</code></pre>

<p>Then when you use <code>ode45</code> you should tell it to pass the <code>t</code> and <code>y</code> variables as arguments to <code>dxdt</code> like</p>

<pre><code>[t,x] = ode45(@(t,y) dxdt(t,y), tspan, x0);
</code></pre>

<p>This should then work. In this case, as <code>dxdt</code> only takes the default arguments, you could also write</p>

<pre><code>[t,x] = ode45(@dxdt, tspan, x0);
</code></pre>

<p>The error you got indicates that at some point you made <code>dxdt</code> into a vector of length 69, whilst MATLAB was expecting to get back 1 value for <code>dxdt</code> when it passed one <code>t</code> and one <code>y</code> to your <code>dxdt</code> 'function'. Whenever you get errors like this, I'd recommend putting </p>

<p><strike><code>clear all</code></strike></p>

<pre><code>`clearvars` % better than clear all - see am304's comment below
</code></pre>

<p>at the top of your script to avoid polluting your workspace with previously defined variables.</p>
",6459621,1,1,,,Coordinates
123,3319,55038180,How to use the uArm Pro Python library,|python-2.7|robotics|,"<p>I am just getting started with uArm Pro and need to write a test controller in Python. I came across the python library for uArm on Github. How can I use it to my advantage?</p>
",3/7/2019 7:24,,985,1,0,-1,,10133412,"Haldia, West Bengal, India",7/25/2018 12:25,12,55038414,"<p>uArm provides baisc Movement on Python. The library only supports uArm Swift/SwiftPro. For Metal, please use pyuarm or pyuf instead.</p>

<p>pyuarm can be installed from PyPI, either manually downloading the files and installing as described below or using:</p>

<pre><code>pip install pyuarm
</code></pre>

<p>Download the archive from <a href=""https://github.com/uArm-Developer/uArm-Python-SDK"" rel=""nofollow noreferrer"">https://github.com/uArm-Developer/uArm-Python-SDK</a> . Unpack the archive, enter the uArm-Python-SDK directory and run:</p>

<pre><code>python setup.py install
</code></pre>

<p>Try examples using this link <a href=""https://github.com/uArm-Developer/uArm-Python-SDK/tree/2.0/examples/api"" rel=""nofollow noreferrer"">https://github.com/uArm-Developer/uArm-Python-SDK/tree/2.0/examples/api</a></p>
",7484853,1,0,,,Error
124,3342,55258526,Accelerometer too noisy,|python|robotics|,"<p>I am working on a group project for building a robot and we are trying to implement an accelerometer that triggers an alarm if the robot is picked up/pushed on the way. Right now our accelerometer is very noisy and not very reliable. Could someone point me in the right direction on how to fix this?
Current code:</p>

<pre><code> def accel_alarm(self):
        cur_accel = self.read_smooth_accel()
        accel_x, accel_y, accel_z = cur_accel
        base_x, base_y, base_z = (-32, 31, 1075)  # values when robot is static
        if abs(accel_x - base_x) &gt; 350 or abs(
                accel_y - base_y) &gt; 350 or abs(accel_z - base_z) &gt; 350:
            if not (self.alarm):
                self.send_alarm()
                self.alarm = True
                print(
                    ""[accel_alarm] ALARM STATE "" + str(accel_x) + "" X "" + str(
                        accel_y) + "" Y  "" + str(accel_z) + "" Z"")

    # Smooth accelerometer output by taking the average of the last n values
    # where n = len(self.accel_data)
    def read_smooth_accel(self):
        cur_accel, _ = self.accel.read()
        self.accel_data.pop()
        self.accel_data.appendleft(cur_accel)
        # For the first len(accel_data) values the average is not
        # representative - just return current value
        if [0, 0, 0] in self.accel_data:
            return cur_accel

        av_accel = [sum(i) / float(len(i)) for i in zip(*self.accel_data)]
        return av_accel
</code></pre>

<p>The second method is trying to tackle the noisiness but it is still not very nice... </p>
",3/20/2019 10:26,,40,0,4,0,,10585692,,10/31/2018 11:53,16,,,,,,97255737,"Basically, when it is turned on now and the robot is even static, it prints out the coordinates to the console every millisecond. We want to try to reduce the number of outputs to only appear when the robot moves more than 10cm in direction.",Incoming
125,3347,55313613,How rotate source vector to target (to nearest vector) by using only 2 Euler angles,|matrix|vector|geometry|coordinates|robotics|,"<p>I need to rotate source 3D vector to target vector by rotation around only two axes YZ.</p>

<p>I have mechanism with 2 motors rotating around Y and then around Z. I have random source vector attached to this mechanism.</p>

<p>I think that source vector can't be rotated to random target 3D vector by using only 2 angles and I need to rotate it to the nearest to target vector.</p>

<p>I need to align only orientation, not coordinates.</p>

<p>What is the best way to do it with rotation matrices, quaternions, etc?</p>

<p>I can calculate shortestArcQuat from source vector to target. Then multiply target vector by inverse of this quat. And then get YZ angles from rotation matrix with the result of prev operation as Z vector but I think it's wrong.</p>

<p>Solution should be analytic.</p>
",3/23/2019 12:14,55315048,456,1,0,0,,11246956,,3/23/2019 11:49,2,55315048,"<p>For clarity in the notation let me define the unit vectors </p>

<pre><code>E1 = (1,0,0)
E2 = (0,1,0)
E3 = (0,0,1)
</code></pre>

<p>Given a rotation matrix R the goal is deconpose it in two rotation matrices rotating an angle ""a"" around axis E2 and an angle ""b"" around axis E3:</p>

<pre><code>R = exp(a E2) exp(b E3)
</code></pre>

<p>Multiplying both sides by E3</p>

<pre><code>R E3 = exp(a E2) exp(b E3) E3
</code></pre>

<p>We get:</p>

<pre><code>W = exp(a E2) E3
</code></pre>

<p>Where W is the vector E3 rotated by R: W = R E3</p>

<pre><code>a = atan2( W • E1, W • E3)
</code></pre>

<p>Where (•) is the dot product.</p>

<p>Now taking the transpose of R we get:</p>

<pre><code>R^T = exp(b E3)^T exp(a E2)^T
</code></pre>

<p>Multiplying both sides by E2:</p>

<pre><code>R^T E2 = exp(b E3)^T exp(a E2)^T E2

R^T E2 = exp(-b E3) exp(-a E2) E2

S = exp(-b E3) E2
</code></pre>

<p>Where S is the vector E2 rotated by R^T: S = R^T E2</p>

<pre><code>b = - atan2( S • E1, S • E2)
</code></pre>

<p>I have just derived those equations so this is untested and there might be some mistake. Take it as is.</p>
",9147444,0,0,,,Coordinates
126,3366,55429137,How to control a 4 DOF robotic arm using Ardunio?,|arduino|arduino-uno|robotics|,"<p>I recently bought a 4 DOF <em>(Degree Of Freedom)</em> robotic arm kit. I successfully assembled it and now I want to program the arduino to control it.
I know how to make the servos work using arduino but could not figure out how to move the hand to specific positions.</p>

<p>I tried manually creating a two dimensonal array with rotational values for each motor in degrees. This works but it is very hard to get the values and create the array. Currently I adjusted the values by trial and error.</p>

<p><strong>The array I created manually :</strong></p>

<pre><code>short first[] = { 180 , 80 , 0 , 90 };
short pos[][4] = 
{
  { 180 , 80 , 00 , 85 },
  { 180 , 85 , 00 , 80 },
  { 180 , 90 , 00 , 75 },
  { 180 , 95 , 00 , 75 },
  { 180 , 100 , 0 , 70 },
  { 180 , 110 , 0 , 70 },
  { 180 , 115 , 0 , 70 },
  { 180 , 120 , 0 , 65 },
  { 180 , 125 , 0 , 65 },
  { 180 , 130 , 0 , 65 },
  { 180 , 135 , 0 , 65 },
  { 180 , 140 , 0 , 65 },
  { 180 , 145 , 0 , 65 },
  { 180 , 150 , 0 , 65 },
  { 180 , 150 , 0 , 70 },
  { 180 , 150 , 0 , 75 },
  { 180 , 150 , 0 , 80 },  
  { 180 , 150 , 0 , 90 },
  { 180 , 145 , 0 , 90 },
  { 180 , 140 , 0 , 90 },
  { 180 , 135 , 0 , 90 },
  { 180 , 130 , 0 , 90 },
  { 180 , 125 , 0 , 90 },
  { 180 , 120 , 0 , 90 },
  { 180 , 115 , 0 , 90 },
  { 180 , 110 , 0 , 90 },
  { 170 , 110 , 0 , 90 },
  { 160 , 110 , 0 , 90 },
  { 150 , 110 , 0 , 90 }, 
  { 140 , 110 , 0 , 90 }, 
  { 130 , 110 , 0 , 90 },
  { 130 , 115 , 0 , 90 }, 
  { 120 , 120 , 0 , 90 },
  { 120 , 125 , 0 , 90 },
  { 120 , 130 , 0 , 90 },
  { 120 , 135 , 0 , 90 },
  { 120 , 137 , 0 , 90 },
  { 120 , 139 , 0 , 90 },
  { 120 , 140 , 0 , 85 },
  { 120 , 140 , 0 , 80 },
  { 120 , 140 , 0 , 75 },
  { 120 , 140 , 0 , 70 },   

};
</code></pre>

<p><strong>The complete code that I wrote</strong> :</p>

<pre><code>/*
 * claws - 90 close 75 open
 * elbow - 0 to 100
 * sholder - 30 to 180
*/

Servo Servos[4];

void setup()
{
  Servos[0].attach(3);
  Servos[1].attach(5);
  Servos[2].attach(9);
  Servos[3].attach(11);

  reset();
  run();
  Servos[0].detach();
  Servos[1].detach();
  Servos[2].detach();
  Servos[3].detach();
}

void run()
{
  for(int i=0; i&lt;sizeof(pos) / sizeof(short) /4 ; i++)
  {
    for(int j=3; j&gt;=0; j--)
    {
      Servos[j].write(pos[i][j]);
      delay(15);
    }
    delay(15);
  }
  for(int i=-1+ sizeof(pos) / sizeof(short) /4;i&gt;=0 ; i--)
  {
    for(int j=3; j&gt;=0; j--)
    {
      Servos[j].write(pos[i][j]);
      delay(15);
    }
    delay(15);
  }
  delay(3000);
}

void reset()
{
  for(int i=3; i&gt;=0; i--)Servos[i].write(first[i]);
}


void loop(){}
</code></pre>

<p>I want some function to calculate the values of the array for any given coordinate or something like that.(That is the moves of each servo to position the end of the arm at that point)</p>

<p><strong>Photo of the Arm</strong> : </p>

<p><strong><a href=""https://i.stack.imgur.com/2ZPL8.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2ZPL8.jpg"" alt=""Robotic Arm""></a></strong> 
<a href=""https://i.stack.imgur.com/yObAw.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yObAw.jpg"" alt=""Robotic Arm""></a></p>

<p><strong>Here is the product page of the actual arm</strong> : </p>

<p><a href=""https://www.amazon.in/gp/product/B07LDNY9J3/ref=ppx_yo_dt_b_asin_title_o03_s00?ie=UTF8&amp;psc=1"" rel=""nofollow noreferrer"">https://www.amazon.in/gp/product/B07LDNY9J3/ref=ppx_yo_dt_b_asin_title_o03_s00?ie=UTF8&amp;psc=1</a></p>
",3/30/2019 7:17,57345583,3582,1,4,0,,10182024,"Kerala, India",8/5/2018 6:28,164,57345583,"<p>I finally solved the problem ! I tried figuring out the inverse kinematics for the arms but I found out it is very hard and due to the uncertainty in the hardware it doesn't work good either. The actual solution was to use sensors. I put distance sensors (Ultra-sonic) on the arm so that I can measure distance between arm parts in real time. Since I know the length of each segment of the arm (I don't have to worry about uncertainty here.) and also the distance between them I can do simple trigonometry to calculate the coordinates of the tip of the arm. This means that I can simply use a feedback loop to position the arm with ineradicable accuracy and overcome the limitations of the Hardware.</p>

<p>I do understand that this is not the exact answer for the asked question but I found out from my experience that this method is the most suitable for the situation (When compared to the attempt according to the question).</p>
",10182024,1,0,97643335,"You need to find the inverse kinematics equations for your robot configuration. Probably, as it is a kit, someone else has calculated them and you can find them online. If that's not the case, you need to obtain the Denavit-Hartemberg parameters for the configuration, and then obtain the matrices for each DOF.",Actuator
127,3372,55608358,"How to fix Mujoco CmakeLists build error ""/usr/bin/ld: cannot find -lglfw""?",|c++|cmake|shared-libraries|simulator|robotics|,"<p>I'm trying to simulate physics of a robot with Mujoco in C++. Because the project is a part of a bigger workspace, I need to use cmake to build the executable. However, I cannot seem to properly link all the dependant libraries, so I cannot get rid of the error:</p>

<pre><code>~: /usr/bin/ld: cannot find -lglfw
</code></pre>

<p>I did a bit of research on the web on how to properly set up Mujoco in CmakeLists, and found some examples <a href=""http://www.mujoco.org/forum/index.php?threads/c-makefile-cmakelists-txt.3840/"" rel=""nofollow noreferrer"">here</a>, <a href=""http://www.mujoco.org/forum/index.php?threads/cmake-request.3361/"" rel=""nofollow noreferrer"">here</a>, and <a href=""https://github.com/atabakd/MuJoCo-Tutorials"" rel=""nofollow noreferrer"">here</a>.</p>

<p>I replicated the CmakeLists files from the examples above, but the error still persisted. Here are the relevant snippets from my files. I defined an environment variable <code>MUJOCO_PATH</code> to point to the Mujoco folder on my machine. Concretely <code>$HOME/.mujoco/mujoco200</code>.</p>

<p>CmakeLists.txt</p>

<pre><code>######################################################
# define the include directory of all ${CATKIN_PKGS} #
######################################################
include_directories(
    ${PROJECT_SOURCE_DIR}/include
    ${catkin_INCLUDE_DIRS}
    ${Eigen_INCLUDE_DIRS}
    $ENV{MUJOCO_PATH}/include
)

########################################################
# manage the creation of the libraries and executables #
########################################################
set(USE_GL 1)

link_directories($ENV{MUJOCO_PATH}/bin)

#Finding main mujoco library
if(${USE_GL})
    file(GLOB LIB_MUJOCO $ENV{MUJOCO_PATH}/bin/libmujoco[0-9][0-9][0-9].so)
else()
    file(GLOB LIB_MUJOCO $ENV{MUJOCO_PATH}/bin/libmujoco[0-9][0-9][0-9]nogl.so)
endif()
#Showing mujoco library found
message(STATUS ""MuJoCo lib: "" ${LIB_MUJOCO})

add_subdirectory(src)
</code></pre>

<p>src/CmakeLists.txt</p>

<pre><code>set(BIN_NAME mujoco_finger_test)

add_executable(${BIN_NAME} ${BIN_NAME}.cpp)
target_link_libraries(${BIN_NAME} ${LIB_MUJOCO})

# Standard libraries for GL
target_link_libraries(${BIN_NAME} GL GLU glut )

# Additional libraries from mujoco package
target_link_libraries(${BIN_NAME} libglew.so libglfw.so.3 libglewegl.so libglewosmesa.so)
</code></pre>

<p>Does anyone have an idea why this could be the case? Am I missing something from these examples?</p>

<p>Thanks!</p>
",4/10/2019 8:47,55608420,9020,1,0,1,,5576434,,11/18/2015 11:56,63,55608420,"<p>You should find the GL/GLW package instead ot this: <code>target_link_libraries(${BIN_NAME} libglew.so libglfw.so.3 libglewegl.so libglewosmesa.so)</code>. This doesn't ensure that these libraries are available and can be found, whereas the <code>FIND_PACKAGE(GLEW)</code>.</p>

<p>See <a href=""https://stackoverflow.com/questions/27472813/linking-glew-with-cmake"">Linking GLEW with CMake</a> for more information on the subject.</p>
",2266772,3,0,,,Error
128,3375,55787126,Can someone explain how to add the slider object so it controls its own individual servo motor?,|user-interface|arduino|processing|computer-science|robotics|,"<p>The problem is that every time I add a slider object and try to connect it to a different servo motor through a different pin the sliders both only control the same servo motor. It won't allow for me to add a servo motor that controls each servo motor independently through the user-interface. I'm using Processing as the interface and Arduino as the IDE. </p>

<p>I have tried adding other slider objects but they all still control the same servo. I do not know if the issue is through Arrduino or Processing. When I add the other sliders, I connect them to their own pins but it still doesn't allow for them to be controlled individually.</p>

<p>Processing code:</p>

<pre><code>import processing.serial.*;
import cc.arduino.*;
import controlP5.*;
ControlP5 controlP5;
Arduino arduino;
int servoAngle = 90;
void setup() {

 size(400,400);

 println(Arduino.list());
 arduino = new Arduino(this, Arduino.list()[0], 57600);

 for (int i = 0; i &lt;= 13; i++)
 arduino.pinMode(i, Arduino.OUTPUT);

 controlP5 = new ControlP5(this);
 controlP5.addSlider(""servoAngle"",0,180,servoAngle,20,10,180,20);

}
void draw() {
 arduino.analogWrite(9, servoAngle);
 //delay(15);
}
</code></pre>

<p>Arduino code:</p>

<pre><code>    #include &lt;Servo.h&gt;
    #include &lt;Firmata.h&gt;
    Servo servos[MAX_SERVOS];
    byte servoPinMap[TOTAL_PINS];
    byte servoCount = 0;

void analogWriteCallback(byte pin, int value)
{
  if (IS_PIN_DIGITAL(pin)) {
    servos[servoPinMap[pin]].write(value);
  }
}

void systemResetCallback()
{
  servoCount = 0;
}

void setup()
{
  byte pin;

  Firmata.setFirmwareVersion(FIRMATA_FIRMWARE_MAJOR_VERSION, 
  FIRMATA_FIRMWARE_MINOR_VERSION);
  Firmata.attach(ANALOG_MESSAGE, analogWriteCallback);
  Firmata.attach(SYSTEM_RESET, systemResetCallback);

  Firmata.begin(57600);
  systemResetCallback();

  // attach servos from first digital pin up to max number of
  // servos supported for the board
  for (pin = 0; pin &lt; TOTAL_PINS; pin++) {
    if (IS_PIN_DIGITAL(pin)) {
      if (servoCount &lt; MAX_SERVOS) {
        servoPinMap[pin] = servoCount;
        servos[servoPinMap[pin]].attach(PIN_TO_DIGITAL(pin));
        servoCount++;
      }
    }
  }
}

void loop()
{
  while (Firmata.available())
    Firmata.processInput();
}
</code></pre>

<p>I would like to be able to add 3 more sliders that to the one already created but be able to control 4 servo motors, each controlled by its own slider, but the result I'm getting is that each additional slider is controlling the same motors.</p>
",4/21/2019 21:59,,285,1,0,0,,11392380,"Chicago, IL, USA",4/21/2019 21:38,1,56297074,"<p>totally depend on which hardware you are running.
but i think you have to change a few places</p>

<p>(1) decelar all trackbar value and the pwm</p>

<pre><code> int servoAngle = 90; int servoAngle1 = 90; int servoAngle2 = 90; int servoAngle3 = 90;
</code></pre>

<p>(2)
is add track bar</p>

<pre><code>controlP5.addSlider(""servoAngle"",0,180,servoAngle,20,10,180,20);
controlP5.addSlider(""servoAngle1"",0,180,servoAngle1,20,10,180,20);
controlP5.addSlider(""servoAngle2"",0,180,servoAngle2,20,10,180,20);
controlP5.addSlider(""servoAngle3"",0,180,servoAngle3,20,10,180,20);
</code></pre>

<p>(3) link up the hardware pin. totally hardware depended, not sure which 1 u are using. ill jus tput here as sample</p>

<pre><code> arduino.analogWrite(9, servoAngle);
 arduino.analogWrite(10, servoAngle);
 arduino.analogWrite(11, servoAngle);
 arduino.analogWrite(12, servoAngle);
 //delay(15);
}
</code></pre>

<p>that should more or less do it. You know many years ago, we have to write our own processing to arduino serial protocol to do this.</p>
",11530294,-1,0,,,Remote
129,3385,55997039,Algorithms for a line follower robot (with camera) capable following very sharp turns and junction,|python|opencv|computer-vision|robotics|opencv3.1|,"<p>I want to write code (python,opencv) for a line follower robot equipped with a camera and Raspberry Pi. I want to make to robot go has fast as possible</p>

<ol>
<li>The course has few very sharp turns like this: <a href=""https://i.stack.imgur.com/g7G8C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g7G8C.png"" alt=""https://imgur.com/a/h07WlAD/""></a> I'm assuming that using ROI (region of interest) will not work well when the robot in near the turn (it will also capture/""see"" the other line) - for example as shown below. What is the best approach here?</li>
</ol>

<p><a href=""https://i.stack.imgur.com/walJK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/walJK.png"" alt=""https://imgur.com/a/b9K66Rp""></a></p>

<ol start=""2"">
<li><p>In the course there is a junction as shown in below image,
<a href=""https://i.stack.imgur.com/tZgNA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tZgNA.png"" alt=""enter image description here""></a></p>

<p>How to ""understand"" that this is a junction? and if the robot is
coming for the bottom of the image, how to make the algorithm
continue driving straight and not get confused by the horizontal
line?</p></li>
</ol>
",5/5/2019 22:42,,2045,1,3,1,,11455950,,5/5/2019 16:25,8,62512400,"<p>I can recommend this awesome video on YouTube:</p>
<p><a href=""https://www.youtube.com/watch?v=tpwokAPiqfs&amp;t=868s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=tpwokAPiqfs&amp;t=868s</a></p>
<p>It contains several episodes and teachs a lot of useful stuff. Good luck!!!</p>
",13749389,0,0,98641189,"I'm experimenting, I wrote a code that slices the input image from the camera (horizontal). On each slice I run threshold-> findcontours-> find the centeroid of the contour. I keep track on the line using a sliding window, because on the right to of the line (in some places) there are signaling lines and I need to filter them out. When I get close to a sharp  turn, the sliding doesn't work well",Moving
130,3390,56067475,"Doing things (driving servos, turning on led) using joystick input",|python|robotics|joystick|evdev|,"<p>I'm building a basic DVa Mech robot (hobby).  This is non-bipedal.  Wheeled chassis. All code in python. </p>

<p>How do I continuously perform activities while holding down a button, pushing a joystick?  And can I even do two (or more) at once: move wheels forward, turn torso, fire guns?</p>

<p>I'm reading joystick input fine. Servos work too.
I can't seem to figure out the logic loops of 'while button pushed - do something - and keep scanning for more input'</p>

<p>Tried various things...they didn't work so they are out of the code below.</p>

<p>Running 6 continuous servos (4 for chassis, two for mini-guns)
Logitech F710 joystick</p>

<pre><code>from evdev import InputDevice, categorize, ecodes, KeyEvent
from adafruit_servokit import ServoKit
import time
kit = ServoKit(channels = 16)
gamepad = InputDevice('/dev/input/event7')
print (gamepad)
for event in gamepad.read_loop():
    if event.type == ecodes.EV_KEY:
            keyevent = categorize(event)
            if keyevent.keystate == KeyEvent.key_down:
                    print(keyevent)
                    ....
                    elif keyevent.keycode == 'BTN_TL':
                            print (""Guns"")
    elif event.type == ecodes.EV_ABS:
            absevent = categorize(event)
            print(absevent.event.code)
            if ecodes.bytype[absevent.event.type][absevent.event.code] == 'ABS_HAT0X':
                    if absevent.event.value == -1:
                            print('left')
                    elif absevent.event.value == 1:
                            print('right')
            if ecodes.bytype[absevent.event.type][absevent.event.code] == 'ABS_HAT0Y':
                    if absevent.event.value == -1:
                            print('forward')
                    elif absevent.event.value == 1:
                            print('back')
</code></pre>

<p>Fairly basic...when BTN_TL is pressed, servos 5 and 6 should spin until the button is released</p>

<p>Likewise with HAT0X and 0Y the servos should move forward/back/left/right while pressed.</p>

<p>I've tried while loops and whatnot...but there's a logic/timing sequence in joystick input I'm not putting in the right place</p>
",5/9/2019 20:58,,445,1,0,0,,11477889,,5/9/2019 20:34,0,56068204,"<p>For the <strong>servo</strong> part and based on <a href=""https://github.com/adafruit/Adafruit_CircuitPython_ServoKit"" rel=""nofollow noreferrer"">Servokit documentation</a> there is two ways to control servos:</p>

<ol>
<li>Set desired shaft angle:</li>
</ol>

<pre><code>    kit.servo[servonum].angle = 180
</code></pre>

<ol start=""2"">
<li>Indicate rotation direction (<code>1</code>: forward, <code>-1</code>: backward, <code>0</code>: stop) like:</li>
</ol>

<pre><code>    kit.continuous_servo[servonum].throttle = 1 
    kit.continuous_servo[servonum].throttle = -1
    kit.continuous_servo[servonum].throttle = 0
</code></pre>

<p>I'd rather use angles, even if I'd have to increment/decrement their values (according to time) in the loop. It would give you a hand on speed (or speed curve) and position.</p>

<p>For the <strong>Joystick</strong> part, the <a href=""https://ericgoebelbecker.com/2015/07/raspberry-pi-and-gamepad-programming-part-3-adding-a-gun-turret/"" rel=""nofollow noreferrer"">Eric Goebelbecker</a>'s ""tutorial"" worth a reading.</p>

<p>EDIT: Solution with <code>continuous_servo</code> (for future reading)</p>

<p><code>ABS_HAT0{X,Y}</code> notifies DOWN event with -1 or +1 on the active axis. And UP with 0 value.</p>

<pre><code>axis_servo = {
    'ABS_HAT0X': 5,
    'ABS_HAT0Y': 6,
}

[...]
        axis = ecodes.bytype[absevent.event.type][absevent.event.code]
        if axis == 'ABS_HAT0X':
            servonum = axis_servo[axis]
            kit.continuous_servo[servonum].throttle = absevent.event.value
        if axis == 'ABS_HAT0Y':
            servonum = axis_servo[axis]
            # Reverse Y axis (1 -&gt; up direction)
            kit.continuous_servo[servonum].throttle = -absevent.event.value
</code></pre>

<p>Without <code>continuous_servo</code>, one should consider the use of <code>select()</code> with <strong>timeout</strong> (<code>select([gamepad], [], [], timeout)</code>) as described in <a href=""https://python-evdev.readthedocs.io/en/latest/tutorial.html#reading-events-from-multiple-devices-using-select"" rel=""nofollow noreferrer"">readthedoc: python-evdev</a>.</p>

<p>The timeout will allow angle computation.</p>
",425540,0,1,,,Remote
131,3393,56360142,What does it actually mean by Task Planning?,|robotics|,"<p>I have just started studying robotics. This is my first question to get started with robotics. Pardon me if this question is too naive.</p>

<p>What does it actually mean by <a href=""https://ieeexplore.ieee.org/document/56654"" rel=""nofollow noreferrer"">Task Planning</a> in case of robotics?</p>

<p>How is it different from conventional ""robot control""?</p>
",5/29/2019 12:00,56373815,40,1,0,0,,159072,,6/2/2009 18:25,6588,56373815,"<p><strong>Robot Motion control</strong> is generally dealt with low-level navigation like localization (what is the current location of the robot), path planning (chose a path from a to b by avoiding obstacles).</p>

<p><strong>Tasking planning</strong> is generally dealing with high-level planning. For example, task for a robot with a manipulator is to clean the desk (go to desk, find objects on desk and choose the order to pickup items, move the picked items to dusbin).</p>
",1595504,1,0,,,Other
132,3404,56592230,In general is it ok to loop if statements with goto under else?,|lua|robotics|,"<p>So I have a task to be done which is to program the robot (AUBO) to pick different objects and place them in a certain order (Point A, B, C, D). 
I'm using some vision system known as pim60. So if an object is detected it will go and pick and the rest of the program are waypoints to drop products. The first problem is I want it to go to the next waypoint to drop the and the second thing is, the next drop point cannot be skipped until an object is detected for that drop point. </p>

<p>In my own code, I wrote a rather lengthy program like this.</p>

<pre><code>::LoopA::
script_common_interface(""SICKCamera"",""takePhoto"")
script_common_interface(""SICKCamera"",""getResult"")
Located = script_common_interface(""SICKCamera"",""partLocated"")
if(Located == 1) then
.
.
.
Drop at position A
else 
goto LoopA
end

::LoopB::
script_common_interface(""SICKCamera"",""takePhoto"")
script_common_interface(""SICKCamera"",""getResult"")
Located = script_common_interface(""SICKCamera"",""partLocated"")
if(Located == 1) then
.
.
.
Drop at position B
else 
goto LoopB
end

::LoopC::
script_common_interface(""SICKCamera"",""takePhoto"")
script_common_interface(""SICKCamera"",""getResult"")
Located = script_common_interface(""SICKCamera"",""partLocated"")
if(Located == 1) then
.
.
.
Drop at position C
else 
goto LoopC
end

::LoopD::
script_common_interface(""SICKCamera"",""takePhoto"")
script_common_interface(""SICKCamera"",""getResult"")
Located = script_common_interface(""SICKCamera"",""partLocated"")
if(Located == 1) then
.
.
.
Drop at position D
else 
goto LoopD
end
</code></pre>

<p>There is no error and the program runs as expected. However, I'm wondering if there is any better way to do it.</p>
",6/14/2019 6:00,56602237,117,1,6,2,,11640175,Singapore,6/13/2019 4:23,32,56602237,"<p>The only generally accepted use-case for <code>goto</code> is error handling, e.g. to jump forward to the cleanup code. But even for that it usually can and should be avoided.</p>

<p>You can probably do something like this:</p>

<pre><code>-- loop B
repeat
  take photo, etc.
  located = ...
until(located == 1)

Drop at position B
</code></pre>

<p>Also, if you're repeating the same code three times, you should extract it into a function, and maybe give the position as a parameter. Or at least put the whole thing into a <code>for</code> loop.</p>
",235548,1,3,99773559,My noobish program makes it appear so. And I had posted incorrect stuff after not touching the code for some time.,Moving
133,3442,57038494,how can I use gpiozero robot library to change speeds of motors via L298N,|python-3.x|raspberry-pi2|robotics|gpiozero|,"<p>In my raspberry pi, i need to run two motors with a L298N.
I can pwm on enable pins to change speeds. But i saw that gpiozero robot library can make things a lot easier. But 
When using gpiozero robot library, how can i alter speeds of those motors by giving signel to the enable pins.</p>
",7/15/2019 11:09,61753240,778,2,0,1,,11390692,,4/21/2019 10:50,128,59144212,"<p>I have exactly the same situation.  You can of course program the motors separately but it is nice to use the robot class.
Looking into the gpiocode for this, I find that in our case the left and right tuples have a third parameter which is the pin for PWM motor speed control. (GPIO Pins  12 13 18 19 have hardware PWM support).  The first two outout pins in the tuple are to be signalled as 1, 0 for forward, 0,1 for back. 
So here is my line of code:
    Initio = Robot(left=(4, 5, 12), right=(17, 18, 13))</p>

<p>Hope it works for you!
I have some interesting code on the stocks for controlling the robot's absolute position, so it can explore its environment.</p>
",5559525,1,0,,,Actuator
134,3445,57065924,What is the data contained in ROS topic velodyne_msgs/VelodynePacket Message?,|ros|robotics|lidar|,"<p>I am doing a light weight program to monitor received beams for lidar. Preferably, I do not want to cache the entire UDP data packet or point cloud data due to the light weight nature.</p>

<p>The question is what is the data contained in ROS message velodyne_msgs/VelodynePacket. This message contains smaller data but I do not know if it is related. </p>

<p>By read the <a href=""http://wiki.ros.org/velodyne_msgs"" rel=""nofollow noreferrer"">Ros Wiki</a> on this topic but the link for velodynepackt did not provide useful info on the content. </p>
",7/16/2019 21:52,,706,1,0,0,,4318755,,12/3/2014 6:24,20,57068895,"<p>Check the message definition to see what fields a message contains and their types. Message files will usually either have field names that are self explanatory or will have comments (<code>## text</code>) describing the fields. You can look at the message definitions either online or locally. To look at them locally use <code>roscd</code> to get to the package directory <code>roscd &lt;package_name&gt;/msg</code> and then using <code>cat</code> to see the contents of the message file. In your case, this would be: </p>

<p><code>roscd velodyne_msgs/msg
cat VelodynePacket.msg
cat VelodyneScan.msg</code> </p>

<p>The relevant message files are available online from the page you linked to:
<a href=""http://docs.ros.org/api/velodyne_msgs/html/msg/VelodyneScan.html"" rel=""nofollow noreferrer"">http://docs.ros.org/api/velodyne_msgs/html/msg/VelodyneScan.html</a>
<a href=""http://docs.ros.org/api/velodyne_msgs/html/msg/VelodynePacket.html"" rel=""nofollow noreferrer"">http://docs.ros.org/api/velodyne_msgs/html/msg/VelodynePacket.html</a></p>

<p>In regards to your specific question about creating a lightweight application, you have a few options. </p>

<ol>
<li>Use the provided ROS message and subscribe to it. Most of the time if you don't have a ton of large data traveling around, you'll be okay and will be able to keep up with real time data. The majority of the time associated with ROS usually comes from the network transport, so if that's a problem, you'll need to not pass the data over ROS. </li>
<li>Put your code in a ROS Nodelet. This gives you the advantages of ROS data abstractions while eliminating the network data transfer that occurs between nodes. This is akin to using a pointer to the data field. </li>
<li>If you really don't want all the scan data, but still want to use ROS, you can write your own driver node. This will read from the LIDAR the data you want and discard the data you don't. You can do the raw data processing in that node (no ROS message required) or publish the data you care about and do the processing in another node. </li>
</ol>
",3954529,-1,2,,,Incoming
135,3461,57675404,Data structure to store a grid (that will have negative indices),|c++|data-structures|ros|robotics|,"<p>I'm studying robotics at the university and I have to implement on my own SLAM algorithm. To do it I will use ROS, Gazebo and C++.</p>

<p>I have a doubt about what data structure I have to use to store the map (and what I'm going to store it, but this is another story).</p>

<p>I have thought to represent the map as a 2D grid and robot's start location is (0,0). But I don't know where exactly is the robot on the world that I have to map. It could be at the top left corner, at the middle of the world, or in any other unknonw location inside the world.</p>

<p>Each cell of the grid will be 1x1 meters. I will use a laser to know where are the obstacles. Using current robot's location, I will set to 1 on all the cells that represent an obstacle. For example, it laser detects an obstacle at 2 meters in front of the robot, I will set to 1 the cell at (0,2).</p>

<p>Using a vector, or a 2D matrix, here is a problem, because, vector and matrices indices start at 0, and there could be more room behind the robot to map. And that room will have an obstacle at (-1,-3).</p>

<p>On this data structure, I will need to store the cells that have an obstacle and the cells that I know they are free.</p>

<p>Which kind of data structure will I have to use?</p>

<p><strong>UPDATE:</strong><br/></p>

<p>The process to store the map will be the following:</p>

<ol>
<li>Robot starts at (0,0) cell. It will detect the obstacles and store them in the map.</li>
<li>Robot moves to (1,0) cell. And again, detect and store the obstacles in the map.</li>
<li>Continue moving to free cells and storing the obstacles it founds.</li>
</ol>

<p>The robot will detect the obstacles that are in front of it and to the sides, but never behind it.</p>

<p>My problem comes when the robot detects an obstacle on a negative cell (like (0,-1). I don't know how to store that obstacle if I have previously stored only the obstacle on ""positive"" cells. So, maybe the ""offset"", it is not a solution here (or maybe I'm wrong).</p>
",8/27/2019 13:07,,1516,6,11,2,0,68571,"Barcelona, Spain",2/19/2009 19:03,4696,57675538,"<p>You can use a <code>std::set</code> to represent a grid layout by using a <code>position</code> class you create. It contains a <code>x</code> and <code>y</code> variable and can therefore be used to intuitively be used to find points inside the grid. You can also use a <code>std::map</code> if you want to store information about a certain location inside the grid.</p>

<p>Please don't forget to fulfill the C++ named requirements for <a href=""https://en.cppreference.com/w/cpp/container/set/set"" rel=""nofollow noreferrer""><code>set</code></a>/<a href=""https://en.cppreference.com/w/cpp/container/map/map"" rel=""nofollow noreferrer""><code>map</code></a> such as <a href=""https://en.cppreference.com/w/cpp/named_req/Compare"" rel=""nofollow noreferrer"">Compare</a> if you don't want to provide a comparison operator externally.</p>

<p>example:
position.h</p>

<pre><code>/* this class is used to store the position of things
 * it is made up by a horizontal and a vertical position.
 */
class position{
private:
    int32_t horizontalPosition;
    int32_t verticalPosition;
public:
    position::position(const int hPos = 0,const int vPos = 0) : horizontalPosition{hPos}, verticalPosition{vPos}{}
    position::position(position&amp; inputPos) : position(inputPos.getHorPos(),inputPos.getVerPos()){}
    position::position(const position&amp; inputPos) : position((inputPos).getHorPos(),(inputPos).getVerPos()){}

    //insertion operator, it enables the use of cout on this object: cout &lt;&lt; position(0,0) &lt;&lt; endl;
    friend std::ostream&amp; operator&lt;&lt;(std::ostream&amp; os, const position&amp; dt){
        os &lt;&lt; dt.getHorPos() &lt;&lt; "","" &lt;&lt; dt.getVerPos();
        return os;
    }

    //greater than operator
    bool operator&gt;(const position&amp; rh) const noexcept{
        uint64_t ans1 = static_cast&lt;uint64_t&gt;(getVerPos()) | static_cast&lt;uint64_t&gt;(getHorPos())&lt;&lt;32;
        uint64_t ans2 = static_cast&lt;uint64_t&gt;(rh.getVerPos()) | static_cast&lt;uint64_t&gt;(rh.getHorPos())&lt;&lt;32;

        return(ans1 &lt; ans2);
    }

    //lesser than operator
    bool operator&lt;(const position&amp; rh) const noexcept{
        uint64_t ans1 = static_cast&lt;uint64_t&gt;(getVerPos()) | static_cast&lt;uint64_t&gt;(getHorPos())&lt;&lt;32;
        uint64_t ans2 = static_cast&lt;uint64_t&gt;(rh.getVerPos()) | static_cast&lt;uint64_t&gt;(rh.getHorPos())&lt;&lt;32;

        return(ans1 &gt; ans2);
    }

    //equal comparison operator
    bool operator==(const position&amp; inputPos)const noexcept {
        return((getHorPos() == inputPos.getHorPos()) &amp;&amp; (getVerPos() == inputPos.getVerPos()));
    }

    //not equal comparison operator
    bool operator!=(const position&amp; inputPos)const noexcept {
        return((getHorPos() != inputPos.getHorPos()) || (getVerPos() != inputPos.getVerPos()));
    }

    void movNorth(void) noexcept{
        ++verticalPosition;
    }
    void movEast(void) noexcept{
        ++horizontalPosition;
    }
    void movSouth(void) noexcept{
        --verticalPosition;
    }
    void movWest(void) noexcept{
        --horizontalPosition;
    }

    position getNorthPosition(void)const noexcept{
        position aPosition(*this);
        aPosition.movNorth();
        return(aPosition);
    }
    position getEastPosition(void)const noexcept{
        position aPosition(*this);
        aPosition.movEast();
        return(aPosition);
    }
    position getSouthPosition(void)const noexcept{
        position aPosition(*this);
        aPosition.movSouth();
        return(aPosition);
    }
    position getWestPosition(void)const noexcept{
        position aPosition(*this);
        aPosition.movWest();
        return(aPosition);
    }

    int32_t getVerPos(void) const noexcept {
        return(verticalPosition);
    }
    int32_t getHorPos(void) const noexcept {
        return(horizontalPosition);
    }
};
</code></pre>

<pre><code>std::set&lt;position&gt; gridNoData;
std::map&lt;position, bool&gt; gridWithData;

gridNoData.insert(point(1,1));
gridWithData.insert(point(1,1),true);

gridNoData.insert(point(0,0));
gridWithData.insert(point(0,0),true);

auto search = gridNoData.find(point(0,0));
if (search != gridNoData.end()) {
    std::cout &lt;&lt; ""0,0 exists"" &lt;&lt; '\n';
} else {
    std::cout &lt;&lt; ""0,0 doesn't exist\n"";
}

auto search = gridWithData.find(point(0,0));
if (search != gridWithData.end()) {
    std::cout &lt;&lt; ""0,0 exists with value"" &lt;&lt; search-&gt;second  &lt;&lt; '\n';
} else {
    std::cout &lt;&lt; ""0,0 doesn't exist\n"";
}
</code></pre>

<p><br>
The above class was used by me in a similar setting and we used a <code>std::map</code> defined as: </p>

<pre><code>std::map&lt;position,directionalState&gt; exploredMap;
</code></pre>

<p>To store if we had found any walls at a certain position.
<br>
By using this <code>std::map</code> based method you avoid having to do math to know what offset you have to have inside an 2D array (or some structure like that). It also allows you to move freely as there is no chance that you'll travel outside of the predefined bounds you set at construction. This structure is also more space efficient against a 2D array as this structure only saves the areas where the robot has been. This is also a C++ way of doing things: relying on the STL instead of creating your own 2D map using C constructs.</p>
",8548828,1,1,101798646,I have updated my question to better clarify my problem.,Programming
136,3546,58625888,Generate probability density after single sensor measurement,|robotics|,"<p>In MCL localization, the robot is able to generate a density map after receiving a single sensor scan. An example is given on page 10 of the following link.</p>

<p><a href=""http://robots.stanford.edu/papers/thrun.robust-mcl.pdf"" rel=""nofollow noreferrer"">http://robots.stanford.edu/papers/thrun.robust-mcl.pdf</a></p>

<p>I wonder how Figure 3 is generated given only a sensor scan and a wall map?</p>
",10/30/2019 12:56,,29,1,0,0,,3284573,"Taichung City, Taiwan",2/7/2014 15:54,75,58629570,"<blockquote>
  <p>Fig. 3c probability distribution for different poses</p>
</blockquote>

<p>It indicates probability of scan <code>o</code> being taken at different locations {x1, x2, ...xn}. </p>

<p><a href=""https://i.stack.imgur.com/M50m7.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M50m7.gif"" alt=""enter image description here""></a></p>

<p>In <code>Fig.3c</code>, these probabilities are plotted such that higher the probability higher the darkness.</p>

<p>I guess these positions of particle are taken from <code>Fig. 6a</code> as example. You can see in <code>Fig. 3c</code> that particles in the corridor have high probability (more dark). </p>
",1595504,0,0,,,Incoming
137,3572,58681419,Ultra96 or Pynq-Z2 kit?,|python|arm|iot|robotics|,"<p>Which one is more suitable for my project? I want to build a human gesture mimicking Robotic arm. The application is to be built using an open source PYNQ framework. It requires me to choose either one of the kits Ultra96 or PYNQ-Z2 kit for my project so that they can provide me with it. I am unable to find the exact difference between the functionalities of the two, and when to use which? Please help me.</p>
",11/3/2019 15:02,,198,1,0,0,,6024781,,3/6/2016 8:44,6,61090911,"<p>I have been using the Z2 board and it would seem to work just fine for your project. Although I can't speak for the ultra96 it's worth noting that it is a community board and so is not officially supported by Pynq. </p>
",1748173,0,0,,,Specifications
138,3578,58744003,Custom addon not displayed in the addons menu in G1ANT studio,|c#|automation|robotics|rpa|g1ant|,"<p>I am trying to  create a new addon but the addon is not being displayed in the addons menu in G1ANT Studio. Even other addons installed from the marketplace are also not displayed. I am using the latest version. I have tried running G1ANT studio as administrator. Yet it makes no difference. </p>

<p>Here is the Addon.cs file of my addon:</p>

<pre><code>using System.Collections.Generic;
using System.Linq;
using System.Text;
using G1ANT.Language;

// Please remember to refresh G1ANT.Language.dll in references

namespace G1ANT.Addon.LibreOffice
{
    [Addon(Name = ""libreoffice"", Tooltip = ""Provides commands to automate LibreOffice"")]
    [Copyright(Author = ""G1ANT LTD"", Copyright = ""G1ANT LTD"", Email = ""support@g1ant.com"", Website = ""www.g1ant.com"")]
    [License(Type = ""LGPL"", ResourceName = ""License.txt"")]
    [CommandGroup(Name = ""calc"", Tooltip = ""Commands connected with creating editing and generally working on calc"")]
    public class LibreOfficeAddon : Language.Addon
    {

        public override void Check()
        {
            base.Check();
            // Check integrity of your Addon
            // Throw exception if this Addon needs something that doesn't exists
        }

        public override void LoadDlls()
        {
            base.LoadDlls();
            // All dlls embeded in resources will be loaded automatically,
            // but you can load here some additional dlls:

            // Assembly.Load(""..."")
        }

        public override void Initialize()
        {
            base.Initialize();
            // Insert some code here to initialize Addon's objects
        }

        public override void Dispose()
        {
            base.Dispose();
            // Insert some code here which will dispose all unnecessary objects when this Addon will be unloaded
        }
    }
}
</code></pre>

<p>The addon also references some other DLLs as dependencies. </p>
",11/7/2019 7:38,58818729,90,2,0,2,,11998441,"Chennai, Tamil Nadu, India",8/30/2019 8:39,42,58818077,"<p>There are no errors in your code. Have you ever compiled the HelloWorld example from this tutorial? <a href=""https://github.com/G1ANT-Robot/G1ANT.Addon.Tutorials/tree/master/G1ANT.Addon.Command.HelloWorld"" rel=""nofollow noreferrer"">https://github.com/G1ANT-Robot/G1ANT.Addon.Tutorials/tree/master/G1ANT.Addon.Command.HelloWorld</a></p>

<p>Remember
1. All dlls in the solution should be marked as ""Resource"" and will be embeded into your addon
2. The target .NET Framework of your project should be 4.6.1</p>
",12360767,3,1,,,Error
139,3586,58808291,Printing serial data from IRobot Create 2,|c++|byte|robotics|,"<p>I'm working on an interface for the irobot create 2 and am having trouble reading a single incoming data packet. Where can I find more detailed information about doing this via the use of termios, read(), write(), and printf()? I'm fairly new to this kind of programming, aside from some robotics projects during college, and am probably missing some key points. Please spare my ignorance.</p>

<p>So far I've successfully confirmed sending of commands which initialize the robot, start it in various modes, start/stop the IO, and turn the robot off. To read the data, which comes in as a single byte, I've sent the commands to locate the designated sensor packet via the write function and this is where I get confused. I've allocated a vector for a single byte and am trying to read() the returned value and then printf() it. I'm not sure which data types to use when printing the value or if I'm even getting what I want. </p>

<pre><code>*yungBot.cpp*
#include ""yungBot.h""
#include &lt;iostream&gt;


/*initialize robot*/
int yungBot::init(const char *yungin){

    fd = open(yungin, O_RDWR | O_NOCTTY | O_NDELAY);
    if(fd == -1){ 
        perror(""Error, failed to connect"");
    }
    else{
        fcntl(fd,F_SETFL,0);
        tcflush (fd, TCIFLUSH);
    }
    struct termios parameters;

    int get = tcgetattr(fd, &amp;parameters);
    if(get == -1){ 
        perror(""Error getting attributes"");
    }
    else{ 
        printf(""%s\n"", ""Get attributes: success"");
    }
    cfmakeraw (&amp;parameters); 
    //sets input and output baud rate
    cfsetispeed(&amp;parameters,B115200);
    cfsetospeed(&amp;parameters,B115200);

    // or forces values to 1; and forces all values to 0 (off); 
    parameters.c_iflag &amp;= ~(IXON | IXOFF); //flow control off; 
    parameters.c_cflag |=(CLOCAL | CREAD);
    parameters.c_cflag &amp;= ~(PARENB | CSTOPB);//no parity 
    parameters.c_cflag &amp;= ~CSIZE; //mask the character bits
    parameters.c_cflag |= (CS8); //8 bit character size 
    parameters.c_oflag = 0;
    parameters.c_lflag = 0;//ICANON=canonical mode
    parameters.c_cc[VMIN] = 0; // 1 input byte is enough to return from read()
    parameters.c_cc[VTIME] = 1;// Timer off 

    //set attribute immediately
    int set = tcsetattr(fd, TCSANOW, &amp;parameters); 
    if(set == -1){
        perror(""Error setting attributes \n"");
    }
    if (fd == -1){
        perror(yungin);
        return -1;
    }   
    usleep(200000);         
    return fd;
}


/*stream desired data packets of 1 unsigned byte*/
int yungBot::stream(int packet){
    unsigned char command[]={142,packet};
    if(write(fd, command, sizeof(command))==-1){
        perror(""failed to retrieve data packet"");
        return -1;
    }
    unsigned char response[1];
    if(read(fd,response,sizeof(response))!=1){
        perror(""failed to write data packet"");
        return -1;
    }
    //shift through the byte for individual bits
    /*unsigned char bit = response[1]; 
    for(int i = 0; i &lt; CHAR_BIT; i++){
    printf(""%d"", (bit&gt;&gt;i)&amp;1);
    }
    */
    printf(""%i"",response[0]);
return response[0];
}
</code></pre>

<p>I'm honestly not sure what to expect. If I read something such as the charge state it seems to work returning a value of 0 when not charged and currently 3 when charged bc the battery is full. When returning something such as the bump and wheel drops sensors (which are sent as a single byte and have a range of 0-15, i'm not sure what to do. I get different values within that range when pressing different parts of the bumper etc..but 4 bits of the byte correspond to 4 different values and the other 4 are ""reserved"". How do I go about reading something like this? I've noticed for the wheel drop/bump sensors that i get certain values depending where I press/lift the robot, for example:</p>

<pre><code>right bump=1
left bump=2
middle bump=3

right drop=4
left drop =8
both dropped = 12
</code></pre>

<p>Is this all i need?</p>
",11/11/2019 20:30,58808429,78,1,0,0,0,12293457,,10/29/2019 19:31,23,58808429,"<p>Oh, this is pretty simple.  You can read those bits with something like...</p>

<pre><code>#define LEFT_BUMPER 1
#define RIGHT_BUMPER 2
#define REAR_BUMPER 4
#define FRONT_BUMPER 8

if(bumper_state &amp; LEFT_BUMPER){ printf(""left bumper actuated\n"") }
if(bumper_state &amp; RIGHT_BUMPER){ printf(""right bumper actuated\n"") }
</code></pre>

<p>The best data type to use us usually an unsigned type of the same size as the thing you're reading, in this case a uint8_t would be good.</p>

<p>Also, you don't need an array, you could read into a single byte if you wanted.  It'll probably help avoid confusion later.</p>
",405312,0,6,,,Connections
140,3588,58836477,Robot won't connect to a new connection on our SQL server to AWS,|sql|automation|robotics|rpa|g1ant|,"<p>I’ve been connecting the G1ANT robot to SQL server and it works fine.
However now I've created a new connection on our SQL server to AWS but it's not connecting to it.</p>

<p>This connection string works fine:</p>

<p><code>Server=GARDSQLDEV01;Database=ColumbusMk1;Trusted_Connection=True;</code></p>

<p>This connection string does not work:</p>

<p><code>Server=columbus-dev.csalh0f00gat.eu-west-2.rds.amazonaws.com,1433;Database=ColumbusMk1; User ID=sa; Password=***;   Trusted_Connection=True;</code></p>

<p>Replaced the password with *** for security reasons.</p>

<p>Please help.</p>
",11/13/2019 11:49,58836538,42,1,0,1,,11697489,,6/25/2019 12:00,7,58836538,"<p>Use special character ‴long text‴ for long connection strings with spaces. It should work fine:</p>

<pre><code>database ‴Server=columbus-dev.csalh0f00gat.eu-west-2.rds.amazonaws.com,
1433;Database=ColumbusMk1; User ID=sa; Password=********;   Trusted_Connection=True;‴
</code></pre>

<p>Good luck! </p>
",11261776,0,0,,,Connections
141,3598,58904495,"FTC Robotics code is slow to start, becasue of ""problem with ""imu""""",|java|robotics|,"<p>I am running some code for my FTC robotics team, but when I press init, an orange error appears: ""problem with ""imu"""". There is no variable in our code named imu, and it is an orange runtime error, which is usually caused by a problem with the rev hub. After pressing run, there is a long delay before anything happens, after which the program runs as normal. What does this error mean?</p>
",11/17/2019 19:44,59814602,296,1,2,1,,11842161,"Moscow, ID",7/26/2019 14:51,25,59814602,"<p>If you are using two rev hubs that are connect together check to make sure that in the I2C Bus 0 there is not two instances of IMU. Because the IMU is used for tasks like compass and gyro if you have two imu's in your configuration the app gets confused.</p>
",11986567,1,0,104071790,Do you know what IMU stands for?,Error
142,3600,58934308,fast light and accurate person-detection algorithm to run on raspberry pi,|python|computer-vision|raspberry-pi3|object-detection|robotics|,"<p>Hope you are doing well.</p>

<p>I am trying to build a following robot which follows a person.
I have a raspberry pi and and a calibrated stereo camera setup.Using the camera setup,i can find depth value of any pixel with respect to the reference frame of the camera. </p>

<p>My plan is to use feed from the camera to detect person and then using the stereo camera to find the average depth value thus calculating distance and from that calculate the position of the person with respect to the camera and run the motors of my robot accordingly using PID.</p>

<p>Now i have the robot running and person detection using HOGdescriptor that comes opencv.But the problem is,even with nomax suppression, the detector is not stable to implement on a robot as too many false positives and loss of tracking occurs pretty often.</p>

<p><strong>So my question is,can u guys suggest a good way to track only people. Mayb a light NN of some sort,as i plan to run it on a raspberry pi 3b+.
I am using intel d435 as my depth camera. 
TIA</strong></p>
",11/19/2019 12:37,58934824,1003,2,1,0,,11381978,,4/19/2019 3:55,36,58934824,"<p>You can use pretrained model. Nowadays there's <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"" rel=""nofollow noreferrer"">plenty</a> of them to choose from. There're also lighter versions for mobile devices. Check <a href=""https://medium.com/@madhawavidanapathirana/real-time-human-detection-in-computer-vision-part-2-c7eda27115c6"" rel=""nofollow noreferrer"">this</a> blog post. It's also worth to check <a href=""https://www.tensorflow.org/lite"" rel=""nofollow noreferrer"">TensorFlow Lite</a>. Some architectures will give you boundig boxes, some masks. Guess you'd be more interested in masks.</p>
",9230562,0,0,104127910,Maybe d435 can't help you match and some sort of IR camera will suit better.,Incoming
143,3615,59229077,Custom Python Transpiler,|python|assembly|robotics|transpiler|lego-mindstorms|,"<p><br/>
I am participating in FLL (FIRST LEGO League, a Mindstorm robotics competition) and this year they piloted a program allow teams to code in Python. I have found a way to disassemble the LEGO Mindstorm block language to the LEGO assembly code. How can I write a python-based transpiler to convert the python to the assembly code or vice versa? I will try to upload the assembly code. </p>

<p>Many thanks for any help you can provide!</p>
",12/7/2019 18:32,,167,0,2,0,,11846654,"New York, NY, USA",7/27/2019 22:35,33,,,,,,104671444,maybe ask author of [Transcrypt](https://www.transcrypt.org/) which transpiles Python to JavaScript,Specifications
144,3623,59476046,Webots - BoundingObject of Robot becomes null after world reload (how to prevent that?),|robotics|webots|,"<p>I've a Robot node with children = [SolidCylinderJoint]. SolidCylinderJoint is a proto that I created, which defines a DEF node as a field, i.e.,</p>
<pre><code>field SFNode geometry DEF BODY Cylinder {
                height 0.1
                radius 0.05
            }
</code></pre>
<p>Now I USE the BODY DEF node as the boundingObject of the robot, like this:</p>
<p><a href=""https://i.stack.imgur.com/9K0G3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9K0G3.png"" alt=""enter image description here"" /></a></p>
<p>Now, this works great, but as soon as I hit &quot;Reload World&quot; or restart webots, the boundingObject becomes NULL again. I think this is happening because the robot node is loaded before the Proto, and at the time it's trying to set the boundingObject to BODY, it doesn't find that definition and hence defaults to NULL.</p>
<p>World file: <a href=""https://pastecode.xyz/view/fab1533d"" rel=""nofollow noreferrer"">https://pastecode.xyz/view/fab1533d</a></p>
<p>Proto file: <a href=""https://pastecode.xyz/view/f558d13c"" rel=""nofollow noreferrer"">https://pastecode.xyz/view/f558d13c</a></p>
",12/25/2019 7:08,59576668,151,1,0,1,,1210650,"Seattle, WA, USA",2/15/2012 6:49,483,59576668,"<p>First, there is an issue in your PROTO, you are not allowed to make an IS in the default argument of the fields (i.e. baseColor IS baseColor):</p>

<pre><code>field SFVec3f baseColor 0.985946 0 0.0481575
field SFNode appearance PBRAppearance { baseColor IS baseColor metalness 0.3 }
</code></pre>

<p>About the issue with the DEF-USE, this is indeed a bug, it seems default argument of the PROTO are created after the root node and therefore not found at the creation of the root node.
I have reported this here and hopefully it will be fixed in the next version of Webots:
<a href=""https://github.com/cyberbotics/webots/issues/1231"" rel=""nofollow noreferrer"">https://github.com/cyberbotics/webots/issues/1231</a></p>
",8427891,1,0,,,Error
145,3630,59866125,"Find the next 3D point given a starting point, a orientation quaternion, and a distance travelled",|math|quaternions|robotics|slam|,"<p>What is the formula I need to use to find the second 3D point (P1) given:</p>

<ol>
<li>The first point P0 = [x0, y0, z0]</li>
<li>An orientation quaternion Q0 = [q0, q1, q2, q3]</li>
<li>The distance traveled S</li>
</ol>

<p>I'm guessing that the distance traveled S needs to be split up into it's constituent X, Y and Z components. Is there an easy way to do this using quaternions?</p>
",1/22/2020 18:27,59872425,837,1,0,0,0,12448898,"Vancouver, BC, Canada",11/28/2019 0:20,20,59872425,"<p>Components of direction vector (forward-vector) are:</p>

<pre><code>x = 2 * (q1*q3 + q0*q2)
y = 2 * (q2*q3 - q0*q1)
z = 1 - 2 * (q1*q1 + q2*q2)
</code></pre>

<p>This formula is calculated from Quaternion-to-Matrix (below) with multiplication by <code>(0,0,1)</code> vector. </p>

<p>Normalize <code>D=(x,y,z)</code> if it is not unit, and calculate <code>P_New.x= P0.x + S * D.x</code> and other components.</p>

<hr>

<p>To get up- and left- vector of orientation (perhaps your orientation refers to another base frame orientation - OX or OY as forward), use another columns of  the matrix cited below:</p>

<p><a href=""https://www.euclideanspace.com/maths/geometry/rotations/conversions/quaternionToMatrix/index.htm"" rel=""nofollow noreferrer"">Link:</a>
Quaternion multiplication and orthogonal matrix multiplication can both be used to represent rotation. If a quaternion is represented by <code>qw + i qx + j qy + k qz</code> , then the equivalent matrix, to represent the same rotation, is:</p>

<pre><code>1 - 2*qy2 - 2*qz2   2*qx*qy - 2*qz*qw   2*qx*qz + 2*qy*qw
2*qx*qy + 2*qz*qw   1 - 2*qx2 - 2*qz2   2*qy*qz - 2*qx*qw
2*qx*qz - 2*qy*qw   2*qy*qz + 2*qx*qw   1 - 2*qx2 - 2*qy2
</code></pre>
",844416,0,3,,,Coordinates
146,3632,60002484,Fusing asynchronous measurements from multiple sensors,|embedded|sensors|robotics|sensor-fusion|,"<p>I have a set of 12 IMUs mounted on the end effector of my robot arm which I read using a micro controller to determine it's movement. With my controller I can read two sensors simultaneously using direct memory access. After acquiring the measurements I would like to fuse them to make up for the sensor error and generate a more reliable reading than having only one sensor.</p>

<p>After some research my understanding is that I can use a Kalman filter to reach my desired outcome, but still have the problem of all the sensor values having different time stamps, since I can read only two at a time and even if both time stamps will be synchronized perfectly, the next pair will have a different time stamp if only in the µs range. </p>

<p>Now I know controls engineering principles but am completely new to the topic of sensor fusion and google presents me with too many results to find a solution in a reasonable amount of time. 
Therefore my question, can anybody point me into the right direction by naming me a certain keyword I need to look for or literature I should work through to better understand that topic, please?</p>

<p>Thank you!</p>
",1/31/2020 10:52,60005677,254,1,0,0,,9395961,Germany,2/22/2018 11:18,3,60005677,"<p>The topic you are dealing with is not an easy one. Try to have a look at the multi-rate kalman filters.</p>

<p>The idea is that you design different kalman filters for each combination of sensor that you can available at the same time, and use it when you have the data from those sensors, while the system state is passed between the various filters.  </p>
",12753914,0,1,,,Incoming
147,3633,60086354,Finding fastest path for robotic drive base,|algorithm|physics|path-finding|robotics|,"<p>I am trying to create an algorithm to find the fastest path a robot can take between 2 points in terms of time. The robot I would be using would be powered by a drive wheel on each side and have limited acceleration and velocity. I was also hoping to build in some obstacle avoidance so the robot is able to path around obstacles. I am familiar with pathfinding algorithms such as a* that find the fastest path in terms of distance between two points but this algorithm does not always find the fastest path for a robot with bounded acceleration and velocity.</p>

<p>I've been thinking about this for a couple days and honestly I'm not really sure where to start or where to find resources on the topic so any help is appreciated.</p>

<p>Example:</p>

<p><a href=""https://i.stack.imgur.com/21ZXz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/21ZXz.png"" alt=""img""></a></p>

<p>Lets say our robot is 10 units wide and each wheel has a maximum speed of 100 units/second and a maximum acceleration of 10 units/second/second.</p>

<p>At point A (x = 0, y = 0) the velocities of the robots wheels are 50 and 30 for the left and right wheels respectively and the robot is at a 30 degree angle relative to the x-axis. At point B (x = 1000, y = -600) we want the robot to be stationary and have a -75 degree angle relative to the x-axis</p>

<p>What is the most time efficient path for the robot to take from point A to point B given it's starting and ending velocities and headings while also avoiding the obstacles?</p>
",2/6/2020 1:11,,125,0,5,1,,10887179,,1/9/2019 1:12,3,,,,,,106272787,The search terms `hybrid a*` and `hybrid a* stanford` get some interesting hits.,Moving
148,3660,60640639,Minibrick8 doesnt connect to computer via RS-232 serial cable when there is a Snap-action-switch (microswitch) attatched,|robotics|dmx512|,"<p>I am using a MiniBrick8 (from Gilderfluke &amp; Company in Burbank, California), as part of a lighting/ projector rig. The MiniBrick8 needs an input from a button (using a switch) to start a program running (PCMacs). At the moment the MiniBrick8 is also connected to a BrightSign Media Player.</p>

<p>Minibrick8 doesnt connect to computer via RS-232 serial cable when there is a Snap-action-switch (microswitch) attatched and drawing power from the '9-24 VDC' screw output.</p>

<p>The switch has two output parts: NC, NO, and a power input (COM). (NC and NO being normally closed and normally open respectively).</p>

<p>The NC is connected to ground, so when the switch is not pressed down power is routed directly from the VDC through the switch into ground. Then when the button is pressed power is instead routed from the switch into the MiniBrick8's A-input.</p>

<p>I then take the A-inputs return cable and ground it.</p>

<p>Maybe the fact that while the switch is not pressed down current is constantly flowing through the switch into ground? Is this a problem?</p>

<p>Thank you for your help.</p>
",3/11/2020 16:32,60726352,22,1,0,0,,8564483,"Dorset, United Kingdom",9/5/2017 15:28,8,60726352,"<p>SOLVED:</p>

<p>It was perhaps as I thought.</p>

<p>I changed the switch to get a power input into the NO (normally open) connector, leaving the NC (normally closed) connector without any cable attached, connecting the output via the COM connector to one of the A-trigger/input terminals (the other going to ground).</p>

<p>I had previously thought (incorrectly obviously) that all the switch's connectors had to have wires connecting them to something.</p>

<p>Here is a source: <a href=""https://youtu.be/pf_Mngbx32w"" rel=""nofollow noreferrer"">YouTube Video: https://youtu.be/pf_Mngbx32w</a></p>

<p>^^ This helped me figure it out.</p>
",8564483,1,0,,,Connections
149,3663,60682233,Problem with robot orientation due to euler angles,|python|ros|robotics|euler-angles|gazebo-simu|,"<p>I am trying to implement a basic path planning algorithm using python with a 4 wheels robot created for ROS and Gazebo, RViz.</p>

<p>The only thing required for my algorithm is to make my robot oriented towards a given (using mouse) point in the x,y plane.</p>

<p>The problem I am facing is that I am converting from quaternion to Euler angles and I always (no mater how I manipulate my code) come to a situation like the following:</p>

<pre><code>Theta to target:257.106918658
Theta_deg:-179.85
dTheta:-436.96
Theta to target:257.106918658
Theta_deg:-179.85
dTheta:-436.96
Theta to target:257.118158708
Theta_deg:179.99
dTheta:-77.13
Theta to target:257.118158708
Theta_deg:179.99
dTheta:-77.13
Theta to target:257.118158708
Theta_deg:179.99
dTheta:-77.13
Theta to target:257.118158708
Theta_deg:179.99
</code></pre>

<p>So:</p>

<p>Theta to target: Euler [-180, 180] Angle in [deg] of the line connecting the center of mass of robot to the given point.</p>

<p>Theta_deg: Euler [-180, 180] Angle in [deg] of the orientation (vertical vector on the front face of the robot) to X axis.</p>

<p>As you can see from the data above, <code>Theta_deg</code> experiences a non-continuous step from:</p>

<pre><code>Theta_deg:-179.99
</code></pre>
Minibrick8
<p>to</p>

<pre><code>Theta_deg:179.99
</code></pre>

<p>I know that this is an issue with using Euler angles. 
How can I overcome this issue?</p>

<p>Relative code:</p>

<pre><code>while not rospy.is_shutdown():
    theta_deg = (round(180*(theta/math.pi),2))
    old_distance = round(distance_to_target,2)

    theta_to_target =180*(math.pi/2+math.atan2((y-goal.y),(x-goal.x)))/math.pi        
    dTheta =(round(((theta_deg)-(theta_to_target)),2))

    dtt = round((math.sqrt((goal.x - x)**2 + (goal.y - y)**2)),2)

    if dtt &lt; 0.1:
        print ""&lt;&lt;&lt; Destination Reached &gt;&gt;&gt;""
    else:
        theta_to_target =180*(math.pi/2+math.atan2((y-goal.y),(x-goal.x)))/math.pi
        dTheta =(round(((theta_deg)-(theta_to_target)),2))
        print ""Theta to target:"" + str(theta_to_target)
        print ""Theta_deg:"" + str(theta_deg)
        print ""dTheta:"" + str(dTheta)
        # if dTheta&lt;-360: dTheta=dTheta+360
        # if dTheta&gt;360: dTheta=dTheta-360

        if abs(dTheta)&gt;10:
            if abs(theta_to_target - theta_deg)&gt;180:
                speed.angular.z = -0.2
                speed.linear.x = 0.0
                pub.publish(speed)
            else:
                speed.angular.z = +0.2
                speed.linear.x = 0.0
                pub.publish(speed)
        else:
            speed.linear.x = 0.6


    pub.publish(speed)
    r.sleep()
</code></pre>
",3/14/2020 11:29,,464,0,2,0,,12969878,,2/26/2020 23:42,10,,,,,,107371483,"Instead of fixing the maths aspect, can you make it so that your robot always goes in the shortest rotation direction? For example from `-179.99` to `+179.99` degrees would not be a full turn but `-0.02` degrees. I think `dθ = min(dθ%360, dθ%360 + 360, key=abs)` would work.",Coordinates
150,3665,60883932,Controller_manager : [ERROR] Controler Spawner couldn't find the expected controller_manager ROS interface,|controller|runtime-error|ros|robot|gazebo-simu|,"<p>I know there is already a lot of questions on this particular error but none of the ones i found solved the issue for me...</p>

<p>I'm trying to implement the ROS differential drive controller for a two wheeled robot in gazebo, but when launching the controller spawner I get the following output :</p>

<pre><code>[INFO] [1585302057.569863, 0.000000]: Controller Spawner: Waiting for service controller_manager/load_controller
[WARN] [1585302087.735023, 40.162000]: Controller Spawner couldn't find the expected controller_manager ROS interface.
</code></pre>

<p>In fact, trying to list controller_manager services gives no output :</p>

<pre><code>$ rosservice list | grep controller_manager
$
</code></pre>

<p>I'm running ros melodic on Ubuntu 18.04.</p>

<p>Here is my config file <code>diff_drive.yaml</code>:</p>

<pre><code>wheelchair_controler:
  type        : ""diff_drive_controller/DiffDriveController""
  left_wheel  : 'left_wheel_motor'
  right_wheel : 'right_wheel_motor'
  publish_rate: 50.0               # default: 50
  pose_covariance_diagonal : [0.001, 0.001, 1000000.0, 1000000.0, 1000000.0, 1000.0]
  twist_covariance_diagonal: [0.001, 0.001, 1000000.0, 1000000.0, 1000000.0, 1000.0]

  # Wheel separation and diameter. These are both optional.
  # diff_drive_controller will attempt to read either one or both from the
  # URDF if not specified as a parameter
  wheel_separation : 0.52
  wheel_radius : 0.3048

  # Wheel separation and radius multipliers
  wheel_separation_multiplier: 1.0 # default: 1.0
  wheel_radius_multiplier    : 1.0 # default: 1.0

  # Velocity commands timeout [s], default 0.5
  cmd_vel_timeout: 0.25

  # Base frame_id
  base_frame_id: base_link #default: base_link

  # Velocity and acceleration limits
  # Whenever a min_* is unspecified, default to -max_*
  linear:
    x:
      has_velocity_limits    : true
      max_velocity           : 1.0  # m/s
      min_velocity           : -0.5 # m/s
      has_acceleration_limits: true
      max_acceleration       : 0.8  # m/s^2
      min_acceleration       : -0.4 # m/s^2
      has_jerk_limits        : true
      max_jerk               : 5.0  # m/s^3
  angular:
    z:
      has_velocity_limits    : true
      max_velocity           : 1.7  # rad/s
      has_acceleration_limits: true
      max_acceleration       : 1.5  # rad/s^2
      has_jerk_limits        : true
      max_jerk               : 2.5  # rad/s^3

</code></pre>

<p>Here is the launch file. I tried to put the spawner in a separate launch file to make sure gazebo add time to launch properly.</p>

<pre><code>&lt;?xml version=""1.0""?&gt;
&lt;launch&gt;

  &lt;!-- Controllers --&gt;
  &lt;rosparam file=""$(find wheelchair_simulation)/config/diff_drive.yaml"" command=""load"" /&gt;
  &lt;node name=""wheelchair_controller_spawner"" pkg=""controller_manager"" type=""spawner"" respawn=""false"" output=""screen"" args=""wheelchair_controler"" /&gt;


&lt;/launch&gt;

</code></pre>

<p>In my <code>.xacro</code> file I use a macro to define my wheels, joints, and gazebo tags. I also added a transmission inside it :</p>

<pre><code>&lt;xacro:macro name=""main_wheel"" params=""prefix reflect""&gt;
    &lt;link name=""main_${prefix}_wheel""&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;cylinder length=""${main_wheel_length}"" radius=""${main_wheel_radius}""/&gt;
        &lt;/geometry&gt;
        &lt;material name=""black""/&gt;
      &lt;/visual&gt;
      &lt;collision&gt;
        &lt;geometry&gt;
          &lt;cylinder length=""${main_wheel_length}"" radius=""${main_wheel_radius}""/&gt;
        &lt;/geometry&gt;
      &lt;/collision&gt;
      &lt;inertial&gt;
        &lt;xacro:cylinder_inertia length=""${main_wheel_length}"" radius=""${main_wheel_radius}"" weight=""${main_wheel_mass}""/&gt;
      &lt;/inertial&gt;
    &lt;/link&gt;

    &lt;joint name=""${prefix}_wheel_motor"" type=""continuous""&gt;
      &lt;axis xyz=""0 0 1""/&gt;
      &lt;parent link=""base_link""/&gt;
      &lt;child link=""main_${prefix}_wheel""/&gt;
      &lt;origin rpy=""${-reflect*1.5708} 0 0"" xyz=""0 ${reflect*((total_width - main_wheel_length)/2 + 0.001)} 0""/&gt;
      &lt;gazebo&gt;
        &lt;implicitSpringDamper&gt;true&lt;/implicitSpringDamper&gt;
      &lt;/gazebo&gt;
      &lt;dynamic friction=""0.1""/&gt;
    &lt;/joint&gt;

    &lt;transmission name=""${prefix}_wheel_transmission""&gt;
      &lt;type&gt;transmission_interface/SimpleTransmission&lt;/type&gt;
      &lt;joint name=""${prefix}_wheel_motor""&gt;
        &lt;hardwareInterface&gt;hardware_interface/VelocityJointInterface&lt;/hardwareInterface&gt;
      &lt;/joint&gt;
      &lt;actuator name=""${prefix}_wheel_motor""&gt;
        &lt;mechanicalReduction&gt;1&lt;/mechanicalReduction&gt;
        &lt;hardwareInterface&gt;VelocityJointInterface&lt;/hardwareInterface&gt;
      &lt;/actuator&gt;
    &lt;/transmission&gt;

    &lt;gazebo reference=""main_${prefix}_wheel""&gt;
      &lt;mu1&gt;0.8&lt;/mu1&gt;
        &lt;mu2&gt;0.8&lt;/mu2&gt;
      &lt;turnGravityOff&gt;false&lt;/turnGravityOff&gt;
      &lt;material&gt;Gazebo/Black&lt;/material&gt;
    &lt;/gazebo &gt;
  &lt;/xacro:macro&gt;
</code></pre>

<p>I made sure to install <code>gazebo_ros_controle</code>:</p>

<pre><code>$ sudo apt-get install ros-melodic-gazebo-ros-controle
</code></pre>

<p>And to link it in my description file :</p>

<pre><code>&lt;gazebo&gt;
        &lt;plugin name=""gazebo_ros_control"" filename=""libgazebo_ros_contol.so""&gt;
        &lt;/plugin&gt;
&lt;/gazebo&gt;
</code></pre>

<p>Finally, I checked the dependencies and everything looks ok :</p>

<pre><code>$ rosdep check controller_manager
All system dependencies have been satisified
</code></pre>

<p>EDIT : I add the description of the base_link and base_footprint links, in case it's necessary as I saw somewhere that the frame for the controller must have inertia</p>

<pre><code>&lt;!-- Dummy link because first link should not have any inertia. Located on the ground between both wheels for easier control --&gt;

  &lt;link name=""base_footprint""/&gt;

&lt;!-- Footprint and main inertia of the chair --&gt;

  &lt;link name=""base_link""&gt;
    &lt;visual&gt;
      &lt;geometry&gt;
        &lt;box size=""${total_length} ${total_width} ${seat_height - ground_clearence}""/&gt;
      &lt;/geometry&gt;
      &lt;origin xyz=""${-main_wheel_radius + total_length/2} 0 ${a}""/&gt;
      &lt;material name=""white""/&gt;
    &lt;/visual&gt;
    &lt;collision&gt;
      &lt;geometry&gt;
        &lt;box size=""${total_length} ${total_width} ${seat_height - ground_clearence}""/&gt;
      &lt;/geometry&gt;
      &lt;origin xyz=""${-main_wheel_radius + total_length/2} 0 ${a+0.1}""/&gt;
    &lt;/collision&gt;
    &lt;inertial&gt;
      &lt;xacro:box_inertia height=""${seat_height - ground_clearence}"" length=""${total_length}"" width=""${total_width}"" weight=""${total_mass-2*main_wheel_mass}""/&gt;
    &lt;/inertial&gt;
  &lt;/link&gt;

  &lt;joint name=""base_link_joint"" type=""fixed""&gt;
    &lt;parent link=""base_footprint""/&gt;
    &lt;child link=""base_link""/&gt;
    &lt;origin xyz=""${-total_length/2 + main_wheel_radius} 0 ${main_wheel_radius}""/&gt;
  &lt;/joint&gt;
</code></pre>

<p>I sincerely hope some of you can find the issue because I have no clue about where it comes from... Feel free to ask any missing details.</p>

<p>Thank you in advance !!</p>
",3/27/2020 10:14,,5974,2,0,3,,13134356,"Cannes, France",3/27/2020 9:45,25,63610055,"<p>Please check gazebo log. If there is a problem with the urdf / xacro file, gazebo is not going to initialize the robot simulation interface, and will not start the gazebo_ros_control plugin.</p>
<p>Here you have an example. Once I corrected the urdf file, the controller load and no longer I had this error.</p>
<pre class=""lang-none prettyprint-override""><code>[ INFO] [1598502340.159974962]: waitForService: Service [/gazebo/set_physics_properties] is now available.
[ INFO] [1598502340.212629654]: Physics dynamic reconfigure ready.

[ INFO] [1598502340.568106895]: Loading gazebo_ros_control plugin

[ INFO] [1598502340.568395067]: Starting gazebo_ros_control plugin in namespace: /plotter

[ INFO] [1598502340.569824694]: gazebo_ros_control plugin is waiting for model URDF in parameter [/robot_description] on the ROS param server.

[ERROR] [1598502340.698456387]: This robot has a joint named &quot;link_00__link_01&quot; which is not in the gazebo model.

[FATAL] [1598502340.698639178]: Could not initialize robot simulation interface
</code></pre>
<p>As you can see in the two last lines, gazebo is not finishing ok.</p>
",13939780,0,0,,,Error
151,3678,61001497,How to publish odom (nav_msgs/Odometry) from MD49 encoders outputs?,|python|ros|robotics|odometry|,"<p>I am using MD49 Motor Drive with its motors</p>

<p><a href=""https://www.robot-electronics.co.uk/htm/md49tech.htm"" rel=""nofollow noreferrer"">https://www.robot-electronics.co.uk/htm/md49tech.htm</a></p>

<p><a href=""http://wiki.ros.org/md49_base_controller"" rel=""nofollow noreferrer"">http://wiki.ros.org/md49_base_controller</a></p>

<p>How to subscribe (encoder_l and encoder_r) from md49_base_controller package and publish (vx , vy ,and vth ) as a form odom (nav_msgs/Odometry) ?</p>

<p><strong>There are two problem:</strong></p>

<p>1-The first is that the encoders outputs are not correct ""the package needs to be modified.</p>

<p>2-The second is the I want to create a package that subscribe the right and left wheels encoder counts (encoder_l and encoder_r) and publish (vx , vy ,and vth) as a form odom (nav_msgs/Odometry) to be compatable wth imu MPU9250</p>

<p><a href=""http://wiki.ros.org/robot_pose_ekf"" rel=""nofollow noreferrer"">http://wiki.ros.org/robot_pose_ekf</a></p>

<p><strong>The proposed package is:</strong></p>

<p>1- We have to convert  (encoder_l and encoder_r)  to (RPM_l and RPM_r) as follow:</p>

<pre><code>if (speed_l&gt;128){newposition1 = encoder_l;}
else if  (speed_l&lt;128){ newposition1 = 0xFFFFFFFF-encoder_l;}
else if  (speed_l==128) {newposition1=0;}

newtime1 = millis();
RPM_l = ((newposition1-oldposition1)*1000*60)/((newtime1-oldtime1)*980);
oldposition1 = newposition1;
oldtime1 = newtime1;
delay(250);

if (speed_r&gt;128){ newposition2 = encoder_r;}
else if  (speed_r&lt;128){ newposition2 = 0xFFFFFFFF-encoder_r;}
else if   (speed_r==128) { newposition2=0;}
newtime2 = millis();
RPM_r = ((newposition2-oldposition2)*1000*60)/((newtime2-oldtime2)*980);
oldposition2 = newposition2;
oldtime2= newtime2;
delay(250);
</code></pre>

<p>2- We have to convert (RPM_l and RPM_r) to (vx, vy, and vth) as follow:</p>

<pre><code>vx=(r/2)*RPM_l*math.cos(th)+(r/2)*RPM_r*math.cos(th);
vx=(r/2)*RPM_l*math.sin(th)+(r/2)*RPM_r*math.sin(th);
vth=(r/B)*omega_l-(r/B)*omega_r;
</code></pre>

<p>Hint: r and B are wheel radius and vehicle width respectively.</p>

<p>3- The odom (nav_msgs/Odometry) package is:</p>

<pre><code>#!/usr/bin/env python

import math
from math import sin, cos, pi

import rospy
import tf
from nav_msgs.msg import Odometry
from geometry_msgs.msg import Point, Pose, Quaternion, Twist, Vector3
from md49_messages.msg import md49_encoders

rospy.init_node('odometry_publisher')

odom_pub = rospy.Publisher(""odom"", Odometry, queue_size=50)
odom_broadcaster = tf.TransformBroadcaster()


x = 0.0
y = 0.0
th = 0.0

vx =0.1
vy = -0.1
vth = 0.1

current_time = rospy.Time.now()
last_time = rospy.Time.now()

r = rospy.Rate(1.0)
while not rospy.is_shutdown():
    current_time = rospy.Time.now()


    # compute odometry in a typical way given the velocities of the robot
    dt = (current_time - last_time).to_sec()
    delta_x = (vx * cos(th) - vy * sin(th)) * dt
    delta_y = (vx * sin(th) + vy * cos(th)) * dt
    delta_th = vth * dt

    x += delta_x
    y += delta_y
    th += delta_th

    # since all odometry is 6DOF we'll need a quaternion created from yaw
    odom_quat = tf.transformations.quaternion_from_euler(0, 0, th)

    # first, we'll publish the transform over tf
    odom_broadcaster.sendTransform(
        (x, y, 0.),
        odom_quat,
        current_time,
        ""base_link"",
        ""odom""
    )

    # next, we'll publish the odometry message over ROS
    odom = Odometry()
    odom.header.stamp = current_time
    odom.header.frame_id = ""odom""

    # set the position
    odom.pose.pose = Pose(Point(x, y, 0.), Quaternion(*odom_quat))

    # set the velocity
    odom.child_frame_id = ""base_link""
    odom.twist.twist = Twist(Vector3(vx, vy, 0), Vector3(0, 0, vth))

    # publish the message
    odom_pub.publish(odom)

    last_time = current_time
    r.sleep()
</code></pre>
",4/2/2020 21:32,,3802,2,0,0,,6020636,,3/4/2016 23:16,33,61122286,"<p>First off, you need to import nav_msgs/Odometry as the following:</p>

<p><strong>from nav_msgs.msg import Odometry</strong></p>

<p>You must have a function that performs those conversions and then in rospy.Subscriber import those variables, like this:</p>

<pre><code>def example(data):
    data.vx=&lt;conversion&gt;
    data.vth=&lt;conversion&gt;

def listener():
    rospy.Subscriber('*topic*', Odometry, example)
    rospy.spin()

if __name__ == 'main':
    listener()
</code></pre>

<p>I think this would work</p>
",13185389,0,1,,,Coordinates
152,3681,61055315,Change speed of TestFX robots?,|java|javafx|robot|testfx|,"<p>Specifically the <code>WriteRobot</code> / <code>WriteRobotImpl</code>. It seems to write things rather slowly and I'd like to make it write faster.</p>

<p><strong>Edit</strong><br>
In response to M.S.'s comment I tried this (NB at this point I hadn't worked out that <code>WriteRobot</code> was involved, not <code>TypeRobot</code>):</p>

<pre><code>setup(){
...
    setFinalStatic( org.testfx.robot.impl.TypeRobotImpl.class.getDeclaredField(""SLEEP_AFTER_KEY_CODE_IN_MILLIS""), 5 );
}
...
static void setFinalStatic(Field field, Object newValue) throws Exception {
    field.setAccessible(true);
    Field modifiersField = Field.class.getDeclaredField(""modifiers"");
    modifiersField.setAccessible(true);
    modifiersField.setInt(field, field.getModifiers() &amp; ~Modifier.FINAL);
    field.set(null, newValue);
}
</code></pre>

<p>Unfortunately it appears to make no difference to the typing speed, even when set to 1 ms.</p>

<p><strong>Edit</strong><br>
I note the comment by Slaw.</p>

<p>I set the <code>System</code> property <code>testfx.robot.write_sleep</code> before running the test: this had no effect, despite one being able to see that it might have, from the source code at the top of WriteRobotImpl.java (see below). When I set this to 500 ms it also had no effect, making me conclude that the property was not being seen by the code there for some reason, so the default 25 ms was being set.</p>

<p>NB possible other causes: following the code there, it appears that <code>WriteRobot.write</code> always results in a call to <code>WriteRobot.typeCharacterInScene</code>, which in turn calls <code>BaseRobot.typeKeyboard</code> and <code>WaitForAsyncUtils.waitForFxEvents</code>. The latter might be a ""difficult customer"": if each pressed key then has to ""wait for events"" to bubble up, there may well be nothing to be done about things. </p>

<p>Still trying to work out why the following lines at the top of org.testfx.robot.impl.WriteRobotImpl.java would fail to see the <code>System</code> property:</p>

<pre><code>private static final int SLEEP_AFTER_CHARACTER_IN_MILLIS;

static {
    int writeSleep;
    try {
        writeSleep = Integer.getInteger(""testfx.robot.write_sleep"", 25);
    }
    catch (NumberFormatException e) {
        System.err.println(""\""testfx.robot.write_sleep\"" property must be a number but was: \"""" +
                System.getProperty(""testfx.robot.write_sleep"") + ""\"".\nUsing default of \""25\"" milliseconds."");
        e.printStackTrace();
        writeSleep = 25;
    }
    SLEEP_AFTER_CHARACTER_IN_MILLIS = writeSleep;
}
</code></pre>

<p>I also wondered whether maybe that <code>static{...}</code> code block happens so early than you need to set the <code>System</code> property before the tests are run. I tried setting this property in gradle.build. Still no success.</p>
",4/6/2020 7:58,,580,0,10,0,,595305,"London, United Kingdom",1/29/2011 21:24,2736,,,,,,108025333,[WriteRobotImpl](https://github.com/TestFX/TestFX/blob/ca90e066d9339282973e3872212d608402a8d9d6/subprojects/testfx-core/src/main/java/org/testfx/robot/impl/WriteRobotImpl.java)?,Actuator
153,3692,61319036,How to control a physical robot using a web interface,|ubuntu-18.04|robot|ros2|,"<p>I would like to know how I could control the movement of a physical robot using a web interface. For example, I have created a web interface with four movement buttons (front, back, left, right) but do not know how to connect that interface to the physical robot and control its movements. I have experience in controlling a simulated Turtlebot (in Gazebo) with the interface locally on my laptop using ROSBRIDGE and SimpleHTTPServer. Would I have to use these as well to control a physical robot?</p>

<p>I'm running ROS2 Crystal, Ubuntu 18.04. Thank you!</p>
",4/20/2020 9:22,,512,2,0,1,,13151812,,3/30/2020 9:29,8,61363449,"<p>Yes,  The interface to control a physical robot would be the same as simulation.<br>
You will need to to publish control command to <code>/cmd_vel</code> topic and then you can subscribe to the topic to convert those velocity commands to actual motor commands.   </p>

<p>You can also look into using <a href=""http://robotwebtools.org/"" rel=""nofollow noreferrer"">Robot Web Tools</a> for the web interface.   </p>

<p>Additionally if you could provide more detiails about your setup I could give more information.</p>
",2661982,2,0,,,Remote
154,3694,61363106,ROS move_base node is generating pre-Hydro warning,|c++|navigation|ros|robotics|catkin|,"<p>I am working with move_base from navigation stack. However, I am getting the warning that</p>

<p>""local_costmap: preHydro parameter ""static_map"" unused since ""plugins"" is provided""</p>

<p>In terms of costmap definition here are the common and local config files I have been using:</p>

<pre><code>footprint: [ [-0.15,-0.15], [0.15,-0.15], [0.15,0.15], [-0.15,0.15] ]
transform_tolerance: 0.5
map_type: costmap
obstacle_layer:
 enabled: true
 obstacle_range: 3.0
 raytrace_range: 3.5
 inflation_radius: 0.2
 track_unknown_space: false
 combination_method: 1
 observation_sources: laser_scan_sensor
 laser_scan_sensor: {sensor_frame: scanmatcher_frame, data_type: LaserScan, topic: /scan, marking: true, clearing: false}
inflation_layer:
  enabled:              true
  cost_scaling_factor:  1.0  
  inflation_radius:     0.2
obstacle_layer:
     enabled: true
     obstacle_range: 5.0
     raytrace_range: 1.0
     observation_sources: ""/scan""
     observation_persistence: 0.0
     inf_is_valid: false
     scan:
       data_type: LaserScan
       topic: scan

local_costmap:
 global_frame: map
 robot_base_frame: base_link
 update_frequency: 0.5
 publish_frequency: 0.25
 static_map: false
 rolling_window: true
 width: 50
 height: 50
 width: 8
 height: 8
 origin_x: -4
 origin_y: -4
 resolution: 0.1
 transform_tolerance: 0.5 
 plugins:
   - {name: inflation_layer,        type: ""costmap_2d::InflationLayer""}
   - {name: obstacle_layer,      type: ""costmap_2d::ObstacleLayer""}
</code></pre>

<p>Now, I have followed the navigation tutorial page which exactly addresses this issue but without success. Interestingly,my global costmap throws the same warning while receiving the map correctly with a message that ""Recieved a 250x250 map at 0.1 m/px"". My global yaml file looks like this:</p>

<pre><code>global_costmap:
 global_frame: map
 robot_base_frame: base_link
 update_frequency: 0.5
 publish_frequency: 0.25
 always_send_full_costmap: true
 width: 250
 height: 250
 origin_x: -125
 origin_y: -125
 resolution: 0.1
 static_map: true
 plugins:
   - {name: static_layer,        type: ""costmap_2d::StaticLayer""}
</code></pre>
",4/22/2020 10:40,61363290,5569,1,0,0,,13184375,,4/1/2020 13:17,18,61363290,"<p>The <code>static_map</code> parameter has been deprecated and is no longer used.</p>

<p>Just remove the <code>static_map: false</code> line inside the <code>local_costmap</code> parameters to make the warning go away.</p>
",2661982,0,1,,,Other
155,3695,61379723,Recovery behavior clears the obstacle layer,|c++|navigation|mapping|ros|robot|,"<p>I am using RP-Lidar /scan topic together with move_base from navigation stack. Although the obstacle layer is parameterized to get the LaserScan type data from /scan topic, I am recieving the message that ""Recover behavior will clear layer 'obstacles'"". I would like to mention that the UniTest over /scan topic and /odom topic are working fine. Thus in my RVIZ, the obstacle are not shown neither the planner takes them into account to prevent a collision.
For clarity here is my common config file:</p>

<pre><code>footprint: [ [-0.15,-0.15], [0.15,-0.15], [0.15,0.15], [-0.15,0.15] ]
transform_tolerance: 0.5
map_type: costmap
obstacle_layer:
 enabled: true
 obstacle_range: 3.0
 raytrace_range: 3.5
 inflation_radius: 0.2
 track_unknown_space: false
 combination_method: 1
 observation_sources: laser_scan_sensor
 laser_scan_sensor: {sensor_frame: scanmatcher_frame, data_type: LaserScan, topic: /scan, marking: true, clearing: false}
inflation_layer:
  enabled:              true
  cost_scaling_factor:  1.0  
  inflation_radius:     0.2
obstacle_layer:
     enabled: true
     obstacle_range: 5.0
     raytrace_range: 1.0
     observation_sources: ""/scan""
     observation_persistence: 0.0
     inf_is_valid: false
     scan:
       data_type: LaserScan
       topic: scan  
</code></pre>
",4/23/2020 5:08,61400149,1761,1,0,0,,13184375,,4/1/2020 13:17,18,61400149,"<p>Your <code>observation_sources</code> should be <code>scan</code> not <code>""/scan""</code>.</p>

<p>As mentioned in the <a href=""http://wiki.ros.org/costmap_2d/hydro/obstacles"" rel=""nofollow noreferrer"">Obstacle Layer wiki</a>:</p>

<blockquote>
  <p>A list of observation source names separated by spaces. This defines
  each of the  namespaces defined below. Each source_name
  in observation_sources defines a namespace in which parameters can be
  set:</p>
  
  <p>~//topic (string, default: source_name)</p>
  
  <p>The topic on which sensor data comes in for this source. Defaults to
  the name of the source.</p>
</blockquote>
",2661982,1,0,,,Moving
156,3708,62019726,Real Time Stepper Motors Control using ESP8266 and Blynk app,|c++|arduino|arduino-esp8266|robot|blynk|,"<p>I wanted to control 2 Stepper Motors for running a robot using the joystick of Blynk App and NodeMCU/ESP8266. But when I searched for the codes of real time controlling of Stepper Motors online I didn't get much code and most of them were not real time.</p>

<p>This is code I am currently working on:-</p>

<pre><code>#include &lt;ESP8266WiFi.h&gt;
#include &lt;BlynkSimpleEsp8266.h&gt;

#define RightMotorSpeed D7
#define RightMotorDir   D8  

const int enPin = D2;
const int enPin2 = D3;

#define LeftMotorSpeed  D6  
#define LeftMotorDir    D5


// You should get Auth Token in the Blynk App.
// Go to the Project Settings (nut icon).
// Use your own WiFi settings
char auth[] = ""LRTCZUnCI06P-pqh5rlPXRbuOUgQ_uGH"";
char ssid[] = ""Airtel_7599998800"";
char pass[] = ""air71454"";

// neutral zone settings for x and y
// joystick must move outside these boundary numbers to activate the motors
// makes it a little easier to control the wifi car
int minRange = 312;
int maxRange = 712;

// analog speeds from 0 (lowest) - 1023 (highest)
// 3 speeds used -- 0 (noSpeed), 350 (minSpeed), 850 (maxSpeed).
// use whatever speeds you want...too fast made it a pain in the ass to control
int minSpeed = 450;
int maxSpeed = 1023;
int noSpeed = 0;


void moveControl(int x, int y)
{
  // movement logic
  // move forward

   // y je vetsi jak maxrange a současně x je vetsi jak minRange a současne mensi jak max range 
  while(y &gt;= maxRange &amp;&amp; x &gt;= minRange &amp;&amp; x &lt;= maxRange) //zataci R
  {
    digitalWrite(RightMotorDir,HIGH);  
    digitalWrite(LeftMotorDir,HIGH);

    analogWrite(RightMotorSpeed,maxSpeed); 
    analogWrite(LeftMotorSpeed,maxSpeed);

    delayMicroseconds(500);

    digitalWrite(RightMotorSpeed,0); 
    digitalWrite(LeftMotorSpeed,0);

    delayMicroseconds(500);
  }

  // move forward right
  while(x &gt;= maxRange &amp;&amp; y &gt;= maxRange)   //zataci R
  {
    digitalWrite(RightMotorDir,HIGH);
    digitalWrite(LeftMotorDir,HIGH);
   analogWrite(RightMotorSpeed,minSpeed); 
    analogWrite(LeftMotorSpeed,maxSpeed);
  }

  // move forward left
  while(x &lt;= minRange &amp;&amp; y &gt;= maxRange)
  {
    digitalWrite(RightMotorDir,HIGH);
    digitalWrite(LeftMotorDir,HIGH);
    analogWrite(RightMotorSpeed,maxSpeed); 
    analogWrite(LeftMotorSpeed,minSpeed);
  }

  // neutral zone
  while(y &lt; maxRange &amp;&amp; y &gt; minRange &amp;&amp; x &lt; maxRange &amp;&amp; x &gt; minRange)
  {
    analogWrite(RightMotorSpeed,noSpeed); 
    analogWrite(LeftMotorSpeed,noSpeed);
  }

 // move back
  while(y &lt;= minRange &amp;&amp; x &gt;= minRange &amp;&amp; x &lt;= maxRange)
  {
    digitalWrite(RightMotorDir,LOW);
    digitalWrite(LeftMotorDir,LOW);
   analogWrite(RightMotorSpeed,maxSpeed); 
    analogWrite(LeftMotorSpeed,maxSpeed);
  }

  // move back and right
 while(y &lt;= minRange &amp;&amp; x &lt;= minRange)
  {
   digitalWrite(RightMotorDir,LOW);
    digitalWrite(LeftMotorDir,LOW);
    analogWrite(RightMotorSpeed,minSpeed); 
    analogWrite(LeftMotorSpeed,maxSpeed);  
  }

  // move back and left
  while(y &lt;= minRange &amp;&amp; x &gt;= maxRange)
  {
    digitalWrite(RightMotorDir,LOW);
    digitalWrite(LeftMotorDir,LOW);
    analogWrite(RightMotorSpeed,maxSpeed); 
    analogWrite(LeftMotorSpeed,minSpeed);
  }
}

void setup()
{
  // initial settings for motors off and direction forward
  pinMode(RightMotorSpeed, OUTPUT);
  pinMode(LeftMotorSpeed, OUTPUT);
  pinMode(RightMotorDir, OUTPUT);
  pinMode(LeftMotorDir, OUTPUT);
  digitalWrite(RightMotorSpeed, LOW);
  digitalWrite(LeftMotorSpeed, LOW);
  digitalWrite(RightMotorDir, HIGH);
  digitalWrite(LeftMotorDir,HIGH);



    Serial.begin(9600);

    pinMode(enPin,OUTPUT);
    digitalWrite(enPin,LOW);

    pinMode(enPin2,OUTPUT);
    digitalWrite(enPin2,LOW);



  Blynk.begin(auth, ssid, pass);
 }


void loop()
{
  Blynk.run();
}


BLYNK_WRITE(V1)
{
  int x = param[0].asInt();
  int y = param[1].asInt();
  moveControl(x,y); 
}
</code></pre>

<p>Here I have defined by 2 Stepper motors as Right and Left and since I am using TB6600 Motor Driver therefore their Pulse and Direction Pins are also Defined. That is the main reason that I am unable to use the Stepper motor Library for the code.</p>

<p>Running the code I see that Both the motors runs fine once for 3 to 5 seconds and the the Blynk Server Disconnects and Reconnects again causing the motors to stop and not creating a real time communication. Some one Please help me create a code for these 2 stepper motors to run at realtime.</p>

<p>I think that <strong>Blink.run()</strong> causes the server to reconnect and stop the motor.</p>

<p>I also searched for this cause and found that instead of Stepper motor Library I should use AccelStepper Library but that is also not achieved . Please help me with this. Any correct reference is also appreciable. Thanks in advance.</p>
",5/26/2020 10:12,,1609,1,1,0,,8913187,India,11/9/2017 11:08,8,68532470,"<p>Blynk requires constants pings to keep the connection alive. A while loop prevents any form of communication between your device and the Blynk server.
The preferred option is to use Timers to interactively call your functions and advance through the code. Another method is to force a server ping.</p>
<p>For example, you could add the following and call softDelay(1) within each of your while loops.</p>
<pre><code>void softDelay(uint32_t t) {
  unsigned long currentTime = millis();`
  unsigned long newTime = currentTime + t;
  while (currentTime &lt;= newTime)
  {
     Blynk.run();
     timer.run();
     currentTime = millis();
  }
}
</code></pre>
",16530042,0,0,109699787,"I think you should find out what real time means. I doubt that most code for constrolling stepper motors is not realtime as you claim. also note that it is not a very good idea to search for complete solutions and tutorials on your specific project. rather break your project down into sub-problems which are more common. you won't find a tutorial on how to look like Arnold Schwarzenegger with pink hair, but you'll find plenty of resources on how to do body building and on how to dye hair properly.",Remote
157,3714,62046666,Find 3D coordinate with respect to the camera using 2D image coordinates,|matlab|image-processing|robotics|coordinate-systems|,"<p>I need to calculate the X,Y coordinates in the world with respect to the camera using u,v coordinates in the 2D image. I am using an S7 edge camera to send a 720x480 video feed to MATLAB.</p>

<p>What I know: Z i.e the depth of the object from the camera, size of the camera pixels (1.4um), focal length (4.2mm)</p>

<p>Let's say the image point is at (u,v) = (400,400).</p>

<p>My approach is as follows:</p>

<ol>
<li>Subtract the pixel value of center point (240,360) from the u,v pixel coordinates of the point in the image. This should give us the pixel coordinates with respect to the camera's optical axis (z axis). The origin is now at the center of the image. So new coordinates are: (160, -40)</li>
<li>Multiply the new u,v pixel values with pixel size to obtain the distance of the point from the origin in physical units. Let's call it (x,y). We get (x,y) = (0.224,-0.056) in mm units.</li>
<li>Use the formula X = xZ/f &amp; Y = yZ/f to calculate X,Y coordinates in the real world with respect to the camera's optical axis.</li>
</ol>

<p>Is my approach correct? </p>
",5/27/2020 15:19,,7616,1,0,2,0,8555995,,9/3/2017 20:44,27,62047722,"<p>Your approach is going in the right way, but it would be easier if you use a more standardize approach. What we usually do is use <a href=""https://en.wikipedia.org/wiki/Pinhole_camera_model"" rel=""nofollow noreferrer"">Pinhole Camera Model</a> to give you a transformation between the world coordinates <code>[X, Y, Z]</code> to the pixel <code>[x, y]</code>. Take a look in <a href=""https://staff.fnwi.uva.nl/r.vandenboomgaard/IPCV20172018/LectureNotes/CV/PinholeCamera/PinholeCamera.html"" rel=""nofollow noreferrer"">this guide</a> which describes step-by-step the process of building your transformation.</p>

<p>Basically you have to define you Internal Camera Matrix to do the transformation:</p>

<p><a href=""https://i.stack.imgur.com/1UHnW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1UHnW.png"" alt=""enter image description here""></a></p>

<ul>
<li><strong>fx and fy</strong> are your focal length scaled to use as pixel distance. You can calculate this with your FOV and the total pixel in each direction. Take a look <a href=""https://photo.stackexchange.com/questions/97213/finding-focal-length-from-image-size-and-fov"">here</a> and <a href=""http://www.cs.toronto.edu/~jepson/csc420/notes/imageProjection.pdf"" rel=""nofollow noreferrer"">here</a> for more info. </li>
<li><p><strong>u0 and v0</strong> are the piercing point. Since our pixels are not centered in the <code>[0, 0]</code> these parameters represents a translation to the center of the image. (intersection of the optical axis with the image plane provided in pixel coordinates).</p></li>
<li><p>If you need, you can also add a the <strong>skew factor a</strong>, which you can use to correct shear effects of your camera. Then, the Internal Camera Matrix will be:</p></li>
</ul>

<p><a href=""https://i.stack.imgur.com/cBp9s.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cBp9s.png"" alt=""enter image description here""></a></p>

<p>Since your depth is fixed, just fix your Z and continue the transformation without a problem. </p>

<p><strong>Remember:</strong> If you want the inverse transformation (camera to world) just invert you Camera Matrix and be happy!</p>

<p><a href=""https://www.mathworks.com/help/vision/ug/camera-calibration.html#bu0nh2_"" rel=""nofollow noreferrer"">Matlab has also a very good guide for this transformation. Take a look.</a></p>
",11522398,4,4,,,Incoming
158,3722,62414292,NVIDIA Jetson Nano with Realsense 435i via Isaac - Camera not found,|robotics|nvidia-jetson|realsense|slam|nvidia-jetson-nano|,"<p>I posted about this over on the Isaac forums, but listing it here for visibility as well. I am trying to get the Isaac Realsense examples working on a Jetson Nano with my 435i (firmware downgraded to 5.11.15 per the Isaac documentation), but I've been unable to so far. I've got a Nano flashed with Jetpack4.3 and have installed all dependencies on both the desktop and the Nano. The realsense-viewer works fine, so I know the camera is functioning properly and is being detected by the Nano. However, when I run <code>./apps/samples/realsense_camera/realsense_camera</code> it throws an error:</p>

<pre><code>ERROR engine/alice/components/Codelet.cpp@229: Component 'camera/realsense' of type 'isaac::RealsenseCamera' reported FAILURE:

    No device connected, please connect a RealSense device
ERROR engine/alice/backend/event_manager.cpp@42: Stopping node 'camera' because it reached status 'FAILURE'
</code></pre>

<p>I've attached the log of this output as well. I get the same error running locally on my desktop, but that's running through WSL so I was willing to write that off. Any suggestions would be greatly appreciated!</p>

<pre><code>0m2020-06-15 17:18:20.620 INFO  engine/alice/tools/websight.cpp@166: Loading websight...0m
33m2020-06-15 17:18:20.621 WARN  engine/alice/backend/application_json_loader.cpp@174: This application does not have an explicit scheduler configuration. One will be autogenerated to the best of the system's abilities if possible.0m
0m2020-06-15 17:18:20.622 INFO  engine/alice/backend/redis_backend.cpp@40: Successfully connected to Redis server.
0m
33m2020-06-15 17:18:20.623 WARN  engine/alice/backend/backend.cpp@201: This application does not have an execution group configuration. One will be autogenerated to the best of the systems abilities if possible.0m
33m2020-06-15 17:18:20.623 WARN  engine/gems/scheduler/scheduler.cpp@337: No default execution groups specified. Attempting to create scheduler configuration for 4 remaining cores. This may be non optimal for the system and application.0m
0m2020-06-15 17:18:20.623 INFO  engine/gems/scheduler/scheduler.cpp@290: Scheduler execution groups are:0m
0m2020-06-15 17:18:20.623 INFO  engine/gems/scheduler/scheduler.cpp@299: __BlockerGroup__: Cores = [3], Workers = No0m
0m2020-06-15 17:18:20.623 INFO  engine/gems/scheduler/scheduler.cpp@299: __WorkerGroup__: Cores = [0, 1, 2], Workers = Yes0m
0m2020-06-15 17:18:20.660 INFO  engine/alice/backend/modules.cpp@226: Loaded module 'packages/realsense/librealsense_module.so': Now has 45 components total0m
0m2020-06-15 17:18:20.679 INFO  engine/alice/backend/modules.cpp@226: Loaded module 'packages/rgbd_processing/librgbd_processing_module.so': Now has 51 components total0m
0m2020-06-15 17:18:20.696 INFO  engine/alice/backend/modules.cpp@226: Loaded module 'packages/sight/libsight_module.so': Now has 54 components total0m
0m2020-06-15 17:18:20.720 INFO  engine/alice/backend/modules.cpp@226: Loaded module 'packages/viewers/libviewers_module.so': Now has 83 components total0m
90m2020-06-15 17:18:20.720 DEBUG engine/alice/application.cpp@348: Loaded 83 components: isaac::RealsenseCamera, isaac::alice::BufferAllocatorReport, isaac::alice::ChannelMonitor, isaac::alice::CheckJetsonPerformanceModel, isaac::alice::CheckOperatingSystem, isaac::alice::Config, isaac::alice::ConfigBridge, isaac::alice::ConfigLoader, isaac::alice::Failsafe, isaac::alice::FailsafeHeartbeat, isaac::alice::InteractiveMarkersBridge, isaac::alice::JsonToProto, isaac::alice::LifecycleReport, isaac::alice::MessageLedger, isaac::alice::MessagePassingReport, isaac::alice::NodeStatistics, isaac::alice::Pose, isaac::alice::Pose2Comparer, isaac::alice::PoseFromFile, isaac::alice::PoseInitializer, isaac::alice::PoseMessageInjector, isaac::alice::PoseToFile, isaac::alice::PoseToMessage, isaac::alice::PoseTree, isaac::alice::PoseTreeJsonBridge, isaac::alice::PoseTreeRelink, isaac::alice::ProtoToJson, isaac::alice::PyCodelet, isaac::alice::Random, isaac::alice::Recorder, isaac::alice::RecorderBridge, isaac::alice::Replay, isaac::alice::ReplayBridge, isaac::alice::Scheduling, isaac::alice::Sight, isaac::alice::SightChannelStatus, isaac::alice::Subgraph, isaac::alice::Subprocess, isaac::alice::TcpPublisher, isaac::alice::TcpSubscriber, isaac::alice::Throttle, isaac::alice::TimeOffset, isaac::alice::TimeSynchronizer, isaac::alice::UdpPublisher, isaac::alice::UdpSubscriber, isaac::map::Map, isaac::map::ObstacleAtlas, isaac::map::OccupancyGridMapLayer, isaac::map::PolygonMapLayer, isaac::map::WaypointMapLayer, isaac::navigation::DistanceMap, isaac::navigation::NavigationMap, isaac::navigation::RangeScanModelClassic, isaac::navigation::RangeScanModelFlatloc, isaac::rgbd_processing::DepthEdges, isaac::rgbd_processing::DepthImageFlattening, isaac::rgbd_processing::DepthImageToPointCloud, isaac::rgbd_processing::DepthNormals, isaac::rgbd_processing::DepthPoints, isaac::rgbd_processing::FreespaceFromDepth, isaac::sight::AliceSight, isaac::sight::SightWidget, isaac::sight::WebsightServer, isaac::viewers::BinaryMapViewer, isaac::viewers::ColorCameraViewer, isaac::viewers::DepthCameraViewer, isaac::viewers::Detections3Viewer, isaac::viewers::DetectionsViewer, isaac::viewers::FiducialsViewer, isaac::viewers::FlatscanViewer, isaac::viewers::GoalViewer, isaac::viewers::ImageKeypointViewer, isaac::viewers::LidarViewer, isaac::viewers::MosaicViewer, isaac::viewers::ObjectViewer, isaac::viewers::OccupancyMapViewer, isaac::viewers::PointCloudViewer, isaac::viewers::PoseTrailViewer, isaac::viewers::SegmentationCameraViewer, isaac::viewers::SegmentationViewer, isaac::viewers::SkeletonViewer, isaac::viewers::TensorViewer, isaac::viewers::TrajectoryListViewer, 0m
33m2020-06-15 17:18:20.723 WARN  engine/alice/application.cpp@164: The function Application::findComponentByName is deprecated. Please use `getNodeComponentOrNull` instead. Note that the new method requires a node name instead of a component name. (argument: 'websight/isaac.sight.AliceSight')0m
0m2020-06-15 17:18:20.723 INFO  engine/alice/application.cpp@255: Starting application 'realsense_camera' (instance UUID: 'e24992d0-af66-11ea-8bcf-c957460c567e') ...0m
90m2020-06-15 17:18:20.723 DEBUG engine/gems/scheduler/execution_groups.cpp@476: Launching 0 pre-start job(s)0m
90m2020-06-15 17:18:20.723 DEBUG engine/gems/scheduler/execution_groups.cpp@485: Replaying 0 pre-start event(s)0m
90m2020-06-15 17:18:20.723 DEBUG engine/gems/scheduler/execution_groups.cpp@476: Launching 0 pre-start job(s)0m
90m2020-06-15 17:18:20.723 DEBUG engine/gems/scheduler/execution_groups.cpp@485: Replaying 0 pre-start event(s)0m
0m2020-06-15 17:18:20.723 INFO  engine/alice/backend/asio_backend.cpp@33: Starting ASIO service0m
0m2020-06-15 17:18:20.727 INFO  packages/sight/WebsightServer.cpp@216: Sight webserver is loaded0m
0m2020-06-15 17:18:20.727 INFO  packages/sight/WebsightServer.cpp@217: Please open Chrome Browser and navigate to http://&lt;ip address&gt;:30000m
33m2020-06-15 17:18:20.727 WARN  engine/alice/backend/codelet_canister.cpp@225: Codelet 'websight/isaac.sight.AliceSight' was not added to scheduler because no tick method is specified.0m
33m2020-06-15 17:18:20.728 WARN  engine/alice/components/Codelet.cpp@53: Function deprecated. Set tick_period to the desired tick paramater0m
33m2020-06-15 17:18:20.728 WARN  engine/alice/backend/codelet_canister.cpp@225: Codelet '_check_operating_system/isaac.alice.CheckOperatingSystem' was not added to scheduler because no tick method is specified.0m
33m2020-06-15 17:18:20.728 WARN  engine/alice/components/Codelet.cpp@53: Function deprecated. Set tick_period to the desired tick paramater0m
33m2020-06-15 17:18:20.730 WARN  engine/alice/components/Codelet.cpp@53: Function deprecated. Set tick_period to the desired tick paramater0m
1;31m2020-06-15 17:18:20.741 ERROR engine/alice/components/Codelet.cpp@229: Component 'camera/realsense' of type 'isaac::RealsenseCamera' reported FAILURE:

    No device connected, please connect a RealSense device
0m
1;31m2020-06-15 17:18:20.741 ERROR engine/alice/backend/event_manager.cpp@42: Stopping node 'camera' because it reached status 'FAILURE'0m
33m2020-06-15 17:18:20.743 WARN  engine/alice/backend/codelet_canister.cpp@225: Codelet 'camera/realsense' was not added to scheduler because no tick method is specified.0m
0m2020-06-15 17:18:21.278 INFO  packages/sight/WebsightServer.cpp@113: Server connected / 10m
0m2020-06-15 17:18:30.723 INFO  engine/alice/backend/allocator_backend.cpp@57: Optimized memory CPU allocator.0m
0m2020-06-15 17:18:30.724 INFO  engine/alice/backend/allocator_backend.cpp@66: Optimized memory CUDA allocator.0m

</code></pre>
",6/16/2020 17:28,,536,0,0,1,,9459427,,3/7/2018 23:27,6,,,,,,,,Connections
159,3729,62630556,"Mapping, and autonomous drones",|artificial-intelligence|augmented-reality|robotics|,"<p>What is a good software to be able to navigate a drone in an augmented reality, is there anything that exists for individual and educational use in this area?</p>
",6/29/2020 2:58,,61,1,0,0,,13830207,"Fortuna, CA, USA",6/28/2020 17:40,6,62701790,"<p>I'm not sure this is the platform to ask this question, but here is an answer. There were a few companies that tackled this issue in the past, dARwing was doing exactly that and edgybees did something similar. Both companies are no longer in this field.</p>
",3360466,0,1,,,Specifications
160,3735,62761806,How can I use an Arduino Ultrasonic Sensor connected to an Arduino UNO to measure distance using Pyfirmata or Python Generally?,|python|arduino|robotics|,"<p>Here is the Code I have in Arduino. Pyfirmata, as I am concerned doesn't have a PulseIn function, so how can I bypass this obstacle? I want to turn an LED on when the sensor senses an object that it's distance from it is 20cm or smaller!</p>
<pre><code>// defines pins numbers
const int trigPin = 12;
const int echoPin = 11;
int LED = 5;
// defines variables
long duration;
int distance;

void setup() {
    pinMode(trigPin, OUTPUT); // Sets the trigPin as an Output
    pinMode(echoPin, INPUT); // Sets the echoPin as an Input
    Serial.begin(9600); // Starts the serial communication
    pinMode (LED,OUTPUT);
}

void loop() {
    // Clears the trigPin
    digitalWrite(trigPin, LOW);
    delayMicroseconds(2);
    // Sets the trigPin on HIGH state for 10 micro seconds
    digitalWrite(trigPin, HIGH);
    delayMicroseconds(10);
    digitalWrite(trigPin, LOW);
    // Reads the echoPin, returns the sound wave travel time in microseconds
    duration = pulseIn(echoPin, HIGH);
    // Calculating the distance
    distance= duration*0.034/2;
    // Prints the distance on the Serial Monitor
    Serial.print(&quot;Distance: &quot;);
    Serial.println(distance);

    if (distance &lt;=20){
      digitalWrite(LED,HIGH);
    }
    else{
      digitalWrite (LED,LOW);
    }
}
</code></pre>
",7/6/2020 17:58,67685151,1137,1,1,1,,13349700,,4/18/2020 19:14,3,67685151,"<p>I came across the same problem and was trying to solve. I saw your question had no answers :D. It took me two days to figure out I went all out like a hungry wolf hunting.
Here's what u need to do.</p>
<p>First some back story.
PyFirmata is a library which acts like a bridge between Python and Arduino AVR. Which means in order for functions to work you need to define them in both the languages at eh backhand program (AVR and Python) so they can communicate on same terms.
Files in AVR are StandardFirmata.ino. Files in Python are pyFirmata.py. There are more as well u can see from here. These Files communicate between each other when program runs.</p>
<ol>
<li><a href=""https://github.com/tino/pyFirmata/pull/45/files"" rel=""nofollow noreferrer"">https://github.com/tino/pyFirmata/pull/45/files</a></li>
<li><a href=""https://github.com/jgautier/arduino-1/tree/pulseIn"" rel=""nofollow noreferrer"">https://github.com/jgautier/arduino-1/tree/pulseIn</a></li>
</ol>
<p>Some guys tried to add a PULSE_IN feature to calculate the distance from their Ultrasonic sensor and it worked fine. To make it work u have to update your standardfirmata file in arduino and pyFirmata file in Python manually to add the PULSE_IN codes from the link above. Here are the steps.</p>
<ol>
<li>Open your StandardFirmata sketch in Arduino IDE.</li>
<li>Copy paste this code to replace teh old code. this will add the PULSE_IN features i extracted from the links above and added to my standardfirmata.</li>
</ol>
<pre><code>/*
 * Firmata is a generic protocol for communicating with microcontrollers
 * from software on a host computer. It is intended to work with
 * any host computer software package.
 *
 * To download a host software package, please clink on the following link
 * to open the download page in your default browser.
 *
 * http://firmata.org/wiki/Download
 */
/*
  Copyright (C) 2006-2008 Hans-Christoph Steiner.  All rights reserved.
  Copyright (C) 2010-2011 Paul Stoffregen.  All rights reserved.
  Copyright (C) 2009 Shigeru Kobayashi.  All rights reserved.
  Copyright (C) 2009-2011 Jeff Hoefs.  All rights reserved.
  
  This library is free software; you can redistribute it and/or
  modify it under the terms of the GNU Lesser General Public
  License as published by the Free Software Foundation; either
  version 2.1 of the License, or (at your option) any later version.
 
  See file LICENSE.txt for further informations on licensing terms.
  formatted using the GNU C formatting and indenting
*/
/* 
 * TODO: use Program Control to load stored profiles from EEPROM
 */
#include &lt;Servo.h&gt;
#include &lt;Wire.h&gt;
#include &lt;Firmata.h&gt;
// move the following defines to Firmata.h?
#define I2C_WRITE B00000000
#define I2C_READ B00001000
#define I2C_READ_CONTINUOUSLY B00010000
#define I2C_STOP_READING B00011000
#define I2C_READ_WRITE_MODE_MASK B00011000
#define I2C_10BIT_ADDRESS_MODE_MASK B00100000
#define MAX_QUERIES               8
#define MINIMUM_SAMPLING_INTERVAL 10
#define REGISTER_NOT_SPECIFIED    -1
#define PULSE_IN                  0x74 // send a pulse in command

/*==============================================================================
 * GLOBAL VARIABLES
 *============================================================================*/
/* Ultrasonic Distance Measurement variables */
# include &quot;LiquidCrystal.h&quot;  //lcd libary                                       
LiquidCrystal lcd(22, 23, 24, 25, 26, 27);   //LCD object Parameters: (rs, enable, d4, d5, d6, d7)
const int trigPin = 35; //trig pin connection 
const int echoPin = 34;  //echopin connection 
long duration;
int distanceCm;
float liquid;
/* analog inputs */
int analogInputsToReport = 0; // bitwise array to store pin reporting
/* digital input ports */
byte reportPINs[TOTAL_PORTS];       // 1 = report this port, 0 = silence
byte previousPINs[TOTAL_PORTS];     // previous 8 bits sent
/* pins configuration */
byte pinConfig[TOTAL_PINS];         // configuration of every pin
byte portConfigInputs[TOTAL_PORTS]; // each bit: 1 = pin in INPUT, 0 = anything else
int pinState[TOTAL_PINS];           // any value that has been written
/* timer variables */
unsigned long currentMillis;        // store the current value from millis()
unsigned long previousMillis;       // for comparison with currentMillis
int samplingInterval = 19;          // how often to run the main loop (in ms)
/* i2c data */
struct i2c_device_info {
  byte addr;
  byte reg;
  byte bytes;
};
/* for i2c read continuous more */
i2c_device_info query[MAX_QUERIES];
byte i2cRxData[32];
boolean isI2CEnabled = false;
signed char queryIndex = -1;
unsigned int i2cReadDelayTime = 0;  // default delay time between i2c read request and Wire.requestFrom()
Servo servos[MAX_SERVOS];
/*==============================================================================
 * FUNCTIONS
 *============================================================================*/
void readAndReportData(byte address, int theRegister, byte numBytes) {
  // allow I2C requests that don't require a register read
  // for example, some devices using an interrupt pin to signify new data available
  // do not always require the register read so upon interrupt you call Wire.requestFrom()  
  if (theRegister != REGISTER_NOT_SPECIFIED) {
    Wire.beginTransmission(address);
    #if ARDUINO &gt;= 100
    Wire.write((byte)theRegister);
    #else
    Wire.send((byte)theRegister);
    #endif
    Wire.endTransmission();
    delayMicroseconds(i2cReadDelayTime);  // delay is necessary for some devices such as WiiNunchuck
  } else {
    theRegister = 0;  // fill the register with a dummy value
  }
  Wire.requestFrom(address, numBytes);  // all bytes are returned in requestFrom
  // check to be sure correct number of bytes were returned by slave
  if(numBytes == Wire.available()) {
    i2cRxData[0] = address;
    i2cRxData[1] = theRegister;
    for (int i = 0; i &lt; numBytes; i++) {
      #if ARDUINO &gt;= 100
      i2cRxData[2 + i] = Wire.read();
      #else
      i2cRxData[2 + i] = Wire.receive();
      #endif
    }
  }
  else {
    if(numBytes &gt; Wire.available()) {
      Firmata.sendString(&quot;I2C Read Error: Too many bytes received&quot;);
    } else {
      Firmata.sendString(&quot;I2C Read Error: Too few bytes received&quot;); 
    }
  }
  // send slave address, register and received bytes
  Firmata.sendSysex(SYSEX_I2C_REPLY, numBytes + 2, i2cRxData);
}
void outputPort(byte portNumber, byte portValue, byte forceSend)
{
  // pins not configured as INPUT are cleared to zeros
  portValue = portValue &amp; portConfigInputs[portNumber];
  // only send if the value is different than previously sent
  if(forceSend || previousPINs[portNumber] != portValue) {
    Firmata.sendDigitalPort(portNumber, portValue);
    previousPINs[portNumber] = portValue;
  }
}
/* -----------------------------------------------------------------------------
 * check all the active digital inputs for change of state, then add any events
 * to the Serial output queue using Serial.print() */
void checkDigitalInputs(void)
{
  /* Using non-looping code allows constants to be given to readPort().
   * The compiler will apply substantial optimizations if the inputs
   * to readPort() are compile-time constants. */
  if (TOTAL_PORTS &gt; 0 &amp;&amp; reportPINs[0]) outputPort(0, readPort(0, portConfigInputs[0]), false);
  if (TOTAL_PORTS &gt; 1 &amp;&amp; reportPINs[1]) outputPort(1, readPort(1, portConfigInputs[1]), false);
  if (TOTAL_PORTS &gt; 2 &amp;&amp; reportPINs[2]) outputPort(2, readPort(2, portConfigInputs[2]), false);
  if (TOTAL_PORTS &gt; 3 &amp;&amp; reportPINs[3]) outputPort(3, readPort(3, portConfigInputs[3]), false);
  if (TOTAL_PORTS &gt; 4 &amp;&amp; reportPINs[4]) outputPort(4, readPort(4, portConfigInputs[4]), false);
  if (TOTAL_PORTS &gt; 5 &amp;&amp; reportPINs[5]) outputPort(5, readPort(5, portConfigInputs[5]), false);
  if (TOTAL_PORTS &gt; 6 &amp;&amp; reportPINs[6]) outputPort(6, readPort(6, portConfigInputs[6]), false);
  if (TOTAL_PORTS &gt; 7 &amp;&amp; reportPINs[7]) outputPort(7, readPort(7, portConfigInputs[7]), false);
  if (TOTAL_PORTS &gt; 8 &amp;&amp; reportPINs[8]) outputPort(8, readPort(8, portConfigInputs[8]), false);
  if (TOTAL_PORTS &gt; 9 &amp;&amp; reportPINs[9]) outputPort(9, readPort(9, portConfigInputs[9]), false);
  if (TOTAL_PORTS &gt; 10 &amp;&amp; reportPINs[10]) outputPort(10, readPort(10, portConfigInputs[10]), false);
  if (TOTAL_PORTS &gt; 11 &amp;&amp; reportPINs[11]) outputPort(11, readPort(11, portConfigInputs[11]), false);
  if (TOTAL_PORTS &gt; 12 &amp;&amp; reportPINs[12]) outputPort(12, readPort(12, portConfigInputs[12]), false);
  if (TOTAL_PORTS &gt; 13 &amp;&amp; reportPINs[13]) outputPort(13, readPort(13, portConfigInputs[13]), false);
  if (TOTAL_PORTS &gt; 14 &amp;&amp; reportPINs[14]) outputPort(14, readPort(14, portConfigInputs[14]), false);
  if (TOTAL_PORTS &gt; 15 &amp;&amp; reportPINs[15]) outputPort(15, readPort(15, portConfigInputs[15]), false);
}
// -----------------------------------------------------------------------------
/* sets the pin mode to the correct state and sets the relevant bits in the
 * two bit-arrays that track Digital I/O and PWM status
 */
void setPinModeCallback(byte pin, int mode)
{
  if (pinConfig[pin] == I2C &amp;&amp; isI2CEnabled &amp;&amp; mode != I2C) {
    // disable i2c so pins can be used for other functions
    // the following if statements should reconfigure the pins properly
    disableI2CPins();
  }
  if (IS_PIN_SERVO(pin) &amp;&amp; mode != SERVO &amp;&amp; servos[PIN_TO_SERVO(pin)].attached()) {
    servos[PIN_TO_SERVO(pin)].detach();
  }
  if (IS_PIN_ANALOG(pin)) {
    reportAnalogCallback(PIN_TO_ANALOG(pin), mode == ANALOG ? 1 : 0); // turn on/off reporting
  }
  if (IS_PIN_DIGITAL(pin)) {
    if (mode == INPUT) {
      portConfigInputs[pin/8] |= (1 &lt;&lt; (pin &amp; 7));
    } else {
      portConfigInputs[pin/8] &amp;= ~(1 &lt;&lt; (pin &amp; 7));
    }
  }
  pinState[pin] = 0;
  switch(mode) {
  case ANALOG:
    if (IS_PIN_ANALOG(pin)) {
      if (IS_PIN_DIGITAL(pin)) {
        pinMode(PIN_TO_DIGITAL(pin), INPUT); // disable output driver
        digitalWrite(PIN_TO_DIGITAL(pin), LOW); // disable internal pull-ups
      }
      pinConfig[pin] = ANALOG;
    }
    break;
  case INPUT:
    if (IS_PIN_DIGITAL(pin)) {
      pinMode(PIN_TO_DIGITAL(pin), INPUT); // disable output driver
      digitalWrite(PIN_TO_DIGITAL(pin), LOW); // disable internal pull-ups
      pinConfig[pin] = INPUT;
    }
    break;
  case OUTPUT:
    if (IS_PIN_DIGITAL(pin)) {
      digitalWrite(PIN_TO_DIGITAL(pin), LOW); // disable PWM
      pinMode(PIN_TO_DIGITAL(pin), OUTPUT);
      pinConfig[pin] = OUTPUT;
    }
    break;
  case PWM:
    if (IS_PIN_PWM(pin)) {
      pinMode(PIN_TO_PWM(pin), OUTPUT);
      analogWrite(PIN_TO_PWM(pin), 0);
      pinConfig[pin] = PWM;
    }
    break;
  case SERVO:
    if (IS_PIN_SERVO(pin)) {
      pinConfig[pin] = SERVO;
      if (!servos[PIN_TO_SERVO(pin)].attached()) {
          servos[PIN_TO_SERVO(pin)].attach(PIN_TO_DIGITAL(pin));
      }
    }
    break;
  case I2C:
    if (IS_PIN_I2C(pin)) {
      // mark the pin as i2c
      // the user must call I2C_CONFIG to enable I2C for a device
      pinConfig[pin] = I2C;
    }
    break;
  default:
    Firmata.sendString(&quot;Unknown pin mode&quot;); // TODO: put error msgs in EEPROM
  }
  // TODO: save status to EEPROM here, if changed
}
void analogWriteCallback(byte pin, int value)
{
  if (pin &lt; TOTAL_PINS) {
    switch(pinConfig[pin]) {
    case SERVO:
      if (IS_PIN_SERVO(pin))
        servos[PIN_TO_SERVO(pin)].write(value);
        pinState[pin] = value;
      break;
    case PWM:
      if (IS_PIN_PWM(pin))
        analogWrite(PIN_TO_PWM(pin), value);
        pinState[pin] = value;
      break;
    }
  }
}
void digitalWriteCallback(byte port, int value)
{
  byte pin, lastPin, mask=1, pinWriteMask=0;
  if (port &lt; TOTAL_PORTS) {
    // create a mask of the pins on this port that are writable.
    lastPin = port*8+8;
    if (lastPin &gt; TOTAL_PINS) lastPin = TOTAL_PINS;
    for (pin=port*8; pin &lt; lastPin; pin++) {
      // do not disturb non-digital pins (eg, Rx &amp; Tx)
      if (IS_PIN_DIGITAL(pin)) {
        // only write to OUTPUT and INPUT (enables pullup)
        // do not touch pins in PWM, ANALOG, SERVO or other modes
        if (pinConfig[pin] == OUTPUT || pinConfig[pin] == INPUT) {
          pinWriteMask |= mask;
          pinState[pin] = ((byte)value &amp; mask) ? 1 : 0;
        }
      }
      mask = mask &lt;&lt; 1;
    }
    writePort(port, (byte)value, pinWriteMask);
  }
}
// -----------------------------------------------------------------------------
/* sets bits in a bit array (int) to toggle the reporting of the analogIns
 */
//void FirmataClass::setAnalogPinReporting(byte pin, byte state) {
//}
void reportAnalogCallback(byte analogPin, int value)
{
  if (analogPin &lt; TOTAL_ANALOG_PINS) {
    if(value == 0) {
      analogInputsToReport = analogInputsToReport &amp;~ (1 &lt;&lt; analogPin);
    } else {
      analogInputsToReport = analogInputsToReport | (1 &lt;&lt; analogPin);
    }
  }
  // TODO: save status to EEPROM here, if changed
}
void reportDigitalCallback(byte port, int value)
{
  if (port &lt; TOTAL_PORTS) {
    reportPINs[port] = (byte)value;
  }
  // do not disable analog reporting on these 8 pins, to allow some
  // pins used for digital, others analog.  Instead, allow both types
  // of reporting to be enabled, but check if the pin is configured
  // as analog when sampling the analog inputs.  Likewise, while
  // scanning digital pins, portConfigInputs will mask off values from any
  // pins configured as analog
}
/*==============================================================================
 * SYSEX-BASED commands
 *============================================================================*/
void sysexCallback(byte command, byte argc, byte *argv)
{
  byte mode;
  byte slaveAddress;
  byte slaveRegister;
  byte data;
  unsigned int delayTime; 
  
  switch(command) {
  case I2C_REQUEST:
    mode = argv[1] &amp; I2C_READ_WRITE_MODE_MASK;
    if (argv[1] &amp; I2C_10BIT_ADDRESS_MODE_MASK) {
      Firmata.sendString(&quot;10-bit addressing mode is not yet supported&quot;);
      return;
    }
    else {
      slaveAddress = argv[0];
    }
    switch(mode) {
    case I2C_WRITE:
      Wire.beginTransmission(slaveAddress);
      for (byte i = 2; i &lt; argc; i += 2) {
        data = argv[i] + (argv[i + 1] &lt;&lt; 7);
        #if ARDUINO &gt;= 100
        Wire.write(data);
        #else
        Wire.send(data);
        #endif
      }
      Wire.endTransmission();
      delayMicroseconds(70);
      break;
    case I2C_READ:
      if (argc == 6) {
        // a slave register is specified
        slaveRegister = argv[2] + (argv[3] &lt;&lt; 7);
        data = argv[4] + (argv[5] &lt;&lt; 7);  // bytes to read
        readAndReportData(slaveAddress, (int)slaveRegister, data);
      }
      else {
        // a slave register is NOT specified
        data = argv[2] + (argv[3] &lt;&lt; 7);  // bytes to read
        readAndReportData(slaveAddress, (int)REGISTER_NOT_SPECIFIED, data);
      }
      break;
    case I2C_READ_CONTINUOUSLY:
      if ((queryIndex + 1) &gt;= MAX_QUERIES) {
        // too many queries, just ignore
        Firmata.sendString(&quot;too many queries&quot;);
        break;
      }
      queryIndex++;
      query[queryIndex].addr = slaveAddress;
      query[queryIndex].reg = argv[2] + (argv[3] &lt;&lt; 7);
      query[queryIndex].bytes = argv[4] + (argv[5] &lt;&lt; 7);
      break;
    case I2C_STOP_READING:
      byte queryIndexToSkip;      
      // if read continuous mode is enabled for only 1 i2c device, disable
      // read continuous reporting for that device
      if (queryIndex &lt;= 0) {
        queryIndex = -1;        
      } else {
        // if read continuous mode is enabled for multiple devices,
        // determine which device to stop reading and remove it's data from
        // the array, shifiting other array data to fill the space
        for (byte i = 0; i &lt; queryIndex + 1; i++) {
          if (query[i].addr = slaveAddress) {
            queryIndexToSkip = i;
            break;
          }
        }
        
        for (byte i = queryIndexToSkip; i&lt;queryIndex + 1; i++) {
          if (i &lt; MAX_QUERIES) {
            query[i].addr = query[i+1].addr;
            query[i].reg = query[i+1].addr;
            query[i].bytes = query[i+1].bytes; 
          }
        }
        queryIndex--;
      }
      break;
    default:
      break;
    }
    break;
  case I2C_CONFIG:
    delayTime = (argv[0] + (argv[1] &lt;&lt; 7));
    if(delayTime &gt; 0) {
      i2cReadDelayTime = delayTime;
    }
    if (!isI2CEnabled) {
      enableI2CPins();
    }
    
    break;
  case SERVO_CONFIG:
    if(argc &gt; 4) {
      // these vars are here for clarity, they'll optimized away by the compiler
      byte pin = argv[0];
      int minPulse = argv[1] + (argv[2] &lt;&lt; 7);
      int maxPulse = argv[3] + (argv[4] &lt;&lt; 7);
      if (IS_PIN_SERVO(pin)) {
        if (servos[PIN_TO_SERVO(pin)].attached())
          servos[PIN_TO_SERVO(pin)].detach();
        servos[PIN_TO_SERVO(pin)].attach(PIN_TO_DIGITAL(pin), minPulse, maxPulse);
        setPinModeCallback(pin, SERVO);
      }
    }
    break;
  case SAMPLING_INTERVAL:
    if (argc &gt; 1) {
      samplingInterval = argv[0] + (argv[1] &lt;&lt; 7);
      if (samplingInterval &lt; MINIMUM_SAMPLING_INTERVAL) {
        samplingInterval = MINIMUM_SAMPLING_INTERVAL;
      }      
    } else {
      //Firmata.sendString(&quot;Not enough data&quot;);
    }
    break;
  case EXTENDED_ANALOG:
    if (argc &gt; 1) {
      int val = argv[1];
      if (argc &gt; 2) val |= (argv[2] &lt;&lt; 7);
      if (argc &gt; 3) val |= (argv[3] &lt;&lt; 14);
      analogWriteCallback(argv[0], val);
    }
    break;
  case CAPABILITY_QUERY:
    Serial.write(START_SYSEX);
    Serial.write(CAPABILITY_RESPONSE);
    for (byte pin=0; pin &lt; TOTAL_PINS; pin++) {
      if (IS_PIN_DIGITAL(pin)) {
        Serial.write((byte)INPUT);
        Serial.write(1);
        Serial.write((byte)OUTPUT);
        Serial.write(1);
      }
      if (IS_PIN_ANALOG(pin)) {
        Serial.write(ANALOG);
        Serial.write(10);
      }
      if (IS_PIN_PWM(pin)) {
        Serial.write(PWM);
        Serial.write(8);
      }
      if (IS_PIN_SERVO(pin)) {
        Serial.write(SERVO);
        Serial.write(14);
      }
      if (IS_PIN_I2C(pin)) {
        Serial.write(I2C);
        Serial.write(1);  // to do: determine appropriate value 
      }
      Serial.write(127);
    }
    Serial.write(END_SYSEX);
    break;
  case PIN_STATE_QUERY:
    if (argc &gt; 0) {
      byte pin=argv[0];
      Serial.write(START_SYSEX);
      Serial.write(PIN_STATE_RESPONSE);
      Serial.write(pin);
      if (pin &lt; TOTAL_PINS) {
        Serial.write((byte)pinConfig[pin]);
    Serial.write((byte)pinState[pin] &amp; 0x7F);
    if (pinState[pin] &amp; 0xFF80) Serial.write((byte)(pinState[pin] &gt;&gt; 7) &amp; 0x7F);
    if (pinState[pin] &amp; 0xC000) Serial.write((byte)(pinState[pin] &gt;&gt; 14) &amp; 0x7F);
      }
      Serial.write(END_SYSEX);
    }
    break;
  case ANALOG_MAPPING_QUERY:
    Serial.write(START_SYSEX);
    Serial.write(ANALOG_MAPPING_RESPONSE);
    for (byte pin=0; pin &lt; TOTAL_PINS; pin++) {
      Serial.write(IS_PIN_ANALOG(pin) ? PIN_TO_ANALOG(pin) : 127);
    }
    Serial.write(END_SYSEX);
    break;
  case PULSE_IN:
    unsigned long duration;
    byte responseArray[5];
    byte timeoutArray[4] = {
        (argv[2] &amp; 0x7F) | ((argv[3] &amp; 0x7F) &lt;&lt; 7)
       ,(argv[4] &amp; 0x7F) | ((argv[5] &amp; 0x7F) &lt;&lt; 7)
       ,(argv[6] &amp; 0x7F) | ((argv[7] &amp; 0x7F) &lt;&lt; 7)
       ,(argv[8] &amp; 0x7F) | ((argv[9] &amp; 0x7F) &lt;&lt; 7)
    };
    unsigned long timeout = ((unsigned long)timeoutArray[0] &lt;&lt; 24)
              | ((unsigned long)timeoutArray[1] &lt;&lt; 16)
              | ((unsigned long)timeoutArray[2] &lt;&lt; 8)
              | ((unsigned long)timeoutArray[3]);
    duration = pulseIn(argv[0],argv[1],timeout);
    responseArray[0] = argv[0];
    responseArray[1] = ((timeout &gt;&gt; 24) &amp; 0xFF) ;
    responseArray[2] = ((timeout &gt;&gt; 16) &amp; 0xFF) ;
    responseArray[3] = ((timeout &gt;&gt; 8) &amp; 0xFF);
    responseArray[4] = ((timeout &amp; 0xFF));
    Firmata.sendSysex(PULSE_IN,5,responseArray);
  }
}

void enableI2CPins()
{
  byte i;
  // is there a faster way to do this? would probaby require importing 
  // Arduino.h to get SCL and SDA pins
  for (i=0; i &lt; TOTAL_PINS; i++) {
    if(IS_PIN_I2C(i)) {
      // mark pins as i2c so they are ignore in non i2c data requests
      setPinModeCallback(i, I2C);
    } 
  }
   
  isI2CEnabled = true; 
  
  // is there enough time before the first I2C request to call this here?
  Wire.begin();
}
/* disable the i2c pins so they can be used for other functions */
void disableI2CPins() {
    isI2CEnabled = false;
    // disable read continuous mode for all devices
    queryIndex = -1;
    // uncomment the following if or when the end() method is added to Wire library
    // Wire.end();
}
/*==============================================================================
 * SETUP()
 *============================================================================*/
void systemResetCallback()
{
  // initialize a defalt state
  // TODO: option to load config from EEPROM instead of default
  if (isI2CEnabled) {
    disableI2CPins();
  }
  for (byte i=0; i &lt; TOTAL_PORTS; i++) {
    reportPINs[i] = false;      // by default, reporting off
    portConfigInputs[i] = 0;    // until activated
    previousPINs[i] = 0;
  }
  // pins with analog capability default to analog input
  // otherwise, pins default to digital output
  for (byte i=0; i &lt; TOTAL_PINS; i++) {
    if (IS_PIN_ANALOG(i)) {
      // turns off pullup, configures everything
      setPinModeCallback(i, ANALOG);
    } else {
      // sets the output to 0, configures portConfigInputs
      setPinModeCallback(i, OUTPUT);
    }
  }
  // by default, do not report any analog inputs
  analogInputsToReport = 0;
  /* send digital inputs to set the initial state on the host computer,
   * since once in the loop(), this firmware will only send on change */
  /*
  TODO: this can never execute, since no pins default to digital input
        but it will be needed when/if we support EEPROM stored config
  for (byte i=0; i &lt; TOTAL_PORTS; i++) {
    outputPort(i, readPort(i, portConfigInputs[i]), true);
  }
  */
}
void setup() 
{
  Firmata.setFirmwareVersion(FIRMATA_MAJOR_VERSION, FIRMATA_MINOR_VERSION);
  Firmata.attach(ANALOG_MESSAGE, analogWriteCallback);
  Firmata.attach(DIGITAL_MESSAGE, digitalWriteCallback);
  Firmata.attach(REPORT_ANALOG, reportAnalogCallback);
  Firmata.attach(REPORT_DIGITAL, reportDigitalCallback);
  Firmata.attach(SET_PIN_MODE, setPinModeCallback);
  Firmata.attach(START_SYSEX, sysexCallback);
  Firmata.attach(SYSTEM_RESET, systemResetCallback);
  Firmata.begin(57600);
  systemResetCallback();  // reset to default config

 /*Distance measurement with Ultrasonic Sensor */
  Serial.begin(9600);
  lcd.begin(16,2);                                                   
  pinMode(trigPin, OUTPUT);
  pinMode(echoPin, INPUT);
  lcd.setCursor(0,0);
  lcd.print(&quot;  Distance    &quot;);
  lcd.setCursor(0,1);
  lcd.print(&quot;  Measurement  &quot;);
  delay(2000);
  lcd.clear();
  lcd.setCursor(0,0);
  lcd.print(&quot;    Made By    &quot;);
  lcd.setCursor(0,1);
  lcd.print(&quot;    HASH    &quot;);
  delay(2000);
  lcd.clear();
}

/*==============================================================================
 * LOOP()
 *============================================================================*/
void loop() 
{
  byte pin, analogPin;
  /* DIGITALREAD - as fast as possible, check for changes and output them to the
   * FTDI buffer using Serial.print()  */
  checkDigitalInputs();  
  /* SERIALREAD - processing incoming messagse as soon as possible, while still
   * checking digital inputs.  */
  while(Firmata.available())
    Firmata.processInput();
  /* SEND FTDI WRITE BUFFER - make sure that the FTDI buffer doesn't go over
   * 60 bytes. use a timer to sending an event character every 4 ms to
   * trigger the buffer to dump. */
  currentMillis = millis();
  if (currentMillis - previousMillis &gt; samplingInterval) {
    previousMillis += samplingInterval;
    /* ANALOGREAD - do all analogReads() at the configured sampling interval */
    for(pin=0; pin&lt;TOTAL_PINS; pin++) {
      if (IS_PIN_ANALOG(pin) &amp;&amp; pinConfig[pin] == ANALOG) {
        analogPin = PIN_TO_ANALOG(pin);
        if (analogInputsToReport &amp; (1 &lt;&lt; analogPin)) {
          Firmata.sendAnalog(analogPin, analogRead(analogPin));
        }
      }
    }
    // report i2c data for all device with read continuous mode enabled
    if (queryIndex &gt; -1) {
      for (byte i = 0; i &lt; queryIndex + 1; i++) {
        readAndReportData(query[i].addr, query[i].reg, query[i].bytes);
      }
    }
  }
  /*Distance measurement with Ultrasonic Sensor */
  digitalWrite(trigPin, LOW);
  delayMicroseconds(2);
  digitalWrite(trigPin, HIGH);
  delayMicroseconds(10);
  digitalWrite(trigPin, LOW);
  duration = pulseIn(echoPin, HIGH);
  distanceCm= duration*0.034/2;                                                                                 
  lcd.setCursor(0,0);                                                 
  lcd.print(&quot;Distance Measur.&quot;);
  delay(10);
  lcd.setCursor(0,1);
  lcd.print(&quot;Distance:&quot;);
  lcd.print(distanceCm);
  lcd.print(&quot; Cm &quot;);
  Serial.print(distanceCm);
  delay(10);

}
</code></pre>
<ol start=""3"">
<li><p>You will see the Firmata.h and Firmata.cpp files in link#2 as well. Open notepad copy paste thier codes make two files on ur dekstop with the same name. Go where u have installed arduino. Find firmata folder ull see both files in there and replace the old files with these new files.
50% DONE.</p>
</li>
<li><p>Open LINK#1 and u'll see pyFirmata.py and util.py file which has the PULSE_IN feature added. The green areas are the additions  done in the code. Ull see three dots button on the left for more options select view file.</p>
</li>
<li><p>Open Python IDE open pyfirmata.py and util.py and replace it with the new code. Save.</p>
</li>
</ol>
<p>Now we have added the code for PULSE_IN in both the platforms Python and Arduino AVR. See the examples in Link#1 to make use of PULSE_IN function. ENJOY.</p>
<p>P.S. I didnt add the test.py file, couldnt find where to add it and what file to replace. If u have error try adding that as well. If u find anything do post.</p>
",14288649,4,0,110987711,FYI: [arduino.se],Incoming
161,3842,63516972,how to convert coordinates from azimuth/altitude to tilt-tilt,|robotics|astronomy|kinematics|,"<p>I'm trying to build a lightweight antenna tracker with two servos. For mechanical reasons, I'm first mounting servo1 on a base so that it tilts forward/backwards, then mount servo2 on it twisted 90˚so it can tilt left/right.</p>
<p>I can basically use the first servo to select a from the great circles that go through azimuth=0˚ and alt=0˚ and az=180˚ and alt=0˚, and use hte 2nd servo to move on the chosen great circle. This way, I should be able to point at the entire upper hemisphere even though I might need to reposition the antenna when crossing the midline (the servos only have 180 degrees of movement.)</p>
<p>I'm trying to find the function that maps az/alt to the tilt/tilt servo angles. I suspect that should be similar to how equatorial telescope mounts work, but I couldn't find a good reference on how to do it - neither do I trust my own math.</p>
<p>I found this astronomy lecture notes vaguely helpful: <a href=""http://ircamera.as.arizona.edu/Astr_518/ametry.pdf"" rel=""nofollow noreferrer"">http://ircamera.as.arizona.edu/Astr_518/ametry.pdf</a> especially page 22/23 on the ecliptic coordinate system, but I think the problem solved here is slightly different.</p>
<p>This seems a standard kinematics problem, it bothers me I can't figure it out or even find online resources. I'd be super thankful for any pointers. Happy to give more details on the servo setup.</p>
",8/21/2020 5:25,,690,1,0,-1,,12835990,,2/4/2020 0:56,54,63571545,"<p>I think I figured this out on math.stackexchange.com:</p>
<p><a href=""https://math.stackexchange.com/questions/3799191/direct-conversion-from-az-el-to-ecliptic-coordinates"">https://math.stackexchange.com/questions/3799191/direct-conversion-from-az-el-to-ecliptic-coordinates</a></p>
<p>Short answer (for python, consider parameter order for atan2 in your language):</p>
<p>$\epsilon=\atan2(\sin\delta, \cos\delta\cdot\sin\alpha)$</p>
<p>$\lambda = \arccos(\cos\alpha\cdot\cos\delta)$</p>
<p>where $\alpha$ is azimuth, $\delta$ is elevation or altitude, $epsilon$ is the angle for the first and $lambda$ the angle for the 2nd servo. This seems to work for all values of $\alpha$ and for values in $[0,\pi/2]$ for $\delta$.</p>
",12835990,0,0,,,Coordinates
162,3846,63705598,unity3d cant open port,|c#|unity-game-engine|robotics|,"<p>I'm trying to control my Dynamixel AX-12+ servo using unity3d, using the dynamixel SDK (C#). the servo is connected to my windows 10 pc using a u2d2 (not an Arduino!) and it works fine using visual studio. I imported the dll to unity and wrote a script for controlling the servomotor and there are no errors in the script but when I try to run it, it fails to open the port and after that, the visual studio code also fails to open the port (until i disconnect and reconnect the USB).</p>
<p>the part of the code trying to open the port:</p>
<pre><code>  // Open port (COM9)
  if (dynamixel.openPort(port_num))
  {
    Debug.Log(&quot;Succeeded to open the port!&quot;);
  }
  else
  {
    Debug.Log(&quot;Failed to open the port!&quot;);
    
  }
</code></pre>
<p>i used the example SDK codes to see if the motor works correctly with visual studio <a href=""https://emanual.robotis.com/docs/en/software/dynamixel/dynamixel_sdk/sample_code/csharp_read_write_protocol_1_0/#csharp-read-write-protocol-10"" rel=""nofollow noreferrer"">(like this one)</a></p>
<p>a complete unity script :</p>
<pre><code>    using System;
using System.Runtime.InteropServices;
using System.Collections;
using System.Collections.Generic;
using UnityEngine;

namespace dynamixelunity {
    
    //further below you can find the &quot;DynamixelObject&quot; class.
    public class dynamixel : MonoBehaviour
      {
        const string dll_path = &quot;dxl_x64_c&quot;;

        #region PortHandler
        [DllImport(dll_path)]
        public static extern int    portHandler         (string port_name);

        [DllImport(dll_path)]
        public static extern bool   openPort            (int port_num);
        [DllImport(dll_path)]
        public static extern void   closePort           (int port_num);
        [DllImport(dll_path)]
        public static extern void   clearPort           (int port_num);

        [DllImport(dll_path)]
        public static extern void   setPortName         (int port_num, string port_name);
        [DllImport(dll_path)]
        public static extern string getPortName         (int port_num);

        [DllImport(dll_path)]
        public static extern bool   setBaudRate         (int port_num, int baudrate);
        [DllImport(dll_path)]
        public static extern int    getBaudRate         (int port_num);

        [DllImport(dll_path)]
        public static extern int    readPort            (int port_num, byte[] packet, int length);
        [DllImport(dll_path)]
        public static extern int    writePort           (int port_num, byte[] packet, int length);

        [DllImport(dll_path)]
        public static extern void   setPacketTimeout    (int port_num, UInt16 packet_length);
        [DllImport(dll_path)]
        public static extern void   setPacketTimeoutMSec(int port_num, double msec);
        [DllImport(dll_path)]
        public static extern bool   isPacketTimeout     (int port_num);
        #endregion

        #region PacketHandler
        [DllImport(dll_path)]
        public static extern void   packetHandler       ();

        [DllImport(dll_path)]
        public static extern IntPtr getTxRxResult       (int protocol_version, int result);
        [DllImport(dll_path)]
        public static extern IntPtr getRxPacketError    (int protocol_version, byte error);

        [DllImport(dll_path)]
        public static extern int    getLastTxRxResult   (int port_num, int protocol_version);
        [DllImport(dll_path)]
        public static extern byte   getLastRxPacketError(int port_num, int protocol_version);

        [DllImport(dll_path)]
        public static extern void   setDataWrite        (int port_num, int protocol_version, UInt16 data_length, UInt16 data_pos, UInt32 data);
        [DllImport(dll_path)]
        public static extern UInt32 getDataRead         (int port_num, int protocol_version, UInt16 data_length, UInt16 data_pos);

        [DllImport(dll_path)]
        public static extern void   txPacket            (int port_num, int protocol_version);

        [DllImport(dll_path)]
        public static extern void   rxPacket            (int port_num, int protocol_version);

        [DllImport(dll_path)]
        public static extern void   txRxPacket          (int port_num, int protocol_version);

        [DllImport(dll_path)]
        public static extern void   ping                (int port_num, int protocol_version, byte id);

        [DllImport(dll_path)]
        public static extern UInt16 pingGetModelNum     (int port_num, int protocol_version, byte id);

        [DllImport(dll_path)]
        public static extern void   broadcastPing       (int port_num, int protocol_version);
        [DllImport(dll_path)]
        public static extern bool   getBroadcastPingResult(int port_num, int protocol_version, int id);

        [DllImport(dll_path)]
        public static extern void   reboot              (int port_num, int protocol_version, byte id);

        [DllImport(dll_path)]
        public static extern void   factoryReset        (int port_num, int protocol_version, byte id, byte option);

        [DllImport(dll_path)]
        public static extern void   readTx              (int port_num, int protocol_version, byte id, UInt16 address, UInt16 length);
        [DllImport(dll_path)]
        public static extern void   readRx              (int port_num, int protocol_version, UInt16 length);
        [DllImport(dll_path)]
        public static extern void   readTxRx            (int port_num, int protocol_version, byte id, UInt16 address, UInt16 length);

        [DllImport(dll_path)]
        public static extern void   read1ByteTx         (int port_num, int protocol_version, byte id, UInt16 address);
        [DllImport(dll_path)]
        public static extern byte   read1ByteRx         (int port_num, int protocol_version);
        [DllImport(dll_path)]
        public static extern byte   read1ByteTxRx       (int port_num, int protocol_version, byte id, UInt16 address);

        [DllImport(dll_path)]
        public static extern void   read2ByteTx         (int port_num, int protocol_version, byte id, UInt16 address);
        [DllImport(dll_path)]
        public static extern UInt16 read2ByteRx         (int port_num, int protocol_version);
        [DllImport(dll_path)]
        public static extern UInt16 read2ByteTxRx       (int port_num, int protocol_version, byte id, UInt16 address);

        [DllImport(dll_path)]
        public static extern void   read4ByteTx         (int port_num, int protocol_version, byte id, UInt16 address);
        [DllImport(dll_path)]
        public static extern UInt32 read4ByteRx         (int port_num, int protocol_version);
        [DllImport(dll_path)]
        public static extern UInt32 read4ByteTxRx       (int port_num, int protocol_version, byte id, UInt16 address);

        [DllImport(dll_path)]
        public static extern void   writeTxOnly         (int port_num, int protocol_version, byte id, UInt16 address, UInt16 length);
        [DllImport(dll_path)]
        public static extern void   writeTxRx           (int port_num, int protocol_version, byte id, UInt16 address, UInt16 length);

        [DllImport(dll_path)]
        public static extern void   write1ByteTxOnly    (int port_num, int protocol_version, byte id, UInt16 address, byte data);
        [DllImport(dll_path)]
        public static extern void   write1ByteTxRx      (int port_num, int protocol_version, byte id, UInt16 address, byte data);

        [DllImport(dll_path)]
        public static extern void   write2ByteTxOnly    (int port_num, int protocol_version, byte id, UInt16 address, UInt16 data);
        [DllImport(dll_path)]
        public static extern void   write2ByteTxRx      (int port_num, int protocol_version, byte id, UInt16 address, UInt16 data);

        [DllImport(dll_path)]
        public static extern void   write4ByteTxOnly    (int port_num, int protocol_version, byte id, UInt16 address, UInt32 data);
        [DllImport(dll_path)]
        public static extern void   write4ByteTxRx      (int port_num, int protocol_version, byte id, UInt16 address, UInt32 data);

        [DllImport(dll_path)]
        public static extern void   regWriteTxOnly      (int port_num, int protocol_version, byte id, UInt16 address, UInt16 length);
        [DllImport(dll_path)]
        public static extern void   regWriteTxRx        (int port_num, int protocol_version, byte id, UInt16 address, UInt16 length);

        [DllImport(dll_path)]
        public static extern void   syncReadTx          (int port_num, int protocol_version, UInt16 start_address, UInt16 data_length, UInt16 param_length);
        // syncReadRx   -&gt; GroupSyncRead
        // syncReadTxRx -&gt; GroupSyncRead

        [DllImport(dll_path)]
        public static extern void   syncWriteTxOnly     (int port_num, int protocol_version, UInt16 start_address, UInt16 data_length, UInt16 param_length);

        [DllImport(dll_path)]
        public static extern void   bulkReadTx          (int port_num, int protocol_version, UInt16 param_length);
        // bulkReadRx   -&gt; GroupBulkRead
        // bulkReadTxRx -&gt; GroupBulkRead

        [DllImport(dll_path)]
        public static extern void   bulkWriteTxOnly     (int port_num, int protocol_version, UInt16 param_length);
        #endregion

        #region GroupBulkRead
        [DllImport(dll_path)]
        public static extern int    groupBulkRead       (int port_num, int protocol_version);

        [DllImport(dll_path)]
        public static extern bool   groupBulkReadAddParam   (int group_num, byte id, UInt16 start_address, UInt16 data_length);
        [DllImport(dll_path)]
        public static extern void   groupBulkReadRemoveParam(int group_num, byte id);
        [DllImport(dll_path)]
        public static extern void   groupBulkReadClearParam (int group_num);

        [DllImport(dll_path)]
        public static extern void   groupBulkReadTxPacket   (int group_num);
        [DllImport(dll_path)]
        public static extern void   groupBulkReadRxPacket   (int group_num);
        [DllImport(dll_path)]
        public static extern void   groupBulkReadTxRxPacket (int group_num);

        [DllImport(dll_path)]
        public static extern bool   groupBulkReadIsAvailable(int group_num, byte id, UInt16 address, UInt16 data_length);
        [DllImport(dll_path)]
        public static extern UInt32 groupBulkReadGetData    (int group_num, byte id, UInt16 address, UInt16 data_length);
        #endregion

        #region GroupBulkWrite
        [DllImport(dll_path)]
        public static extern int    groupBulkWrite            (int port_num, int protocol_version);

        [DllImport(dll_path)]
        public static extern bool   groupBulkWriteAddParam    (int group_num, byte id, UInt16 start_address, UInt16 data_length, UInt32 data, UInt16 input_length);
        [DllImport(dll_path)]
        public static extern void   groupBulkWriteRemoveParam (int group_num, byte id);
        [DllImport(dll_path)]
        public static extern bool   groupBulkWriteChangeParam (int group_num, byte id, UInt16 start_address, UInt16 data_length, UInt32 data, UInt16 input_length, UInt16 data_pos);
        [DllImport(dll_path)]
        public static extern void   groupBulkWriteClearParam  (int group_num);

        [DllImport(dll_path)]
        public static extern void   groupBulkWriteTxPacket    (int group_num);
        #endregion

        #region GroupSyncRead
        [DllImport(dll_path)]
        public static extern int    groupSyncRead             (int port_num, int protocol_version, UInt16 start_address, UInt16 data_length);

        [DllImport(dll_path)]
        public static extern bool   groupSyncReadAddParam     (int group_num, byte id);
        [DllImport(dll_path)]
        public static extern void   groupSyncReadRemoveParam  (int group_num, byte id);
        [DllImport(dll_path)]
        public static extern void   groupSyncReadClearParam   (int group_num);

        [DllImport(dll_path)]
        public static extern void   groupSyncReadTxPacket     (int group_num);
        [DllImport(dll_path)]
        public static extern void   groupSyncReadRxPacket     (int group_num);
        [DllImport(dll_path)]
        public static extern void   groupSyncReadTxRxPacket   (int group_num);

        [DllImport(dll_path)]
        public static extern bool   groupSyncReadIsAvailable  (int group_num, byte id, UInt16 address, UInt16 data_length);
        [DllImport(dll_path)]
        public static extern UInt32 groupSyncReadGetData      (int group_num, byte id, UInt16 address, UInt16 data_length);
        #endregion

        #region GroupSyncWrite
        [DllImport(dll_path)]
        public static extern int    groupSyncWrite            (int port_num, int protocol_version, UInt16 start_address, UInt16 data_length);

        [DllImport(dll_path)]
        public static extern bool   groupSyncWriteAddParam    (int group_num, byte id, UInt32 data, UInt16 data_length);
        [DllImport(dll_path)]
        public static extern void   groupSyncWriteRemoveParam (int group_num, byte id);
        [DllImport(dll_path)]
        public static extern bool   groupSyncWriteChangeParam (int group_num, byte id, UInt32 data, UInt16 data_length, UInt16 data_pos);
        [DllImport(dll_path)]
        public static extern void   groupSyncWriteClearParam  (int group_num);

        [DllImport(dll_path)]
        public static extern void   groupSyncWriteTxPacket    (int group_num);
        #endregion
      }



    public class DynamixelObject : MonoBehaviour {

        // Control table address
        public const int ADDR_MX_TORQUE_ENABLE           = 24;                  // Control table address is different in Dynamixel model
        public const int ADDR_MX_GOAL_POSITION           = 30;
        public const int ADDR_MX_PRESENT_POSITION        = 36;

        // Protocol version
        public const int PROTOCOL_VERSION                = 1;                   // See which protocol version is used in the Dynamixel

        // Default setting
        public const int DXL_ID                          = 1;                   // Dynamixel ID: 1 
        public const int BAUDRATE                        = 1000000;
        public const string DEVICENAME                   = &quot;COM9&quot;;              // Check which port is being used on your controller
                                                                                // ex) Windows: &quot;COM1&quot;   Linux: &quot;/dev/ttyUSB0&quot; Mac: &quot;/dev/tty.usbserial-*&quot;

        public const int TORQUE_ENABLE                   = 1;                   // Value for enabling the torque
        public const int TORQUE_DISABLE                  = 0;                   // Value for disabling the torque
        public const int DXL_MINIMUM_POSITION_VALUE      = 100;                 // Dynamixel will rotate between this value
        public const int DXL_MAXIMUM_POSITION_VALUE      = 4000;                // and this value (note that the Dynamixel would not move when the position value is out of movable range. s
        public const int DXL_MOVING_STATUS_THRESHOLD     = 10;                  // Dynamixel moving status threshold

        public const byte ESC_ASCII_VALUE                = 0x1b;

        public const int COMM_SUCCESS                    = 0;                   // Communication Success result value
        public const int COMM_TX_FAIL                    = -1001;               // Communication Tx Failed

        // Initialize PortHandler Structs
          // Set the port path
          // Get methods and members of PortHandlerLinux or PortHandlerWindows
          
        int port_num = dynamixel.portHandler(DEVICENAME);
        
        void start(){
            

          // Initialize PacketHandler Structs
          dynamixel.packetHandler();

          int index = 0;
          int dxl_comm_result = COMM_TX_FAIL;                                   // Communication result
          UInt16[] dxl_goal_position = new UInt16[2]{ DXL_MINIMUM_POSITION_VALUE, DXL_MAXIMUM_POSITION_VALUE };         // Goal position

          byte dxl_error = 0;                                                   // Dynamixel error
          UInt16 dxl_present_position = 0;                                      // Present position

          // Open port (COM9)
          if (dynamixel.openPort(port_num))
          {
            Debug.Log(&quot;Succeeded to open the port!&quot;);
          }
          else
          {
            Debug.Log(&quot;Failed to open the port!&quot;);

          }

          // Set port baudrate
          if (dynamixel.setBaudRate(port_num, BAUDRATE))
          {
            Debug.Log(&quot;Succeeded to change the baudrate!&quot;);
          }
          else
          {
            Debug.Log(&quot;Failed to change the baudrate!&quot;);
          }

        }
        
        void Update()
        {
            if (Input.GetKeyDown(KeyCode.Space))
            {
                //Enable motor torque
                dynamixel.write1ByteTxRx(port_num, PROTOCOL_VERSION, DXL_ID, ADDR_MX_TORQUE_ENABLE, TORQUE_ENABLE);
            }

            if (Input.GetKeyUp(KeyCode.Space))
            {
                //disable motor torque
                dynamixel.write1ByteTxRx(port_num, PROTOCOL_VERSION, DXL_ID, ADDR_MX_TORQUE_ENABLE, TORQUE_DISABLE);
            }
        }
    }
}
</code></pre>
<p>Does anyone have any suggestions?</p>
",9/2/2020 12:31,63814395,319,1,7,0,,11782176,,7/14/2019 9:35,2,63814395,"<p>Ok, I found out what was wrong!
I forgot to close the port after exiting unity's play mode.
the code below closes the port after exiting play mode:</p>
<pre><code>    [InitializeOnLoad]
public static class PlayStateNotifier
{

    static PlayStateNotifier()
    {
        EditorApplication.playModeStateChanged += ModeChanged;
    }

    static void ModeChanged(PlayModeStateChange playModeState)
    {
        if (playModeState == PlayModeStateChange.EnteredEditMode)
        {
            Debug.Log(&quot;Entered Edit mode.&quot;);
            Debug.Log(&quot;Closing dynamixel port! :D&quot;);
            dynamixel.closePort(DynamixelObject.port_num);
        }
    }
}
</code></pre>
",11782176,0,0,112710303,"Did you see following on link : USB2DYNAMIXELs purchased before July 2015 may not work with the latest FTDI drivers 2.12.00. For a fix, please see our blog post on the matter",Connections
163,3893,64386937,Navigating the robot by finding the shortest way,|python|for-loop|shortest-path|robotics|,"<p>I have this task where I have to make my robot find the shortest way when I enter a destination point. I've created some functions to assign numbers (distances) for each square and counts its way back to my robot by deleting the other options. Then the robot should only follow the numbers.</p>
<p><img src=""https://i.stack.imgur.com/ZzM52.jpg"" alt=""Here's also a screenshot of the map it navigates in:
"" /></p>
<p>My code works so far, however I think I should be able to reach the same outcome by using less for loops and with a more efficient writing. I believe if I see different ways of thinking in my earlier stages, I can have a broader perspective in the future. So, any ideas on how to reach my goal with a shorter code?</p>
<pre><code>    #Your Code Starts Here
&quot;&quot;&quot;
for x in range(0, map.width):
    for y in range(0, map.height): 
        if map.blocked(x, y):
            map.value[x, y] = -1 
        else:
            map.value[x, y] = 0
&quot;&quot;&quot;
def fill_around(n,m,number):
    for x in range (n-1,n+2):
        for y in range (m-1,m+2):
            if map.value[x,y] != 0:
                pass
            elif x==n+1 and y==m+1 or x==n-1 and y==m-1 or x==n-1 and y==m+1 or x==n+1 and y==m-1:
                pass
            elif map.blocked(x,y) == True:
                map.value[x,y]= -1            
            elif x==n and y==m:
                map.value[x,y]= number
            else:
                map.value[x,y]= number+1


def till_final(final_x,final_y):
final_x=9 
final_y=1
fill_around(1,1,1)
for p in range(2,17):
    for k in range(0, map.width):
        for l in range(0, map.height):
            if map.value[final_x,final_y] ==0 and map.value[k,l]==p:
                fill_around(k,l,p)

def delete_duplicates(final_x,final_y):
for k in range(0, map.width):
    for l in range(0, map.height):
        if map.value[k,l] == map.value[final_x,final_y] and k != final_x and l != final_y:
            map.value[k,l] = 0
</code></pre>
",10/16/2020 9:56,,126,0,2,1,,10329083,,9/7/2018 6:27,1,,,,,,113854038,"If your code works and you are looking for advice on improving it, your question is off-topic for SO, but would probably be a good fit for https://codereview.stackexchange.com/.",Other
164,3898,64417385,How to find the transformation between two IMU measurements?,|quaternions|robotics|imu|,"<p>I am looking at the Clubs Dataset <a href=""https://clubs.github.io/"" rel=""nofollow noreferrer"">https://clubs.github.io/</a> for some research into multiway registration of point clouds. I am initially trying to use the ICP registration method in a sequential order adding one point cloud at a time.</p>
<p>The dataset has the RGB parameters for the various poses of the object.</p>
<pre><code>1525694104,-0.0913515162993169,-0.16815189159691318,0.4504956847817425,0.4591556084159897,-0.7817619820248951,-0.3682132612946675,0.20601769355692517
1525694157,-0.22510740390250225,-0.32514596548025265,0.45221561140129063,0.2388698281592328,-0.8750788198918591,-0.4081451880445991,0.10293936130543749
1525694174,-0.4179094161019803,-0.39403349319958664,0.4522321523188167,-0.004021371419719342,0.9070013543525104,0.4210736433584828,0.005455788205342825
</code></pre>
<p>The columns are namely</p>
<blockquote>
<p>filename, translation across 3 axes, quaternion for rotation.</p>
</blockquote>
<p>I converted the images into point clouds and when I try to align the point clouds, I would need to have a good estimation of the transformation. Given the measurements at two points, how would I find out the transformation between those points. I would use that as my intiial estimate of my ICP registration algorithm.</p>
",10/18/2020 19:18,,131,0,0,1,,3961840,,8/20/2014 20:26,4,,,,,,,,Coordinates
165,3910,64563152,Issue with Google's edge TPU compiler,|artificial-intelligence|robotics|tpu|google-coral|,"<p>I tried to install edge TPU compiler on my raspberrypi 3b+ but apparently it is no longer supported on 32bit. Can I install compiler on a 64bit machine, compile my code, turn it into a code readable by the edge tpu and bring this code back and run it on my raspberry pi? Or will there be conflict of some sort?</p>
",10/27/2020 21:28,,115,1,2,1,,14531758,,10/27/2020 21:23,2,64579829,"<ul>
<li><p>As far as application goes:
You can write your application code on the rpi. The code that you compiles on an x86-64 machine won't be executable on the rpi due to architecture differences. Unless you are using a cross compiler.</p>
</li>
<li><p>Now on the edgetpu compiler:
The edgetpu compiler can only be installed on the <code>x86_64</code> machine, although since the format of the model is the same for all platform (flat buffer), you can use the same model compiled on the <code>x86_64</code> platform on your rpi just fine!</p>
</li>
</ul>
",6262757,0,0,114167270,"Assuming that you have connected a TPU device(Like USB Accelerator) with your RPI. You can run the compiled model on that RPI. Please see the requirements at : https://coral.ai/docs/accelerator/get-started/#requirements.
It's just that the compiler cannot be installed on a 32-bit machine but the output (.tflite) format can be executed on TPU connected to a ARMv7 32-bit machine.",Programming
166,3924,65115603,lost in 3D space - tilt values (euler?) from rotation matrix (javafx affine) only works partially,|javafx|rotation|robotics|,"<p>it is a while ago that I asked this question:
<a href=""https://stackoverflow.com/questions/48850937/javafx-how-to-apply-yaw-pitch-and-roll-deltas-not-euler-to-a-node-in-respec"">javafx - How to apply yaw, pitch and roll deltas (not euler) to a node in respect to the nodes rotation axes instead of the scene rotation axes?</a></p>
<p>Today I want to ask, how I can get the tilt (fore-back and sideways) relative to the body (not to the room) from the rotation matrix. To make the problem understandable, I took the final code from the fantastic answer of José Pereda and basicly added a method &quot;getEulersFromRotationMatrix&quot;. This is working a bit, but at some point freaks out.</p>
<p>Attached find the whole working example. The problem becomes clear with the following click path:</p>
<pre><code>// right after start
tilt fore
tilt left  // all right
tilt right
tilt back  // all right

// right after start
turn right
turn right
turn right
tilt fore
tilt back  // all right
tilt left  // bang, tilt values are completely off
</code></pre>
<p>While the buttons move the torso as expected, the tilt values (printed out under the buttons) behave wrong at some point.</p>
<pre><code>import javafx.application.Application;
import javafx.application.Platform;
import javafx.event.ActionEvent;
import javafx.event.EventHandler;
import javafx.geometry.Point3D;
import javafx.scene.Group;
import javafx.scene.Parent;
import javafx.scene.PerspectiveCamera;
import javafx.scene.Scene;
import javafx.scene.SceneAntialiasing;
import javafx.scene.SubScene;
import javafx.scene.control.Button;
import javafx.scene.control.Label;
import javafx.scene.layout.HBox;
import javafx.scene.layout.StackPane;
import javafx.scene.layout.VBox;
import javafx.scene.paint.Color;
import javafx.scene.paint.PhongMaterial;
import javafx.scene.shape.Box;
import javafx.scene.transform.Affine;
import javafx.scene.transform.Rotate;
import javafx.stage.Stage;


public class PuppetTestApp extends Application {
    
    private final int width = 800;
    private final int height = 500;
    private XGroup torsoGroup;
    private final double torsoX = 50;
    private final double torsoY = 80;

    private Label output = new Label();

    public Parent createRobot() {
        Box torso = new Box(torsoX, torsoY, 20);
        torso.setMaterial(new PhongMaterial(Color.RED));
        Box head = new Box(20, 20, 20);
        head.setMaterial(new PhongMaterial(Color.YELLOW.darker()));
        head.setTranslateY(-torsoY / 2 -10);

        Box x = new Box(200, 2, 2);
        x.setMaterial(new PhongMaterial(Color.BLUE));
        Box y = new Box(2, 200, 2);
        y.setMaterial(new PhongMaterial(Color.BLUEVIOLET));
        Box z = new Box(2, 2, 200);
        z.setMaterial(new PhongMaterial(Color.BURLYWOOD));

        torsoGroup = new XGroup();
        torsoGroup.getChildren().addAll(torso, head, x, y, z);
        return torsoGroup;
    }

    public Parent createUI() {
        HBox buttonBox = new HBox();

        Button b;
        buttonBox.getChildren().add(b = new Button(&quot;Exit&quot;));
        b.setOnAction( (ActionEvent arg0) -&gt; { Platform.exit(); } );

        buttonBox.getChildren().add(b = new Button(&quot;tilt fore&quot;));
        b.setOnAction(new TurnAction(torsoGroup.rx, 15) );

        buttonBox.getChildren().add(b = new Button(&quot;tilt back&quot;));
        b.setOnAction(new TurnAction(torsoGroup.rx, -15) );

        buttonBox.getChildren().add(b = new Button(&quot;tilt left&quot;));
        b.setOnAction(new TurnAction(torsoGroup.rz, 15) );

        buttonBox.getChildren().add(b = new Button(&quot;tilt right&quot;));
        b.setOnAction(new TurnAction(torsoGroup.rz, -15) );

        buttonBox.getChildren().add(b = new Button(&quot;turn left&quot;));
        b.setOnAction(new TurnAction(torsoGroup.ry, -28) ); // not 30 degree to avoid any gymbal lock problems

        buttonBox.getChildren().add(b = new Button(&quot;turn right&quot;));
        b.setOnAction(new TurnAction(torsoGroup.ry, 28) ); // not 30 degree to avoid any gymbal lock problems

        VBox vbox = new VBox();
        vbox.getChildren().add(buttonBox);
        vbox.getChildren().add(output);
        return vbox;
    }

    class TurnAction implements EventHandler&lt;ActionEvent&gt; {
        final Rotate rotate;
        double deltaAngle;

        public TurnAction(Rotate rotate, double targetAngle) {
            this.rotate = rotate;
            this.deltaAngle = targetAngle;
        }

        @Override
        public void handle(ActionEvent arg0) {
            addRotate(torsoGroup, rotate, deltaAngle);
        } 
    }

    private void addRotate(XGroup node, Rotate rotate, double angle) {
        Affine affine = node.getTransforms().isEmpty() ? new Affine() : new Affine(node.getTransforms().get(0));
        double A11 = affine.getMxx(), A12 = affine.getMxy(), A13 = affine.getMxz(); 
        double A21 = affine.getMyx(), A22 = affine.getMyy(), A23 = affine.getMyz(); 
        double A31 = affine.getMzx(), A32 = affine.getMzy(), A33 = affine.getMzz(); 

        Rotate newRotateX = new Rotate(angle, new Point3D(A11, A21, A31));
        Rotate newRotateY = new Rotate(angle, new Point3D(A12, A22, A32));
        Rotate newRotateZ = new Rotate(angle, new Point3D(A13, A23, A33));

        affine.prepend(rotate.getAxis() == Rotate.X_AXIS ? newRotateX : 
                rotate.getAxis() == Rotate.Y_AXIS ? newRotateY : newRotateZ);

        EulerValues euler= getEulersFromRotationMatrix(affine);
        output.setText(String.format(&quot;tilt fore/back=%3.0f    tilt sideways=%3.0f&quot;, euler.forward, euler.leftSide));
        
        node.getTransforms().setAll(affine);
    }

    public class XGroup extends Group {
        public Rotate rx = new Rotate(0, Rotate.X_AXIS);
        public Rotate ry = new Rotate(0, Rotate.Y_AXIS);
        public Rotate rz = new Rotate(0, Rotate.Z_AXIS);
    }

    @Override 
    public void start(Stage stage) throws Exception {
        Parent robot = createRobot();
        SubScene subScene = new SubScene(robot, width, height, true, SceneAntialiasing.BALANCED);
        PerspectiveCamera camera = new PerspectiveCamera(true);
        camera.setNearClip(0.01);
        camera.setFarClip(100000);
        camera.setTranslateZ(-400);
        subScene.setCamera(camera);

        Parent ui = createUI();
        StackPane combined = new StackPane(ui, subScene);
        combined.setStyle(&quot;-fx-background-color: linear-gradient(to bottom, cornsilk, midnightblue);&quot;);

        Scene scene = new Scene(combined, width, height);
        stage.setScene(scene);
        stage.show();
    }

    /**
     * Shall return the tilt values relative to the body (not relative to the room)
     * (Maybe euler angles are not the right term here, but anyway)
     */
    private EulerValues getEulersFromRotationMatrix(Affine rot) {
        double eulerX;  // turn left/right
        double eulerY;  // tilt fore/back
        double eulerZ;  // tilt sideways

        double r11 = rot.getMxx();
        double r12 = rot.getMxy();
        double r13 = rot.getMxz();

        double r21 = rot.getMyx();

        double r31 = rot.getMzx();
        double r32 = rot.getMzy();
        double r33 = rot.getMzz();

        // used instructions from https://www.gregslabaugh.net/publications/euler.pdf
        
        if (r31 != 1.0 &amp;&amp; r31 != -1.0) {
            eulerX  = -Math.asin(r31);  // already tried with the 2nd solution as well
            double cosX = Math.cos(eulerX);
            eulerY = Math.atan2(r32/cosX, r33/cosX);
            eulerZ = Math.atan2(r21/cosX, r11/cosX);
        }
        else {
            eulerZ = 0;
            if (r31 == -1) {
                eulerX = Math.PI / 2;
                eulerY = Math.atan2(r12, r13);
            }
            else {
                eulerX = -Math.PI / 2;
                eulerY = Math.atan2(-r12, -r13);
            }
        }
        
        return new EulerValues(
                eulerY / Math.PI * 180.0, 
                eulerZ / Math.PI * 180.0, 
                -eulerX / Math.PI * 180.0);     
    }
    
    public class EulerValues {
        public double leftTurn;
        public double forward;
        public double leftSide;

        public EulerValues(double forward, double leftSide, double leftTurn) {
            this.forward = forward;
            this.leftSide = leftSide;
            this.leftTurn = leftTurn;
        }
    }


    public static void main(String[] args) {
        launch(args);
    }
}

</code></pre>
<p>PS: This may look like I have close to no progress, but this is only because I try to reduce the question to the possible minimum. If you want to see how this stuff is embedded in my main project, you can watch this little video I just uploaded (but does not add anything to the question): <a href=""https://www.youtube.com/watch?v=R3t8BIHeo7k"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=R3t8BIHeo7k</a></p>
",12/2/2020 20:23,65382260,249,1,3,0,,2081279,"Munich, Germany",2/17/2013 20:41,87,65382260,"<p>I think I got it by myself now: What I computed was the &quot;default&quot; euler angles, sometimes refered to as z x' z'', where the 1st and 3th rotation is around the same axis. But what I am looking for are the angles that can be applied to the z, y' and x'' achses (in that order) to reach the position presented by the rotation matrix. (and then ignore the z rotation).</p>
<p>Or even better compute the z y' x'' eulers and the z x' y'' eulers and
only use the x' and y' values.</p>
<p>Added:
No, that was wrong. I indeed calculated the Tait-Bryan x y z rotations. So this was not the solution.</p>
<p>Ok, new explanation:</p>
<p>The rotation axes wthat I calculate are room relative rotations (not object relative rotations), and the 2nd rotation is at the vertical axe (which I am not interested in). But because it is &quot;in the middle&quot;, it can cancel out the 1st and 3th rotation, and this is what happens.</p>
<p>So the solution should be the change the rotation order, that comes out of my matrix-to-euler algorithm. But how to do this?</p>
<p>I just exchanged all &quot;y&quot; and &quot;z&quot;:</p>
<pre><code>    r11 = rot.getMxx();
    r12 = rot.getMxz();
    r13 = rot.getMxy();

    r21 = rot.getMzx();

    r31 = rot.getMyx();
    r32 = rot.getMyz();
    r33 = rot.getMyy();
</code></pre>
<p>and now it really does what I want.  :)</p>
",2081279,0,0,115405053,"Ok - I was re-testing with a different version. Back to the version I posted above: When I do 3x turn-right, then tilt-fore and tilt-back, then I get values as expected: tilt-fore/back=0 and tilt-sideways=0. But when I add on single tilt-left to that, then I get tilt-fore/back=68 and tilt-sideways=69. Isn't this completely off from what we should expect, as both tilts were zero before, and we just did a 30° tilting to one side? Or do you get different values?",Coordinates
167,3937,65649719,PID Control: Is adding a delay before the next loop a good idea?,|c++|robotics|control-theory|pid-controller|,"<p>I am implementing PID control in c++ to make a differential drive robot turn an accurate number of degrees, but I am having many issues.</p>
<p><strong>Exiting control loop early due to fast loop runtime</strong></p>
<p>If the robot measures its error to be less than .5 degrees, it exits the control loop and consider the turn &quot;finished&quot; (the .5 is a random value that I might change at some point). It appears that the control loop is running so quickly that the robot can turn at a very high speed, turn <em>past</em> the setpoint, and exit the loop/cut motor powers, because it <em>was</em> at the setpoint for a short instant. I know that this is the entire purpose of PID control, to accurately reach the setpoint without overshooting, but this problem is making it very difficult to tune the PID constants. For example, I try to find a value of kp such that there is steady oscillation, but there is never any oscillation because the robot thinks it has &quot;finished&quot; once it passes the setpoint. To fix this, I have implemented a system where the robot has to be at the setpoint for a certain period of time before exiting, and this has been effective, allowing oscillation to occur, but the issue of exiting the loop early seems like an unusual problem and my solution may be incorrect.</p>
<p><strong>D term has no effect due to fast runtime</strong></p>
<p>Once I had the robot oscillating in a controlled manner using only P, I tried to add D to prevent overshoot. However, this was having no effect for the majority of the time, because the control loop is running so quickly that 19 loops out of 20, the rate of change of error is 0: the robot did not move or did not move enough for it to be measured in that time. I printed the change in error and the derivative term each loop to confirm this and I could see that these would both be 0 for around 20 loop cycles before taking a reasonable value and then back to 0 for another 20 cycles. Like I said, I think that this is because the loop cycles are so quick that the robot literally hasn't moved enough for any sort of noticeable change in error. This was a big problem because it meant that the D term had essentially no effect on robot movement because it was almost always 0. To fix this problem, I tried using the last non-zero value of the derivative in place of any 0 values, but this didn't work well, and the robot would oscillate randomly if the last derivative didn't represent the current rate of change of error.</p>
<p><strong>Note: I am also using a small feedforward for the static coefficient of friction, and I call this feedforward &quot;f&quot;</strong></p>
<p><strong>Should I add a delay?</strong></p>
<p>I realized that I think the source of both of these issues is the loop running very very quickly, so something I thought of was adding a wait statement at the end of the loop. However, it seems like an overall bad solution to intentionally slow down a loop. Is this a good idea?</p>
<pre><code>turnHeading(double finalAngle, double kp, double ki, double kd, double f){
    std::clock_t timer;
    timer = std::clock();

    double pastTime = 0;
    double currentTime = ((std::clock() - timer) / (double)CLOCKS_PER_SEC);

    const double initialHeading = getHeading();
    finalAngle = angleWrapDeg(finalAngle);

    const double initialAngleDiff = initialHeading - finalAngle;
    double error = angleDiff(getHeading(), finalAngle);
    double pastError = error;

    double firstTimeAtSetpoint = 0;
    double timeAtSetPoint = 0;
    bool atSetpoint = false;

    double integral = 0;
    double derivative = 0;
    double lastNonZeroD = 0;

    while (timeAtSetPoint &lt; .05)
    {
        updatePos(encoderL.read(), encoderR.read());
        error = angleDiff(getHeading(), finalAngle);

        currentTime = ((std::clock() - timer) / (double)CLOCKS_PER_SEC);
        double dt = currentTime - pastTime;

        double proportional = error / fabs(initialAngleDiff);
        integral += dt * ((error + pastError) / 2.0);
        double derivative = (error - pastError) / dt;
        
        //FAILED METHOD OF USING LAST NON-0 VALUE OF DERIVATIVE
        // if(epsilonEquals(derivative, 0))
        // {
        //     derivative = lastNonZeroD;
        // }
        // else
        // {
        //     lastNonZeroD = derivative;
        // }

        double power = kp * proportional + ki * integral + kd * derivative;

        if (power &gt; 0)
        {
            setMotorPowers(-power - f, power + f);
        }
        else
        {
            setMotorPowers(-power + f, power - f);
        }

        if (fabs(error) &lt; 2)
        {
            if (!atSetpoint)
            {
                atSetpoint = true;
                firstTimeAtSetpoint = currentTime;
            }
            else //at setpoint
            {
                timeAtSetPoint = currentTime - firstTimeAtSetpoint;
            }
        }
        else //no longer at setpoint
        {
            atSetpoint = false;
            timeAtSetPoint = 0;
        }
        pastTime = currentTime;
        pastError = error;
    }
    setMotorPowers(0, 0);
}

turnHeading(90, .37, 0, .00004, .12);
</code></pre>
",1/10/2021 2:49,,202,0,3,0,,14486787,Austin,10/20/2020 16:15,5,,,,,,116072604,"You might get more results posting the in the Electrical Engineering group. Lots of experienced control theory people there. Also, if you log position, time, and motor control you can get a lot of info to adjust feedback constants.",Actuator
168,3940,65679789,Obtaining input from Joystick with C# and DirectX using Tutorial,|c#|directx|visual-studio-2019|robotics|joystick|,"<p>I'm using Visual Studio 2019 my question is closely aligned with the use of this <a href=""https://www.codeproject.com/Tips/850730/Obtaining-Input-Form-a-Joystick-with-Csharp-and-Di?msg=5633819#xx5633819xx"" rel=""nofollow noreferrer"">Tutorial</a>. I've done all the steps such as:</p>
<ol>
<li>Creation of a Window Form in C#</li>
<li>Adding existing file Joystick.cs</li>
<li>Adding Reference item</li>
<li>Added Application Configuration File and pasted the code provided</li>
<li>Pasted example code into Form1.cs</li>
</ol>
<p>My question: What am I missing? I can't access the library for joystick <a href=""https://i.stack.imgur.com/ehI7w.png"" rel=""nofollow noreferrer"">Screenshot of Error</a></p>
<p>Here is the whole Form1.cs code:</p>
<pre><code>using System;
using System.Collections.Generic;
using System.ComponentModel;
using System.Data;
using System.Drawing;
using System.Linq;
using System.Text;
using System.Threading.Tasks;
using System.Windows.Forms;

namespace RunJoyStickOnLocalMachine{

public partial class Form1 : Form
{
    public Form1()
    {
        InitializeComponent();
    }

    private void Form1_Load(object sender, EventArgs e)
    {

    }
    private void joystickTimer_Tick_1(object sender, EventArgs e)
    {
        try
        {
            joystick.UpdateStatus();
            joystickButtons = joystick.buttons;

            if (joystick.Xaxis == 0)
                output.Text += &quot;Left\n&quot;;

            if (joystick.Xaxis == 65535)
                output.Text += &quot;Right\n&quot;;

            if (joystick.Yaxis == 0)
                output.Text += &quot;Up\n&quot;;

            if (joystick.Yaxis == 65535)
                output.Text += &quot;Down\n&quot;;

            for (int i = 0; i &lt; joystickButtons.Length; i++)
            {
                if (joystickButtons[i] == true)
                    output.Text += &quot;Button &quot; + i + &quot; Pressed\n&quot;;
            }
        }
        catch
        {
            joystickTimer.Enabled = false;
            connectToJoystick(joystick);
        }
    }
}
}
</code></pre>
<p>If there is anything else that I need to provide please notify me.</p>
<p>Thank You!</p>
<hr />
<p>Edit: I've solved this problem by comparing both the tutorial file and the step by step tutorial file that the blog has.</p>
",1/12/2021 7:44,65695088,4319,1,2,0,0,10965128,,1/25/2019 2:39,9,65695088,"<p>You can download the project from the article you mentioned to check the code.</p>
<p>Based on my test, you can use the following code in the form1.cs code after you add <code>theJoystick.cs</code> to your project.</p>
<pre><code>public partial class Form1 : Form
    {

        private Joystick joystick;        // define the type Joystick
        private bool[] joystickButtons;  // here  define the bool array
        public Form1()
        {
            InitializeComponent();
            joystick = new Joystick(this.Handle);
            connectToJoystick(joystick);
        }

        private void joystickTimer_Tick(object sender, EventArgs e)
        {
            try
            {
                joystick.UpdateStatus();
                joystickButtons = joystick.buttons;

                if (joystick.Xaxis == 0)
                    output.Text += &quot;Left\n&quot;;    // output is the name of richtextbox

                if (joystick.Xaxis == 65535)
                    output.Text += &quot;Right\n&quot;;

                if (joystick.Yaxis == 0)
                    output.Text += &quot;Up\n&quot;;

                if (joystick.Yaxis == 65535)
                    output.Text += &quot;Down\n&quot;;

                for (int i = 0; i &lt; joystickButtons.Length; i++)
                {
                    if (joystickButtons[i] == true)
                        output.Text += &quot;Button &quot; + i + &quot; Pressed\n&quot;;
                }
            }
            catch
            {
                joystickTimer.Enabled = false;
                connectToJoystick(joystick);
            }
        }
        private void enableTimer()
        {
            if (this.InvokeRequired)
            {
                BeginInvoke(new ThreadStart(delegate ()
                {
                    joystickTimer.Enabled = true;
                }));
            }
            else
                joystickTimer.Enabled = true;
        }
        private void connectToJoystick(Joystick joystick)
        {
            while (true)
            {
                string sticks = joystick.FindJoysticks();
                if (sticks != null)
                {
                    if (joystick.AcquireJoystick(sticks))
                    {
                        enableTimer();
                        break;
                    }
                }
            }
        }
    }
</code></pre>
",11507778,0,0,116153378,"Note that tutorial uses legacy Managed DirectX which is *ancient*, and only deployed by the legacy DirectX End-User Runtime. You should use [SlimDX](https://code.google.com/archive/p/slimdx/) or [SharpDX](http://sharpdx.org/) for DirectX interop instead.",Remote
169,3977,66340344,Compile the Robot Control Library for a different beaglebone cape,|beagleboneblack|robotics|,"<p>How should I go about modifying and/or compiling the <a href=""http://strawsondesign.com/docs/librobotcontrol/"" rel=""nofollow noreferrer"">Robot Control Library</a> for use with a different beaglebone cape that uses slightly different pin assignments?</p>
<p>My primary reason for wanting to re-use the Robot Control Library is the ability to read a fourth encoder via the PRU.  Beyond that, I only need access to the encoder and pwm modules.</p>
",2/23/2021 20:07,66394399,372,1,1,1,0,3830019,,7/11/2014 15:21,12,66394399,"<h1>TL;DR</h1>
<p>Modifying the PRU firmware to read the encoder signal from a different pin was easy.  Figuring out how to assemble a working device tree for the combination of features I needed was way harder.</p>
<p>I would welcome any feedback on how I <strong>should</strong> have done this, or how I could improve upon what I currently have.</p>
<h1>Robot Control Library + Motor Cape</h1>
<p>The Robotics Cape and the BeagleBone Blue provide a turnkey solution for servo controlling four motors,
IFF you are satisfied with driving them at 8V (e.g. a 2S LIPO battery).  The Motor Cape can handle a
higher drive voltage (and more current), but does not include encoders.  Plugging encoders into the P8 &amp; P9
headers on the Motor Cape is simple enough, but the BeagleBone itself only has 3 encoder counters (eQEP).<br />
The Robot Control Library solves this problem by reading the fourth encoder with PRU0.  However, some of the
pins conflict between the Motor Cape and what the Robot Control Library expects on the Robotics Cape.</p>
<p>So, how hard could it be to use the Robot Control Library to read encoders and drive motors with a slightly
different pinout on the Motor Cape?  Probably not difficult at all if you are already competent with BeagleBone
device tree overlays, which I am not...</p>
<h1>It all starts with a plan -- Pin Selection</h1>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Pin</th>
<th>PRU Bit</th>
<th>Robotics Cape</th>
<th>Motor Cape</th>
</tr>
</thead>
<tbody>
<tr>
<td>P8_15</td>
<td>15</td>
<td>Enc 4B</td>
<td>--</td>
</tr>
<tr>
<td>P8_16</td>
<td>14</td>
<td>Enc 4A</td>
<td>M2 Dir</td>
</tr>
<tr>
<td>P9_25</td>
<td>7</td>
<td>IMU</td>
<td>--</td>
</tr>
</tbody>
</table>
</div>
<p>The Robot Control Library expects the fourth encoder to show up on P8_15 and P8_16, but the
Motor Cape has P8_16 wired as a direction signal.  There are only 12 pins than be configured
as inputs to PRU0 and I eventually selected  P9_25 because I did not need the IMU functionality.</p>
<p>The best reference I found for what pins can be used for what purposes were these pdfs:</p>
<ul>
<li><a href=""https://ofitselfso.com/BeagleNotes/BeagleboneBlackP8HeaderPinMuxModes.pdf"" rel=""nofollow noreferrer"">https://ofitselfso.com/BeagleNotes/BeagleboneBlackP8HeaderPinMuxModes.pdf</a></li>
<li><a href=""https://ofitselfso.com/BeagleNotes/BeagleboneBlackP9HeaderPinMuxModes.pdf"" rel=""nofollow noreferrer"">https://ofitselfso.com/BeagleNotes/BeagleboneBlackP9HeaderPinMuxModes.pdf</a></li>
</ul>
<h1>The Easy Part -- Modifying the PRU Code</h1>
<p>The Robot Control Library defines the encoder signal input bits in <code>pru_firmware/src/pur0-encoder.asm</code> as</p>
<pre><code>; Encoder counting definitions
; these pin definitions are specific to SD-101D Robotics Cape
    .asg    r0,         OLD     ; keep last known values of chA and B in memory
    .asg    r1,         EXOR    ; place to store the XOR of old with new AB vals
    .asg    14,         A
    .asg    15,         B
</code></pre>
<p>This can be modified to look for the A channel on bit 7 (of register 31, which is used for all inputs) as</p>
<pre><code>; Encoder counting definitions
; these pin definitions are specific to SD-101D Robotics Cape
    .asg    r0,         OLD     ; keep last known values of chA and B in memory
    .asg    r1,         EXOR    ; place to store the XOR of old with new AB vals
    .asg    07,         A
    .asg    15,         B
</code></pre>
<p><strong>N.B.</strong> The PRU firmware has to be separately compiled by by running <code>make</code> and <code>sudo make install</code>
inside the <code>pru_firmware</code> directory.  It is <strong>not</strong> compiled automatically as part as part of building the rest
of the library from the top-level Makefile.</p>
<h3>Helpful Tip: What version am I actually running?</h3>
<p>There are instructions on modifying the repored version of <code>librobotcontrol</code> in
<code>library/version_updating_howto.txt</code>.  I followed these instructions to create my own
&quot;private&quot; version number so that I could confirm that I was actually running my modified
version of the libray.  This version is reported by <code>rc_test_drivers</code>.</p>
<p>However... as noted above, the PRU firmware was not getting compiled by the top-level Makefile,
so for a while I was running my &quot;new&quot; version of <code>librobotcontrol</code> with &quot;old&quot; firmware in the PRU.</p>
<h1>The Part that Almost Worked -- The Device Tree</h1>
<p>I found references in both the documentation and code for <code>librobotcontrol</code> that device tree overlays
were no longer needed because the Robotics Cape used its own device tree.</p>
<blockquote>
<p>The overlay is deprecated now, instead the cape gets its own complete device tree.</p>
</blockquote>
<p>I also observed that running the recommended <code>configure_robotics_dt.sh</code> replaced <code>/boot/uEnv.txt</code> with
the following simplified version that loads a single device tree binary (.dtb)</p>
<pre><code>uname_r=4.19.94-ti-r42
dtb=am335x-boneblack-roboticscape.dtb
cmdline=coherent_pool=1M
</code></pre>
<p>My favorite reference that I found for general information about the device tree, pinmux, etc was
<a href=""http://www.ofitselfso.com/BeagleNotes/AboutTheDeviceTree.pdf"" rel=""nofollow noreferrer"">http://www.ofitselfso.com/BeagleNotes/AboutTheDeviceTree.pdf</a>  However, I now realize that some of the
details are a bit out of date, so be careful.</p>
<p>Because I had very little idea of where else to start, I set out to modify the robotics cape device tree
just enough to eliminate the conflicts with the Motor Cape.  I forked and cloned
<a href=""https://github.com/beagleboard/BeagleBoard-DeviceTrees"" rel=""nofollow noreferrer"">https://github.com/beagleboard/BeagleBoard-DeviceTrees</a> and created two new files</p>
<ul>
<li><code>am335x-boneblack-custom.dts</code>
<ul>
<li>copy of <code>am335x-boneblack-roboticscape.dts</code></li>
<li>changed <code>model</code> to make the new device tree recognizable</li>
<li>changed <code>#include</code> to point to <code>am335x-custom.dtsi</code> instead of <code>am335x-roboticscape.dtsi</code></li>
</ul>
</li>
<li><code>am335x-custom.dtsi</code>
<ul>
<li>copy of <code>am335x-roboticscape.dtsi</code></li>
<li>deleted a whole bunch of stuff I thought I didn't need anymore</li>
<li>routed P9_25 (instead of P8_16) to PRU0</li>
</ul>
</li>
</ul>
<p>Before</p>
<pre><code>            /* PRU encoder input */
            0x03c 0x36  /* P8_15,PRU0_r31_15,MODE6 */
            0x038 0x36  /* P8_16,PRU0_r31_14,MODE6 */
</code></pre>
<p>After</p>
<pre><code>            /* PRU encoder input */
            0x03c 0x36  /* P8_15,PRU0_r31_15,MODE6 */
            0x1ac 0x36  /* P9_25,PRU0_r31_7,MODE6 */
</code></pre>
<p>After compiling and installing the modified device trees (<code>make</code>, <code>sudo make install</code> in the
<code>BeagleBoard-DeviceTrees</code> repo), I modified <code>/boot/uEnv.txt</code> to call my new custom device tree</p>
<pre><code>uname_r=4.19.94-ti-r42
dtb=am335x-boneblack-custom.dtb
cmdline=coherent_pool=1M
</code></pre>
<p>I was able to boot the BeagleBone with no cape installed, plug encoders directly into the desired pins
on P8_and P9 (including enc4a on P9_25) and read all four encoders using <code>sudo rc_test_encoders</code>.
I thought I had won and went to bed...</p>
<h3>Motor Cape Won't Boot</h3>
<p>After a good night's sleep, I plugged the Motor Cape onto the BeagleBone expecting nothing to change since
I was only passing encoder signals directly through the P8 and P9 headers.  I thought the next step would be
to make similar tweaks to a few of the pwm direction pins.</p>
<p>However, the BeagleBone refused to boot my custom device tree with the MotorCape installed.  I went back to
the &quot;standard&quot; <code>am335x-boneblack-roboticscape.dtb</code> device tree and observed that it would not boot either with
the Motor Cape intalled.  I also became suspicious that the &quot;factory&quot; installation of the Robotics Cape might have
been using overlays after all</p>
<p>I had been torn from the beginning about whether I should be starting from the Robotics Cape device tree and removing
things I did not need in order to eliminate resource conflicts, versus starting with the &quot;naked&quot; BeagleBone device tree
and adding the things that I did need.  Whether accurate or not, in my mind that kind-of mapped into trying to specify
a full device tree versus providing an overlay to apply on top of the base device tree.  The latter seemed like
the conceptually more correct path, so once the Motor Cape failed to boot with the robotics-cape-derived device tree,
I decided to bite the bullet and try to figure out device tree overlays.</p>
<h3>Unanswered Questions</h3>
<ul>
<li>[ ] Why won't the BB boot from <code>am335x-boneblack-roboticscape.dtb</code> with the motor cape installed?  What is the actual error?</li>
<li>[ ] Does a &quot;normal&quot; installation of <code>librobotcontrol</code> install the simplified <code>uEnv.txt</code> above or does it use an overlay?  Does it work?</li>
</ul>
<p>I did not yet have USB-to-TTL serial cable that could fit under an installed cape, so I know very little about
why or how this was failing to boot.</p>
<h1>The Part that Finally Worked -- Device Tree Overlays</h1>
<p>I eventually figured out that the collection of device tree overlays is avialable both at
<a href=""https://github.com/beagleboard/bb.org-overlays"" rel=""nofollow noreferrer"">https://github.com/beagleboard/bb.org-overlays</a> and in the <code>v4.19.x-ti-overlays</code> branch at
<a href=""https://github.com/beagleboard/BeagleBoard-DeviceTrees"" rel=""nofollow noreferrer"">https://github.com/beagleboard/BeagleBoard-DeviceTrees</a>.  I suspect that this might be an
in-progress migration, but there was more documentation associated with the <code>bb.org-overlays</code>
repository so that is what I chose to use.</p>
<p>A few documentation links that I wish I had found earlier:</p>
<ul>
<li><a href=""https://elinux.org/Beagleboard:BeagleBoneBlack_Debian#U-Boot_Overlays"" rel=""nofollow noreferrer"">https://elinux.org/Beagleboard:BeagleBoneBlack_Debian#U-Boot_Overlays</a></li>
<li><a href=""https://github.com/cdsteinkuehler/beaglebone-universal-io"" rel=""nofollow noreferrer"">https://github.com/cdsteinkuehler/beaglebone-universal-io</a></li>
<li><a href=""https://vadl.github.io/beagleboneblack/2016/07/29/setting-up-bbb-gpio"" rel=""nofollow noreferrer"">https://vadl.github.io/beagleboneblack/2016/07/29/setting-up-bbb-gpio</a></li>
</ul>
<p>I created forked, cloned, and branched the <code>bb.org-overlays</code> repo and created a new overlay at
<code>src/arm/CustomCape-00A0.dts</code> by hacking together pieces from <code>BBORG_MOTOR-00A2.dts</code> and <code>RoboticsCape-00A0.dts</code></p>
<pre><code>/*
 * Device Tree Overlay for custom cape trying to reuse Robot Control Library's
 * reading of 4x optical encoders.
 */

/*
pinmux control byte map courtesy of http://beaglebone.cameon.net/
Bit 5: 1 - Input, 0 - Output
Bit 4: 1 - Pull up, 0 - Pull down
Bit 3: 1 - Pull disabled, 0 - Pull enabled
Bit 2 \
Bit 1 |- Mode
Bit 0 /
 */

/dts-v1/;
/plugin/;

/ {
    compatible = &quot;ti,beaglebone-black&quot;;

    /* identification */
    part-number = &quot;CustomCape&quot;;

    /* version */
    version = &quot;00A0&quot;;

    exclusive-use =
        &quot;P8.11&quot;,    /*QEP_2B*/
        &quot;P8.12&quot;,    /*QEP_2A*/
        &quot;P8.16&quot;,    /*PRU_ENCODER_A*/
        &quot;P8.33&quot;,    /*QEP_1B*/
        &quot;P8.35&quot;,    /*QEP_1A*/
        &quot;P9.27&quot;,    /*QEP_0B*/
        &quot;P9.41&quot;,    /*MOT_STBY*/
        &quot;P9.42&quot;;    /*QEP_0A*/

    /*
     * Helper to show loaded overlays under: /proc/device-tree/chosen/overlays/
     */
    fragment@0 {
        target-path=&quot;/&quot;;
        __overlay__ {
            chosen {
                overlays {
                    CustomCape-00A0 = __TIMESTAMP__;
                };
            };
        };
    };

fragment@1 {
    target = &lt;&amp;am33xx_pinmux&gt;;
    __overlay__ {
        /****************************************
        *           pinmux helper
        ****************************************/
        mux_helper_pins: pins {
            pinctrl-single,pins = &lt;

            /* EQEP */
            0x1A0 0x31  /* P9_42,EQEP0A, MODE1 */
            0x1A4 0x31  /* P9_27,EQEP0B, MODE1 */
            0x0D4 0x32  /* P8_33,EQEP1B, MODE2 */
            0x0D0 0x32  /* P8_35,EQEP1A, MODE2 */
            0x030 0x34  /* P8_12,EQEP2A_in, MODE4 */
            0x034 0x34  /* P8_11,EQEP2B_in, MODE4 */

            /* PRU encoder input */
            0x03c 0x36  /* P8_15,PRU0_r31_15,MODE6 */
            0x1ac 0x36  /* P9_25,PRU0_r31_7,MODE6 */
            &gt;;
        };
    };
};

/****************************************
    Pinmux Helper
    activates the pinmux helper list of pin modes
****************************************/
fragment@2 {
    target = &lt;&amp;ocp&gt;;
        __overlay__ {
            test_helper: helper {
            compatible = &quot;bone-pinmux-helper&quot;;
            pinctrl-names = &quot;default&quot;;
            pinctrl-0 = &lt;&amp;mux_helper_pins&gt;;
            status = &quot;okay&quot;;
        };
    };
};


    /*
     * Free up the pins used by the cape from the pinmux helpers.
     */
    fragment@3 {
        target = &lt;&amp;ocp&gt;;
        __overlay__ {
            P8_11_pinmux { status = &quot;disabled&quot;; };  /* enc3b */
            P8_12_pinmux { status = &quot;disabled&quot;; };  /* enc3a */
            P8_15_pinmux { status = &quot;disabled&quot;; };  /* enc4b */
            P8_33_pinmux { status = &quot;disabled&quot;; };  /* enc0  */
            P8_35_pinmux { status = &quot;disabled&quot;; };  /* enc0  */
            P9_25_pinmux { status = &quot;disabled&quot;; };  /* enc4a */
            P9_27_pinmux { status = &quot;disabled&quot;; };  /* enc1b */
            P9_92_pinmux { status = &quot;disabled&quot;; };  /* enc1a */
        };
    };

/****************************************
        Encoders
****************************************/
fragment@9 {
    target = &lt;&amp;eqep0&gt;;
    __overlay__ {
        count_mode = &lt;0&gt;;  /* 0 - Quadrature mode, normal 90 phase offset cha &amp; chb.  1 - Direction mode.  cha input = clock, chb input = direction */
        swap_inputs = &lt;0&gt;; /* Are channel A and channel B swapped? (0 - no, 1 - yes) */
        invert_qa = &lt;1&gt;;   /* Should we invert the channel A input?  */
        invert_qb = &lt;1&gt;;   /* Should we invert the channel B input? I invert these because my encoder outputs drive transistors that pull down the pins */
        invert_qi = &lt;0&gt;;   /* Should we invert the index input? */
        invert_qs = &lt;0&gt;;   /* Should we invert the strobe input? */

        status = &quot;okay&quot;;
    };
};

fragment@10 {
    target = &lt;&amp;eqep1&gt;;
    __overlay__ {
        count_mode = &lt;0&gt;;  /* 0 - Quadrature mode, normal 90 phase offset cha &amp; chb.  1 - Direction mode.  cha input = clock, chb input = direction */
        swap_inputs = &lt;0&gt;; /* Are channel A and channel B swapped? (0 - no, 1 - yes) */
        invert_qa = &lt;1&gt;;   /* Should we invert the channel A input?  */
        invert_qb = &lt;1&gt;;   /* Should we invert the channel B input? I invert these because my encoder outputs drive transistors that pull down the pins */
        invert_qi = &lt;0&gt;;   /* Should we invert the index input? */
        invert_qs = &lt;0&gt;;   /* Should we invert the strobe input? */
        status = &quot;okay&quot;;
    };
};

fragment@11 {
    target = &lt;&amp;eqep2&gt;;
    __overlay__ {
        count_mode = &lt;0&gt;;  /* 0 - Quadrature mode, normal 90 phase offset cha &amp; chb.  1 - Direction mode.  cha input = clock, chb input = direction */
        swap_inputs = &lt;0&gt;; /* Are channel A and channel B swapped? (0 - no, 1 - yes) */
        invert_qa = &lt;1&gt;;   /* Should we invert the channel A input?  */
        invert_qb = &lt;1&gt;;   /* Should we invert the channel B input? I invert these because my encoder outputs drive transistors that pull down the pins */
        invert_qi = &lt;0&gt;;   /* Should we invert the index input? */
        invert_qs = &lt;0&gt;;   /* Should we invert the strobe input? */
        status = &quot;okay&quot;;
    };
};


/****************************************
        PRU
****************************************/
fragment@31 {
    target = &lt;&amp;pruss&gt;;
    __overlay__ {
        status = &quot;okay&quot;;
    };
};
};

</code></pre>
<p>I added my custom overlay to <code>/boot/uEnv.txt</code> and disabled the video and audio overlays</p>
<pre><code>uname_r=4.19.94-ti-r42
#uuid=
#dtb=

###U-Boot Overlays###
###Documentation: http://elinux.org/Beagleboard:BeagleBoneBlack_Debian#U-Boot_Overlays
###Master Enable
enable_uboot_overlays=1
###
###Additional custom capes
uboot_overlay_addr4=/lib/firmware/CustomCape-00A0.dtbo
###
###Custom Cape
#dtb_overlay=/lib/firmware/&lt;file8&gt;.dtbo
###
###Disable auto loading of virtual capes (emmc/video/wireless/adc)
#disable_uboot_overlay_emmc=1
disable_uboot_overlay_video=1
disable_uboot_overlay_audio=1
#disable_uboot_overlay_wireless=1
#disable_uboot_overlay_adc=1
###
###PRUSS OPTIONS
###pru_rproc (4.19.x-ti kernel)
uboot_overlay_pru=/lib/firmware/AM335X-PRU-RPROC-4-19-TI-00A0.dtbo
###
###Cape Universal Enable
enable_uboot_cape_universal=1
###
###U-Boot Overlays###

cmdline=coherent_pool=1M net.ifnames=0 lpj=1990656 rng_core.default_quality=100 quiet
</code></pre>
<p>I make no claims of optimality or even correctness, but this configuration boots with or without the Motor Cape
installed and I can read all four encoders with <code>rc_test_encoders</code>.  When the Motor Cape is installed, uBoot
is correctly picking up and applying the <code>BBORG_MOTOR-00A2</code> overlay.  I honestly thought that I would need a lot more
configuration of the PRU to get the PRU-based encoder counter from the Robot Control Library working, but this seeems
to do the trick.</p>
<p>I would welcome any feedback on how I <strong>should</strong> have done this, or how I could improve upon what I currently have.</p>
<h3>Helpful Tip: Watch the serial terminal!</h3>
<p>I am embarrassed that I even attempted to debug device tree booting issues without first having a serial terminal
open to the beaglebone so that I could observe the boot sequence.  With the help of some 5-minute epoxy, I was
eventually able to make a 90-degree header to bring the JTAG port out from under an installed cape.</p>
<p><a href=""https://elinux.org/Beagleboard:BeagleBone_Black_Serial"" rel=""nofollow noreferrer"">https://elinux.org/Beagleboard:BeagleBone_Black_Serial</a></p>
",3830019,3,0,117331116,"You've asked a question without providing any relevant details whatsoever, in which case the answer would be ""follow instructions to update the code to match your specifications"".",Error
170,3998,66947260,How to control this robot with a PS4 DualShock controller (Python)?,|python|loops|object|robotics|,"<p>I am trying to control a robot with a ps4 controller connected via Bluetooth to raspberry pi, but have a problem with the while loop I need to make it move. The robot cycles leg positions in a pattern that makes it walk. I made a successful program that uses the keyboard where holding w, a, s, or d will move the robot in the usual directions and stop the robot once released. However, the ps4 controller uses functions for input in a way that confuses me.</p>
<p>Here is the keyboard code:</p>
<pre><code>from move import move, steady, steady_X
import sys, tty, termios, time
import Adafruit_PCA9685

def getch():
    fd = sys.stdin.fileno()
    old_settings = termios.tcgetattr(fd)
    try:
        tty.setraw(sys.stdin.fileno())
        ch = sys.stdin.read(1)
    finally:
        termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)
    return ch

pwm =  Adafruit_PCA9685.PCA9685()

if __name__ == '__main__':
    step = 1
    move_stu = 1
    try:
        while 1:
            char = getch()
            if char == 'x':
                pwm.set_all_pwm(0,300)
                time.sleep(1)
                break
    
            if char == 'w':
                move(step, 35, 'forward')
                step += 1
                if step &gt; 4:
                    step = 1
                time.sleep(0.08)

            if char == 's':
                move(step, 35, 'backward')
                step += 1
                if step &gt; 4:
                    step = 1
                time.sleep(0.08) 

            if char == 'a':
                move(step, 35, 'left')
                step += 1
                if step &gt; 4:
                    step = 1
                time.sleep(0.08)

            if char == 'd':
                move(step, 35, 'right')
                step += 1
                if step &gt; 4:
                    step = 1
                time.sleep(0.08)

            if char == 'e':
                pwm.set_all_pwm(0,300)
                time.sleep(0.1)
                steady_X()

            if char == ' ':
                steady()

    except KeyboardInterrupt:
        pwm.set_all_pwm(0,300)
        time.sleep(1)
</code></pre>
<p>where 35 is the speed of the robot.</p>
<p>The controller code is:</p>
<pre><code>from pyPS4Controller.controller import Controller
from move import move, steady, steady_X
import sys, tty, termios, time
import Adafruit_PCA9685



class MyController(Controller):


    def __init__(self, **kwargs):
        Controller.__init__(self, **kwargs)
        self.step = 1
        self.pwm = Adafruit_PCA9685.PCA9685()
        self.command = False

    def on_x_press(self):
        self.command = True
        while self.command == True:
            move(self.step, 35, 'forward')
            self.step +=1
            if self.step &gt; 4:
                self.step = 1
            time.sleep(0.08)

    def on_x_release(self):
        self.command = False



controller = MyController(interface=&quot;/dev/input/js0&quot;, connecting_using_ds4drv = False, event_definition = None)
controller.listen()
</code></pre>
<p>Where once controller.listen() is called it starts taking inputs from the controller.
Right now, once x is pressed the robot will move forward and not stop when it is released. So my question is how can I make it stop moving when x is released and the on_x_release(self) function is called?</p>
",4/5/2021 1:19,,617,1,0,0,,15382824,,3/12/2021 13:07,2,67334609,"<p>I might not have a direct fix to your problem because I do not have the hardware to test it out, but you may consider writing a <code>stop()</code> function where you explicitly set all motors to the 0 position <em>(if you use a H-bridge controller, this means setting both of the channels to off)</em> . Then on calling the function <code>on_x_release(self)</code>, you will set the self.command flag to False as you normally do, but also, call the <code>stop()</code> function to explicitly stop the motors from moving.</p>
<pre><code>from pyPS4Controller.controller import Controller
from move import move, steady, steady_X
import sys, tty, termios, time
import Adafruit_PCA9685

class MyController(Controller):


    def __init__(self, **kwargs):
        Controller.__init__(self, **kwargs)
        self.step = 1
        self.pwm = Adafruit_PCA9685.PCA9685()
        self.command = False

    def on_x_press(self):
        self.command = True
        while self.command == True:
            move(self.step, 35, 'forward')
            self.step +=1
            if self.step &gt; 4:
                self.step = 1
            time.sleep(0.08)

    def on_x_release(self):
        self.command = False
        stop() # (or modify the move function to accept a 'stop' string)


controller = MyController(interface=&quot;/dev/input/js0&quot;, connecting_using_ds4drv = False, 
event_definition = None)
controller.listen()
</code></pre>
<p>On a separate note, I think this issue might be because you do not have a &quot;default&quot; condition where the robot does nothing. Usually the way I would go about with such a problem is to define the general behaviour of the robot as doing nothing, and define the x key as an interrupt. So, whenever the key is pressed, an event occurs, and the equivalent eventHandler is called <em>(think of ISR if you know microcontrollers well)</em> . Designing it in this way will probably give you more control.</p>
",8955618,0,0,,,Remote
171,4002,67072289,Eye-In-Hand Calibration OpenCV,|python|opencv|camera-calibration|robotics|,"<p>I have a setup where a (2D) camera is mounted on the end-effector of a robot arm - similar to the OpenCV <a href=""https://docs.opencv.org/master/d9/d0c/group__calib3d.html"" rel=""nofollow noreferrer"">documentation</a>:
<a href=""https://i.stack.imgur.com/pRWnu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pRWnu.png"" alt=""enter image description here"" /></a></p>
<p>I want to calibrate the camera and find the transformation from camera to end-effector.
I have already calibrated the camera using this OpenCV guide, <a href=""https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_calib3d/py_calibration/py_calibration.html"" rel=""nofollow noreferrer"">Camera Calibration</a>, with a checkerboard where the undistorted images are obtained.</p>
<p>My problem is about finding the transformation from camera to end-effector. I can see that OpenCV has a function, <a href=""https://docs.opencv.org/master/d9/d0c/group__calib3d.html"" rel=""nofollow noreferrer"">calibrateHandEye()</a>, which supposely should achieve this. I already have the &quot;gripper2base&quot; vectors and are missing the &quot;target2cam&quot; vectors. Should this be based on the size of the checkerboard squares or what am I missing?
Any guidance in the right direction will be appreciated.</p>
",4/13/2021 9:35,67155501,6224,1,4,3,0,11166171,,3/7/2019 14:53,33,67155501,"<p>You are close to the answer.</p>
<p>Yes, it is based on the size of the checkerboard. But instead of directly taking those parameters and an image, this function is taking target2cam. How to get target2cam? Just simply move your robot arm above the chessboard so that the camera can see the chessboard and take a picture. From the picture of the chessboard and camera intrinsics, you can find target2cam. Calculating the extrinsic from the chessboard is already given in opencv.</p>
<p>Repeat this a couple of times at different robot poses and collect multiple target2cam. Put them calibrateHandEye() and you will get what you need.</p>
",2104452,4,1,118704183,"Are you implementing everything in ROS, in ROS we can set the TF of camera and the EF.",Incoming
172,4006,67081917,How do I get my code to loop until I press a button (Lego EV3),|python|robotics|lego-mindstorms|lego-mindstorms-ev3|,"<p>The question pure and simple. I've tried using while loops, rearranging code, but all that happens is that the program starts when i press the button or does absolutely nothing at all. Can anyone see what I am doing wrong?</p>
<pre><code>#!/usr/bin/env python3
from ev3dev2.motor import MoveTank, OUTPUT_B, OUTPUT_C
from ev3dev2.button import Button
from ev3dev2.sound import Sound
from time import sleep
import time

sound = Sound()
btn = Button()
tank_pair = MoveTank(OUTPUT_B, OUTPUT_C)

def movement(left_speed, right_speed, rotations):
tank_pair.on_for_rotations(left_speed, right_speed, rotations, brake=True, block=True)


while not btn.any():
movement(20,20,10)
movement(-100,0,1.5)
movement(20,20,10)
sleep(0.01)

while btn.any():
sleep(0.01)
tank_pair.off()
sound.beep()
exit()
</code></pre>
",4/13/2021 20:25,,814,2,2,0,,15331530,Regina,3/4/2021 17:38,4,67082399,"<p>you can break your main loop using this</p>
<pre><code>pip install keyboard
</code></pre>
<p>then</p>
<pre><code>import keyboard

while True:
    # do something
    if keyboard.is_pressed(&quot;q&quot;):
        print(&quot;q pressed, ending loop&quot;)
        break
</code></pre>
",10413299,0,1,118574083,what do you mean?,Remote
173,4034,67731514,How do I get eulerangles from two Vector3 coordinates based on openpose 3d?,|python|robotics|openpose|,"<p>In short. I want to make following program.</p>
<p>Input: Two Vector3 coordinates
P1 = (x1, y1, z1)
P2 = (x2, y2, z2)</p>
<p>output: one Eulerangles (P1-&gt;P2 or P2-&gt;P1).</p>
<p>I'm trying to apply 3d openpose joint data to robot arm control.
3d openpose data is constructed by Vector3 (x, y, z).
but I must use EulerAngles to control a robot arm.</p>
<p>Please tell me how to calculate EulerAngles from two Vector3 coordinates.</p>
<p>The following diagram outlines what I want to do.
Sorry for the hand-drawn illustration.<br />
<a href=""https://i.stack.imgur.com/ZmqqC.jpg"" rel=""nofollow noreferrer"">outline diagram</a></p>
<p>The following is a brief summary of code</p>
<pre class=""lang-py prettyprint-override""><code>def convert_pos2angle(P1, P2):
    
    ## some code here.
    

    return angle


def main():
    #sample input
    P1 = [0, 0, 0]
    P2 = [1, 1, 1]
    
    #convert
    angle = convert_pos2angle(P1, P2)

    print(angle)

    
</code></pre>
",5/28/2021 0:42,67835293,161,1,2,0,,16049595,,5/27/2021 13:52,2,67835293,"<p>I was able to solve this problem on my own.
I found the project &quot;video2bvh&quot; on GitHub.
It Converts openpose to BVH data.
These programs work very well.</p>
<p>GitHub: <a href=""https://github.com/KevinLTT/video2bvh"" rel=""nofollow noreferrer"">https://github.com/KevinLTT/video2bvh</a></p>
",16049595,0,0,119724260,"I am sorry, but I have not made any code.
I added a diagram and brief summary of code to help people understand what I want to do.",Coordinates
174,4040,68225798,"How stepper motor torque will behave for two different supply, 24v/5A and 36v/5A",|robotics|stepper|,"<p>How stepper motor torque will behave for two different supply, 24v/5A and 36v/5A. I am using three Nema 23 , 10kg-cm stepper motors. Using TB6600 Driver which will limit my current to rated current from the supply. It accepts 12-36v and 2.8 is the rated current.</p>
<p>I want to achieve max torque. I went through T depends on Current.
What is my motor torque behavior, when 24v/5A, 36v/5A. Speed will be very less in my use case - kind of robitics arm.</p>
",7/2/2021 13:09,,240,1,0,0,,10527120,,10/19/2018 3:17,14,68550350,"<p>If the TB6600 is current limiting the motor to 2.8A, then the torque will be identical for any power supply that can supply over 2.8A.</p>
<p>Internally I expect the TB6600 is using a chopper driver to limit the total average current - <a href=""https://i.stack.imgur.com/rfaoz.png"" rel=""nofollow noreferrer"">Chopper driver waveform</a> from <a href=""https://www.linearmotiontips.com/what-is-a-chopper-drive-for-a-stepper-motor/"" rel=""nofollow noreferrer"">https://www.linearmotiontips.com/what-is-a-chopper-drive-for-a-stepper-motor/</a></p>
<p>So if you increase the voltage supplied to the driver, you might see the torque ripple increase in frequency, but the average torque will still be the same.</p>
",15339016,0,0,,,Actuator
175,4041,68257668,Correct tf frames setting in ndt_matching,|ros|robotics|,"<p>ndt_matching succeeded in autoware, but the vehicle model cannot be set correctly.</p>
<ol>
<li>How do I set the correct angle for the vehicle model?</li>
<li>What does the frame &quot;mobility&quot; mean?</li>
</ol>
<p>tf.launch</p>
<pre><code>&lt;node pkg=&quot;tf&quot;  type=&quot;static_transform_publisher&quot; name=&quot;world_to_map&quot; args=&quot;0 0 0 0 0 0 /world /map 10&quot; /&gt;
&lt;node pkg=&quot;tf&quot;  type=&quot;static_transform_publisher&quot; nurdf.xacro file roboticsame=&quot;map_to_points_map&quot; args=&quot;0 0 0 0 0 0 /map /points_map 10&quot; /&gt;
&lt;node pkg=&quot;tf&quot;  type=&quot;static_transform_publisher&quot; name=&quot;velodyne_to_lidar_top&quot; args=&quot;0 0 0 0 0 0 /velodyne /lidar_top 10&quot; /&gt;
</code></pre>
<p><a href=""https://i.stack.imgur.com/s77bf.png"" rel=""nofollow noreferrer"">Image for RViz</a></p>
<p><a href=""https://i.stack.imgur.com/Crr17.png"" rel=""nofollow noreferrer"">Image for TF Tree</a></p>
",7/5/2021 14:15,68463650,384,1,5,1,,9724286,,5/1/2018 5:54,4,68463650,"<p>The settings in the TF file were correct.
To change the angle of the vehicle model, I made the following settings.</p>
<ol>
<li>Change the yaw setting of <code>Baselink to Localizer</code> in the <code>Setup</code> tab (in the direction you want the vehicle model to point).</li>
<li>Set the yaw setting of <code>ndt_matching</code> to offset it.(if baselink angle(1) is -1.55, here it is +1.55)</li>
</ol>
<p>I wrote an article about these issues, Thank you JWCS!</p>
<p><a href=""https://medium.com/yodayoda/localization-with-autoware-3e745f1dfe5d"" rel=""nofollow noreferrer"">https://medium.com/yodayoda/localization-with-autoware-3e745f1dfe5d</a></p>
",9724286,1,0,120741210,Cool... Would you like to post an answer that describes what you did to fix it?,Coordinates
176,4049,68386445,Installing MRPT on Fedora,|robotics|mobile-robot-toolkit|,"<p>Can anyone provide a detailed procedure for installing MRPT on Fedora 33 Scientific (one of the Fedora Labs which has a KDE interface)? The MRPT installation instructions for Ubuntu mentions something about cmake/cmake-gui. Checking the man pages, F33Sci has no such thing. It must be possible to accomplish this somehow, because Fedora Robotics Lab includes MRPT. I've already tried &quot;$sudo dnf install mrpt&quot;, resulting in &quot;Error: Unable to find a match: mrpt&quot;. However, &quot;$dnf search mrpt&quot; results in a bunch of items from mrpt-base... to mrpt-stereo-camera-calibration.</p>
",7/15/2021 0:21,,88,1,0,1,,4561673,,2/13/2015 0:41,3,68453480,"<p>The version of MRPT that ships with Fedora is really outdated, so you do well in building from sources.</p>
<p><code>cmake-gui</code> is not 100% required, it is only mentioned in the instructions to make things easier to those preferring GUIs, but you should be able to compile using the console commands <a href=""https://docs.mrpt.org/reference/latest/compiling.html#using-the-console"" rel=""nofollow noreferrer"">here</a> (that is, the standard workflow with cmake).</p>
<p>Next, the CMake configuration process will warn you about missing libraries. Most are optional, but at least make sure of installing eigen3, opencv and wxwidgets. Those should be installed with the standard commands used in Fedora...</p>
",1631514,0,2,,,Error
177,4067,69101225,PID implementation in line following bot using turtlebot,|python|opencv|ros|robotics|,"<p>I am using ROS melodic,turtlebot 2 on Ubuntu 18.04.</p>
<p>The idea is to create an environment consisting of lines as a path (slightly curved), and to program the turtlebot to follow the lines. Basically, a line following bot.</p>
<p>The idea is to click photos using the camera mounted on the turtlebot,process them to segment out the path which needs to be followed, and then control the bot's orientation, by using PID to control the angular velocity, so that it moves along the path.</p>
<p>I have created a program called <code>take_photo_mod.py</code>, which successfully keeps clicking photos and saving them as <code>image.jpg</code>, at a frequency that can be controlled.</p>
<pre><code> #!/usr/bin/env python
    from __future__ import print_function
    import sys
    import rospy
    import cv2
    from std_msgs.msg import String
    from sensor_msgs.msg import Image
    from cv_bridge import CvBridge, CvBridgeError

    class TakePhoto:
        def __init__(self):
    
            self.bridge = CvBridge()
            self.image_received = False
            img_topic = &quot;/camera/rgb/image_raw&quot;
            self.image_sub = rospy.Subscriber(img_topic, Image, self.callback)
            rospy.sleep(1)
    
        def callback(self, data):
         
            try:
                cv_image = self.bridge.imgmsg_to_cv2(data, &quot;bgr8&quot;)
            except CvBridgeError as e:
                print(e)
    
            self.image_received = True
            self.image = cv_image
    
        def take_picture(self, img_title):
            if self.image_received:
               
                cv2.imwrite(img_title, self.image)
                rospy.loginfo(&quot;received&quot;)
    
                return True
            else:
                return False
    
    if __name__ == '__main__':
    
     
        rospy.init_node('take_photo', anonymous=False)
        camera = TakePhoto()
    
        while not rospy.is_shutdown():
            
            img_title = rospy.get_param('~image_title', 'image'.jpg')
            if camera.take_picture(img_title):
                rospy.loginfo(&quot;Saved image &quot; + img_title)
            else:
                rospy.loginfo(&quot;No images received&quot;)
    
            rospy.sleep(1)
</code></pre>
<p>As for the next part, I plan to create a program <code>go_forward.py</code>, which incorporates two things:</p>
<p><strong>Part 1.</strong> <strong>The openCV aspect</strong>: processing the image clicked to segment out the path. I have successfully managed to do this:</p>
<pre><code>import cv2
import numpy as np
import math as m

def contor(lst):
    if len(lst)&gt;1:
        m = list()
        for i in lst:
            for j in i:
                m.append(j[0])
        m = np.array(m)
    else:
        m=np.array([[w//2,h-150],[w//2,h-40]])
    print(&quot;m=&quot;,m)
    m = m[m[:, 1].argsort()]
    return m

def aver(i, p):
    a = p[i]
    y = int(np.mean([a[:, 0]]))
    x = int(np.mean([a[:, 1]]))
    return (y, x)

def extract(frame):
    theta=0
    phi=0
    print(&quot;h,w =&quot;,h,w)
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    lb = np.array([0, 0, 0])
    ub = np.array([255, 50, 160])
    mask = cv2.inRange(hsv, lb, ub)
    res = cv2.bitwise_and(frame, frame, mask=mask)

   

     res2 = cv2.cvtColor(res, cv2.COLOR_BGR2GRAY)
        ret, thres = cv2.threshold(res2, 100, 255, cv2.THRESH_BINARY)
        countours, hierarchy = cv2.findContours(thres, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
        cv2.drawContours(frame, countours, -1, (255, 0, 255), 3)
    
        points = contor(countours)
        i = np.where((points[:, 1] &gt; h - 80) &amp; (points[:, 1] &lt; h))
        #print('i1', len(i[0]))
        if len(i[0]) &gt; 0:
            end = aver(i, points) #coordinates (x2,y2)
        else:
            end = (w // 2, h)
        i = np.where((points[:, 1] &gt; h - 190) &amp; (points[:, 1] &lt; h - 110))
        #print('i2', len(i[0]))
        if len(i[0]) &gt; 0:
            strt = aver(i, points) #coordinates (x1,y1)
        else:
            strt = (w // 2, h - 200)
            
    
        
        frame=cv2.line(frame,strt,end,(255,0,0),9)
        cv2.imshow(&quot;aa&quot;,frame)
        cv2.waitKey()
        cv2.destroyAllWindows()
        
        if (strt[0] - end[0]) == 0:
            phi=m.pi/2 #angle with horizontal
            theta=0 #angle with vertical
        else:
            slope = (strt[1] - end[1]) / (end[0] - strt[0])
            phi=m.atan2(slope) #angle with horizontal
            theta= (m.pi/2) - phi #angle with vertical
                
    
        return theta
    
    
    
    frame=cv2.imread(&quot;image.jpg&quot;,1)
    h, w, _ = frame.shape
    i,d=0,0
    error=extract(frame)
    print(error)
</code></pre>
<p>image.jpg ( which I was using for testing the openCV aspect. When take_photo_mod.py runs , image.jpg will keep changing at the frequency with which the bot will take photos)</p>
<p><a href=""https://i.stack.imgur.com/w3KAI.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w3KAI.jpg"" alt=""enter image description here"" /></a></p>
<p>This draws a line that approximately gives the direction along the path. Further, this calculates the angle with the angle with the <strong>vertical</strong>.</p>
<p><strong>Part 2. PID aspect</strong>: This is the part with which I am struggling. The camera is mounted on the bot, so whatever we are calculating on the image is in the bot's frame,and the orientation of the bot in the bot's frame is along the vertical. Hence the angle which we calculate with the vertical acts as the &quot;error&quot; term: the angle by which the bot should turn to be along the path. Now, I think it's the angular velocity which will serve as the control signal.</p>
<p>I already have a script that just makes the bot go forward till its terminated.It had  angular velocity fixed at 0. So,the program &quot;goforward.py&quot; will basically be the combination of the openCV aspect ,and tweaking the &quot;simply go forward&quot; script to use PID to set it's angular velocity (instead of it being fixed at 0). Then, the two programs(nodes) <code>take_photo_mod.py</code> and <code>goforward.py</code> will be launched together using a launch file.</p>
<p>What I tried for <code>goforward.py</code>:</p>
<pre><code>#!/usr/bin/env python

import rospy
import cv2
import numpy as np
import math as m
from std_msgs.msg import String
from sensor_msgs.msg import Image
from cv_bridge import CvBridge, CvBridgeError
from geometry_msgs.msg import Twist


#openCV aspect

def contor(lst):
    if len(lst)&gt;1:
        m = list()
        for i in lst:
            for j in i:
                m.append(j[0])
        m = np.array(m)
    else:
        m=np.array([[w//2,h-150],[w//2,h-40]])
    #print(&quot;m=&quot;,m)
    m = m[m[:, 1].argsort()]
    return m

def aver(i, p):
    a = p[i]
    y = int(np.mean([a[:, 0]]))
    x = int(np.mean([a[:, 1]]))
    return (y, x)


def extract(frame):
    
    h, w, _ = frame.shape
    #print(h,w)
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    lb = np.array([0, 0, 0])
    ub = np.array([255, 50, 160])
    mask = cv2.inRange(hsv, lb, ub)
    res = cv2.bitwise_and(frame, frame, mask=mask)

    res2 = cv2.cvtColor(res, cv2.COLOR_BGR2GRAY)
    ret, thres = cv2.threshold(res2, 100, 255, cv2.THRESH_BINARY)
    countours, hierarchy = cv2.findContours(thres, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    cv2.drawContours(frame, countours, -1, (255, 0, 255), 3)

    points = contor(countours)
    i = np.where((points[:, 1] &gt; h - 80) &amp; (points[:, 1] &lt; h))
    
    if len(i[0]) &gt; 0:
        end = aver(i, points)
    else:
        end = (w // 2, h)
    i = np.where((points[:, 1] &gt; h - 190) &amp; (points[:, 1] &lt; h - 110))
    
    if len(i[0]) &gt; 0:
        strt = aver(i, points)
    else:
        strt = (w // 2, h - 200)



    if (strt[0]-end[0]) == 0:
        phi=m.pi/2 #angle with horizontal
        theta = 0 #angle with vertical
    else:
        slope = (strt[1] - end[1]) / (end[0] - strt[0])
        phi=m.atan2(slope) #angle with horizontal
        theta= (m.pi/2) - phi #angle with vertical

    return theta



#PID

def PID (kp,ki,kd,dt,error):
    


#making the bot move

class GoForward():

    def _init_(self):
        rospy.init_node('GoForward', anonymous=False)
        rospy.loginfo(&quot;To stop TurtleBot CTRL + C&quot;)
        rospy.on_shutdown(self.shutdown)
        self.cmd_vel = rospy.Publisher('cmd_vel_mux/input/navi', Twist, queue_size=10)
        r = rospy.Rate(10)
        move_cmd = Twist()
        move_cmd.linear.x = 0.2
        move_cmd.angular.z = 0
        

        while not rospy.is_shutdown():
            frame=cv2.imread(&quot;image.jpg&quot;,1)
            error=extract(frame)
            w=0
            kp=0.15
            ki=0.02
            kd=0.02
            dt=0.01
            w= PID(kp,ki,kd,dt,error) 
            velocity_control=0.5 #when the bot is turning, its velocity should be slowed down.
            move_cmd.linear.x = 0.2-(veclocity_control *abs(w))
            move_cmd.angular.z = -w  #-sign because we are controlling angular velocity in -z direction (clockwise),if the error is positive, and vice versa
            self.cmd_vel.publish(move_cmd)
            r.sleep()
    


    def shutdown(self):
        rospy.loginfo(&quot;Stop TurtleBot&quot;)
        self.cmd_vel.publish(Twist())
        rospy.sleep(1)



if _name_ == '_main_':
    try:
        GoForward()
    except:
        rospy.loginfo(&quot;GoForward node terminated.&quot;)
</code></pre>
<p>The problem I'm having is to implement the PID part properly. How exactly should I approach writing the PID (...) function?</p>
",9/8/2021 10:17,,568,1,1,0,,14407957,,10/7/2020 14:05,57,69103416,"<p>As a comment suggested, there's a lot to tackle here, so I'll just focus on what you asked in your title. When concering PIDs in the ROS ecosystem I'd almost always suggest using the <a href=""http://wiki.ros.org/pid"" rel=""nofollow noreferrer"">PID</a> package. From your description it should do everything you need.</p>
",11245187,0,0,122130376,"that's a lot of different tasks all at once. break it down, distribute into separate questions... perhaps one at a time. honestly, if you wanted to ask about PID, this question contains too much stuff to sift through to give an answer (besides, PID tuning is likely a math/engineering question, unless you really want to talk about implementing the math).",Incoming
178,4075,69125009,ROS - problem adding ultrasonic data to range_sensor_layer,|ros|robotics|arduino-ultra-sonic|,"<p>I need some help with a problem I encountered while adding ultrasonic sensors to a robot (loosely based on Linorobot), already equipped with an RPlidar. Hw/Sw: Raspi3B w/ Ubuntu 16.04.6 LTS, ROS kinetic, a Teensy, 2 Nano.</p>
<p>The robot was working fine with just the lidar, but I need to be able to detect correctly glass and some reflective surfaces, so I'm adding the ultrasonic sensors.
The hardware and microcontroller (rosserial) parts seem to be working fine, I suspect it's an error from my part, maybe related to namespaces or transform frames... or maybe I'm missing something gargantuan. I checked and re-checked against online tutorials, examples and other questions similar to this one, but I couldn't identify the culprit.</p>
<p>After executing the launch files I get the standard messages (same as before trying to setup the ultrasonic sensors), plus:</p>
<pre><code>[ INFO] [1631195261.554945536]: global_costmap/sonar_layer: ALL as input_sensor_type given
[ INFO] [1631195261.596176257]: RangeSensorLayer: subscribed to topic /ultrasound_front
</code></pre>
<p>and I guess that's good.
Unfortunately from that moment onward I get (with increasingly high figures, of course):</p>
<pre><code>[ WARN] [1631195265.533631740]: No range readings received for 4.02 seconds, while expected at least every 2.00 seconds.
</code></pre>
<p>here's a sensor message (from &quot;rostopic echo /ultrasound_front&quot;):</p>
<pre><code>----
header: 
  seq: 1124
  stamp: 
    secs: 1631192726
    nsecs: 301432058
  frame_id: &quot;sonar_front&quot;
radiation_type: 0
field_of_view: 0.259999990463
min_range: 0.0
max_range: 100.0
range: 52.0
----
</code></pre>
<p>So, the topic is published and the massages should be ok...</p>
<p>My costmap_common_params.yaml:</p>
<pre><code>map_type: costmap

transform_tolerance: 1

footprint: [[-0.25, -0.25], [-0.25, 0.25], [0.25, 0.25], [0.25, -0.25]]

inflation_layer:
  inflation_radius: 0.28
  cost_scaling_factor: 3

obstacle_layer:
  obstacle_range: 2.5
  raytrace_range: 3.0
  observation_sources: scan
  observation_persistence: 0.0
  scan:
    data_type: LaserScan
    topic: scan
    marking: true
    clearing: true

sonar_layer:
  frame: sonar_front
  topics: [&quot;/ultrasound_front&quot;]
  no_readings_timeout: 2.0
  clear_on_max_reading: true
  clear_threshold: 0.2
  mark_threshold: 0.80
</code></pre>
<p>My global_costmap_params.yaml:</p>
<pre><code>global_costmap:
  global_frame: /map
  robot_base_frame: /base_footprint
  update_frequency: 1
  publish_frequency: 0.5
  static_map: true
  transform_tolerance: 1
  plugins:
    - {name: static_layer,    type: &quot;costmap_2d::StaticLayer&quot;}
    - {name: sonar_layer,   type: &quot;range_sensor_layer::RangeSensorLayer&quot;}
    - {name: obstacle_layer,  type: &quot;costmap_2d::ObstacleLayer&quot;}
    - {name: inflation_layer, type: &quot;costmap_2d::InflationLayer&quot;}
</code></pre>
<p>My local_costmap_params.yaml:</p>
<pre><code>local_costmap:
  global_frame: /odom
  robot_base_frame: /base_footprint
  update_frequency: 1
  publish_frequency: 5.0
  static_map: false
  rolling_window: true
  width: 3
  height: 3
  resolution: 0.02
  transform_tolerance: 1
  observation_persistence: 0.0

  plugins:
    - {name: obstacle_layer,  type: &quot;costmap_2d::ObstacleLayer&quot;}
    - {name: sonar_layer, type: &quot;range_sensor_layer::RangeSensorLayer&quot;}
    - {name: inflation_layer, type: &quot;costmap_2d::InflationLayer&quot;}
</code></pre>
<p>And my barebone URDF:</p>
<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;
   &lt;robot name=&quot;linorobot&quot;&gt;

    &lt;link name=&quot;base_link&quot;&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;box size=&quot;0.50 0.33 0.09&quot;/&gt;
        &lt;/geometry&gt;
        &lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0.0 0.00 0.085&quot;/&gt;
        &lt;material name=&quot;blue&quot;&gt;
          &lt;color rgba=&quot;0 0 .8 1&quot;/&gt;
        &lt;/material&gt;
      &lt;/visual&gt;
    &lt;/link&gt;

    &lt;link name=&quot;perception_deck&quot;&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;box size=&quot;0.18 0.33 0.08&quot;/&gt;
        &lt;/geometry&gt;
        &lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0.0 0.0 0.17&quot;/&gt;
        &lt;material name=&quot;blue&quot;&gt;
          &lt;color rgba=&quot;0 0 .8 1&quot;/&gt;
        &lt;/material&gt;
      &lt;/visual&gt;
    &lt;/link&gt;


    &lt;link name=&quot;wheel_left_front&quot;&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;cylinder length=&quot;0.03&quot; radius=&quot;0.06&quot;/&gt;
        &lt;/geometry&gt;
        &lt;origin rpy=&quot;1.57 0 0&quot; xyz=&quot;0.163 0.222 0.03&quot;/&gt;
        &lt;material name=&quot;black&quot;&gt;
          &lt;color rgba=&quot;0 0 0 1&quot;/&gt;
        &lt;/material&gt;
      &lt;/visual&gt;
    &lt;/link&gt;

    &lt;link name=&quot;wheel_right_front&quot;&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;cylinder length=&quot;0.03&quot; radius=&quot;0.06&quot;/&gt;
        &lt;/geometry&gt;
        &lt;origin rpy=&quot;1.57 0 0&quot; xyz=&quot;0.163 -0.222 0.03&quot;/&gt;
        &lt;material name=&quot;black&quot;&gt;
          &lt;color rgba=&quot;0 0 0 1&quot;/&gt;
        &lt;/material&gt;
      &lt;/visual&gt;
    &lt;/link&gt;

    &lt;link name=&quot;wheel_left_rear&quot;&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;cylinder length=&quot;0.03&quot; radius=&quot;0.06&quot;/&gt;
        &lt;/geometry&gt;
        &lt;origin rpy=&quot;1.57 0 0&quot; xyz=&quot;-0.163 0.222 0.03&quot;/&gt;
        &lt;material name=&quot;black&quot;&gt;
          &lt;color rgba=&quot;0 0 0 1&quot;/&gt;
        &lt;/material&gt;
      &lt;/visual&gt;
    &lt;/link&gt;

    &lt;link name=&quot;wheel_right_rear&quot;&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;cylinder length=&quot;0.03&quot; radius=&quot;0.06&quot;/&gt;
        &lt;/geometry&gt;
        &lt;origin rpy=&quot;1.57 0 0&quot; xyz=&quot;-0.163 -0.222 0.03&quot;/&gt;
        &lt;material name=&quot;black&quot;&gt;
          &lt;color rgba=&quot;0 0 0 1&quot;/&gt;
        &lt;/material&gt;
      &lt;/visual&gt;
    &lt;/link&gt;

    &lt;link name=&quot;laser&quot;&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;cylinder length=&quot;0.065&quot; radius=&quot;0.035&quot;/&gt;
        &lt;/geometry&gt;
        &lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0.0 0.0 0.2825&quot;/&gt;
        &lt;material name=&quot;black&quot;/&gt;
      &lt;/visual&gt;
    &lt;/link&gt;

    &lt;link name=&quot;chassis&quot;&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;box size=&quot;0.5 0.5 0.8&quot;/&gt;
        &lt;/geometry&gt;
        &lt;origin rpy=&quot;0 0 0&quot; xyz=&quot;0.0 0.0 0.0&quot;/&gt;
        &lt;material name=&quot;silver&quot;&gt;
          &lt;color rgba=&quot;192 192 192 0.6&quot;/&gt;
        &lt;/material&gt;
      &lt;/visual&gt;
    &lt;/link&gt;

    &lt;link name=&quot;sonar_front&quot;&gt;
      &lt;visual&gt;
       &lt;/geometry&gt;
        &lt;origin rpy=&quot;1.5708 0.2618 0&quot; xyz=&quot;-0.21 0.0 0.235&quot;/&gt;
        &lt;material name=&quot;silver&quot;&gt;
          &lt;color rgba=&quot;192 192 192 0.6&quot;/&gt;
        &lt;/material&gt;
      &lt;/visual&gt;
    &lt;/link&gt;

    &lt;link name=&quot;sonar_rear&quot;&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;box size=&quot;0.02 0.025 0.07&quot;/&gt;
        &lt;/geometry&gt;
        &lt;origin rpy=&quot;1.5708 0.2618 3.1416&quot; xyz=&quot;0.23 0.0 0.235&quot;/&gt;
        &lt;material name=&quot;silver&quot;&gt;
          &lt;color rgba=&quot;192 192 192 0.6&quot;/&gt;
        &lt;/material&gt;
      &lt;/visual&gt;
    &lt;/link&gt;

    &lt;link name=&quot;sonar_left&quot;&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;box size=&quot;0.02 0.025 0.07&quot;/&gt;
        &lt;/geometry&gt;
        &lt;origin rpy=&quot;1.5708 -0.2618 1.5708&quot; xyz=&quot;0.0 0.18 0.235&quot;/&gt;
        &lt;material name=&quot;silver&quot;&gt;
          &lt;color rgba=&quot;192 192 192 0.6&quot;/&gt;
        &lt;/material&gt;
      &lt;/visual&gt;
    &lt;/link&gt;

    &lt;link name=&quot;sonar_right&quot;&gt;
      &lt;visual&gt;
        &lt;geometry&gt;
          &lt;box size=&quot;0.02 0.025 0.07&quot;/&gt;
        &lt;/geometry&gt;
        &lt;origin rpy=&quot;1.5708 -0.2618 -1.5708&quot; xyz=&quot;0.0 -0.19 0.235&quot;/&gt;
        &lt;material name=&quot;silver&quot;&gt;
          &lt;color rgba=&quot;192 192 192 0.6&quot;/&gt;
        &lt;/material&gt;
      &lt;/visual&gt;
    &lt;/link&gt;

    &lt;joint name=&quot;base_to_wheel_left_front&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;wheel_left_front&quot;/&gt;
      &lt;origin xyz=&quot;0 0 0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_wheel_right_front&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;wheel_right_front&quot;/&gt;
      &lt;origin xyz=&quot;0 0 0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_wheel_left_rear&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;wheel_left_rear&quot;/&gt;
      &lt;origin xyz=&quot;0 0 0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_wheel_right_rear&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;wheel_right_rear&quot;/&gt;
      &lt;origin xyz=&quot;0 0 0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_laser&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;laser&quot;/&gt;
      &lt;origin xyz=&quot;0.0 0.0 0.0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_left_sonar&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;sonar_left&quot;/&gt;
      &lt;origin xyz=&quot;0.0 0.0 0.0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_right_sonar&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;sonar_right&quot;/&gt;
      &lt;origin xyz=&quot;0.0 0.0 0.0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_rear_sonar&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;sonar_rear&quot;/&gt;
      &lt;origin xyz=&quot;0.0 0.0 0.0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_front_sonar&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;sonar_front&quot;/&gt;
      &lt;origin xyz=&quot;0.0 0.0 0.0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_perception_deck&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
    &lt;joint name=&quot;base_to_laser&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;laser&quot;/&gt;
      &lt;origin xyz=&quot;0.0 0.0 0.0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_left_sonar&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;sonar_left&quot;/&gt;
      &lt;origin xyz=&quot;0.0 0.0 0.0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_right_sonar&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;sonar_right&quot;/&gt;
      &lt;origin xyz=&quot;0.0 0.0 0.0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_rear_sonar&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;sonar_rear&quot;/&gt;
      &lt;origin xyz=&quot;0.0 0.0 0.0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_front_sonar&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;sonar_front&quot;/&gt;
      &lt;origin xyz=&quot;0.0 0.0 0.0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_perception_deck&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;perception_deck&quot;/&gt;
      &lt;origin xyz=&quot;0 0 0.0&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;base_to_chassis&quot; type=&quot;fixed&quot;&gt;
      &lt;parent link=&quot;base_link&quot;/&gt;
      &lt;child link=&quot;chassis&quot;/&gt;
      &lt;origin xyz=&quot;0 0 0.44&quot;/&gt;
    &lt;/joint&gt;
  &lt;/robot&gt;
</code></pre>
<p>Thanks!</p>
<p>EDITS<br />
after getting the messages, &quot;rostopic hz /ultrasound_front&quot; gives:</p>
<pre><code>subscribed to [/ultrasound_front]
average rate: 3.494
    min: 0.267s max: 0.305s std dev: 0.01919s window: 3
average rate: 3.384
    min: 0.250s max: 0.353s std dev: 0.03533s window: 6
average rate: 3.362
    min: 0.250s max: 0.353s std dev: 0.02813s window: 9
average rate: 3.352
    min: 0.250s max: 0.353s std dev: 0.02625s window: 13
average rate: 3.349
    min: 0.250s max: 0.353s std dev: 0.02447s window: 16
average rate: 3.344
    min: 0.250s max: 0.353s std dev: 0.02547s window: 20
average rate: 3.341
    min: 0.250s max: 0.353s std dev: 0.02368s window: 23
average rate: 3.256
    min: 0.250s max: 0.490s std dev: 0.04349s window: 26
average rate: 3.336
    min: 0.110s max: 0.490s std dev: 0.05406s window: 30
average rate: 3.335
    min: 0.110s max: 0.490s std dev: 0.05176s window: 33
</code></pre>
<p>and so on. Publishing interval in the MCU code is 250ms.</p>
<p>&quot;max_range:1.0&quot; in &quot;rostopic echo /ultrasound_front&quot; has been corrected (was an error in the original MCU code), the behaviour doesn't change. I modified the output above to reflect the current version.</p>
<p>&quot;rostopic info /ultrasound_front&quot;, after the massages started, gives: (Thank you @BTables!)</p>
<pre><code>Type: sensor_msgs/Range

Publishers: 
 * /rosserial_NANO_sensors (http://192.168.2.54:34525/)

Subscribers: 
 * /move_base (http://192.168.2.54:40149/)
</code></pre>
",9/9/2021 21:41,,1149,1,2,2,,16873199,,9/9/2021 20:19,3,69407772,"<p>I finally solved some of the problems that emerged after adding the ultrasound sensors. Because of the nature of the errors, and the extremely large amount of different hw/sw configurations possible, I will put here my findings, with some more general info, hoping to help others:</p>
<ul>
<li>Double check the UNIT of MEASURE used in the range fields in the microcontroller code. For example, the library and examples I used and referred to had everything in cm.<br />
This isn't good for ROS navigation layer, the range/distance numbers passed in the messages should be in meters (min_range, max_range, range).<br />
HOWEVER the microcontroller code could be passing the data, and using some internal calculations or logic, in centimeters (like here 'https://www.intorobotics.com/how-to-use-sensor_msgs-range-ros-for-multiple-sensors-with-rosserial/', for example), so some changes are probably needed (also regarding the logic behind the clearing of the costmap, but that's a problem for another question).</li>
<li>A message rate of 20Hz should be ok, it should not produce missing data messages, sync errors, etc. However please note that it's possible this frequency has to be modified, depending on the hardware involved.</li>
<li>The costmap YAML parameter <code>clear_on_max_reading</code> behaviour depends on how the data is presented by your ultrasound sensor (or sensors) MCU code. It's a good idea to try both settings and check which one is more appropriate for your case. You can then modify the MCU code to accomodate for the library logic behind this setting (or the other way around, modifying the libraries).</li>
<li>Verify that your RVIZ configuration contains all the necessary information to visualize your ultrasound (range) sensor data (<a href=""http://wiki.ros.org/rviz/DisplayTypes/Range"" rel=""nofollow noreferrer"">http://wiki.ros.org/rviz/DisplayTypes/Range</a>)</li>
<li>The URDF usually gives clear messages if something related to the transforms and related data is not working, once the real problems are solved, it's possible to see the cones and axes in Rviz (IF the unit of measure isn't too small!), so it's easy to correct orientation and position errors.</li>
<li>Use <code>check_urdf</code> to verify the validity of the URDF file (<a href=""http://wiki.ros.org/urdf#Verification"" rel=""nofollow noreferrer"">http://wiki.ros.org/urdf#Verification</a>), and <code>urdf_to_graphiz</code> to have a visual representation with some more data, that could give some clues on malfunctions or errors (<a href=""http://wiki.ros.org/urdf#Visualization"" rel=""nofollow noreferrer"">http://wiki.ros.org/urdf#Visualization</a>). Also <code>rqt_graph</code> with <code>enable_statistics</code> set to &quot;true&quot; can give useful clues (<a href=""http://wiki.ros.org/rqt_graph"" rel=""nofollow noreferrer"">http://wiki.ros.org/rqt_graph</a>).</li>
</ul>
",16873199,0,0,122200468,"Yes, I updated the question, thanks!",Incoming
179,4114,69425729,Why does my program makes my robot turn the power off?,|python|raspberry-pi|controls|robotics|pid-controller|,"<p>I'm trying to put together a programmed robot that can navigate the room by reading instructions off signs (such as bathroom-right). I'm using the AlphaBot2 kit and an RPI 3B+.</p>
<p>the image processing part works well, but for some reason, the MOTION CONTROL doesn't work.
I wrote a simple PID controller that &quot;feeds&quot; the motor, but as soon as motors start turning, the robot turns off.</p>
<pre><code>iPWM = 20 # initial motor power in duty cycle
MAX_PWM = 100 
dt = 0.001 # time step

#PID PARAMETERS#
KP = 0.2
KD = 0.01
KI = 0.00005

targets = ['BATHROOM', 'RESTAURANT', 'BALCONY']

class PID(object):
    def __init__(self,target):
        
        self.target = target
        self.kp = KP
        self.ki = KI
        self.kd = KD 
        self.setpoint = 320
        self.error = 0
        self.integral_error = 0
        self.error_last = 0
        self.derivative_error = 0
        self.output = 0
        
    def compute(self, pos):
        self.error = self.setpoint - pos
        #self.integral_error += self.error * TIME_STEP
        self.derivative_error = (self.error - self.error_last) / dt
        self.error_last = self.error
        self.output = self.kp*self.error + self.ki*self.integral_error + self.kd*self.derivative_error
        if(abs(self.output)&gt;= MAX_PWM-iPWM and (((self.error&gt;=0) and (self.integral_error&gt;=0))or((self.error&lt;0) and (self.integral_error&lt;0)))):
            #no integration
            self.integral_error = self.integral_error
        else:
            #rectangular integration
            self.integral_error += self.error * dt
        
        if self.output&gt;= MAX_PWM-iPWM:
            self.output = MAX_PWM-iPWM
    
            
        elif self.output &lt;= -MAX_PWM:
            self.output = iPWM - MAX_PWM
        return self.output
        



class MOTORS(object):
    
    def __init__(self,ain1=12,ain2=13,ena=6,bin1=20,bin2=21,enb=26):
        self.AIN1 = ain1
        self.AIN2 = ain2
        self.BIN1 = bin1
        self.BIN2 = bin2
        self.ENA = ena
        self.ENB = enb
        self.PA  = iPWM
        self.PB  = iPWM

        GPIO.setmode(GPIO.BCM)
        GPIO.setwarnings(False)
        GPIO.setup(self.AIN1,GPIO.OUT)
        GPIO.setup(self.AIN2,GPIO.OUT)
        GPIO.setup(self.BIN1,GPIO.OUT)
        GPIO.setup(self.BIN2,GPIO.OUT)
        GPIO.setup(self.ENA,GPIO.OUT)
        GPIO.setup(self.ENB,GPIO.OUT)
        self.PWMA = GPIO.PWM(self.ENA,500)
        self.PWMB = GPIO.PWM(self.ENB,500)
        self.PWMA.start(self.PA)
        self.PWMB.start(self.PB)
        self.stop()

    def forward(self):
        self.PWMA.ChangeDutyCycle(self.PA)
        self.PWMB.ChangeDutyCycle(self.PB)
        GPIO.output(self.AIN1,GPIO.LOW)
        GPIO.output(self.AIN2,GPIO.HIGH)
        GPIO.output(self.BIN1,GPIO.LOW)
        GPIO.output(self.BIN2,GPIO.HIGH)
    
    def updatePWM(self,value):
        if value&lt;0:
            self.PA = iPWM+abs(value)
            self.PB = iPWM
            self.PWMA.ChangeDutyCycle(self.PA)
            self.PWMB.ChangeDutyCycle(self.PB)
        if value&gt;0:
            self.PA = iPWM
            self.PB = iPWM+value
            self.PWMB.ChangeDutyCycle(self.PB)
            self.PWMA.ChangeDutyCycle(self.PA)
        if value ==0:
            self.PA = iPWM
            self.PB = iPWM
            self.PWMB.ChangeDutyCycle(self.PB)
            self.PWMA.ChangeDutyCycle(self.PA)
            
        GPIO.output(self.AIN1,GPIO.LOW)
        GPIO.output(self.AIN2,GPIO.HIGH)
        GPIO.output(self.BIN1,GPIO.LOW)
        GPIO.output(self.BIN2,GPIO.HIGH)
        
        
    
    def stop(self):
        self.PWMA.ChangeDutyCycle(0)
        self.PWMB.ChangeDutyCycle(0)
        GPIO.output(self.AIN1,GPIO.LOW)
        GPIO.output(self.AIN2,GPIO.LOW)
        GPIO.output(self.BIN1,GPIO.LOW)
        GPIO.output(self.BIN2,GPIO.LOW)
</code></pre>
<p>then I have the loop over the camera captures, where i identify the nearest sign and calculate his width and x coordiante of its center:</p>
<pre><code>cx = int(x+w//2)

    if d&lt;= 60:
        mot.stop()
        GPIO.cleanup()
dutyCycle = pid.compute(cx)
pwm = mot.updatePWM(dutyCycle)


</code></pre>
",10/3/2021 14:22,69425825,87,1,2,0,,4912015,,5/18/2015 12:22,21,69425825,"<p>It is probably not the software. Your power supply is not sufficient or stable enough to power your motors and the Raspberry Pi.  It is a very common problem. Either:</p>
<ul>
<li>Use separate power supplies which is recommended</li>
<li>Or Increase your main power supply and use some short of stabilization of power</li>
</ul>
<p>What power supply and power configuration are you using?</p>
",14649310,1,5,122710213,is it possible that if ill turn off the VNC viewer and won't show any image will sort it out?,Actuator
180,4124,69553909,How can I get z position of hector_quadrotor in ROS?,|c++|simulation|ros|robotics|,"<p>I am trying to get z position of hector_quadrotor in simulation. I can get X and Y axis coordinates but I couldn't get Z coordinate. I tried to get it by using GPS but the values are not correct. So I want to get Z coordinate by using barometer or another sensor.</p>
<p>Here the a part of pose_estimation_node.cpp (You can find full version on github source):</p>
<pre><code>void PoseEstimationNode::gpsCallback(const sensor_msgs::NavSatFixConstPtr&amp; gps, const 
geometry_msgs::Vector3StampedConstPtr&amp; gps_velocity) {
  boost::shared_ptr&lt;GPS&gt; m = boost::static_pointer_cast&lt;GPS&gt;(pose_estimation_-&gt;getMeasurement(&quot;gps&quot;));

  if (gps-&gt;status.status == sensor_msgs::NavSatStatus::STATUS_NO_FIX) {
    if (m-&gt;getStatusFlags() &gt; 0) m-&gt;reset(pose_estimation_-&gt;state());
    return;
  }

  GPS::Update update;
  update.latitude = gps-&gt;latitude * M_PI/180.0;
  update.longitude = gps-&gt;longitude * M_PI/180.0;
  update.velocity_north =  gps_velocity-&gt;vector.x;
  update.velocity_east  = -gps_velocity-&gt;vector.y;
  m-&gt;add(update);

  if (gps_pose_publisher_ || sensor_pose_publisher_) {
    geometry_msgs::PoseStamped gps_pose;
    pose_estimation_-&gt;getHeader(gps_pose.header);
    gps_pose.header.stamp = gps-&gt;header.stamp;
    GPSModel::MeasurementVector y = m-&gt;getVector(update, pose_estimation_-&gt;state());

    if (gps_pose_publisher_) {
      gps_pose.pose.position.x = y(0);
      gps_pose.pose.position.y = y(1);
      gps_pose.pose.position.z = gps-&gt;altitude - pose_estimation_-&gt;globalReference()- &gt;position().altitude;
      double track = atan2(gps_velocity-&gt;vector.y, gps_velocity-&gt;vector.x);
      gps_pose.pose.orientation.w = cos(track/2);
      gps_pose.pose.orientation.z = sin(track/2);
      gps_pose_publisher_.publish(gps_pose);
    }

    sensor_pose_.pose.position.x = y(0);
    sensor_pose_.pose.position.y = y(1);
    &quot;&quot;&quot;I add it here&quot;&quot;&quot; 
  }
}
</code></pre>
<p>If I add -----&gt;   sensor_pose_.pose.position.z = gps-&gt;altitude,</p>
<p>I can get a Z coordinate on RVIZ simulation or gnome-terminal. But as I said, the values are very meanless (negative values).</p>
<p>Also ------&gt; gps_pose.pose.position.z = gps-&gt;altitude - pose_estimation_-&gt;globalReference()-
&gt;position().altitude;</p>
<p>It is not working because position().altitude return NAN.
There are another measurement method in pose_estimation_node.cpp like barometer. How can I use barometer value.</p>
<p>Here the another part of pose_estimation_node.cpp:</p>
<pre><code>#if defined(USE_HECTOR_UAV_MSGS)
void PoseEstimationNode::baroCallback(const hector_uav_msgs::AltimeterConstPtr&amp; 
altimeter) {
  boost::shared_ptr&lt;Baro&gt; m = boost::static_pointer_cast&lt;Baro&gt;(pose_estimation_-&gt;getMeasurement(&quot;baro&quot;));
  m-&gt;add(Baro::Update(altimeter-&gt;pressure, altimeter-&gt;qnh));
}

#else
void PoseEstimationNode::heightCallback(const geometry_msgs::PointStampedConstPtr&amp; 
height) {
boost::shared_ptr&lt;Height&gt; m = boost::static_pointer_cast&lt;Height&gt;(pose_estimation_-&gt;getMeasurement(&quot;height&quot;));

Height::MeasurementVector update;
update(0) = height-&gt;point.z;
m-&gt;add(Height::Update(update));

if (sensor_pose_publisher_) {
sensor_pose_.pose.position.z = height-&gt;point.z - m-&gt;getElevation();
  }
}
#endif

void PoseEstimationNode::magneticCallback(const geometry_msgs::Vector3StampedConstPtr&amp; magnetic) {
boost::shared_ptr&lt;Magnetic&gt; m = boost::static_pointer_cast&lt;Magnetic&gt;(pose_estimation_-&gt;getMeasurement(&quot;magnetic&quot;));

Magnetic::MeasurementVector update;
update.x() = magnetic-&gt;vector.x;
update.y() = magnetic-&gt;vector.y;
update.z() = magnetic-&gt;vector.z;
m-&gt;add(Magnetic::Update(update));

if (sensor_pose_publisher_) {
sensor_pose_yaw_ = -(m-&gt;getModel()-&gt;getTrueHeading(pose_estimation_-&gt;state(), update) - pose_estimation_-&gt;globalReference()-&gt;heading());
}
}
</code></pre>
",10/13/2021 10:31,69557069,120,1,0,1,,16475993,,7/18/2021 19:32,8,69557069,"<p>To use a barometer you need to actually add that sensor to your robot. What you're looking at is only the callback meaning such messages are supported. If you want to add a new sensor to your robot I'd suggest looking at <a href=""http://gazebosim.org/tutorials/?tut=add_laser"" rel=""nofollow noreferrer"">this tutorial</a>.</p>
<p>All that being said, if you're getting incorrect altitude values you need to go back and recheck your transforms because the localization built into hector should work fine.</p>
",11245187,0,0,,,Coordinates
181,4141,69752391,Detecting an empty circular/rectangular on a surface using a pointcloud,|c++|point-cloud-library|point-clouds|robotics|realsense|,"<p>I am using the Intel Realsense L515 lidar to obtain a pointcloud. I need to detect an empty  area (no objects blocking it) on a surface (i.e. a table or a wall) so that a robotic arm has enough room to apply pressure to the surface at that point.</p>
<p>So far I have segmented the pointcloud in PCL in order to find a plane, to first detect the surface.
Afterwards I try segmenting for a 10-20cm circle anywhere in the area in order to attempt to find an empty space for the robot to operate.</p>
<p><a href=""https://i.stack.imgur.com/ElwEF.jpg"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/ElwEF.jpg</a></p>
<p>However as you can see, the circle (yellow points) is hollow instead of a full disc (so there could be objects lying inside it) and it has holes in it so it's not really an empty place on the table as the holes are created by objects eliminated in the planar segmentation stage (in this case that's my foot in the middle of the circle).</p>
<p>Is there any way to obtain the true non-object cluttered area on a planar surface?</p>
<p>Current code:</p>
<pre><code>//Plane segmentation processing start
pcl::ModelCoefficients::Ptr coefficients (new pcl::ModelCoefficients);
pcl::PointIndices::Ptr inliers (new pcl::PointIndices);
// Create the plane segmentation object
pcl::SACSegmentation&lt;pcl::PointXYZ&gt; seg;
// Optional
seg.setOptimizeCoefficients (true);
// Mandatory
seg.setModelType (pcl::SACMODEL_PLANE);
seg.setMethodType (pcl::SAC_RANSAC);
seg.setDistanceThreshold (0.005);

// Create the filtering object
pcl::ExtractIndices&lt;pcl::PointXYZ&gt; extract;

// Segment the largest planar component from the remaining cloud
seg.setInputCloud(cloud);
pcl::ScopeTime scopeTime(&quot;Test loop plane&quot;);
{
    seg.segment(*inliers, *coefficients);
}
if (inliers-&gt;indices.size() == 0)
{
    std::cerr &lt;&lt; &quot;Could not estimate a planar model for the given dataset.&quot; &lt;&lt; std::endl;
}

// Extract the inliers
extract.setInputCloud(cloud);
extract.setIndices(inliers);
extract.setNegative(false);
extract.filter(*cloud_p);
std::cerr &lt;&lt; &quot;PointCloud representing the planar component: &quot; &lt;&lt; cloud_p-&gt;width * cloud_p-&gt;height &lt;&lt; &quot; data points.&quot; &lt;&lt; std::endl;

//Circle segmentation processing start
pcl::ModelCoefficients::Ptr coefficients_c (new pcl::ModelCoefficients);
pcl::PointIndices::Ptr inliers_c (new pcl::PointIndices);
// Create the circle segmentation object
pcl::SACSegmentation&lt;pcl::PointXYZ&gt; seg_c;
// Optional
seg_c.setOptimizeCoefficients (true);
// Mandatory
seg_c.setModelType (pcl::SACMODEL_CIRCLE2D);
seg_c.setMethodType (pcl::SAC_RANSAC);
seg_c.setDistanceThreshold (0.01);
seg_c.setRadiusLimits(0.1,0.2);

// Create the filtering object
pcl::ExtractIndices&lt;pcl::PointXYZ&gt; extract_c;

// Segment a circle component from the remaining cloud
seg_c.setInputCloud(cloud_p);
pcl::ScopeTime scopeTime2(&quot;Test loop circle&quot;);
{
    seg_c.segment(*inliers_c, *coefficients_c);
}
if (inliers_c-&gt;indices.size() == 0)
{
    std::cerr &lt;&lt; &quot;Could not estimate a circle model for the given dataset.&quot; &lt;&lt; std::endl;
}
std::cerr &lt;&lt; &quot;Circle coefficients: \nCenter coordinates: &quot; &lt;&lt; coefficients_c-&gt;values[0] &lt;&lt; &quot; &quot; &lt;&lt; coefficients_c-&gt;values[1] &lt;&lt; &quot; &quot; &lt;&lt; coefficients_c-&gt;values[2] &lt;&lt; &quot; &quot;;

// Extract the inliers
extract_c.setInputCloud(cloud_p);
extract_c.setIndices(inliers_c);
extract_c.setNegative(false);
extract_c.filter(*cloud_c);
std::cerr &lt;&lt; &quot;PointCloud representing the circle component: &quot; &lt;&lt; cloud_c-&gt;width * cloud_c-&gt;height &lt;&lt; &quot; data points.&quot; &lt;&lt; std::endl;
</code></pre>
",10/28/2021 10:25,,653,0,2,2,,17269511,,10/28/2021 9:56,7,,,,,,123422583,"Thank you, I will look into it. However, I expect I might need a way to compensate for the fact that the density of the points is higher closer to the camera than it is farther away from it perhaps, but I will see how impactful that is while testing. What is the parameter that specifies the size of the voxel? I can't seem to find that information in the documentation. Is it the resolution?",Moving
182,4164,70197279,How to find robot TCP in image with CalibrateHandEye,|c++|opencv|camera-calibration|robotics|,"<p>I'm currently developing a programm for a robot with a camera attached to the end affector.
The goal is to calculate where the robots TCP appears in the camera frame. I'm using opencv in c++.
The robot is a UR5 from Universal Robots.</p>
<p>My plan:</p>
<ol>
<li>collect multiple (8) data-sets:</li>
</ol>
<ul>
<li>robot pose (XYZ in meters, directly from the robot controller)</li>
<li>robot rotation (rx ry rz in radians, directly from the robot controller)</li>
<li>take a picture of calibration pattern for each step</li>
</ul>
<ol start=""2"">
<li><p>run calibrateCamera over the set of pictures to get tvec and rvec for every step</p>
</li>
<li><p>run calibrateHandEye</p>
</li>
</ol>
<ul>
<li>for t_gripper2base i use the robot pose</li>
<li>for R_gripper2base i use the robot rotation</li>
<li>for t_target2cam i use tvec from calibrateCamera</li>
<li>for R_target2cam i use rvec from calibrateCamera</li>
</ul>
<p>I seem to get correct values (I measured the distance from cam to TCP and the t_cam2gripper seems to be correct.</p>
<pre><code>Translation vector target to cam: 
[-0.0001052803107026547;
 -0.0780872727019615;
 -0.1243323507069755]

Rotation matrix target to cam: 
[0.9999922523048892, 0.002655868335207422, -0.002905459271957709;
 -0.001229768871633805, 0.9119334002787367, 0.4103363999508009;
 0.003739384804660116, -0.4103296477461107, 0.9119296010009958]
</code></pre>
<p>The formula to transform the point from TCP coordinates to the image should look like this:</p>
<ul>
<li>(u,v,w) = C * T * (X,Y,Z,1)
But after the division by w my values are still way off (should be around (600,600)</li>
</ul>
<pre><code>Actual image position vector homogenized: 
[1778.542462313536;
 -1626.72483032188;
 1]
</code></pre>
<pre><code>#include &lt;QCoreApplication&gt;

#include &lt;opencv2/core.hpp&gt;
#include &lt;opencv2/highgui.hpp&gt;
#include &lt;opencv2/imgproc.hpp&gt;
#include &lt;opencv2/imgcodecs.hpp&gt;
#include &lt;opencv2/opencv.hpp&gt;

using namespace std;
using namespace cv;

int main(int argc, char *argv[])
{
    QCoreApplication a(argc, argv);

    Mat defaultCameraMatrix = (Mat_&lt;double&gt;(3, 3));
    Mat defaultDistortion = (Mat_&lt;double&gt;(1, 5));
    defaultCameraMatrix.at&lt;double&gt;(0, 0) = 1739.3749;   // default values from previous intrinsic camera calibration
    defaultCameraMatrix.at&lt;double&gt;(0, 1) = 0;
    defaultCameraMatrix.at&lt;double&gt;(0, 2) = 639.5;

    defaultCameraMatrix.at&lt;double&gt;(1, 0) = 0;
    defaultCameraMatrix.at&lt;double&gt;(1, 1) = 1739.3749;
    defaultCameraMatrix.at&lt;double&gt;(1, 2) = 479.5;

    defaultCameraMatrix.at&lt;double&gt;(2, 0) = 0;
    defaultCameraMatrix.at&lt;double&gt;(2, 1) = 0;
    defaultCameraMatrix.at&lt;double&gt;(2, 2) = 1;

    defaultDistortion.at&lt;double&gt;(0, 0) = -0.165909;
    defaultDistortion.at&lt;double&gt;(0, 1) = 0.303675;
    defaultDistortion.at&lt;double&gt;(0, 2) = 0.0;
    defaultDistortion.at&lt;double&gt;(0, 3) = 0.0;
    defaultDistortion.at&lt;double&gt;(0, 4) = 0.0;

    vector&lt;Mat&gt; R_gripper2base, t_gripper2base, R_target2cam, t_target2cam;
    Mat actualRobotPos1 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotPos2 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotPos3 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotPos4 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotPos5 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotPos6 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotPos7 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotPos8 = (Mat_&lt;double&gt;(3, 1));

    actualRobotPos1.at&lt;double&gt;(0,0) = -0.193139;
    actualRobotPos1.at&lt;double&gt;(1,0) = 0.463823;
    actualRobotPos1.at&lt;double&gt;(2,0) = -0.025;
    t_gripper2base.push_back(actualRobotPos1);

    actualRobotPos2.at&lt;double&gt;(0,0) = -0.193139;
    actualRobotPos2.at&lt;double&gt;(1,0) = 0.463823;
    actualRobotPos2.at&lt;double&gt;(2,0) = -0.025;
    t_gripper2base.push_back(actualRobotPos2);

    actualRobotPos3.at&lt;double&gt;(0,0) = -0.21273;
    actualRobotPos3.at&lt;double&gt;(1,0) = 0.4426;
    actualRobotPos3.at&lt;double&gt;(2,0) = -0.0288;
    t_gripper2base.push_back(actualRobotPos3);

    actualRobotPos4.at&lt;double&gt;(0,0) = -0.17213;
    actualRobotPos4.at&lt;double&gt;(1,0) = 0.4103;
    actualRobotPos4.at&lt;double&gt;(2,0) = 0.014;
    t_gripper2base.push_back(actualRobotPos4);

    actualRobotPos5.at&lt;double&gt;(0,0) = -0.13724;
    actualRobotPos5.at&lt;double&gt;(1,0) = 0.45;
    actualRobotPos5.at&lt;double&gt;(2,0) = 0.02978;
    t_gripper2base.push_back(actualRobotPos5);

    actualRobotPos6.at&lt;double&gt;(0,0) = -0.1655;
    actualRobotPos6.at&lt;double&gt;(1,0) = 0.478;
    actualRobotPos6.at&lt;double&gt;(2,0) = -0.0211;
    t_gripper2base.push_back(actualRobotPos6);

    actualRobotPos7.at&lt;double&gt;(0,0) = -0.17018;
    actualRobotPos7.at&lt;double&gt;(1,0) = 0.46458;
    actualRobotPos7.at&lt;double&gt;(2,0) = -0.03761;
    t_gripper2base.push_back(actualRobotPos7);

    actualRobotPos8.at&lt;double&gt;(0,0) = -0.193139;
    actualRobotPos8.at&lt;double&gt;(1,0) = 0.463823;
    actualRobotPos8.at&lt;double&gt;(2,0) = 0.025;
    t_gripper2base.push_back(actualRobotPos8);

    Mat actualRobotRotVec1 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotRotVec2 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotRotVec3 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotRotVec4 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotRotVec5 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotRotVec6 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotRotVec7 = (Mat_&lt;double&gt;(3, 1)),
            actualRobotRotVec8 = (Mat_&lt;double&gt;(3, 1));

    actualRobotRotVec1.at&lt;double&gt;(0,0) = -3.14159;
    actualRobotRotVec1.at&lt;double&gt;(1,0) = 0.00;
    actualRobotRotVec1.at&lt;double&gt;(2,0) = 0.00719124;
    R_gripper2base.push_back(actualRobotRotVec1);

    actualRobotRotVec2.at&lt;double&gt;(0,0) = -2.06;
    actualRobotRotVec2.at&lt;double&gt;(1,0) = -2.36;
    actualRobotRotVec2.at&lt;double&gt;(2,0) = 0.03;
    R_gripper2base.push_back(actualRobotRotVec2);

    actualRobotRotVec3.at&lt;double&gt;(0,0) = 2.39;
    actualRobotRotVec3.at&lt;double&gt;(1,0) = 1.86;
    actualRobotRotVec3.at&lt;double&gt;(2,0) = 0.49;
    R_gripper2base.push_back(actualRobotRotVec3);

    actualRobotRotVec4.at&lt;double&gt;(0,0) = -2.66;
    actualRobotRotVec4.at&lt;double&gt;(1,0) = 0.08;
    actualRobotRotVec4.at&lt;double&gt;(2,0) = 0.09;
    R_gripper2base.push_back(actualRobotRotVec4);

    actualRobotRotVec5.at&lt;double&gt;(0,0) = -2.84;
    actualRobotRotVec5.at&lt;double&gt;(1,0) = 0.19;
    actualRobotRotVec5.at&lt;double&gt;(2,0) = 0.69;
    R_gripper2base.push_back(actualRobotRotVec5);

    actualRobotRotVec6.at&lt;double&gt;(0,0) = 2.1;
    actualRobotRotVec6.at&lt;double&gt;(1,0) = -2.34;
    actualRobotRotVec6.at&lt;double&gt;(2,0) = -0.02;
    R_gripper2base.push_back(actualRobotRotVec6);

    actualRobotRotVec7.at&lt;double&gt;(0,0) = 1.66;
    actualRobotRotVec7.at&lt;double&gt;(1,0) = -2.53;
    actualRobotRotVec7.at&lt;double&gt;(2,0) = -0.23;
    R_gripper2base.push_back(actualRobotRotVec7);

    actualRobotRotVec8.at&lt;double&gt;(0,0) = -3.14159;
    actualRobotRotVec8.at&lt;double&gt;(1,0) = 0.00;
    actualRobotRotVec8.at&lt;double&gt;(2,0) = 0.00719124;
    R_gripper2base.push_back(actualRobotRotVec8);

    //    for(int i = 0; i &lt; t_gripper2base.size(); i++)
    //    {
    //        cout &lt;&lt; t_gripper2base[i] &lt;&lt; endl &lt;&lt; endl;
    //    }

    //    for(int i = 0; i &lt; R_gripper2base.size(); i++)
    //    {
    //        cout &lt;&lt; R_gripper2base[i] &lt;&lt; endl &lt;&lt; endl;
    //    }

    vector&lt;String&gt; fileNames;
    glob(&quot;PATH*.png&quot;, fileNames, false); // directory of images
    vector&lt;vector&lt;Point2f&gt;&gt; corners(fileNames.size());
    Mat chessboardImg, chessboardImgGray;

    vector&lt;Point3f&gt; objp;
    vector&lt;vector&lt;Point3f&gt;&gt; worldCoordinates;
    int checkerBoard[2] = {9,6};
    double fieldSize = 0.008;

    Mat cameraMatrixHandEye, distortionHandEye;
    vector&lt;Mat&gt; rvecs, tvecs;

    for(int i = 1; i &lt; checkerBoard[1]; i++){
        for(int j = 1; j &lt; checkerBoard[0]; j++){
            objp.push_back(Point3f(j*fieldSize, i*fieldSize, 0));
        }
    }

    for(int i = 0; i &lt; 8; i++)
    {
        chessboardImg = imread(fileNames[i]);
        cvtColor(chessboardImg, chessboardImgGray, COLOR_BGR2GRAY);

        bool patternFound = findChessboardCorners(chessboardImgGray, Size(8,5), corners[i],  CALIB_CB_ADAPTIVE_THRESH + CALIB_CB_NORMALIZE_IMAGE);

        if(patternFound)
        {
            cornerSubPix(chessboardImgGray, corners[i],Size(11,11), Size(-1,-1), TermCriteria(TermCriteria::EPS + TermCriteria::MAX_ITER, 30, 0.1));
            drawChessboardCorners(chessboardImg, Size(8,5), corners[i], patternFound);
            worldCoordinates.push_back(objp);
        }
        //***** Check loaded images and detected chessboard *****
        //imshow(&quot;source&quot;, chessboardImgGray);
        //imshow(&quot;chess&quot;, chessboardImg);
        //waitKey(0);
        //*******************************************************
    }
    float reprojectionError = calibrateCamera(worldCoordinates, corners, Size(1280,960), cameraMatrixHandEye, distortionHandEye, rvecs, tvecs, CALIB_FIX_ASPECT_RATIO + CALIB_FIX_K3 +
                                              CALIB_ZERO_TANGENT_DIST + CALIB_FIX_PRINCIPAL_POINT);

    //***** Check camera calibration results *****
    //cout &lt;&lt; &quot;Reprojection Error CHE: &quot; &lt;&lt; endl &lt;&lt; reprojectionError &lt;&lt; endl &lt;&lt; endl;
    //cout &lt;&lt; &quot;Camera Matrix CHE: &quot; &lt;&lt; endl &lt;&lt; cameraMatrixHandEye &lt;&lt; endl &lt;&lt; endl;
    //cout &lt;&lt; &quot;Distortion CHE: &quot; &lt;&lt; endl &lt;&lt; distortionHandEye &lt;&lt; endl &lt;&lt; endl;
    //for(int i = 0; i &lt; numberOfPoses; i++)
    //{
    //    cout &lt;&lt; &quot;No. &quot; &lt;&lt; i+1 &lt;&lt; &quot; Target translation: &quot; &lt;&lt; endl &lt;&lt; tvecs[i] &lt;&lt; endl &lt;&lt; endl;
    //    cout &lt;&lt; &quot;No. &quot; &lt;&lt; i+1 &lt;&lt; &quot; Target rotation: &quot; &lt;&lt; endl &lt;&lt; rvecs[i] &lt;&lt; endl &lt;&lt; endl;
    //}
    //********************************************/

    for(int i = 0; i &lt; rvecs.size(); i++)
    {
        t_target2cam.emplace_back(tvecs[i]);
        R_target2cam.emplace_back(rvecs[i]);
    }

    //    for(int i = 0; i &lt; t_target2cam.size(); i++)
    //    {
    //        cout &lt;&lt; t_target2cam[i] &lt;&lt; endl &lt;&lt; endl;
    //    }

    //    for(int i = 0; i &lt; R_target2cam.size(); i++)
    //    {
    //        cout &lt;&lt; R_target2cam[i] &lt;&lt; endl &lt;&lt; endl;
    //    }

    Mat R_cam2gripper;
    Mat t_cam2gripper = (Mat_&lt;double&gt;(3, 1));
    calibrateHandEye(R_gripper2base, t_gripper2base, R_target2cam, t_target2cam, R_cam2gripper, t_cam2gripper);

    cout &lt;&lt; t_cam2gripper &lt;&lt; endl &lt;&lt; endl;
    cout &lt;&lt; R_cam2gripper &lt;&lt; endl &lt;&lt; endl;

    Mat transformationMat4x4 = (Mat_&lt;double&gt;(4, 4));
    Mat transformationMatInv4x4 = (Mat_&lt;double&gt;(4, 4));
    Mat R_cam2gripperInv = (Mat_&lt;double&gt;(3, 3));
    Mat t_cam2gripperInv = (Mat_&lt;double&gt;(3, 1));

    transformationMat4x4.at&lt;double&gt;(0, 0) = R_cam2gripper.at&lt;double&gt;(0, 0);
    transformationMat4x4.at&lt;double&gt;(0, 1) = R_cam2gripper.at&lt;double&gt;(0, 1);
    transformationMat4x4.at&lt;double&gt;(0, 2) = R_cam2gripper.at&lt;double&gt;(0, 2);
    transformationMat4x4.at&lt;double&gt;(0, 3) = t_cam2gripper.at&lt;double&gt;(0, 0);
    transformationMat4x4.at&lt;double&gt;(1, 0) = R_cam2gripper.at&lt;double&gt;(1, 0);
    transformationMat4x4.at&lt;double&gt;(1, 1) = R_cam2gripper.at&lt;double&gt;(1, 1);
    transformationMat4x4.at&lt;double&gt;(1, 2) = R_cam2gripper.at&lt;double&gt;(1, 2);
    transformationMat4x4.at&lt;double&gt;(1, 3) = t_cam2gripper.at&lt;double&gt;(1, 0);
    transformationMat4x4.at&lt;double&gt;(2, 0) = R_cam2gripper.at&lt;double&gt;(2, 0);
    transformationMat4x4.at&lt;double&gt;(2, 1) = R_cam2gripper.at&lt;double&gt;(2, 1);
    transformationMat4x4.at&lt;double&gt;(2, 2) = R_cam2gripper.at&lt;double&gt;(2, 2);
    transformationMat4x4.at&lt;double&gt;(2, 3) = t_cam2gripper.at&lt;double&gt;(2, 0);
    transformationMat4x4.at&lt;double&gt;(3, 0) = 0;
    transformationMat4x4.at&lt;double&gt;(3, 1) = 0;
    transformationMat4x4.at&lt;double&gt;(3, 2) = 0;
    transformationMat4x4.at&lt;double&gt;(3, 3) = 1;

    transformationMatInv4x4 = transformationMat4x4.inv();
    R_cam2gripperInv.at&lt;double&gt;(0,0) = transformationMatInv4x4.at&lt;double&gt;(0,0);
    R_cam2gripperInv.at&lt;double&gt;(0,1) = transformationMatInv4x4.at&lt;double&gt;(0,1);
    R_cam2gripperInv.at&lt;double&gt;(0,2) = transformationMatInv4x4.at&lt;double&gt;(0,2);

    R_cam2gripperInv.at&lt;double&gt;(1,0) = transformationMatInv4x4.at&lt;double&gt;(1,0);
    R_cam2gripperInv.at&lt;double&gt;(1,1) = transformationMatInv4x4.at&lt;double&gt;(1,1);
    R_cam2gripperInv.at&lt;double&gt;(1,2) = transformationMatInv4x4.at&lt;double&gt;(1,2);

    R_cam2gripperInv.at&lt;double&gt;(2,0) = transformationMatInv4x4.at&lt;double&gt;(2,0);
    R_cam2gripperInv.at&lt;double&gt;(2,1) = transformationMatInv4x4.at&lt;double&gt;(2,1);
    R_cam2gripperInv.at&lt;double&gt;(2,2) = transformationMatInv4x4.at&lt;double&gt;(2,2);

    t_cam2gripperInv.at&lt;double&gt;(0,0) = transformationMatInv4x4.at&lt;double&gt;(0,3);
    t_cam2gripperInv.at&lt;double&gt;(1,0) = transformationMatInv4x4.at&lt;double&gt;(1,3);
    t_cam2gripperInv.at&lt;double&gt;(2,0) = transformationMatInv4x4.at&lt;double&gt;(2,3);

    cout &lt;&lt; transformationMatInv4x4 &lt;&lt; endl &lt;&lt; endl;
    cout &lt;&lt; t_cam2gripperInv &lt;&lt; endl &lt;&lt; endl;


    Point3f objectPoints1, objectPoints2;
    vector&lt;Point3f&gt; objectPoints;

    objectPoints1.x = 0;    //TCP in TCP-Coordinates
    objectPoints1.y = 0;
    objectPoints1.z = 0;
    objectPoints.push_back(objectPoints1);

    vector&lt;Point2f&gt; imagePoints;
    projectPoints(objectPoints, R_cam2gripperInv, t_cam2gripperInv, defaultCameraMatrix, defaultDistortion, imagePoints);
    cout &lt;&lt; imagePoints[0] &lt;&lt; endl &lt;&lt; endl;

    return a.exec();
}
`
</code></pre>
",12/2/2021 9:52,,647,2,0,0,0,17087746,,10/6/2021 8:57,10,70201334,"<p>you need to use solvepnp to get rvec and tvec for each image separately and then you will have a list of rvecs and tvecs. list length equals no of images. To get a similar list of rvec and tvec for Gripper_to_base transformation, you need to derive the R and T matrices based on robot dynamics which take rotation angle as input. Then for each pose you need to input the rotation angle data to R,T matrices to get rvec and tvec for each pose and make list of same length. Then you input them to calibrateHandeye function.</p>
",12114977,0,9,,,Incoming
183,4202,70695409,how to go from pixel coordinates to angle off the optical axis (Object detection alignment),|raspberry-pi|camera|computer-vision|robotics|,"<p>I am making a robot that detects a ball and goes to it.</p>
<p>Because I am doing the detection on a raspberry pi I thought that it will be better to work with images and not with real time detection.</p>
<p>The robot rotates 45 degrees and take a photo. If the ball isn’t detected, it moves another 45 degrees till it detects the ball.</p>
<p>Here it is the problem: after the detection the ball could be anywhere on the image, so I need to make an algorithm that says to the robot how many degrees it should turn to be centered aligned to the ball.</p>
<p>Here is how the robot detects the ball:</p>
<ul>
<li>Google cloud vision API, and if the ball isn't detected...</li>
<li>a TF-lite model detection will run. If the ball isn't detected with this...</li>
<li>it rotates.</li>
</ul>
<p>The camera used for the project: Raspberry pi Noir V2 (res HD)</p>
<p>Language: Python, but mostly I need ideas.</p>
<p>P.S.: I am a newbie to robotics, so any help will be appreciated.
Sorry for missing out some info, it is the first time asking on stackoverflow.</p>
",1/13/2022 10:57,70701562,814,1,2,-1,,17923574,,1/13/2022 10:51,8,70701562,"<p>You're asking how to handle a camera matrix, how to work with the &quot;intrinsics&quot; of a camera, and a little linear algebra.</p>
<p><a href=""https://elinux.org/Rpi_Camera_Module#Technical_Parameters_.28v.2_board.29"" rel=""nofollow noreferrer"">specs for the &quot;Raspberry pi Noir V2 (res HD)&quot;</a>:</p>
<ul>
<li>1.12 µm pixel size (design)</li>
<li>3.04 mm focal length (design)</li>
<li>full resolution: 3280 x 2464 (design)</li>
<li>full resolution FoV: H 62.28°, V 48.83°, D 74.16° (calculated)</li>
</ul>
<p>The camera matrix <code>K</code> generally looks like</p>
<p><a href=""https://docs.opencv.org/3.4/dc/dbb/tutorial_py_calibration.html"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3MaAot.png"" alt=""camera matrix"" /></a></p>
<p>The full resolution focal length (in pixels) is <code>f = (3.04 mm) / (1.12 µm/pixel) = 2714.3 [pixels]</code></p>
<p>The &quot;1920x1080&quot; video mode does no binning, only cropping. That means <code>f = 2714</code> goes for it too. That means the video mode's field of view is actually 38.96° horizontally, 22.5° vertically, 44.2° diagonally.</p>
<p>NB: for binned modes, if the binning is 2x2, then <code>f</code> halves, i.e. use 1357.</p>
<p>I have no information on how still pictures are produced. The full sensor has 4:3 aspect ratio. Assuming a 1920x1080 frame is fully fitted in there (touching sides, cropping top and bottom), the scale factor is 0.585 and f = 1589, with FoV being 62.28° by 37.54°, 69.46° diagonally.</p>
<p>Focal length can also be calculated from resolution and field of view, but pixel pitch and lens focal length are the nominal design parameters and the field of view derives from that (and imperfections like lens distortion).</p>
<p>Then we have <code>cx = (width-1) / 2 = 959.5</code> and <code>cy = (height-1) / 2 = 539.5</code>.</p>
<p>So now you have the values for the matrix.</p>
<p>A 3D point <code>p</code> is projected onto the image by calculating <code>K p</code>, which is a matrix multiplication. The opposite can be done. You can reproject a point on the picture back into the world. It's now a vector, a ray, a direction.</p>
<p>If you have <code>(x,y)</code> as picture coordinates, calculate:</p>
<pre><code>v_x = (x - cx) / f
# v_y = (y - cy) / f
</code></pre>
<p>and finally the horizontal angle:</p>
<pre><code>alpha = atan(v_x) # radians, or
alpha = atan(v_x) * 180/pi # degrees
</code></pre>
<p>(Diagonal angle away from the optical axis would be <code>atan(hypot(v_x, v_y))</code>)</p>
<p>For a point on the left edge (x=0), that would mean turning 19.5 degrees left.</p>
<p>All of this assumes that the picture was not distorted by the lens. For small angles, this doesn't matter. Some cameras have special lenses that hardly distort at all. Some cameras, especially action cameras, intentionally have almost-fisheye lenses (meaning strong distortion).</p>
<p>If you need to deal with lens distortion, that's another topic. There are common models for lens distortion that work with as few as 4 parameters. Both MATLAB and OpenCV come with calibration methods.</p>
",2602877,1,12,124984300,"I need to make an algorithm that calculate the degrees the robot should turn to be aligned,Angle of View: 62.2 x 48.8 degrees,the resolution of the pic(1920x1080),and i know the x1,x2,y1,y2 of the label box.But i want the formula mainly.",Coordinates
184,4210,70994989,Path Planning on a 3d point cloud,|point-clouds|robotics|motion-planning|,"<p>I have a 3d point cloud of a location on which I am trying to design a path planner for a mobile robot. Can anyone please guide me towards the right approach to take in solving the problem. For the point cloud, I have the coordinates of the obstacles on the map (their x,y,z positions). I am trying to solve the problem as a stand-alone general purpose planner for a mobile robot without using ROS.</p>
<p>My current stumbling block lies in the theoretical aspect as well since the fact that the point cloud consists of just x,y,z points, how is a path planning algorithm like A* run on such types of data where you can't define a general grid like that for a 2d case with each grid cell as a node? I have the coordinates of the obstacles on the map (their x,y,z positions).</p>
<p>Would greatly appreciate if anyone can provide me with some guidance on how to move forward.</p>
",2/5/2022 3:09,,435,1,0,0,,18085938,,2/1/2022 0:33,2,72390139,"<p>I am facing a similar problem. The first step should be calculating the normal vectors of the point cloud. Then, the path can be calculated based on the normal vector.</p>
",18632004,0,1,,,Moving
185,4281,71697482,Incorrect distance travelled using optical wheel encoder,|python|raspberry-pi|raspbian|robotics|encoder|,"<p>I started using a raspberry pi 3b+ about 2 months ago, so I'm fairly new. I'm working on a project, in which I have assembled a 4 wheeled small car, it has 4 DC motors, a 3b+, an L289D H bridge and an optical wheel encoder.</p>
<p>I'm in my initial stages of the project where I am testing the kit, whether it travels the correct distance or not. Below is the code that I'm using to test the process:</p>
<pre><code>import RPi.GPIO as GPIO
import time
from time import sleep

GPIO.setmode(GPIO.BOARD)
GPIO.setup(11, GPIO.IN, pull_up_down=GPIO.PUD_UP)

GPIO.setup(13, GPIO.OUT)   #clockwise left
GPIO.setup(15, GPIO.OUT)   #anticlockwise right
GPIO.setup(16, GPIO.OUT)   #anticlockwise left
GPIO.setup(18, GPIO.OUT)   #clockwise right

stateLast = GPIO.input(11)
rotationCount=0
stateCount=0
stateCountTotal=0
flag=0

circ=62.4*3.14 #mm
statesPerRotation=20
distancePerStep= circ/statesPerRotation

GPIO.output(13, GPIO.HIGH)
GPIO.output(18, GPIO.HIGH)
flag=&quot;true&quot;
while flag==&quot;true&quot;:
    stateCurrent = GPIO.input(11)
    if stateCurrent != stateLast:
        stateLast = stateCurrent
        stateCount += 1
        stateCountTotal += 1
        
    if stateCount == statesPerRotation:
        rotationCount += 1
        stateCount = 0
        
    distance = distancePerStep * stateCountTotal
    print(&quot;Distance&quot;,distance)
    if distance &gt; 50:
        flag = &quot;false&quot;

GPIO.output(13, GPIO.LOW)
GPIO.output(18, GPIO.LOW)
</code></pre>
<p>What's happening is that upon running the code, the kit is not travelling the distance I enter in the code above. When I use lower values of distance, the kit travels accurately, but as soon as I increase the distance to greater than 200mm for example, the kit falls short of the actual distance it is supposed to travel. For 300mm, it stops at around 250mm on ground, but the interesting thing is that upon printing the distance from the code, it runs till 300mm. I am failing to understand that why is the code showing that the kit has travelled 300mm but in reality it always stops at around 220mm-250mm.</p>
<p>Another interesting thing I've noticed is that when I use Thonny to run the code, the distance travelled is always inconsistent, no matter the value of distance. For example, if I use distance 20mm, sometimes it ran 10mm, at other moments 30mm, and never accurate. When I ran the code.py file from the terminal, the distance it travelled became consistent, but still inaccurate as stated in the above paragraph.</p>
<p>I'm using remote desktop connection to access the 3b+. I have used another wheel encoder to ensure there's no problem with it and have also checked the battery voltage, which is good. Other than that I have tested the kit on different surfaces as well, but nothing has worked till now.</p>
<p>I would appreciate if anyone helps me out with this.</p>
<p>Thanks!</p>
",3/31/2022 18:27,,132,1,8,0,,18646904,,3/31/2022 18:15,1,71697907,"<pre><code># real-time processing requires fast non blocking code. Try the following 
# and see if the performance improves.
import RPi.GPIO as GPIO
import time
from time import sleep

GPIO.setmode(GPIO.BOARD)
GPIO.setup(11, GPIO.IN, pull_up_down=GPIO.PUD_UP)

GPIO.setup(13, GPIO.OUT)  # clockwise left
GPIO.setup(15, GPIO.OUT)  # anticlockwise right
GPIO.setup(16, GPIO.OUT)  # anticlockwise left
GPIO.setup(18, GPIO.OUT)  # clockwise right

stateLast = GPIO.input(11)
rotationCount = 0
stateCount = 0
stateCountTotal = 0
flag = 0

circ = 62.4 * 3.14  # mm
statesPerRotation = 20
distancePerStep = circ / statesPerRotation

GPIO.output(13, GPIO.HIGH)
GPIO.output(18, GPIO.HIGH)
# flag = &quot;true&quot;

while True: # Use boolean logic not string evaluation
    stateCurrent = GPIO.input(11)
    if stateCurrent != stateLast:
        stateLast = stateCurrent
        stateCount += 1
        stateCountTotal += 1

    if stateCount == statesPerRotation:
        rotationCount += 1
        stateCount = 0

    distance = distancePerStep * stateCountTotal
    # print(&quot;Distance&quot;, distance)  # Do not print in th e loop
    if distance &gt; 50:
        break
print(&quot;Distance&quot;, distance)
GPIO.output(13, GPIO.LOW)
GPIO.output(18, GPIO.LOW)
</code></pre>
",7803702,0,0,126710961,"Yes, you need to understand what the actual frequency of the optical encoder is. You should match the sampling by your code to slightly greater than twice that frequency.",Other
186,4294,71757025,C script for guiding a line follower,|c|state-machine|robotics|,"<p>So for class I am trying to write a C script for a line follower robot. The robot has 3 sensors (ABC) which give logic 1 on black and logic 0 on white, A is on the left side, B in the middle and C on the right when looking straight down on the robot. It also has 2 motors, one on each side.</p>
<p>The board I am using is an <a href=""https://eu.mouser.com/ProductDetail/Texas-Instruments/MSP-EXP430G2?qs=CLImetaeaXWH2pYG%252BA%252B4Vw%3D%3D"" rel=""nofollow noreferrer"">Texas Instruments MSP-EXP430G2</a> and I am using the ports P1.0 - P1.7.</p>
<p>Now, I have literally 0 experience writing C scripts so all pointers are very much appreciated.</p>
<p>Here is the code.</p>
<pre><code># include &quot;msp430G2553.h&quot;
# include &quot;stdio.h&quot;
# include &quot;math.h&quot;

#define Both_On 1 // States
#define Left_On 2
#define Right_On 3
int main (void)
{
char STATE;
P1DIR|=0x0F; // Port1 four lower bits as OUTput, others as INput(0000 1111)
WDTCTL = WDTPW + WDTHOLD;// stop watch dog
STATE = Both_On; // Start here
char MASK1=0x10; //Sensor mask 0001 0000 (P1.0 - P1.7 from left to right)
char L; //Direction switch
while(1)
{
L = P1IN &amp; MASK1; //Sensor value based on input-port
switch (STATE)
{
case Both_On:
    P1OUT = 0x03;//Both motors on
    puts(&quot;Both_On&quot;); //for testing
    if (L == 0x00 ){
        P1OUT=0x01;//left motor on
        STATE = Left_On;}
    else if (L==0x10){
        P1OUT=0x03;
        STATE = Both_On;}
    break;
case Left_On:
    P1OUT = 0x01;//left motor on
    puts(&quot;Left_on&quot;); //for testing
    if (L == 0x10 ){
        P1OUT=0x03;//both motors on
        STATE = Both_On;}
    else if (L==0x00){
        P1OUT= 0x02;
        STATE = Right_On;}
    break;
case Right_On:
    P1OUT = 0x02;//right motor on
    puts(&quot;Right_on&quot;); //for testing
    if (L == 0x10 ){
        P1OUT=0x03;//both motors on
        STATE = Both_On;}
    else if (L==0x00){
        P1OUT= 0x01;
        STATE = Left_On;}
    break;
break;
}//end of Switch
}// end of while
</code></pre>
<p>I think I've understood the method for switching from one active motor to another or both. The first 4 bits are reserved for the input from the sensors and the last 4 bits direct the motors. So P1OUT=0x03 basically means the output is 0000 0011 so the ports P1.6 and P1.7 are active and the motors wired to those ports would turn on.</p>
<p>I've been instructed to use an &quot;L&quot; variable as the direction switch. My problem is that I don't quite understand how this works.</p>
<p>I've been thinking that I would wire the B sensor to the port &quot;L&quot; takes it value from and then saying, if L==1 then both motors should be on and if L==0 then the state should change from &quot;Left_on&quot; to &quot;Right_on&quot; in a loop but this behavior doesn't feel very logical, I would want better detection.</p>
<p>Could I change the mask to be &quot;MASK1=0x70&quot; (0111) which would give me 3 ports to wire the sensors into, and guide the motors using for example:</p>
<pre><code>else if (L==0x40){ //0100 only the left side sensor A is on black
        P1OUT= 0x02; //0010 P1.6
        STATE = Right_On;} // turn the right motor on to keep following the line
    break;
</code></pre>
<p>As I said, all pointers and suggestions are highly appreciated.</p>
",4/5/2022 18:53,,250,0,2,0,,17988251,,1/20/2022 22:14,5,,,,,,126811174,"Then it would be good if you would highlight the exact problem you are having, and what is the actual question. The code you show at the end of the question, so what does it do if you run it on the actual robot?",Incoming
187,4308,71928676,"Cannot find module ""qfi"" for running JdeRobot drone_cat_mouse exercise from source",|python|ros|robotics|,"<p>I want to run JdeRobot drone_cat_mouse on my Ubuntu 20.04. I'm using ROS Noetic and has faithfully followed <a href=""https://github.com/JdeRobot/drones/blob/noetic-devel/installation20.md"" rel=""nofollow noreferrer"">these installation instructions</a>. Everything it told me to test was working properly.</p>
<p>When I first ran roslaunch drone_cat_mouse.launch, there was an import error for teleopWidget and sensorsWidget which I fixed by using relative imports. Then I had an error <code>No module named qfi</code>.</p>
<p><a href=""https://i.stack.imgur.com/RsOcr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RsOcr.png"" alt=""No module named qfi"" /></a></p>
<p>Unlike teleopWidget and sensorsWidget, I couldn't find the qfi module in the <a href=""https://github.com/JdeRobot/drones"" rel=""nofollow noreferrer"">JdeRobot/drones source code.</a> So I googled it, and the only relevant result that popped up was <a href=""https://gsyc.urjc.es/pipermail/jde-developers/2017-April/004421.html"" rel=""nofollow noreferrer"">this</a>, which led to <a href=""https://github.com/JdeRobot/base/issues/738"" rel=""nofollow noreferrer"">this link</a>. They said to:</p>
<p><code>sudo touch /usr/lib/python2.7/dist-packages/qfi/__init__.py</code></p>
<p>But I ran that command and this happened!</p>
<p><a href=""https://i.stack.imgur.com/ivoTP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ivoTP.png"" alt=""qfi module no exist"" /></a></p>
<p>Not even pip has a &quot;qfi&quot; module!</p>
<p><a href=""https://i.stack.imgur.com/qDZuy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qDZuy.png"" alt=""y u no qfi?!"" /></a></p>
<p>So I thought to check <a href=""https://github.com/search?q=org%3AJdeRobot%20qfi&amp;type=code"" rel=""nofollow noreferrer"">JdeRobot's entire repository.</a> Turns out it was in JdeRobot/base, and that repo is not maintained anymore!</p>
<p>After further digging, there was <a href=""https://github.com/JdeRobot/RoboticsAcademy/issues/847"" rel=""nofollow noreferrer"">this issue</a> which basically tells us forget about it and move to the web release! But I can't, circumstances forced me to use the source code option (deliverables are drone_cat_mouse.world and my_solution.py, it's impossible for me to get the former in the docker web version and the latter's format is different between the source code version and the web version).</p>
<p>In a nutshell, how do I fix this <code>qfi module</code> problem so that I can run the exercises from source <a href=""https://www.youtube.com/watch?v=rZCDixH1RV8"" rel=""nofollow noreferrer"">like</a> <a href=""https://www.youtube.com/watch?v=IwAGndG8nx0"" rel=""nofollow noreferrer"">these</a> <a href=""https://www.youtube.com/watch?v=eadm9I4ZDso"" rel=""nofollow noreferrer"">people</a>?</p>
",4/19/2022 17:03,,73,2,0,0,,10618936,,11/7/2018 14:45,28,71928978,"<p>I'm just stupid, as usual. all I need to do was clone <a href=""https://github.com/JdeRobot/ThirdParty"" rel=""nofollow noreferrer"">https://github.com/JdeRobot/ThirdParty</a>, get the qfi module, copy it to
~/catkin_ws/src/drones/rqt_drone_teleop/src/rqt_vel_teleop/ and replace all qfi imports with its relative import version. All common sense</p>
<p>No errors in terminal, gazebo runs, but somehow the rqt widget for drone vision never appears.</p>
<p><a href=""https://i.stack.imgur.com/zZlSf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zZlSf.png"" alt=""why"" /></a></p>
<p>Forget it, I'm giving up on this dumpster fire of a program.</p>
<p>Edit: I did another fresh install, followed the steps, noticed troubleshooting for qfi which required qmake, but same end result</p>
",10618936,0,1,,,Error
188,4367,73277615,"Stereo calibration, do extrinsics change if the lens changes?",|opencv|3d|robotics|stereo-3d|calibration|,"<p>I have a stereo camera setup. Typically I would calibrate the intrinsics of each camera, and then using this result calibrate the extrinsics, so the baseline between the cameras.</p>
<p>What happens now if I change for example the focus or zoom on the lenses? Of course I will have to re-calibrate the intrinsics, but what about the extrinsics?</p>
<p>My first thought would be no, the actual body of the camera didn't move. But on my second thought, doesn't the focal point within the camera change with the changed focus? And isn't the extrinsic calibration actually the calibration between the two focal points of the cameras?</p>
<p>In short: should I re-calibrate the extrinsics of my setup after changing the intrinsics?</p>
<p>Thanks for any help!</p>
",8/8/2022 12:07,,89,1,0,0,,6465752,"Zürich, Schweiz",6/14/2016 17:01,19,73277682,"<p>Yes you should.</p>
<p>It's about the optical center of each camera. Different lenses put that in different places (but hopefully along the optical axis).</p>
",2602877,0,2,,,Incoming
189,4372,73703077,ROS AttributeError: module 'moveit_commander' has no attribute 'MoveGroupCommander',|python|ros|robotics|,"<p><a href=""https://i.stack.imgur.com/Dlgex.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Dlgex.png"" alt=""enter image description here"" /></a>I am using ROS and python to write a robot program. This program can run normally on ROS melodic, but it will raise <em><strong>AttributeError: module 'moveit_commander' has no attribute 'MoveGroupCommander'</strong></em> on noetic, may I ask this is ros Is there a reason for the version? Is there a way to end it?<a href=""https://i.stack.imgur.com/tI2hc.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>this is my code</p>
<pre><code>def pick1(self, forward, side, object, euler, nanko):
        target_x = forward+0.07
        target_y = side
        if object == 1:
            object_placeX = 0.55
            object_placeY = 0.2
        elif object == 2:
            object_placeX = 0.4
            object_placeY = 0.25
        elif object == 3:
            object_placeX = 0.3
            object_placeY = 0.3
        else:
            sys.exit()

        if nanko &gt; 0:
            object_placeX -= 0.05

        # 左手
        left_arm = moveit_commander.MoveGroupCommander(&quot;l_arm_waist_group&quot;)
        left_arm.set_max_velocity_scaling_factor(0.2)
        left_gripper = actionlib.SimpleActionClient(&quot;/sciurus17/controller2/left_hand_controller/gripper_cmd&quot;, GripperCommandAction)
        left_gripper.wait_for_server()

        gripper_goal = GripperCommandGoal()
        gripper_goal.command.max_effort = 2.0

        rospy.sleep(0.3)

        gripper_goal.command.position = -0.9
        left_gripper.send_goal(gripper_goal)
        left_gripper.wait_for_result(rospy.Duration(1.0))

        # SRDFに定義されている&quot;home&quot;の姿勢にする
        left_arm.set_named_target(&quot;l_arm_waist_init_pose&quot;)
        left_arm.go()
        gripper_goal.command.position = 0.0
        left_gripper.send_goal(gripper_goal)
        left_gripper.wait_for_result(rospy.Duration(1.0))

        # 掴む準備をする
        target_pose = geometry_msgs.msg.Pose()
        target_pose.position.x = target_x
        target_pose.position.y = target_y
        target_pose.position.z = 0.3
        q = quaternion_from_euler(-3.14 / 2.0, 0.0, euler)  # 上方から掴みに行く場合
        target_pose.orientation.x = q[0]
        target_pose.orientation.y = q[1]
        target_pose.orientation.z = q[2]
        target_pose.orientation.w = q[3]
        left_arm.set_pose_target(target_pose)  # 目標ポーズ設定
        left_arm.go()  # 実行

        # ハンドを開く
        gripper_goal.command.position = -0.7
        left_gripper.send_goal(gripper_goal)
        left_gripper.wait_for_result(rospy.Duration(1.0))

        # 掴みに行く
        target_pose = geometry_msgs.msg.Pose()
        target_pose.position.x = target_x
        target_pose.position.y = target_y
        target_pose.position.z = 0.11
        q = quaternion_from_euler(-3.14 / 2.0, 0.0, euler)  # 上方から掴みに行く場合
        target_pose.orientation.x = q[0]
        target_pose.orientation.y = q[1]
        target_pose.orientation.z = q[2]
        target_pose.orientation.w = q[3]
        left_arm.set_pose_target(target_pose)  # 目標ポーズ設定
        left_arm.go()  # 実行

        # ハンドを閉じる
        gripper_goal.command.position = -0.3
        left_gripper.send_goal(gripper_goal)
        left_gripper.wait_for_result(rospy.Duration(1.0))

        # 持ち上げる
        target_pose = geometry_msgs.msg.Pose()
        target_pose.position.x = target_x
        target_pose.position.y = target_y
        target_pose.position.z = 0.3
        q = quaternion_from_euler(-3.14 / 2.0, 0.0, euler)  # 上方から掴みに行く場合
        target_pose.orientation.x = q[0]
        target_pose.orientation.y = q[1]
        target_pose.orientation.z = q[2]
        target_pose.orientation.w = q[3]
        left_arm.set_pose_target(target_pose)  # 目標ポーズ設定
        left_arm.go()  # 実行

        # 移動する
        target_pose = geometry_msgs.msg.Pose()
        target_pose.position.x = object_placeX
        target_pose.position.y = object_placeY
        target_pose.position.z = 0.3
        q = quaternion_from_euler(-3.14 / 2.0, 0.0, 0.0)  # 上方から掴みに行く場合
        target_pose.orientation.x = q[0]
        target_pose.orientation.y = q[1]
        target_pose.orientation.z = q[2]
        target_pose.orientation.w = q[3]
        left_arm.set_pose_target(target_pose)  # 目標ポーズ設定
        left_arm.go()  # 実行

        # 下ろす
        target_pose = geometry_msgs.msg.Pose()
        target_pose.position.x = object_placeX
        target_pose.position.y = object_placeY
        target_pose.position.z = 0.14
        q = quaternion_from_euler(-3.14 / 2.0, 0.0, 0.0)  # 上方から掴みに行く場合
        target_pose.orientation.x = q[0]
        target_pose.orientation.y = q[1]
        target_pose.orientation.z = q[2]
        target_pose.orientation.w = q[3]
        left_arm.set_pose_target(target_pose)  # 目標ポーズ設定
        left_arm.go()  # 実行

        # ハンドを開く
        gripper_goal.command.position = -0.7
        left_gripper.send_goal(gripper_goal)
        left_gripper.wait_for_result(rospy.Duration(1.0))

        # 少しだけハンドを持ち上げる
        target_pose = geometry_msgs.msg.Pose()
        target_pose.position.x = object_placeX
        target_pose.position.y = object_placeY
        target_pose.position.z = 0.2
        q = quaternion_from_euler(-3.14 / 2.0, 0.0, 0.0)  # 上方から掴みに行く場合
        target_pose.orientation.x = q[0]
        target_pose.orientation.y = q[1]
        target_pose.orientation.z = q[2]
        target_pose.orientation.w = q[3]
        left_arm.set_pose_target(target_pose)  # 目標ポーズ設定
        left_arm.go()  # 実行

        # SRDFに定義されている&quot;home&quot;の姿勢にする
        left_arm.set_named_target(&quot;l_arm_waist_init_pose&quot;)
        left_arm.go()

        print(&quot;done&quot;)
</code></pre>
",9/13/2022 12:30,,796,1,0,2,,19985720,,9/13/2022 12:20,1,73724246,"<p>The reason it works different between versions is because Noetic uses Python3 and Melodic uses Python2.7. A key difference here is how they handle imports, and thus why you're having a problem. Make sure at the top of your script you have:</p>
<pre><code>import moveit_commander
</code></pre>
<p>If that doesn't work it means you need to install the Noetic version of moveit via: <code>sudo apt install ros-noetic-moveit</code></p>
",11245187,0,2,,,Other
190,4422,73980976,"Python Universal Robots UR-RTDE's API - Getting sporadic ""RTDEReceiveInterface Exception: End of file""",|python|api|exception|robotics|,"<p>Been using happily <a href=""https://pypi.org/project/ur-rtde/"" rel=""nofollow noreferrer"">ur-rtde</a> and lately started getting &quot;RTDEReceiveInterface Exception: End of file&quot; messages in different places of the code.</p>
<p>It seems it happens only when the robot is at rest.</p>
<p>Anyone sees this?</p>
",10/6/2022 23:39,,345,0,0,1,,722453,,4/24/2011 8:34,185,,,,,,,,Error
191,4424,74063852,How to move a robot arm in a circle,|trigonometry|robotics|kinematics|,"<p>I have a 6 joint robot arm, and I want to move it in a circle. I want parameters to choose the radius, degrees, and resolution/quality of the circle.</p>
<p>How do I do this?</p>
<p><a href=""https://i.stack.imgur.com/YqbUA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YqbUA.png"" alt=""arm"" /></a></p>
",10/14/2022 2:58,74063853,403,1,0,0,,15789222,"Canada, Earth",4/29/2021 0:58,27,74063853,"<h2>A quick trig review:</h2>
<p>The hypotenuse is opposite the right angle of the triangle.</p>
<p><a href=""https://i.stack.imgur.com/mczX9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mczX9.png"" alt=""enter image description here"" /></a></p>
<p>The ratio of the height to the hypotenuse is called the sine.</p>
<p>The ratio of the base to the hypotenuse is called the cosine.</p>
<h2>Generating (x,y) coordinates of a circle</h2>
<p>The circle is centered at the point (0,0).
The radius of the circle is 1.
Angles are measured starting from the x-axis.
If we draw a line from the point (0,0) at an angle <em><strong>a</strong></em> from the x-axis, the line will intersect the circle at the point <em><strong>P</strong></em>.</p>
<p><a href=""https://i.stack.imgur.com/vTa1S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vTa1S.png"" alt=""image"" /></a></p>
<p>To generate the coordinates along a circle, let's start with a small example.
We'll use <em><strong>r</strong></em> to refer to the radius of the circle and <em><strong>a</strong></em> to refer to the angles spanned starting from the x-axis.</p>
<p>Let's start with just the five following angles: 0, 90, 180, 270 and 360.</p>
<p>(0 and 360 degrees are the same angle, which is on the positive x-axis).</p>
<pre><code>r = 1

a = 0, 90, 180, 270, 360 (angles in degrees)
</code></pre>
<p>Then, to generate the X and y coordinates along the circle, we use the following equations for each of the angles:</p>
<pre><code>x = r * cos(a)
y = r * sin(a)
</code></pre>
<p>These are the x and y coordinates calculated from the two equations above:</p>
<pre><code>(1, 0)
(0, 1)
(-1, 0)
(0, -1)
(1,0)
</code></pre>
<p>Here's what that looks like on a graph:</p>
<p><a href=""https://i.stack.imgur.com/768cw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/768cw.png"" alt=""image"" /></a></p>
<p>In the above examples, we're only using 4 points, so it doesn't look much like a circle yet.
However, if we use 17 points, we can see the coordinates are approaching a circular shape:</p>
<p><a href=""https://i.stack.imgur.com/kqfNP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kqfNP.png"" alt=""image"" /></a></p>
<h2>Here is a visualization of the math (sin cos wave):</h2>
<p><a href=""https://i.stack.imgur.com/M55sB.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M55sB.gif"" alt=""gif"" /></a></p>
<h2>Here is the Arduino code for a circular movement:</h2>
<pre><code>void moveCircle(float radius, float degrees, float resolution, bool direction)
{
  for (float i = 0; i &lt;= degrees; i += resolution)
  {
    // Get X and Y
    float X = radius * cos(i * DEG_TO_RAD);
    float Y = radius * sin(i * DEG_TO_RAD);

    if (direction)
    {
      // Move circle vertically
      moveTz(X);
      moveTy(Y);
    }
    else
    {
      // Move circle horizontally
      moveTx(X);
      moveZ(Y);
    }
  }
}
</code></pre>
<p>I recommend testing this code by creating a graph in Microsoft Excel to verify that the x y coordinates create a circle.</p>
",15789222,0,0,,,Actuator
192,4429,74116961,Pepper (SoftBank Robotics): Set moveTo speed in Pepper robot via Python (no ROS),|robotics|pepper|nao-robot|slam|,"<p>I'm working on Pepper robot by SoftBank Robotics and I'm trying to write a Python script that dynamically manages the Pepper robot speed .
I'm studying <a href=""http://doc.aldebaran.com/2-5/naoqi/motion/almotion.html"" rel=""nofollow noreferrer"">ALMotion</a> and <a href=""http://doc.aldebaran.com/2-5/naoqi/motion/alnavigation.html"" rel=""nofollow noreferrer"">ALNavigation</a> APIs, but I'm not able to find a solution.</p>
<p>Do you have some ideas?</p>
",10/18/2022 20:14,74135983,188,1,0,0,,11858777,"Bari, BA, Italia",7/30/2019 15:58,36,74135983,"<p><a href=""http://doc.aldebaran.com/2-5/naoqi/motion/control-walk-api.html#almotionproxy-move1"" rel=""nofollow noreferrer"">ALMotion::move</a> and <a href=""http://doc.aldebaran.com/2-5/naoqi/motion/control-walk-api.html#almotionproxy-movetoward1"" rel=""nofollow noreferrer"">ALMotion::moveToward</a> contain velocity/speed as a normalized value from the interval &lt;0; 1&gt; (and &lt;-1; 0&gt; for the opposite direction).</p>
<p>See e.g. the <a href=""https://gitlab.fi.muni.cz/nlp/dialog_coming"" rel=""nofollow noreferrer"">Come with me</a> app to see examples how Pepper's hand joint angles are transformed to its movement speed.</p>
",10822884,2,0,,,Specifications
193,4431,74133720,"How to understand ""pre"" in Preintegration of IMU?",|computer-vision|robotics|slam|,"<p>I see <em>preintegration</em> in IMU-fused SLAM literatures, which mention that <em>preintegration is useful in avoiding to recompute the IMU integration between two consecutive keyframes</em>.</p>
<p>However, when I see some open-sourced SLAM code, e.g. OrbSLAM3, I haven't found anything special in preintegration because it just looks like an ordinary integration (with same time-intervals, same interpolation, same full caculation over multiple intervals), and I don't see an expected pre-calculated value that is reused time and time again.</p>
<p>So my question, is <em>preintegration</em> just an alias of the integration of IMU, or else how to correctly understand &quot;<strong>pre</strong>&quot;?</p>
",10/20/2022 2:12,74188571,335,1,2,1,,14286161,Beijing,9/16/2020 7:53,27,74188571,"<p>Finally I've made clear of <em><strong>Preintegration</strong></em>.</p>
<p>It is a misunderstanding to think of <em>Preintegration</em> as a pre-calculated value that is reusable in each process for calculating any new integration.</p>
<p><strong>In fact</strong>, <em>Preintegration</em> actually means that people <strong>temporarily</strong> calculate integrations <strong>in a rough way</strong> for incoming any time instant (say, instant <em>0, 1, 2, ..., k-1</em>) and, at a key instant <em>k</em>, a bundle adjustment can be performed on the <em>pre-integrations</em> over time instants [<em>0, k</em>] <strong>without recomputing</strong> the integration on time instant <em>0, 1, 2, ..., k-1</em>.</p>
",14286161,1,0,130889103,"@CrisLuengo These two sites are with potential value, thanks!",Moving
194,4452,74301643,About accesing depth images using Python OpenCV,|python|opencv|ros|robotics|realsense|,"<p>I have a array called <code>depth_msg</code> which has a lot of numbers that represents the depth values of a image. When I convert them in <code>cv2</code> message format,</p>
<p>I get this depth image,
<a href=""https://i.stack.imgur.com/YXVU7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YXVU7.png"" alt=""enter image description here"" /></a></p>
<p>And the actual image is ,
<a href=""https://i.stack.imgur.com/WO6lg.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WO6lg.jpg"" alt=""enter image description here"" /></a></p>
<p>How can I get the depth value of the red and yellow bell peppers with the help of the depth image?</p>
<p>My code is,</p>
<pre><code>import cv2
import numpy as np
from cv_bridge import CvBridge

def depth_clbck(depth_msg):

    bridge = CvBridge()
    image = bridge.imgmsg_to_cv2(depth_msg,&quot;32FC1&quot;)
    depth_array = np.array(image, dtype=np.float32)
    depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(image, alpha=0.05), cv2.COLORMAP_HSV)

    cv2.imshow('image', np.array(depth_colormap))
    cv2.waitKey()
</code></pre>
<p>How can I get the depth values of the red and yellow bell-peppers with the help of this depth array and this depth image?</p>
",11/3/2022 10:36,,1966,1,0,0,,13906165,,7/10/2020 13:29,6,74310544,"<p>In order to get the depth value for the red and yellow peeprs in thei mage, you will first have to segment the blobs for the two peppers fro mthe image. Once you have the segmented mask, you can simply obtain through slicing.</p>
<p>For segmentation, you can either manually segment out the peppers or using a image segmentation algorithm - this link shall help you get started:</p>
<p><a href=""https://scikit-image.org/docs/stable/api/skimage.segmentation.html"" rel=""nofollow noreferrer"">https://scikit-image.org/docs/stable/api/skimage.segmentation.html</a></p>
<p>Once a segmentation mask <em>mask</em> is obtained, <em>depth_array[mask&gt;0]</em> will fetch you the depth pixels.</p>
",16744609,0,0,,,Incoming
195,4455,74415334,KheperaIV Test File is more complicated than I expected,|c++|c|robotics|,"<p>I am working on an undergrad project involving the Khepera IV mobile robot, and as I'm reading the files that came with it, I came across this line that confuses me:</p>
<pre><code>for (i=0;i&lt;5;i++) {
    usvalues[i] = (short)(Buffer[i*2] | Buffer[i*2+1]&lt;&lt;8);
...
</code></pre>
<p>From the same file, usvalues[i] is initialized as usvalues[5] for each of the ultrasonic sensors on the robot, Buffer[] is initialized as Buffer[100] i assume for the sample rate of the ultrasonic sensors. But I've never seen a variable set like this. Can someone help me to understand this?</p>
",11/12/2022 18:03,,29,1,2,0,,20028653,,9/18/2022 21:06,4,74415684,"<p>Code reads the <code>Buffer[]</code> array (certainly it has 8-bit elements) 2 successive bytes per iteration in little endian order (lower addressed byte is the least significant byte).  It then forms a 16-bit value to save in <code>usvalues[]</code>.</p>
<pre><code>for (i=0;i&lt;5;i++) {
  usvalues[i] = (short)(Buffer[i*2] | Buffer[i*2+1]&lt;&lt;8);
</code></pre>
<hr />
<p>Code should use <code>uint8_t Buffer[100];</code> to prevent doing a <em>signed</em> left shift.</p>
<p><code>usvalues[]</code> better as some <em>unsigned</em> type like <code>uint16_t</code> or <code>unsigned</code> and use <em>unsigned</em> operations.</p>
<pre><code>uint8_t Buffer[100];
uint16_t /* or unsigned */ usvalues[5 /* or more */];

for (i = 0; i &lt; 5; i++) {
  usvalues[i] = Buffer[i*2] | (unsigned)Buffer[i*2+1] &lt;&lt; 8;
}
</code></pre>
",2410359,1,0,131368173,"Look at this list of operators : https://en.cppreference.com/w/cpp/language/operator_arithmetic. You will find `|` is bitwise or and `<<` is shift left. It is an efficient way to calculate 16 bit values from an 8bit buffer (little endian as Weather Vane said). The cast in C++ should be a static_cast<short> though not the ""C"" style cast (short) though.",Incoming
196,4458,74481393,Arduino mega with L298n and Motors with Encoders not registering encoders,|arduino|robotics|encoder|arduino-c++|motordriver|,"<p>I am trying to follow a tutorial from <a href=""https://www.youtube.com/watch?v=-PCuDnpgiew&amp;list=PLunhqkrRNRhYAffV8JDiFOatQXuU-NnxT&amp;index=9"" rel=""nofollow noreferrer"">youtube</a> on using ROS with Arduino to control motors, and I have connected my <a href=""https://rads.stackoverflow.com/amzn/click/com/B07BK1QL5T"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">L298N</a> with the <a href=""https://rads.stackoverflow.com/amzn/click/com/B00ME3ZH7C"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">battery</a> precisely as the video describes and have uploaded <a href=""https://github.com/joshnewans/ros_arduino_bridge"" rel=""nofollow noreferrer"">sketch</a> 1 with the supporting folder and it loads properly. The Arduino is powered properly via USB, but that connection is not shown in the diagram. When I type the &quot;e&quot; command, I get the proper response of &quot;0 0&quot; and when I do the &quot;o 255 255&quot; it says &quot;OK&quot; and drives properly but upon using &quot;e&quot; to recheck the encoders I am getting the same &quot;0 0&quot;. If anyone can spot something wrong with this, I would really appreciate the help in fixing it. Diagram and Code Below</p>
<p><a href=""https://i.stack.imgur.com/XBk5v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XBk5v.png"" alt=""Diagram"" /></a></p>
<p>Code:</p>
<pre class=""lang-cpp prettyprint-override""><code>#define USE_BASE      // Enable the base controller code
//#undef USE_BASE     // Disable the base controller code

/* Define the motor controller and encoder library you are using */
#ifdef USE_BASE
   /* The Pololu VNH5019 dual motor driver shield */
   //#define POLOLU_VNH5019

   /* The Pololu MC33926 dual motor driver shield */
   //#define POLOLU_MC33926

   /* The RoboGaia encoder shield */
   //#define ROBOGAIA
   
   /* Encoders directly attached to Arduino board */
   #define ARDUINO_ENC_COUNTER

   /* L298 Motor driver*/
   #define L298_MOTOR_DRIVER
#endif

//#define USE_SERVOS  // Enable use of PWM servos as defined in servos.h
#undef USE_SERVOS     // Disable use of PWM servos

/* Serial port baud rate */
#define BAUDRATE      57600

/* Maximum PWM signal */
#define MAX_PWM        255

#if defined(ARDUINO) &amp;&amp; ARDUINO &gt;= 100
#include &quot;Arduino.h&quot;
#else
#include &quot;WProgram.h&quot;
#endif

/* Include definition of serial commands */
#include &quot;commands.h&quot;

/* Sensor functions */
#include &quot;sensors.h&quot;

/* Include servo support if required */
#ifdef USE_SERVOS
   #include &lt;Servo.h&gt;
   #include &quot;servos.h&quot;
#endif

#ifdef USE_BASE
  /* Motor driver function definitions */
  #include &quot;motor_driver.h&quot;

  /* Encoder driver function definitions */
  #include &quot;encoder_driver.h&quot;

  /* PID parameters and functions */
  #include &quot;diff_controller.h&quot;

  /* Run the PID loop at 30 times per second */
  #define PID_RATE           30     // Hz

  /* Convert the rate into an interval */
  const int PID_INTERVAL = 1000 / PID_RATE;
  
  /* Track the next time we make a PID calculation */
  unsigned long nextPID = PID_INTERVAL;

  /* Stop the robot if it hasn't received a movement command
   in this number of milliseconds */
  #define AUTO_STOP_INTERVAL 2000
  long lastMotorCommand = AUTO_STOP_INTERVAL;
#endif

/* Variable initialization */

// A pair of varibles to help parse serial commands (thanks Fergs)
int arg = 0;
int index = 0;

// Variable to hold an input character
char chr;

// Variable to hold the current single-character command
char cmd;

// Character arrays to hold the first and second arguments
char argv1[16];
char argv2[16];

// The arguments converted to integers
long arg1;
long arg2;

/* Clear the current command parameters */
void resetCommand() {
  cmd = NULL;
  memset(argv1, 0, sizeof(argv1));
  memset(argv2, 0, sizeof(argv2));
  arg1 = 0;
  arg2 = 0;
  arg = 0;
  index = 0;
}

/* Run a command.  Commands are defined in commands.h */
int runCommand() {
  int i = 0;
  char *p = argv1;
  char *str;
  int pid_args[4];
  arg1 = atoi(argv1);
  arg2 = atoi(argv2);
  
  switch(cmd) {
  case GET_BAUDRATE:
    Serial.println(BAUDRATE);
    break;
  case ANALOG_READ:
    Serial.println(analogRead(arg1));
    break;
  case DIGITAL_READ:
    Serial.println(digitalRead(arg1));
    break;
  case ANALOG_WRITE:
    analogWrite(arg1, arg2);
    Serial.println(&quot;OK&quot;); 
    break;
  case DIGITAL_WRITE:
    if (arg2 == 0) digitalWrite(arg1, LOW);
    else if (arg2 == 1) digitalWrite(arg1, HIGH);
    Serial.println(&quot;OK&quot;); 
    break;
  case PIN_MODE:
    if (arg2 == 0) pinMode(arg1, INPUT);
    else if (arg2 == 1) pinMode(arg1, OUTPUT);
    Serial.println(&quot;OK&quot;);
    break;
  case PING:
    Serial.println(Ping(arg1));
    break;
#ifdef USE_SERVOS
  case SERVO_WRITE:
    servos[arg1].setTargetPosition(arg2);
    Serial.println(&quot;OK&quot;);
    break;
  case SERVO_READ:
    Serial.println(servos[arg1].getServo().read());
    break;
#endif
    
#ifdef USE_BASE
  case READ_ENCODERS:
    Serial.print(readEncoder(LEFT));
    Serial.print(&quot; &quot;);
    Serial.println(readEncoder(RIGHT));
    break;
   case RESET_ENCODERS:
    resetEncoders();
    resetPID();
    Serial.println(&quot;OK&quot;);
    break;
  case MOTOR_SPEEDS:
    /* Reset the auto stop timer */
    lastMotorCommand = millis();
    if (arg1 == 0 &amp;&amp; arg2 == 0) {
      setMotorSpeeds(0, 0);
      resetPID();
      moving = 0;
    }
    else moving = 1;
    leftPID.TargetTicksPerFrame = arg1;
    rightPID.TargetTicksPerFrame = arg2;
    Serial.println(&quot;OK&quot;); 
    break;
  case MOTOR_RAW_PWM:
    /* Reset the auto stop timer */
    lastMotorCommand = millis();
    resetPID();
    moving = 0; // Sneaky way to temporarily disable the PID
    setMotorSpeeds(arg1, arg2);
    Serial.println(&quot;OK&quot;); 
    break;
  case UPDATE_PID:
    while ((str = strtok_r(p, &quot;:&quot;, &amp;p)) != '\0') {
       pid_args[i] = atoi(str);
       i++;
    }
    Kp = pid_args[0];
    Kd = pid_args[1];
    Ki = pid_args[2];
    Ko = pid_args[3];
    Serial.println(&quot;OK&quot;);
    break;
#endif
  default:
    Serial.println(&quot;Invalid Command&quot;);
    break;
  }
}

/* Setup function--runs once at startup. */
void setup() {
  Serial.begin(BAUDRATE);

// Initialize the motor controller if used */
#ifdef USE_BASE
  #ifdef ARDUINO_ENC_COUNTER
    //set as inputs
    DDRD &amp;= ~(1&lt;&lt;LEFT_ENC_PIN_A);
    DDRD &amp;= ~(1&lt;&lt;LEFT_ENC_PIN_B);
    DDRC &amp;= ~(1&lt;&lt;RIGHT_ENC_PIN_A);
    DDRC &amp;= ~(1&lt;&lt;RIGHT_ENC_PIN_B);
    
    //enable pull up resistors
    PORTD |= (1&lt;&lt;LEFT_ENC_PIN_A);
    PORTD |= (1&lt;&lt;LEFT_ENC_PIN_B);
    PORTC |= (1&lt;&lt;RIGHT_ENC_PIN_A);
    PORTC |= (1&lt;&lt;RIGHT_ENC_PIN_B);
    
    // tell pin change mask to listen to left encoder pins
    PCMSK2 |= (1 &lt;&lt; LEFT_ENC_PIN_A)|(1 &lt;&lt; LEFT_ENC_PIN_B);
    // tell pin change mask to listen to right encoder pins
    PCMSK1 |= (1 &lt;&lt; RIGHT_ENC_PIN_A)|(1 &lt;&lt; RIGHT_ENC_PIN_B);
    
    // enable PCINT1 and PCINT2 interrupt in the general interrupt mask
    PCICR |= (1 &lt;&lt; PCIE1) | (1 &lt;&lt; PCIE2);
  #endif
  initMotorController();
  resetPID();
#endif

/* Attach servos if used */
  #ifdef USE_SERVOS
    int i;
    for (i = 0; i &lt; N_SERVOS; i++) {
      servos[i].initServo(
          servoPins[i],
          stepDelay[i],
          servoInitPosition[i]);
    }
  #endif
}

/* Enter the main loop.  Read and parse input from the serial port
   and run any valid commands. Run a PID calculation at the target
   interval and check for auto-stop conditions.
*/
void loop() {
  while (Serial.available() &gt; 0) {
    
    // Read the next character
    chr = Serial.read();

    // Terminate a command with a CR
    if (chr == 13) {
      if (arg == 1) argv1[index] = NULL;
      else if (arg == 2) argv2[index] = NULL;
      runCommand();
      resetCommand();
    }
    // Use spaces to delimit parts of the command
    else if (chr == ' ') {
      // Step through the arguments
      if (arg == 0) arg = 1;
      else if (arg == 1)  {
        argv1[index] = NULL;
        arg = 2;
        index = 0;
      }
      continue;
    }
    else {
      if (arg == 0) {
        // The first arg is the single-letter command
        cmd = chr;
      }
      else if (arg == 1) {
        // Subsequent arguments can be more than one character
        argv1[index] = chr;
        index++;
      }
      else if (arg == 2) {
        argv2[index] = chr;
        index++;
      }
    }
  }
  
// If we are using base control, run a PID calculation at the appropriate intervals
#ifdef USE_BASE
  if (millis() &gt; nextPID) {
    updatePID();
    nextPID += PID_INTERVAL;
  }
  
  // Check to see if we have exceeded the auto-stop interval
  if ((millis() - lastMotorCommand) &gt; AUTO_STOP_INTERVAL) {;
    setMotorSpeeds(0, 0);
    moving = 0;
  }
#endif

// Sweep servos
#ifdef USE_SERVOS
  int i;
  for (i = 0; i &lt; N_SERVOS; i++) {
    servos[i].doSweep();
  }
#endif
}
</code></pre>
<p>Encoder Pin Designations:</p>
<pre class=""lang-cpp prettyprint-override""><code>/* *************************************************************
   Encoder driver function definitions - by James Nugen
   ************************************************************ */
   
   
#ifdef ARDUINO_ENC_COUNTER
  //below can be changed, but should be PORTD pins; 
  //otherwise additional changes in the code are required
  #define LEFT_ENC_PIN_A PD2  //pin 2
  #define LEFT_ENC_PIN_B PD3  //pin 3
  
  //below can be changed, but should be PORTC pins
  #define RIGHT_ENC_PIN_A PC4  //pin A4
  #define RIGHT_ENC_PIN_B PC5   //pin A5
#endif
   
long readEncoder(int i);
void resetEncoder(int i);
void resetEncoders();

</code></pre>
<p>Encoder Driver:</p>
<pre class=""lang-cpp prettyprint-override""><code>/* *************************************************************
   Encoder definitions
   
   Add an &quot;#ifdef&quot; block to this file to include support for
   a particular encoder board or library. Then add the appropriate
   #define near the top of the main ROSArduinoBridge.ino file.
   
   ************************************************************ */
   
#ifdef USE_BASE

#ifdef ROBOGAIA
  /* The Robogaia Mega Encoder shield */
  #include &quot;MegaEncoderCounter.h&quot;

  /* Create the encoder shield object */
  MegaEncoderCounter encoders = MegaEncoderCounter(4); // Initializes the Mega Encoder Counter in the 4X Count mode
  
  /* Wrap the encoder reading function */
  long readEncoder(int i) {
    if (i == LEFT) return encoders.YAxisGetCount();
    else return encoders.XAxisGetCount();
  }

  /* Wrap the encoder reset function */
  void resetEncoder(int i) {
    if (i == LEFT) return encoders.YAxisReset();
    else return encoders.XAxisReset();
  }
#elif defined(ARDUINO_ENC_COUNTER)
  volatile long left_enc_pos = 0L;
  volatile long right_enc_pos = 0L;
  static const int8_t ENC_STATES [] = {0,1,-1,0,-1,0,0,1,1,0,0,-1,0,-1,1,0};  //encoder lookup table
    
  /* Interrupt routine for LEFT encoder, taking care of actual counting */
  ISR (PCINT2_vect){
    static uint8_t enc_last=0;
        
    enc_last &lt;&lt;=2; //shift previous state two places
    enc_last |= (PIND &amp; (3 &lt;&lt; 2)) &gt;&gt; 2; //read the current state into lowest 2 bits
  
    left_enc_pos += ENC_STATES[(enc_last &amp; 0x0f)];
  }
  
  /* Interrupt routine for RIGHT encoder, taking care of actual counting */
  ISR (PCINT1_vect){
        static uint8_t enc_last=0;
            
    enc_last &lt;&lt;=2; //shift previous state two places
    enc_last |= (PINC &amp; (3 &lt;&lt; 4)) &gt;&gt; 4; //read the current state into lowest 2 bits
  
    right_enc_pos += ENC_STATES[(enc_last &amp; 0x0f)];
  }
  
  /* Wrap the encoder reading function */
  long readEncoder(int i) {
    if (i == LEFT) return left_enc_pos;
    else return right_enc_pos;
  }

  /* Wrap the encoder reset function */
  void resetEncoder(int i) {
    if (i == LEFT){
      left_enc_pos=0L;
      return;
    } else { 
      right_enc_pos=0L;
      return;
    }
  }
#else
  #error A encoder driver must be selected!
#endif

/* Wrap the encoder reset function */
void resetEncoders() {
  resetEncoder(LEFT);
  resetEncoder(RIGHT);
}

#endif
</code></pre>
",11/17/2022 19:56,,2496,1,0,-1,,12527861,"West Orange, NJ, USA",12/12/2019 22:43,42,74674100,"<p>I think if you use a Mega instead of an Uno, the pin ports are different.</p>
<p>So change the port from PD4 to PE4 and PD3 to PE5. Also, change PC4 to PF4 and PC5 to PF5.</p>
<p>In the <code>Encoder.ino</code>, you also have to change the ports accordingly.</p>
<p><code>Encoder.h</code>:</p>
<pre><code>  #define LEFT_ENC_PIN_A PE4  //pin 2
  #define LEFT_ENC_PIN_B PE5  //pin 3
  
  //below can be changed, but should be PORTC pins
  #define RIGHT_ENC_PIN_A PF5  //pin A4
  #define RIGHT_ENC_PIN_B PF5   //pin A5
</code></pre>
<p><code>Encoder.ino</code>:</p>
<pre><code>  /* Interrupt routine for LEFT encoder, taking care of actual counting */
  ISR (PCINT2_vect){
    static uint8_t enc_last=0;
        
    enc_last &lt;&lt;=2; //shift previous state two places
    enc_last |= (PINE &amp; (3 &lt;&lt; 2)) &gt;&gt; 2; //read the current state into lowest 2 bits
  
    left_enc_pos += ENC_STATES[(enc_last &amp; 0x0f)];
  }
  
  /* Interrupt routine for RIGHT encoder, taking care of actual counting */
  ISR (PCINT1_vect){
        static uint8_t enc_last=0;
            
    enc_last &lt;&lt;=2; //shift previous state two places
    enc_last |= (PINF &amp; (3 &lt;&lt; 4)) &gt;&gt; 4; //read the current state into lowest 2 bits
  
    right_enc_pos += ENC_STATES[(enc_last &amp; 0x0f)];
  }
</code></pre>
",20680526,0,0,,,Connections
197,4462,74667949,Robot movement measuring using matlab video processing,|matlab|image-processing|robotics|measurement|,"<p>I'm doing robot project - It need to measure subtle movements in XY direction, while driving in Z direction .
So I was thinking of using a camera with MATLAB and blinking LED attached to a wall - that way using image subtraction I can identify the LED, and with weight matrix locate the center of the light.
Now every period of time I can log the amount of pixels the center moved right-left or up-down directions and check the accuracy of the motion.</p>
<p>But when attempting this sensing solution I had some challenges I couldn't overcome</p>
<ul>
<li>light source like LED/laser has soft edges so the center is not accurate</li>
<li>the camera is not calibrated (and I'm not sure how to calibrate it)</li>
</ul>
<p>Is there other simple solution for this problem?</p>
<p>note: the amount of motion can be proportional.</p>
",12/3/2022 15:11,,45,1,0,0,,10694333,world,11/23/2018 7:19,34,74668342,"<p>You might be able to improve the accuracy of the location of the led by applying some kind of peak interpolation.</p>
<p>For the calibration: Matlab offers an app for camera calibration, maybe that helps you.</p>
",17204618,0,0,,,Coordinates
198,4463,74683769,Microbit doesn't run flashed program when seated in Max:bot,|robotics|bbc-microbit|,"<p>My son bought a <a href=""https://www.pbtech.com/au/product/SBCDFR0005/DFRobot-STEM-Education-Boson-ROB0147-Maxbot-DIY-Pr"" rel=""nofollow noreferrer"">Max:bot DIY Programmable Robot Kit</a>, which uses a BBC Microbit.</p>
<p><a href=""https://i.stack.imgur.com/2q5Jf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2q5Jf.png"" alt=""Max:bot"" /></a></p>
<p>You can see above where the Microbit slots in.</p>
<h3>What works ...</h3>
<p>To set the scene of our problem, we'll set aside the Max:bot for a moment and just consider the Microbit in isolation ...</p>
<p>Using Microsoft MakeCode, we can code a simple program to drive the LEDs on the Microbit.  With the Microbit unseated from the Max:bot, and connected to a Mac over USB, we can successfully flash the Microbit with our program.  As the USB cable provides power for the Microbit, the program begins running, and illuminates the LEDs as expected.  If we press the Microbit reset button, the program runs from the beginning as expected.</p>
<p>Furthermore, if we remove the USB cable (removing the power source), and then re-cable the USB cable (providing power once more), the Microbit immediately runs the program.  This shows to us we have successfully written the program to the Microbit's flash memory as it is persistent across power on/off cycles.</p>
<p>I note that when we power the Microbit via the USB cable, a yellow LED on the rear of the Microbit near the USB port is illuminated.</p>
<h3>What the problem is ...</h3>
<p>Let's now bring the Max:bot back into the fold ...</p>
<p>The Max:bot has a battery pack and (it would seem) provides power to the Microbit independently of the USB cable.</p>
<p>If we do not have the USB cable plugged into the Microbit, and we seat the Microbit in the Max:bot connector, and we turn on the Max:bot, then the Microbit appears to power on but does not run the program stored in its flash.</p>
<p>The Microbit instead initialises with this sequence shown on its LEDs.</p>
<p><a href=""https://i.stack.imgur.com/YlZUS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YlZUS.png"" alt=""boot"" /></a></p>
<p>I note that when powered via the Max:bot, the aforementioned yellow LED on the rear of the Microbit near the USB port does not turn on.</p>
<p>The question is then, why doesn't the Microbit run the flashed program when it is seated in and powered by the Max:bot.</p>
<h3>A final observation ...</h3>
<p>The following sequence does run the program:</p>
<ul>
<li>have the Max:bot powered off</li>
<li>cable the USB to the Mac (Microbit powers on, loads program from flash)</li>
<li>turn on the Max:bot</li>
<li>uncable the USB</li>
</ul>
<p>The question then is why is the USB required to be connected for the Microbit to boot from flash?  (Because it's not practical to do so when you've got a program that actually drives the bot around).</p>
",12/5/2022 5:34,74751567,79,1,3,1,,1212960,,2/16/2012 3:44,1316,74751567,"<p>From the images you have shared of the LEDs on the micro:bit when it is in the Max:bit it appears to be entering &quot;<a href=""https://makecode.microbit.org/v0/reference/bluetooth/bluetooth-pairing"" rel=""nofollow noreferrer"">Bluetooth Pairing Mode</a>&quot;. This mode is entered when holding down buttons <code>A</code> and <code>B</code> on the front of your micro:bit while powering on the device. The signal for those buttons are also available on the <a href=""https://makecode.microbit.org/device/pins"" rel=""nofollow noreferrer"">edge connector</a>.</p>
<p>My assumption here is that Max:bit is using <code>P5</code> and <code>P11</code> and so it appears to the micro:bit that button A and button B are being held down when Max:bit is powered.</p>
<p>Looking at the assembly guide it would appear that there are some LED strips that might be the culprit for this.</p>
<p><a href=""https://i.stack.imgur.com/saMjI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/saMjI.png"" alt=""enter image description here"" /></a></p>
",7721752,1,0,131822110,Thank you @ukBaz I will investigate that angle.,Connections
199,4467,74726722,How to extract points that belong to a vertical plane from a point cloud using PCL?,|c++|point-cloud-library|robotics|lidar|ransac|,"<p>I want to write a program that takes a point cloud data and extract the set of points that belong to a vertical plane. The process mustn’t take more than 100 milliseconds. What is the best way to do this?</p>
<p>I tried using RANSAC filter but, it's slow and also the result is not good.</p>
<pre><code> pcl::SACSegmentation&lt;pcl::PointXYZ&gt; seg;
    seg.setOptimizeCoefficients(true);
    seg.setModelType(pcl::SACMODEL_PLANE);
    seg.setMethodType(pcl::SAC_RANSAC);
    seg.setMaxIterations(1000);
    seg.setDistanceThreshold(0.01);
</code></pre>
",12/8/2022 7:17,74741042,452,1,2,0,,20066403,,9/23/2022 2:21,2,74741042,"<p>First of all, I would recommend to use the latest PCL release (or even the master branch), compiled from source. Compared to PCL 1.9.1, this includes several speed improvements in the sample consensus module. Additionally, by compiling from source, you make sure that you can use everything your computer is capable of (e.g. SIMD instructions).
With the latest PCL release (or master branch) you can also use the parallel RANSAC implementation by calling <code>seg.setNumberOfThreads(0)</code>.
If this is still too slow, you can try to downsample the cloud before passing it to <code>SACSegmentation</code>, e.g. with <a href=""https://pointclouds.org/documentation/classpcl_1_1_random_sample.html"" rel=""nofollow noreferrer"">https://pointclouds.org/documentation/classpcl_1_1_random_sample.html</a></p>
<p>If you only want vertical planes (that is, planes that are parallel to a specified axis), you should use <code>SACMODEL_PARALLEL_PLANE</code> instead of <code>SACMODEL_PLANE</code> and call <a href=""https://pointclouds.org/documentation/classpcl_1_1_s_a_c_segmentation.html#a23abc3e522ccb2b2846a6c9b0cf7b7d3"" rel=""nofollow noreferrer"">https://pointclouds.org/documentation/classpcl_1_1_s_a_c_segmentation.html#a23abc3e522ccb2b2846a6c9b0cf7b7d3</a> and <a href=""https://pointclouds.org/documentation/classpcl_1_1_s_a_c_segmentation.html#a7a2dc31039a1717f83ca281f6970eb18"" rel=""nofollow noreferrer"">https://pointclouds.org/documentation/classpcl_1_1_s_a_c_segmentation.html#a7a2dc31039a1717f83ca281f6970eb18</a></p>
",6540043,0,0,131888874,"What do you mean by ""slow""? Have you measured the time it takes? What do you mean with ""the result is not good""? Please be more precise here so I can make a good suggestion. Which PCL version are you using? There are several speed improvements in the latest PCL versions, and also a parallel RANSAC implementation.",Other
200,4481,75006535,How to calculate the angle between Quaternions,|python|ros|sensors|robotics|imu|,"<p>I have two IMU sensors giving me Euler angles. I convert them to Quaternion by following</p>
<pre><code>quaternion_angle_1=tf.transformations.quaternion_from_euler(roll_1,pitch_1,yaw_1)
quaternion_angle_2=tf.transformations.quaternion_from_euler(roll_2,pitch_2,yaw_2)
</code></pre>
<p>Now I want to caculate the angle between these measurements from the IMU sensors. How can I do that?</p>
",1/4/2023 13:57,,436,2,1,1,,14447735,"Peshawar, Pakistan",10/14/2020 8:46,41,75009966,"<p>Sounds like you want the relative rotation between two quaternions. You could simple calculate the angle between the euler angles, then convert that to a quaternion. But if you need to do it strictly with quaternions <code>tf</code> allows you to do this directly:</p>
<pre><code>relative_quat = quaternion_angle_1 * quaternion_angle_2.inverse()
</code></pre>
",11245187,0,0,132369553,Does this answer your question? [How to obtain the angle between two quaternions?](https://stackoverflow.com/questions/57063595/how-to-obtain-the-angle-between-two-quaternions),Coordinates
201,4487,75081222,PID Control for a DC Motor : Reducing Settling Time,|robotics|pid-controller|,"<p>I want to control the velocity of a DC motor using PID.</p>
<p>In conventional PID Control, we have
u = kp * error + ki * integral_of_error + kd * derivative of error</p>
<p>In order to reduce settling time, I was thinking of writing instead,
u = kp * error + ki * integral_of_error + kd * derivative of error + f(desired_velocity)
, where the function f returns an approximation of the correct signal value.
f(x) = x/(max_velocity) might work.</p>
<p>I'm trying to avoid using a large ki, by reducing the initial error.</p>
<p>Is this a good idea?
Does it achieve anything or would good tuning get the same results?</p>
<p>In my case, the motor's behaviour should be relatively predictable. I don't expect the resistance to motion to vary drastically.</p>
",1/11/2023 10:00,,84,1,0,0,,13056186,,3/13/2020 9:46,5,75777431,"<p>Your additional term f(desired_velocity) is what we call feedforward gain. This is mostly used to achieve better tracking (smaller steady-state error), but it will also make the controller more robust.</p>
<p>In your case, I would try to increase the proportional gain kp as much as possible. Doing so you'll increase the bandwidth of your controller, and so decrease its settling time. Make sure that your system remains controllable (hence stable), and that there is no oscillatory response. The latter happens often when we increase the bandwidth as it also increases noise at the same time.</p>
",20272518,0,0,,,Moving
202,4493,75192356,unable to get the messages from the mqtt topic to ros topic. ||Mqtt to Ros bridge,|python-3.x|mqtt|ros|robotics|ros2|,"<p>Issue: Trying to get mqtt messages from mqtt_topic to ros topic.
I am trying to control my robot which is in ros environment from the outside server using mqtt protocol. I got connected with the outside server. But i didn't get the messages from the mqtt_topic</p>
<p><img src=""https://i.stack.imgur.com/xcXPY.png"" alt=""enter image description here"" /></p>
<p>I am currently using mqtt bridge package https://github.com/groove-x/mqtt_bridge
Please help me to overcome this issue or please recommend some other mqtt ros bridge packages.</p>
",1/21/2023 9:32,,182,0,4,0,,21054569,,1/21/2023 9:16,4,,,,,,132690590,Please show `demo.launch` and the code you use to verify that nothing is received or the `rostopic echo` call you use instead. Your screenshot is not overly useful (and should be text as hardillb says).,Other
203,4502,75367988,"Java error ""OpenCV Assertion failed: (-215:Assertion failed) npoints >= 0 && (depth == CV_32F || depth == CV_32S)""",|java|opencv|image-processing|contour|robotics|,"<p>When I try to run this openCV code on an FTC robot:</p>
<pre><code>        List&lt;MatOfPoint&gt; contours = new ArrayList&lt;&gt;();
        Imgproc.findContours(thresholded, contours, new Mat(), Imgproc.RETR_EXTERNAL, Imgproc.CHAIN_APPROX_SIMPLE);
        contoursAttr = contours;

        MatOfPoint biggestContour = new MatOfPoint();

        for (MatOfPoint curContour : contours) {
            if (Imgproc.contourArea(curContour) &gt; Imgproc.contourArea(biggestContour)) {
                biggestContour = curContour;
            }
        }
</code></pre>
<p>I get the error in the title, namely &quot;OpenCV Assertion failed: (-215:Assertion failed) npoints &gt;= 0 &amp;&amp; (depth == CV_32F || depth == CV_32S)&quot;. I've seen other answers for this question, but they almost all talk about signatures changing across versions, and are somewhat python-specific. However, given that <code>contours</code> is a <code>List&lt;MatOfPoint&gt;</code>, I feel like this should work??</p>
<p>Thanks.</p>
",2/7/2023 0:54,75371933,61,1,0,1,,12240158,"Los Angeles, CA, USA",10/18/2019 18:37,9,75371933,"<p>In the first iteration of the loop, <code>biggestContour</code> is created using default constructor, and has the wrong OpenCV depth.</p>
<p>The following code creates <code>MatOfPoint</code> object with default constructor:</p>
<pre><code>MatOfPoint biggestContour = new MatOfPoint();
</code></pre>
<p>In the first iteration, the following code computes <code>contourArea</code> over the &quot;new&quot; <code>MatOfPoint</code>:</p>
<pre><code>if (Imgproc.contourArea(curContour) &gt; Imgproc.contourArea(biggestContour)
</code></pre>
<p>The result is the error &quot;OpenCV Assertion failed... &amp;&amp; (depth == CV_32F || depth == CV_32S)&quot;<br />
The reason for getting the specific assertion is because the depth of the &quot;new&quot; <code>MatOfPoint</code> is <code>CV_8UC1</code> (not <code>CV_32F</code> and not <code>CV_32S</code>).</p>
<hr />
<p>Suggested solution:<br />
Initialize <code>biggestContour</code> to the first contour in the list:</p>
<pre><code>MatOfPoint biggestContour = contours.get(0); //new MatOfPoint();

for (MatOfPoint curContour : contours) {
    if (Imgproc.contourArea(curContour) &gt; Imgproc.contourArea(biggestContour)) {
        biggestContour = curContour;
    }
}        
</code></pre>
<hr />
<p>It is also a good practice to verify that <code>contours</code> is not empty:</p>
<pre><code>if (!contours.isEmpty())
{
    MatOfPoint biggestContour = contours.get(0);//new MatOfPoint();
    
    for (MatOfPoint curContour : contours) {
        if (Imgproc.contourArea(curContour) &gt; Imgproc.contourArea(biggestContour)) {
            biggestContour = curContour;
        }
    }
}
</code></pre>
",4926757,1,0,,,Incoming
204,4522,75532778,Controller manager not available in ROS2,|ros|robotics|ros2|gazebo-simu|urdf|,"<p>Hi I have added control plugin in robot arm urdf then created launch file but when I run launch file gazebo and robot arm model open but I get an error like below. How can I solve this problem?</p>
<pre><code>[robot_state_publisher-3] Parsing robot urdf xml string.
[robot_state_publisher-3] Error:   Error document empty.
[robot_state_publisher-3]          at line 71 in /tmp/binarydeb/ros-foxy-urdfdom-2.3.3/urdf_parser/src/model.cpp
[robot_state_publisher-3] terminate called after throwing an instance of 'std::runtime_error'
[robot_state_publisher-3]   what():  Unable to initialize urdf::model from robot description
[ros2_control_node-4] terminate called after throwing an instance of 'std::runtime_error'
[ros2_control_node-4]   what():  invalid URDF passed in to robot parser
[spawner.py-5] [INFO] [1677069069.554681723] [spawner_joint_state_broadcaster]: Waiting for /controller_manager services
[spawner.py-6] [INFO] [1677069069.580805205] [spawner_joint_trajectory_controller]: Waiting for /controller_manager services
[spawn_entity.py-2] [INFO] [1677069069.734785422] [spawn_entity]: Spawn Entity started
[spawn_entity.py-2] [INFO] [1677069069.735058596] [spawn_entity]: Loading entity XML from file /home/gursel/robotic_arm_ws/install/robotic_arm/share/robotic_arm/urdf/robotic_arm.urdf
[spawn_entity.py-2] [INFO] [1677069069.736418915] [spawn_entity]: Waiting for service /spawn_entity, timeout = 30
[spawn_entity.py-2] [INFO] [1677069069.736653475] [spawn_entity]: Waiting for service /spawn_entity
[ERROR] [gazebo-1]: process has died [pid 4545, exit code 255, cmd 'gazebo -s libgazebo_ros_factory.so'].
[ERROR] [robot_state_publisher-3]: process has died [pid 4549, exit code -6, cmd '/opt/ros/foxy/lib/robot_state_publisher/robot_state_publisher /home/gursel/robotic_arm_ws/install/robotic_arm/share/robotic_arm/urdf/robotic_arm.urdf --ros-args --params-file /tmp/launch_params_302rtmoc'].
[ERROR] [ros2_control_node-4]: process has died [pid 4551, exit code -6, cmd '/opt/ros/foxy/lib/controller_manager/ros2_control_node --ros-args --params-file /tmp/launch_params_wb0x7xnl --params-file /home/gursel/robotic_arm_ws/install/robotic_arm/share/robotic_arm/config/jtc.yaml'].
[spawner.py-5] [INFO] [1677069071.570302266] [spawner_joint_state_broadcaster]: Waiting for /controller_manager services
[spawner.py-6] [INFO] [1677069071.595951515] [spawner_joint_trajectory_controller]: Waiting for /controller_manager services
[spawner.py-5] [INFO] [1677069073.586642702] [spawner_joint_state_broadcaster]: Waiting for /controller_manager services
[spawner.py-6] [INFO] [1677069073.612373454] [spawner_joint_trajectory_controller]: Waiting for /controller_manager services
[spawner.py-5] [INFO] [1677069075.603461546] [spawner_joint_state_broadcaster]: Waiting for /controller_manager services
[spawner.py-6] [INFO] [1677069075.629372887] [spawner_joint_trajectory_controller]: Waiting for /controller_manager services
[spawner.py-5] [INFO] [1677069077.620421456] [spawner_joint_state_broadcaster]: Waiting for /controller_manager services
[spawner.py-6] [INFO] [1677069077.646128433] [spawner_joint_trajectory_controller]: Waiting for /controller_manager services
[spawner.py-5] [ERROR] [1677069079.637362419] [spawner_joint_state_broadcaster]: Controller manager not available
[spawner.py-6] [ERROR] [1677069079.662836259] [spawner_joint_trajectory_controller]: Controller manager not available
[ERROR] [spawner.py-5]: process has died [pid 4553, exit code 1, cmd '/opt/ros/foxy/lib/controller_manager/spawner.py joint_state_broadcaster --controller-manager /controller_manager --ros-args'].
[ERROR] [spawner.py-6]: process has died [pid 4555, exit code 1, cmd '/opt/ros/foxy/lib/controller_manager/spawner.py joint_trajectory_controller -c /controller_manager --ros-args'].
</code></pre>
<p><em><strong>SOLVED:</strong></em>
The problem was ROS2 version and changing in launch file. It works with <strong>Ubuntu 22.04 Humble</strong> and you should install all control package such as joint_trajectory_controller, joint_state_broadcaster, controller_manager, joint-state-publisher-gui then edit your launch file.</p>
",2/22/2023 12:37,,2393,0,1,1,,18021550,,1/24/2022 20:42,25,,,,,,134548261,"Had the same issue. sudo apt-get install ros-humble-gazebo-ros2-control
solved it for me.",Error
205,4541,75700380,Welding IIWA arm to a Planar Joint,|python|robotics|drake|,"<p>Me and my friends are trying to obtain a mobile IIWA arm using PyDrake library. We are able to manually define a planar joint using two Prismatic joints and one Revolute joint. First, we were playing with a basic box to move around. However, when adding the IIWA arm, our program started to give the following error message:</p>
<pre><code>Traceback (most recent call last):
  File &quot;planar_joint.py&quot;, line 107, in &lt;module&gt;
    simulator.AdvanceTo(12.0)
RuntimeError: Error control wants to select step smaller than minimum allowed (1e-14)
</code></pre>
<p>I read that might be caused by singularity of IIWA but I believe we did not force IIWA to obtain a such position. Another thing I have read was the cause of this problem might be getting an infinite value in the physics engine. It is more likely but I do not know how to fix the problem. You can find the code below.</p>
<p>Main Code:</p>
<pre><code>meshcat = StartMeshcat()

meshcat.Flush()
builder = DiagramBuilder()
plant, scene_graph = AddMultibodyPlantSceneGraph(builder, time_step=0.0005)

parser = Parser(plant)

scene_models = parser.AddModels(&quot;cargobot-models/scene_without_robot.dmd.yaml&quot;)

iiwa = plant.GetModelInstanceByName(&quot;iiwa&quot;)

false_body = plant.AddRigidBody(
        &quot;false_body&quot;, iiwa,
        SpatialInertia(0, [0, 0, 0], UnitInertia(0, 0, 0)))
false_body2 = plant.AddRigidBody(
        &quot;false_body2&quot;, iiwa,
        SpatialInertia(0, [0, 0, 0], UnitInertia(0, 0, 0)))

mobile_base_y = plant.AddJoint(PrismaticJoint(
        &quot;mobile_base_y&quot;, plant.GetFrameByName(&quot;world&quot;), plant.GetFrameByName(&quot;false_body&quot;), 
        [0, 1, 0], -3, 3))
plant.AddJointActuator(&quot;mobile_base_y_actuator&quot;, mobile_base_y)

mobile_base_x = plant.AddJoint(PrismaticJoint(
        &quot;mobile_base_x&quot;, plant.GetFrameByName(&quot;false_body&quot;), plant.GetFrameByName(&quot;false_body2&quot;), 
        [1, 0, 0], -3, 3))
plant.AddJointActuator(&quot;mobile_base_x_actuator&quot;, mobile_base_x)

mobile_base_theta = plant.AddJoint(RevoluteJoint(
        &quot;mobile_base_theta&quot;, plant.GetFrameByName(&quot;false_body2&quot;), plant.GetFrameByName(&quot;iiwa_link_0&quot;), 
        [0, 0, 1],  -np.pi, np.pi))
plant.AddJointActuator(&quot;mobile_base_theta_actuator&quot;, mobile_base_theta)

plant.Finalize()

visualizer = MeshcatVisualizer.AddToBuilder(
    builder, scene_graph, meshcat)

plant_context = plant.CreateDefaultContext()

kp=[100] * plant.num_positions()
ki=[1] * plant.num_positions()
kd=[20] * plant.num_positions()

planar_controller = builder.AddSystem(
    InverseDynamicsController(plant, kp, ki, kd, False))
planar_controller.set_name(&quot;planar_controller&quot;)
builder.Connect(plant.get_state_output_port(),
                planar_controller.get_input_port_estimated_state())
builder.Connect(planar_controller.get_output_port_control(),
                plant.get_actuation_input_port())

diagram = builder.Build()
context = diagram.CreateDefaultContext()

states = [1, 0, 0, -1.57, 0.1, 0, -1.2, 0, 1.6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
planar_controller.GetInputPort('desired_state').FixValue(
    planar_controller.GetMyMutableContextFromRoot(context), states)

simulator = Simulator(diagram, context)
visualizer.StartRecording()
simulator.AdvanceTo(12.0)
visualizer.StopRecording()
visualizer.PublishRecording()
</code></pre>
<p>Yaml file for the scene:</p>
<pre><code>directives:          
    - add_model:
        name: cargo-space
        file: file:///usr/cargobot/cargobot-project/trajopt/cargobot-models/cargo-space.sdf

    - add_weld:
        parent: world
        child: cargo-space::cargo-space
        X_PC:
            rotation: !Rpy { deg: [0, 90, 0 ]}
            translation: [-1.5, 0, 0.4]

    - add_frame: 
        name: table_top_center
        X_PF:
            base_frame: world
            rotation: !Rpy { deg: [0, 0, 0]}
            translation: [0, 0, -0.5]

    - add_model:
        name: table_top
        file: file:///usr/cargobot/cargobot-project/trajopt/cargobot-models/table_top.sdf

    - add_weld: 
        parent: table_top_center
        child: table_top_link

    - add_model:
        name: iiwa
        file: package://drake/manipulation/models/iiwa_description/iiwa7/iiwa7_no_collision.sdf
        default_joint_positions:
            iiwa_joint_1: [-1.57]
            iiwa_joint_2: [0.1]
            iiwa_joint_3: [0]
            iiwa_joint_4: [-1.2]
            iiwa_joint_5: [0]
            iiwa_joint_6: [ 1.6]
            iiwa_joint_7: [0]

    - add_model:
        name: wsg
        file: package://drake/manipulation/models/wsg_50_description/sdf/schunk_wsg_50_with_tip.sdf
    - add_weld:
        parent: iiwa::iiwa_link_7
        child: wsg::body
        X_PC:
            translation: [0, 0, 0.09]
            rotation: !Rpy { deg: [90, 0, 90]}
</code></pre>
<p>We tried to set the robot 1 unit to the left.</p>
",3/10/2023 19:41,,100,0,3,1,,20705465,,12/6/2022 17:42,4,,,,,,133602815,"Adding 2 prismatic joints and 1 revolute joint before the iiwa_link_1 to the given iiwa7 sdf worked! I still do not understand the upper solution did not work, though.",Actuator
206,4563,75940959,Bad Latency with v4l2_camera_node in ROS2,|raspberry-pi|ros|robotics|v4l2|,"<p>I am working on a project that provides a camera feed from a microcontroller running ROS2 to a Unity Scene. Currently I am using the v4l2 package and running the v4ls_camera_node to send the data to the Unity scene, however, I am getting terrible latency of about 12 sec. I am not great at working with image processing and was curious if anyone else has ran into this problem and knew of any solutions to try.</p>
<p>I tried changing some of the parameters with no success.</p>
",4/5/2023 14:55,,104,0,1,0,,21177078,,2/9/2023 3:28,4,,,,,,133958082,Please provide enough code so others can better understand or reproduce the problem.,Incoming
207,4565,75962469,ORB_SLAM3 Camera Calibration file,|opencv|computer-vision|robotics|slam|,"<p>I'm trying to run a a fisheye camera video feed in Monocular ORB_SLAM3 to get a trajectory, but I'm having trouble configuring the proper calibration file for it.</p>
<p><a href=""https://rpg.ifi.uzh.ch/fov.html"" rel=""nofollow noreferrer"">Fisheye Camera Intrinsicts and Image feed</a></p>
<p>Camera Intrinsicts file</p>
<pre><code>640 480 -179.471829787234 0 0.002316743975 -3.635968439375e-06 2.0546506810625e-08 320.0 240.0 1.0 0.0 0.0 256.2124 138.2261 -3.8287 23.8296 8.0091 -0.5033 6.7625 4.3653 -1.2425 -1.2663 -0.1870 0.0
</code></pre>
<p>These are examples of two formats of calibration files for ORB_SLAM3.</p>
<p><a href=""https://github.com/UZ-SLAMLab/ORB_SLAM3/blob/master/Examples_old/Monocular/TUM-VI.yaml"" rel=""nofollow noreferrer"">https://github.com/UZ-SLAMLab/ORB_SLAM3/blob/master/Examples_old/Monocular/TUM-VI.yaml</a></p>
<p><a href=""https://github.com/UZ-SLAMLab/ORB_SLAM3/blob/master/Examples_old/Monocular/TUM1.yaml"" rel=""nofollow noreferrer"">https://github.com/UZ-SLAMLab/ORB_SLAM3/blob/master/Examples_old/Monocular/TUM1.yaml</a></p>
<p>Any help or guidance would be really appreciated.</p>
<p>I tried usig the distortion parameters and both the camera types but there is something thats wrong, the negative focal length also gives an inverted trajectory but still completely inaccurate.</p>
<p><a href=""https://i.stack.imgur.com/i29Nu.png"" rel=""nofollow noreferrer"">Inverted Trajectory</a></p>
<p><a href=""https://i.stack.imgur.com/PKLC9.png"" rel=""nofollow noreferrer"">Inaccurate Trajectory</a></p>
",4/7/2023 22:59,,519,0,0,0,,21592392,,4/7/2023 22:37,2,,,,,,,,Incoming
208,4566,75963064,How to calculate the most likely mobile robot position based on the distance to nearby objects (walls) which have a known position?,|orientation|robotics|least-squares|,"<p>I got the distance and angle to the robot from the  detected obstacles (walls) from 2D LiDAR while robot is moving. So now I would like to calculate the most likely robot  pose based on the distance and angle to nearby objects (walls) which have a known position. So if I use the least squares estimator I can get the position of the robot but then how to get the orientation?</p>
<p>For example this gives me the position <a href=""https://www.th-luebeck.de/fileadmin/media_cosa/Dateien/Veroeffentlichungen/Sammlung/TR-2-2015-least-sqaures-with-ToA.pdf"" rel=""nofollow noreferrer"">robot position estimator using least square method</a> but then how to get the orientation? Any help?</p>
",4/8/2023 2:40,,47,0,0,0,,10319366,Singapore,9/5/2018 9:27,109,,,,,,,,Moving
209,4568,76017501,Roboclaw : [ERROR] : cmd: 59 failed crc_check,|raspberry-pi|ros|robotics|,"<p>I have a problem with my robot. Whenever I try to launch it, I keep getting an error message: [ERROR] cmd:59 failed crc_check (and sometimes [ERROR] cmd:49 failed crc_check).</p>
<p>Although I can see the roboclaw_node in the rqt_graph, but I am unable to send any commands to the motors.
I have a Roboclaw ST 2x45A with the correct configuration.
Can someone please help me identify the problem and suggest a solution?</p>
<p>For the execution and the error I have this:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/ubuntu/catkin_ws/src/drivers/ros_bno055_driver/nodes/bno055_node.py&quot;, line 165, in &lt;module&gt;
    main()
  File &quot;/home/ubuntu/catkin_ws/src/drivers/ros_bno055_driver/nodes/bno055_node.py&quot;, line 136, in main
    node = BNO055Driver()
  File &quot;/home/ubuntu/catkin_ws/src/drivers/ros_bno055_driver/nodes/bno055_node.py&quot;, line 28, in __init__
    if not self.device.begin():
  File &quot;/home/ubuntu/catkin_ws/src/drivers/ros_bno055_driver/nodes/Adafruit_BNO055/BNO055.py&quot;, line 382, in begin
    self._config_mode()
  File &quot;/home/ubuntu/catkin_ws/src/drivers/ros_bno055_driver/nodes/Adafruit_BNO055/BNO055.py&quot;, line 358, in _config_mode
    self.set_mode(OPERATION_MODE_CONFIG)
  File &quot;/home/ubuntu/catkin_ws/src/drivers/ros_bno055_driver/nodes/Adafruit_BNO055/BNO055.py&quot;, line 416, in set_mode
    self._write_byte(BNO055_OPR_MODE_ADDR, mode &amp; 0xFF)
  File &quot;/home/ubuntu/catkin_ws/src/drivers/ros_bno055_driver/nodes/Adafruit_BNO055/BNO055.py&quot;, line 302, in _write_byte
    self._i2c_device.write8(address, value)
  File &quot;/home/ubuntu/catkin_ws/src/drivers/ros_bno055_driver/nodes/Adafruit_GPIO/I2C.py&quot;, line 116, in write8
    self._bus.write_byte_data(self._address, register, value)
  File &quot;/home/ubuntu/catkin_ws/src/drivers/ros_bno055_driver/nodes/Adafruit_PureIO/smbus.py&quot;, line 256, in write_byte_data
    self._device.write(data)
OSError: [Errno 121] Remote I/O error
[ INFO] [1658396096.440849967]: Done initializing likelihood field model.
[bno055_driver-2] process has died [pid 1770, exit code 1, cmd /home/ubuntu/catkin_ws/src/drivers/ros_bno055_driver/nodes/bno055_node.py __name:=bno055_driver __log:=/home/ubuntu/.ros/log/5d988550-08d8-11ed-b5aa-e3ef9a600f4e/bno055_driver-2.log].
log file: /home/ubuntu/.ros/log/5d988550-08d8-11ed-b5aa-e3ef9a600f4e/bno055_driver-2*.log
[WARN] [1658396097.779578]: EXTERNAL_PID expected
[WARN] [1658396097.822070]: Roboclaw Firmware &quot;'USB Roboclaw 2x45a v4.2.8\\n'&quot;
[WARN] [1658396097.833447]: M1 PID 0.000000,0.000000,0.000000,0.000000
[WARN] [1658396097.837850]: M2 PID 0.000000,0.000000,0.000000,0.000000
[WARN] [1658396097.848505]: M1MaxCurrent 10.00 A M2MaxCurrent 10.00 A triggered after 0.200000 s
[ERROR] [1658396098.954525]: cmd: 59 failed crc_check
[ERROR] [1658396098.962611]: Main batt voltage high hysteresis
[WARN] [1658396098.967473]: M1 over current
[WARN] [1658396098.982044]: M2 over current
[WARN] [1658396098.991308]: Main batt voltage high
[ERROR] [1658396099.001628]: cmd: 59 failed crc_check
[ERROR] [1658396099.006135]: End of : Main batt voltage high hysteresis
[WARN] [1658396099.011149]: End of : M1 over current
[WARN] [1658396099.017205]: End of : M2 over current
[WARN] [1658396099.024713]: End of : Main batt voltage high
[ERROR] [1658396099.032435]: cmd: 59 failed crc_check
[ERROR] [1658396099.039953]: cmd: 59 failed crc_check
[ WARN] [1658396099.046330150]: Timed out waiting for transform from base_link to map to become available before running costmap, tf error: canTransform: target_frame map does not exist.. canTransform returned after 0.100352 timeout was 0.1.
[ERROR] [1658396099.054249]: cmd: 59 failed crc_check
[ERROR] [1658396099.068091]: cmd: 59 failed crc_check
[ERROR] [1658396099.093577]: cmd: 59 failed crc_check
[ERROR] [1658396099.119525]: cmd: 59 failed crc_check
[ERROR] [1658396099.147087]: cmd: 59 failed crc_check
[ERROR] [1658396099.167921]: cmd: 59 failed crc_check
[ERROR] [1658396099.193518]: cmd: 59 failed crc_check
[ERROR] [1658396099.222741]: cmd: 59 failed crc_check
Warning: TF_REPEATED_DATA ignoring data with redundant timestamp for frame base_footprint at time 1658396099.246406 according to authority unknown_publisher
         at line 280 in /home/ubuntu/catkin_ws/src/geometry2/tf2/src/buffer_core.cpp
Warning: TF_REPEATED_DATA ignoring data with redundant timestamp for frame base_footprint at time 1658396099.246406 according to authority unknown_publisher
         at line 280 in /home/ubuntu/catkin_ws/src/geometry2/tf2/src/buffer_core.cpp
Warning: TF_REPEATED_DATA ignoring data with redundant timestamp for frame base_footprint at time 1658396099.246406 according to authority unknown_publisher
         at line 280 in /home/ubuntu/catkin_ws/src/geometry2/tf2/src/buffer_core.cpp
[ WARN] [1658396099.256928502]: global_costmap: Parameter &quot;plugins&quot; not provided, loading pre-Hydro parameters
[ERROR] [1658396099.258143]: cmd: 59 failed crc_check
[ERROR] [1658396099.268142]: cmd: 59 failed crc_check
[ERROR] [1658396099.293706]: cmd: 59 failed crc_check
[ INFO] [1658396099.319486335]: global_costmap: Using plugin &quot;static_layer&quot;
[ERROR] [1658396099.320640]: cmd: 59 failed crc_check
[ERROR] [1658396099.352751]: cmd: 59 failed crc_check
[ INFO] [1658396099.357522298]: Requesting the map...
[ERROR] [1658396099.367956]: cmd: 59 failed crc_check
[ERROR] [1658396099.393685]: cmd: 59 failed crc_check
[ERROR] [1658396099.418762]: cmd: 59 failed crc_check
[ERROR] [1658396099.455218]: cmd: 59 failed crc_check
[ERROR] [1658396099.467973]: cmd: 59 failed crc_check
[ERROR] [1658396099.497122]: cmd: 59 failed crc_check
[ERROR] [1658396099.518128]: cmd: 59 failed crc_check
[ERROR] [1658396099.553966]: cmd: 59 failed crc_check
[ERROR] [1658396099.568980]: cmd: 59 failed crc_check
[ INFO] [1658396099.575256002]: Resizing costmap to 2048 X 2048 at 0.050000 m/pix
[ERROR] [1658396099.592809]: cmd: 59 failed crc_check
[ERROR] [1658396099.617774]: cmd: 59 failed crc_check
[ERROR] [1658396099.647493]: cmd: 59 failed crc_check
[ INFO] [1658396099.666048372]: Received a 2048 X 2048 map at 0.050000 m/pix
[ERROR] [1658396099.668508]: cmd: 59 failed crc_check
[ INFO] [1658396099.682784391]: global_costmap: Using plugin &quot;obstacle_layer&quot;
[ERROR] [1658396099.694193]: cmd: 59 failed crc_check
[ INFO] [1658396099.701562409]:     Subscribed to Topics: laser_scan_sensor
[ERROR] [1658396099.719585]: cmd: 59 failed crc_check
[ERROR] [1658396099.744177]: cmd: 59 failed crc_check
[ INFO] [1658396099.767514632]: global_costmap: Using plugin &quot;inflation_layer&quot;
[ERROR] [1658396099.768384]: cmd: 59 failed crc_check
[ERROR] [1658396099.793876]: cmd: 59 failed crc_check
[ERROR] [1658396099.819689]: cmd: 59 failed crc_check
[ERROR] [1658396099.848546]: cmd: 59 failed crc_check
[ERROR] [1658396099.868187]: cmd: 59 failed crc_check
[ERROR] [1658396099.894135]: cmd: 59 failed crc_check
[ERROR] [1658396099.930328]: cmd: 59 failed crc_check
[ERROR] [1658396099.945882]: cmd: 59 failed crc_check```


</code></pre>
",4/14/2023 17:10,,89,0,1,0,,21644642,,4/14/2023 16:54,2,,,,,,134096975,Please provide enough code so others can better understand or reproduce the problem.,Error
210,4574,76058435,Trouble describing a robot's joint origin/axis using URDF and Pybullet,|python|robot|kinematics|pybullet|urdf|,"<p>I have created a complete URDF for my robot in PyBullet, but I'm having trouble getting the joints to rotate correctly. Specifically, when I try to move a joint, it doesn't rotate about the correct coordinate, and the link separates from the body of the robot.</p>
<p>Initially, I kept the joint origin and axis at their default values in the URDF and did not move any joints, which resulted in the robot being in a correct and complete static orientation. However, when I try to make a joint move in pybullet, the joint does not rotate about the correct point, and the link separates from the parent body of the robot.</p>
<p>I have already created reference axes for the servos in my SolidWorks files and attempted to enter this information into the URDF as the joint origin parameter. However, this approach has not resolved the issue.</p>
<p>I suspect that the problem lies in my translation between the parent origin coordinate, joint origin coordinate, and child origin coordinate. I need help in determining the correct joint origin/axis parameter that will allow my joints to rotate properly around the correct vector.</p>
<p>Can anyone offer any advice or suggestions on how to properly determine the joint origin parameter for the joints in my URDF and resolve this issue?</p>
<p>Python code:</p>
<pre><code>import pybullet as p
import time
import pybullet_data
import numpy as np

physicsClient = p.connect(p.GUI)#or p.DIRECT for non-graphical version
p.setAdditionalSearchPath(pybullet_data.getDataPath()) #optionally
p.setGravity(0,0,0)
groundId = p.loadURDF(&quot;plane.urdf&quot;)
robotStartPos = [0,0,.25]
robotStartOrientation = p.getQuaternionFromEuler([1.57,0,0])

urdf = &quot;C:\\Users\\danie\\OneDrive\\Desktop\\Robot_Simulation\\myrobot.urdf&quot; #change to urdf file path
robotId = p.loadURDF(urdf,robotStartPos, robotStartOrientation)

jointIndex = 2 # first joint is number 0
for i in range (10000):
    p.stepSimulation()
    p.setJointMotorControl2(robotId, jointIndex, controlMode = p.POSITION_CONTROL,  targetPosition=0.8+0.6*np.sin(i*0.01))
    time.sleep(1./240.)
    
robotPos, robotOrn = p.getBasePositionAndOrientation(robotId)
print(robotPos, robotOrn)
p.disconnect()
</code></pre>
<p>URDF File:</p>
<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;robot name=&quot;myrobot&quot;&gt;
    &lt;link name=&quot;base&quot;&gt;
        &lt;visual&gt;
            &lt;geometry&gt;
                &lt;mesh filename=&quot;base.stl&quot; scale=&quot;0.001 0.001 0.001&quot;/&gt;
            &lt;/geometry&gt;
            &lt;material name = &quot;Cyan&quot;&gt;
                &lt;color rgba=&quot;0 1.0 1.0 1.0&quot;/&gt;
            &lt;/material&gt;
        &lt;/visual&gt;
        &lt;inertial&gt;
            &lt;mass value=&quot;0.845&quot;/&gt;
            &lt;origin xyz=&quot;-.036 .058 -.123&quot;/&gt;
            &lt;inertia ixx=&quot;.0018&quot;  ixy=&quot;-.00177&quot;  ixz=&quot;0.00374&quot; iyy=&quot;.0156&quot; iyz=&quot;-.006&quot; izz=&quot;.00526&quot; /&gt; 
        &lt;/inertial&gt;
    &lt;/link&gt;

    &lt;link name=&quot;hip_left&quot;&gt;
        &lt;visual&gt;
            &lt;geometry&gt;
                &lt;mesh filename=&quot;hip_left.stl&quot; scale=&quot;0.001 0.001 0.001&quot;/&gt;
            &lt;/geometry&gt;
            &lt;material name = &quot;Orange&quot;&gt;
                &lt;color rgba=&quot;1.0 0.41 0.0 1.0&quot;/&gt;
            &lt;/material&gt;
        &lt;/visual&gt;
        &lt;inertial&gt;
            &lt;mass value=&quot;0.0588&quot;/&gt;
            &lt;origin xyz=&quot;-.036 .028 -.222&quot;/&gt;
            &lt;inertia ixx=&quot;.03&quot;  ixy=&quot;-0.00006&quot;  ixz=&quot;0.0004&quot; iyy=&quot;.002&quot; iyz=&quot;-0.000371&quot; izz=&quot;.000175&quot; /&gt; 
        &lt;/inertial&gt;
    &lt;/link&gt;
    
    &lt;link name=&quot;hip_right&quot;&gt;
        &lt;visual&gt;
            &lt;geometry&gt;
                &lt;mesh filename=&quot;hip_right.stl&quot; scale=&quot;0.001 0.001 0.001&quot;/&gt;
            &lt;/geometry&gt;
            &lt;material name = &quot;Orange&quot;&gt;
                &lt;color rgba=&quot;1.0 0.41 0.0 1.0&quot;/&gt;
            &lt;/material&gt;
        &lt;/visual&gt;
        &lt;inertial&gt;
            &lt;mass value=&quot;0.0588&quot;/&gt;
            &lt;origin xyz=&quot;-.036 .029 -.254&quot;/&gt;
            &lt;inertia ixx=&quot;.03&quot;  ixy=&quot;-0.00006&quot;  ixz=&quot;0.0004&quot; iyy=&quot;.002&quot; iyz=&quot;-0.000371&quot; izz=&quot;.000175&quot; /&gt; 
        &lt;/inertial&gt;
    &lt;/link&gt;

    &lt;link name=&quot;upper_leg_left&quot;&gt;
        &lt;visual&gt;
            &lt;geometry&gt;
                &lt;mesh filename=&quot;upper_leg_left.stl&quot; scale=&quot;0.001 0.001 0.001&quot;/&gt;
            &lt;/geometry&gt;
            &lt;material name = &quot;Green&quot;&gt;
                &lt;color rgba=&quot;0.0 1.0 0.0 1.0&quot;/&gt;
            &lt;/material&gt;
        &lt;/visual&gt;
        &lt;inertial&gt;
            &lt;mass value=&quot;0.05846&quot;/&gt;
            &lt;origin xyz=&quot;-.035 -.092 -.223&quot;/&gt;
            &lt;inertia ixx=&quot;.0035&quot;  ixy=&quot;0.0018&quot;  ixz=&quot;0.000456&quot; iyy=&quot;.0029&quot; iyz=&quot;0.00119&quot; izz=&quot;.00065&quot; /&gt; 
        &lt;/inertial&gt;
    &lt;/link&gt;

    &lt;link name=&quot;upper_leg_right&quot;&gt;
        &lt;visual&gt;
            &lt;geometry&gt;
                &lt;mesh filename=&quot;upper_leg_right.stl&quot; scale=&quot;0.001 0.001 0.001&quot;/&gt;
            &lt;/geometry&gt;
            &lt;material name = &quot;Green&quot;&gt;
                &lt;color rgba=&quot;0.0 1.0 0.0 1.0&quot;/&gt;
            &lt;/material&gt;
        &lt;/visual&gt;
        &lt;inertial&gt;
            &lt;mass value=&quot;0.05846&quot;/&gt;
            &lt;origin xyz=&quot;-.0378 -.091 -.2444&quot;/&gt;
            &lt;inertia ixx=&quot;.0035&quot;  ixy=&quot;0.0018&quot;  ixz=&quot;0.000456&quot; iyy=&quot;.0029&quot; iyz=&quot;0.00119&quot; izz=&quot;.00065&quot; /&gt;
        &lt;/inertial&gt;
    &lt;/link&gt;

    &lt;link name=&quot;lower_leg_left&quot;&gt;
        &lt;visual&gt;
            &lt;geometry&gt;
                &lt;mesh filename=&quot;lower_leg_left.stl&quot; scale=&quot;0.001 0.001 0.001&quot;/&gt;
            &lt;/geometry&gt;
            &lt;material name = &quot;Pink&quot;&gt;
                &lt;color rgba=&quot;1.0 0.0 1.0 1.0&quot;/&gt;
            &lt;/material&gt;
        &lt;/visual&gt;
        &lt;inertial&gt;
            &lt;mass value=&quot;0.05846&quot;/&gt;
            &lt;origin xyz=&quot;-.0427 -.2019 -.2223&quot;/&gt;
            &lt;inertia ixx=&quot;.00369&quot;  ixy=&quot;0.00036&quot;  ixz=&quot;0.00179&quot; iyy=&quot;.00209&quot; iyz=&quot;0.00179&quot; izz=&quot;.001823&quot; /&gt; 
        &lt;/inertial&gt;
    &lt;/link&gt;

    &lt;link name=&quot;lower_leg_right&quot;&gt;
        &lt;visual&gt;
            &lt;geometry&gt;
                &lt;mesh filename=&quot;lower_leg_right.stl&quot; scale=&quot;0.001 0.001 0.001&quot;/&gt;
            &lt;/geometry&gt;
            &lt;material name = &quot;Pink&quot;&gt;
                &lt;color rgba=&quot;1.0 0.0 1.0 1.0&quot;/&gt;
            &lt;/material&gt;
        &lt;/visual&gt;
        &lt;inertial&gt;
            &lt;mass value=&quot;0.05846&quot;/&gt;
            &lt;origin xyz=&quot;-.05398 -.202 -.0253&quot;/&gt;
            &lt;inertia ixx=&quot;.00369&quot;  ixy=&quot;0.00036&quot;  ixz=&quot;0.00179&quot; iyy=&quot;.00209&quot; iyz=&quot;0.00179&quot; izz=&quot;.001823&quot; /&gt;
        &lt;/inertial&gt;
    &lt;/link&gt;

    &lt;joint name=&quot;joint1&quot; type=&quot;revolute&quot;&gt;
        &lt;parent link=&quot;base&quot;/&gt;
        &lt;child link=&quot;hip_left&quot;/&gt;
        &lt;origin xyz=&quot;0 0 0&quot;/&gt;
        &lt;axis xyz=&quot;0 -1 0&quot;/&gt;
        &lt;limit lower=&quot;-3.14159&quot; upper=&quot;3.14159&quot; effort=&quot;10&quot; velocity=&quot;10&quot;/&gt;
    &lt;/joint&gt;
    
    &lt;joint name=&quot;joint2&quot; type=&quot;revolute&quot;&gt;
        &lt;parent link=&quot;base&quot;/&gt;
        &lt;child link=&quot;hip_right&quot;/&gt;
        &lt;origin xyz=&quot;0 0 0&quot;/&gt;
        &lt;axis xyz=&quot;0 -1 0&quot;/&gt;
        &lt;limit lower=&quot;-3.14159&quot; upper=&quot;3.14159&quot; effort=&quot;10&quot; velocity=&quot;10&quot;/&gt;
    &lt;/joint&gt;
    
    &lt;joint name=&quot;joint3&quot; type=&quot;revolute&quot;&gt;
        &lt;parent link=&quot;hip_left&quot;/&gt;
        &lt;child link=&quot;upper_leg_left&quot;/&gt;
        &lt;origin xyz=&quot;0 0 0&quot;/&gt;
        &lt;axis xyz=&quot;0 0 1&quot;/&gt;
        &lt;limit lower=&quot;-3.14159&quot; upper=&quot;3.14159&quot; effort=&quot;10&quot; velocity=&quot;10&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;joint4&quot; type=&quot;revolute&quot;&gt;
        &lt;parent link=&quot;hip_right&quot;/&gt;
        &lt;child link=&quot;upper_leg_right&quot;/&gt;
        &lt;origin xyz=&quot;0 0 0&quot;/&gt;
        &lt;axis xyz=&quot;0 0 -1&quot;/&gt;
        &lt;limit lower=&quot;-3.14159&quot; upper=&quot;3.14159&quot; effort=&quot;10&quot; velocity=&quot;10&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;joint5&quot; type=&quot;revolute&quot;&gt;
        &lt;parent link=&quot;upper_leg_left&quot;/&gt;
        &lt;child link=&quot;lower_leg_left&quot;/&gt;
        &lt;origin xyz=&quot;0 0 0&quot;/&gt;
        &lt;axis xyz=&quot;0 0 1&quot;/&gt;
        &lt;limit lower=&quot;-3.14159&quot; upper=&quot;3.14159&quot; effort=&quot;10&quot; velocity=&quot;10&quot;/&gt;
    &lt;/joint&gt;

    &lt;joint name=&quot;joint6&quot; type=&quot;revolute&quot;&gt;
        &lt;parent link=&quot;upper_leg_right&quot;/&gt;
        &lt;child link=&quot;lower_leg_right&quot;/&gt;
        &lt;origin xyz=&quot;0 0 0&quot;/&gt;
        &lt;axis xyz=&quot;0 0 -1&quot;/&gt;
        &lt;limit lower=&quot;-3.14159&quot; upper=&quot;3.14159&quot; effort=&quot;10&quot; velocity=&quot;10&quot;/&gt;
    &lt;/joint&gt;
&lt;/robot&gt;
</code></pre>
<p>I tried putting in various values for the joint origins in the URDF using various translations but none helped fix the issue. I have yet to attempt to change the joint axis parameter.</p>
",4/19/2023 20:08,,220,0,0,0,,12873598,,2/10/2020 16:20,1,,,,,,,,Actuator
211,4578,76087052,Segmentation fault (core dumped) error when running Python script with pykinect_azure and MoveIt,|python|segmentation-fault|ros|kinect|robotics|,"<p>I'm running a Python script that uses <code>pykinect_azure</code> to track the position of the user's left wrist, and then tries to move a robot arm using the MoveIt library. However, when I run the script, I run into the following error:</p>
<blockquote>
<p>[ INFO] [1682280101.642647859]: Loading robot model 'panda'...
Segmentation fault (core dumped)</p>
</blockquote>
<p>Here is the code I'm using:</p>
<pre class=""lang-py prettyprint-override""><code>import sys
import cv2
import math
import rospy
import pykinect_azure as pykinect
import moveit_commander
from moveit_commander import MoveGroupCommander

pykinect.module_path = '/usr/lib/x86_64-linux-gnu/libk4a.so'

if __name__ == &quot;__main__&quot;:
    # Initialize Kinect and MoveIt libraries
    pykinect.initialize_libraries(track_body=True)
    moveit_commander.roscpp_initialize(sys.argv)
    rospy.init_node('move_robot')

    device_config = pykinect.default_configuration
    device_config.color_resolution = pykinect.K4A_COLOR_RESOLUTION_OFF
    device_config.depth_mode = pykinect.K4A_DEPTH_MODE_WFOV_2X2BINNED
    device = pykinect.start_device(config=device_config)
    bodyTracker = pykinect.start_body_tracker()
    arm = MoveGroupCommander('panda_arm')

    # Initialize variables
    cv2.namedWindow('Depth image with skeleton', cv2.WINDOW_NORMAL)
    left_wrist_init = None
    left_wrist_prev = None

    while True:
        # Update Kinect
        capture = device.update()
        body_frame = bodyTracker.update()
        ret_depth, depth_color_image = capture.get_colored_depth_image()
        ret_color, body_image_color = body_frame.get_segmentation_image()

        if not ret_depth or not ret_color:
            continue

        # Get left wrist position for each detected body
        num_bodies = body_frame.get_num_bodies()
        for body_idx in range(num_bodies):
            body = body_frame.get_body(body_idx)
            if body.is_valid:
                left_wrist_joint = body.joints[pykinect.K4ABT_JOINT_WRIST_LEFT]

                if left_wrist_init is None:
                    left_wrist_init = left_wrist_joint.position
                    left_wrist_prev = left_wrist_joint.position
                else:
                    #  Calculate the difference in wrist position
                    if left_wrist_joint.position is None:
                        continue
                    wrist_diff = [left_wrist_joint.position[i] - left_wrist_prev[i] for i in range(3)]

                    # Update the target pose of the robot arm
                    current_pose = arm.get_current_pose().pose
                    current_pose.position.x += wrist_diff[0]
                    current_pose.position.y += wrist_diff[1]
                    current_pose.position.z += wrist_diff[2]

                    # Move the robot arm to the new position
                    arm.set_pose_target(current_pose)
                    arm.go()

                    # Update the previous wrist position
                    left_wrist_prev = left_wrist_joint.position
    device.stop()
    moveit_commander.roscpp_shutdown()
</code></pre>
<p>Any ideas on what could cause this? When I use the libraries individually the code runs just fine, but now when combining the two the error occurs. I've tries to use the debugger to narrow it down, but it never catches the error but instead just shuts down shortly after it started. Thanks for any answer!</p>
",4/23/2023 20:12,,183,0,0,1,,16431723,,7/12/2021 12:48,23,,,,,,,,Error
212,4582,76194249,How to display a grasps on a pointcloud using open3d (python),|python|point-clouds|robotics|open3d|grasp|,"<p>The main goal is to display the grasp received from a ur10 robot in a pointcoud using open3d. The open3d is used to depict an object as pointclouds and we want to show a grasp skeleton (the idea of grasp skeleton was taken from the s4g repository) around it.
Currently, open3d shows us the pointcloud of the object but the grasp shown is at the wrong orientation.</p>
<p>The ur10 gives us the following output regarding its pose:</p>
<p>position= [0.015238827715672282, 0.026318999049784485, 0.231598307617698]<br />
quats =  [0.7125063395828215, -0.678258641135711, -0.17649390437631773, 0.03390919487441209]</p>
<p>As of now, we are tried to convert 'quats' into oritentation by doing the following: orientation = quat2mat(quats)&gt; We then load the pointcloud and the gripper skeleton, but the gripper always seems to be in a wrong place. there seems to be something wrong in the rotation of the frames.&gt; The rest of the code is as follows&gt;</p>
<pre><code>type here
pc = o3d.io.read_point_cloud(&quot;/home/max_j/Pictures/full_pcd/obj1.pcd&quot;)
vis = o3d.visualization.VisualizerWithEditing()
vis.create_window()
vis.add_geometry(pc)
vis.run()
vis.destroy_window()
vis_list = [pc]

# Gripper Configuration
BACK_COLLISION_MARGIN = 0.0  # points that collide with back hand within this range will not be detected
HALF_BOTTOM_WIDTH = 0.10
BOTTOM_LENGTH = 0.0025
FINGER_WIDTH = 0.0003
HALF_HAND_THICKNESS = 0.0003
FINGER_LENGTH = 0.20
HAND_LENGTH = BOTTOM_LENGTH + FINGER_LENGTH
HALF_BOTTOM_SPACE = HALF_BOTTOM_WIDTH - FINGER_WIDTH
create_box = o3d.geometry.TriangleMesh.create_box
back_hand = create_box(height=2 * HALF_BOTTOM_WIDTH,
                        depth=HALF_HAND_THICKNESS * 2,
                        width=BOTTOM_LENGTH - BACK_COLLISION_MARGIN)

temp_trans = np.eye(4)
temp_trans[0, 3] = -BOTTOM_LENGTH
temp_trans[1, 3] = -HALF_BOTTOM_WIDTH
temp_trans[2, 3] = -HALF_HAND_THICKNESS
back_hand.transform(temp_trans)

finger = create_box((FINGER_LENGTH + BACK_COLLISION_MARGIN),
                    FINGER_WIDTH,
                    HALF_HAND_THICKNESS * 2)
color=(0.1, 0.6, 0.3)
finger.paint_uniform_color(color)
back_hand.paint_uniform_color(color)
left_finger = copy.deepcopy(finger)

temp_trans = np.eye(4)
temp_trans[1, 3] = HALF_BOTTOM_SPACE
temp_trans[2, 3] = -HALF_HAND_THICKNESS
temp_trans[0, 3] = -BACK_COLLISION_MARGIN
left_finger.transform(temp_trans)
temp_trans[1, 3] = -HALF_BOTTOM_WIDTH
finger.transform(temp_trans)

hmt = np.eye(4)  #if we use R to make hmt
hmt[:3,:3] = orientation
hmt[:3,3] = position
T_global_to_local = hmt
print(&quot;the  global to local before correction is: \n&quot;, T_global_to_local)

#T_local_to_global = np.linalg.inv(T_global_to_local)
coord_frame_hand = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1)
coord_frame_hand = coord_frame_hand.transform(T_global_to_local)

# o3d.visualization.draw_geometries([coord_frame_hand_, coord_frame_hand_])

back_hand.transform(T_global_to_local)
finger.transform(T_global_to_local)
left_finger.transform(T_global_to_local)

vis_list_ = [back_hand, left_finger, finger,coord_frame_hand]


hand = vis_list_
o3d.visualization.draw_geometries(hand)



vis_list = vis_list + hand 
o3d.visualization.draw_geometries(vis_list)

</code></pre>
",5/7/2023 13:34,,74,0,0,0,,13295077,,4/12/2020 16:16,29,,,,,,,,Coordinates
213,4585,76226178,Self balancing robot - how to obtain the elapsed angle from gyroscopic data?,|python|raspberry-pi|robotics|mpu6050|,"<p>I am trying to complete my University project building and programming a self balancing robot in python (using RPi) but have hit a road block in my knowledge.</p>
<p>From the MPU6050 I am able to obtain both acceleration and gyroscopic data, furthermore the angles from them (using atan2 function for accel angles and integrating dt for gyro angles). However I cannot think of a way to code this data so that in a loop of given time (dt) it will turn the change in angle (from gyro data) into the total elapsed angle.</p>
<p>I need to be able to loop the program so that it will run for time (dt), store this data, run the program again but expect + or - the previous stored data.
I will then be able to use both angles from the gyro and accel data and run it through a complimentary filter.</p>
<p>I have uploaded the relevant code below.</p>
<pre><code>import smbus                    #import SMBus module of I2C
import time
from time import sleep
import math

#some MPU6050 Registers and their Address
PWR_MGMT_1   = 0x6B
SMPLRT_DIV   = 0x19
CONFIG       = 0x1A 
GYRO_CONFIG  = 0x1B
INT_ENABLE   = 0x38
ACCEL_XOUT_H = 0x3B
ACCEL_YOUT_H = 0x3D
ACCEL_ZOUT_H = 0x3F
GYRO_XOUT_H  = 0x43
GYRO_YOUT_H  = 0x45
GYRO_ZOUT_H  = 0x47

DT = 0.02
rad_to_deg = 180/(math.pi)



def MPU_Init():
    #write to sample rate register
    bus.write_byte_data(Device_Address, SMPLRT_DIV, 7)
    
    #Write to power management register
    bus.write_byte_data(Device_Address, PWR_MGMT_1, 1)
    
    #Write to Configuration register
    bus.write_byte_data(Device_Address, CONFIG, 0)
    
    #Write to Gyro configuration register
    bus.write_byte_data(Device_Address, GYRO_CONFIG, 24)
    
    #Write to interrupt enable register
    bus.write_byte_data(Device_Address, INT_ENABLE, 1)

def read_raw_data(addr):
    #Accelero and Gyro value are 16-bit
        high = bus.read_byte_data(Device_Address, addr)
        low = bus.read_byte_data(Device_Address, addr+1)
    
        #concatenate higher and lower value
        value = ((high &lt;&lt; 8) | low)
        
        #to get signed value from mpu6050
        if(value &gt; 32768):
                value = value - 65536
        return value


bus = smbus.SMBus(1)    # or bus = smbus.SMBus(0) for older version boards
Device_Address = 0x68   # MPU6050 device address
MPU_Init()

print (&quot; Reading Data of Gyroscope and Accelerometer&quot;)


    
while True:
        #Read Accelerometer raw value
        acc_x = read_raw_data(ACCEL_XOUT_H)
        acc_y = read_raw_data(ACCEL_YOUT_H)
        acc_z = read_raw_data(ACCEL_ZOUT_H)
        
        #Read Gyroscope raw value
        gyro_x = read_raw_data(GYRO_XOUT_H)
        gyro_y = read_raw_data(GYRO_YOUT_H)
        gyro_z = read_raw_data(GYRO_ZOUT_H)
        
        #Full scale range +/- 250 degree/C as per sensitivity scale factor
        Ax = acc_x/16384
        Ay = acc_y/16384
        Az = acc_z/16384
        
        Gx = gyro_x/131.0
        Gy = gyro_y/131.0
        Gz = gyro_z/131.0
        
        gyro_x_angle= Gy * DT 

        accel_x_angle = (math.atan2(Az,Ax)-(math.pi/2))*rad_to_deg

        
time.sleep(DT)
</code></pre>
<p>there seems to be many similar self balancing robot projects but very few programmed in python that i can find
Any help would be much appreciated.</p>
<p>I have experimented with lists, for loops, while loops but cannot seem to obtain the required results.</p>
",5/11/2023 9:35,,101,0,0,0,,21879238,,5/11/2023 9:08,2,,,,,,,,Actuator
214,4604,76354503,How to rotate servo when there is object detected?,|python|raspberry-pi|raspberry-pi4|robotics|,"<p>currently I'm working with object detection yolov8 on raspberry pi. My project is to detect these three objects (aluminum can, plastic bottle, glass bottle). I already trained the dataset and export as best.pt file.</p>
<p>Now I got stuck on <strong>how to rotate servo when there is object detected</strong>. Here is my code.</p>
<pre><code>from ultralytics import YOLO
import cv2
from ultralytics.yolo.utils.plotting import Annotator
import torch
from gpiozero import Servo
from time import sleep
from gpiozero.pins.pigpio import PiGPIOFactory

factory = PiGPIOFactory()

servo = Servo(12, min_pulse_width=0.5/1000, max_pulse_width=2.5/1000, pin_factory=factory)

model = YOLO('best.pt')
cap = cv2.VideoCapture(0)
cap.set(3, 640)
cap.set(4, 480)

while True:
_, frame = cap.read()

img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

results = model.predict(img)

for r in results:
    
    annotator = Annotator(frame)
    
    boxes = r.boxes
    for box in boxes:
        
        b = box.xyxy[0]  # get box coordinates in (top, left, bottom, right) format
        c = box.cls
        annotator.box_label(b, model.names[int(c)])
       
      
frame = annotator.result()
  
cv2.imshow('YOLO V8 Obj Detection', frame)

#servo part

if cv2.waitKey(1) &amp; 0xFF == ord(' '):
    break

cap.release()
cv2.destroyAllWindows()
</code></pre>
",5/29/2023 3:23,,157,0,1,0,,21977022,,5/29/2023 3:15,2,,,,,,135665650,"It looks like you've got most things ready to go (including setting up the servo on pin 12).  It's unclear how the detected class(es) map to servo motion: e.g. what does the servo do when there are no detections ? what about multiple detections ? and for a single detection, which servo value should be selected for which class (e.g. -1, 0, 1 for alluminium, plastic, glass) ? (e.g. `servo.value = box.cls - 1.0 # rempap 0,1,2 classes to -1, 0, 1 servo values`)",Moving
215,4619,76481567,How could I expose rqt_graph to tcp port?,|python|c++|ros|robotics|ros2|,"<p>I wrote a project to control a vehicle with ros2. I access this vehicle through ssh and write code directly from there.</p>
<p>Clearly, there is no GUI and I would like to expose the result of rqt_graph on a tcp port so I can see it. Is there any way?</p>
<p>I've tried to use <a href=""https://github.com/dheera/rosboard/"" rel=""nofollow noreferrer"">Rosboard</a> but I didn't find anything to view graph like rqt_graph.</p>
",6/15/2023 10:26,,37,0,0,0,,15496345,,3/27/2021 20:23,4,,,,,,,,Remote
216,4620,76482257,What algorithm should I use to find the quickest path between two points while avoiding obstacles?,|dynamic|path|shortest-path|robotics|motion-planning|,"<p>I have a start and end point and several obstacles (more or less depending on the situation). I also have a robot-like system that moves in a dynamic environnment. My job is to implement an algorithm that will allow my system to arrive to the end-point avoiding the obstacles. Has anyone worked on path/motion planning before and could give me some advice ?
So far I've done a little bit of 'research' and I found that basically there are two groups of algorithms: serach-based and sampling-based. Apprently the sampling-based ones are more suitable for environments having multiple obstacles and they are also mostly used in robotics. Now testing they give a very strange trajectory, having many turns which is not at all what I want. On the other hand, search-based algorithms since they work with a grid, when we have more obstacles take a lot of memory. So, here is where I'm stuck. I was thinking of implementing RRT*, but I really don't like the path found even when I have many iterations.</p>
<p>I looked up algorithms like A*,D*,D<em>Lite,RRT,RRT</em> and PRM.</p>
",6/15/2023 12:01,,69,1,0,0,,15090954,,1/27/2021 11:01,8,76870635,"<p>As you have mentioned, sampling-based methods are a lot more efficient in problems with very high-dimensional state space and you don't have to worry about grid size. The observation of strange and unsmooth paths returned by a sampling-based planner is common. Usually, people perform post-processing to fix this. You can perform shortcutting (see paper: <a href=""https://ieeexplore.ieee.org/document/5509683"" rel=""nofollow noreferrer"">https://ieeexplore.ieee.org/document/5509683</a>), or you can throw the found path into an optimization-based solver (e.g. trajectory optimization) as a good initial guess that takes into account path smoothness as well.</p>
",12004417,1,0,,,Moving
217,4622,76531265,"How to plan a trajectory having path in the form (x_i,y_i,theta_i), robot FK and Invers Kinematic and constraints?",|matlab|robotics|,"<p>I have the following information:
1.Inital path for the robot from start state to goal state in the form (x_i,y_i,theta_i),
2. Robot forward and inverse kinematics,
3. Some constraints to be posed on the robot related to velocity and acceleration.</p>
<p>I want to know how to utilize those known inputs to plan a trajectory then execute it by commanding robot to follow the produced trajectory.</p>
<p>I have done many searches but I still find it confusing, some utilized smoothing algorithms, other used optimization method so on so forth.</p>
",6/22/2023 11:07,,17,0,0,0,,18055837,,1/28/2022 8:13,8,,,,,,,,Moving
218,4623,76626687,Formula derivation in the CHOMP: Gradient Optimization Techniques for Efficient Motion Planning article,|robotics|convex-optimization|motion-planning|,"<p>Has anyone read this article, i.e., <em>CHOMP: Gradient Optimization Techniques for Efficient Motion Planning</em> article?</p>
<p>I'm confused about the derivation of equations 2-3, which is shown in the figure below.</p>
<p>A detailed procedure is expected. Besides, which chapter of Convex Optimization (S. Boyd and L. Vandenberghe) does this derivation belong to？</p>
<p>From which chapter of the book can I learn the derivation process?</p>
<p>Pic:</p>
<p><a href=""https://i.stack.imgur.com/rfKfT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rfKfT.png"" alt=""enter image description here"" /></a></p>
",7/6/2023 7:58,,31,0,0,0,,19644538,,7/29/2022 1:56,26,,,,,,,,Moving
219,4625,76698876,Issue while trying to controlling LEGO Spike Prime motors using Visual Studio Code,|python|error-handling|command-line|integration|robotics|,"<p>I'm using the LEGO SPIKE Prime extension for Visual Studio Code from the GitHub blog (<a href=""https://github.com/sanjayseshan/spikeprime-vscode"" rel=""nofollow noreferrer"">https://github.com/sanjayseshan/spikeprime-vscode</a>) to develop a project. However, I'm encountering a &quot;TypeError: argument of type 'NoneType' is not iterable&quot; error. I'm seeking assistance in resolving this issue.</p>
<p>Project Description:
I'm working on a project that involves controlling the LEGO SPIKE Prime robot using the extension for Visual Studio Code. The extension provides various features and functionalities for interacting with the LEGO SPIKE Prime hardware.</p>
<p>Error Message:
When running the following command in the command line, I encounter the TypeError:</p>
<pre><code>C:\Users\***\Desktop\lego\levov2&gt;python C:\spiketools\spikejsonrpc.py -t COM17 upload &quot;c:\Users\***\Desktop\lego\levov2\teste.py&quot; 1
TypeError: argument of type 'NoneType' is not iterable
</code></pre>
<p>(Note: The username has been obscured for privacy reasons)</p>
<p>Issue:
The error occurs when executing the above command using the LEGO SPIKE Prime extension's &quot;spikejsonrpc.py&quot; script. It seems that the 'NoneType' error is related to an iterable object not being available during the execution.</p>
<p>Expected Behavior:
I expected the command to upload the &quot;teste.py&quot; file to the LEGO SPIKE Prime hub connected to the specified COM port without any errors.</p>
<p>Additional Information:
I have followed the installation steps provided in the GitHub blog for setting up the LEGO SPIKE Prime extension.
The LEGO SPIKE Prime hub is properly connected to COM17 and functional.
The &quot;teste.py&quot; file exists in the specified directory.
Request:
I would appreciate any insights or suggestions on how to resolve this TypeError when using the LEGO SPIKE Prime extension for Visual Studio Code. If there are any specific configurations or additional steps that need to be taken to ensure the proper functionality of the extension and the &quot;spikejsonrpc.py&quot; script, please let me know.</p>
<p>Thank you for your assistance!</p>
",7/16/2023 14:51,,427,0,1,0,,22235563,,7/16/2023 12:43,1,,,,,,135223136,Please provide enough code so others can better understand or reproduce the problem.,Actuator
220,4629,76714383,Mir 100 pause and continue via rest api,|python-3.x|rest|robotics|,"<p>I've been working with mir 100 and using Python to make rest API requests and now I'm trying to pause and unpause the robot via rest API, I'm not working in mir fleet, just one robot, I try to use the action post to make the request to pause the robot via Postman to make some tests, the robot status code was 201 (success) but the robot doesn't stops, someone know how I should make the request correctly?</p>
<p>This is the JSON that I am using:</p>
<pre><code>{
&quot;allowed_methods&quot;:null,
  &quot;descriptions&quot;: [
    null
  ],
  &quot;help&quot;: &quot;string&quot;,
  &quot;parameters&quot;: [
    &quot;pause&quot;
  ],
  &quot;mission_group_id&quot;: &quot;2a4c6731-1fd4-11ee-b4c4-94c691a733c6&quot;,
  &quot;name&quot;: &quot;pause&quot;,
  &quot;action_type&quot;: &quot;pause&quot;,
  &quot;description&quot;: &quot;string&quot;
}
</code></pre>
<p>And this is the request url: http://ip/api/v2.0.0/actions/pause</p>
",7/18/2023 15:28,,192,1,0,0,,,,,,77239306,"<p>I've also been trying to play/pause the mission via RestAPI.</p>
<p>I'm unable to pause the robot from /actions/pause API call.</p>
<p>I can do it from the Put call to /status. The API documentation lets you &quot;try it out&quot; and outlines the parameters for the body to send. You can omit whatever you don't need. I determined the values by playing/pausing the robot and execute the get/status call from the robot's API documentation webpage. Get to it via the robot interface at help/API documentation, then log in and launch as the user for the calls.</p>
<p>The body parameters are:</p>
<pre><code>{
 &quot;map_id&quot;: &quot;string&quot;,
  &quot;mode_id&quot;: 0,
  &quot;state_id&quot;: 0,
  &quot;web_session_id&quot;: &quot;string&quot;,
  &quot;position&quot;: {},
  &quot;serial_number&quot;: &quot;string&quot;,
  &quot;name&quot;: &quot;string&quot;,
  &quot;answer&quot;: &quot;string&quot;,
  &quot;guid&quot;: &quot;string&quot;,
  &quot;datetime&quot;: &quot;2023-10-05T17:12:08.182Z&quot;,
  &quot;clear_error&quot;: true
}
</code></pre>
<p>You can just send the state id, no comma at the end:</p>
<pre><code>{
  &quot;state_id&quot;: 0
}
</code></pre>
<p>Play is state 3 and pause is 4.</p>
",22690906,0,0,,,Actuator
221,4630,76714440,An error in building code in stm32 cube ide (make: error),|robotics|stm32f4discovery|stm32f4|stm32cubeide|,"<p>I'm trying to build code in stm32f401ccu6 blackpill but I keep getting this error :</p>
<blockquote>
<p>make: ***[Drivers/STM32F4xx_HAL_Driver/Src/subdir.mk:61: Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_cortex.o] Error 1</p>
</blockquote>
<p>This is the log from console tab.</p>
<blockquote>
<p>15:51:00 **** Incremental Build of configuration Debug for project WHY ****
make -j4 all
arm-none-eabi-gcc &quot;../Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal.c&quot; -mcpu=cortex-m4 -std=gnu11 -g3 -DDEBUG -DUSE_HAL_DRIVER -DSTM32F401xC -c -I../Core/Inc -I../Drivers/STM32F4xx_HAL_Driver/Inc -I../Drivers/STM32F4xx_HAL_Driver/Src -I../Drivers/STM32F4xx_HAL_Driver/Inc/Legacy -I../Drivers/CMSIS/Device/ST/STM32F4xx/Include -I../Drivers/CMSIS/Include -O0 -ffunction-sections -fdata-sections -Wall -fstack-usage -MMD -MP -MF&quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal.d&quot; -MT&quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal.o&quot; --specs=nano.specs -mfpu=fpv4-sp-d16 -mfloat-abi=hard -mthumb -o &quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal.o&quot;
arm-none-eabi-gcc &quot;../Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_cortex.c&quot; -mcpu=cortex-m4 -std=gnu11 -g3 -DDEBUG -DUSE_HAL_DRIVER -DSTM32F401xC -c -I../Core/Inc -I../Drivers/STM32F4xx_HAL_Driver/Inc -I../Drivers/STM32F4xx_HAL_Driver/Src -I../Drivers/STM32F4xx_HAL_Driver/Inc/Legacy -I../Drivers/CMSIS/Device/ST/STM32F4xx/Include -I../Drivers/CMSIS/Include -O0 -ffunction-sections -fdata-sections -Wall -fstack-usage -MMD -MP -MF&quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_cortex.d&quot; -MT&quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_cortex.o&quot; --specs=nano.specs -mfpu=fpv4-sp-d16 -mfloat-abi=hard -mthumb -o &quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_cortex.o&quot;
arm-none-eabi-gcc &quot;../Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_dma.c&quot; -mcpu=cortex-m4 -std=gnu11 -g3 -DDEBUG -DUSE_HAL_DRIVER -DSTM32F401xC -c -I../Core/Inc -I../Drivers/STM32F4xx_HAL_Driver/Inc -I../Drivers/STM32F4xx_HAL_Driver/Src -I../Drivers/STM32F4xx_HAL_Driver/Inc/Legacy -I../Drivers/CMSIS/Device/ST/STM32F4xx/Include -I../Drivers/CMSIS/Include -O0 -ffunction-sections -fdata-sections -Wall -fstack-usage -MMD -MP -MF&quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_dma.d&quot; -MT&quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_dma.o&quot; --specs=nano.specs -mfpu=fpv4-sp-d16 -mfloat-abi=hard -mthumb -o &quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_dma.o&quot;
arm-none-eabi-gcc &quot;../Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_dma_ex.c&quot; -mcpu=cortex-m4 -std=gnu11 -g3 -DDEBUG -DUSE_HAL_DRIVER -DSTM32F401xC -c -I../Core/Inc -I../Drivers/STM32F4xx_HAL_Driver/Inc -I../Drivers/STM32F4xx_HAL_Driver/Src -I../Drivers/STM32F4xx_HAL_Driver/Inc/Legacy -I../Drivers/CMSIS/Device/ST/STM32F4xx/Include -I../Drivers/CMSIS/Include -O0 -ffunction-sections -fdata-sections -Wall -fstack-usage -MMD -MP -MF&quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_dma_ex.d&quot; -MT&quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_dma_ex.o&quot; --specs=nano.specs -mfpu=fpv4-sp-d16 -mfloat-abi=hard -mthumb -o &quot;Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_dma_ex.o&quot;
arm-none-eabi-gcc: fatal error: cannot execute 'd:/robocon/stm32cubeide_1.10.1/stm32cubeide/plugins/com.st.stm32cube.ide.mcu.externaltools.gnu-tools-for-stm32.10.3-2021.10.win32_1.0.0.202111181127/tools/bin/../lib/gcc/arm-none-eabi/10.3.1/../../../../arm-none-eabi/bin/as.exe': CreateProcess: No such file or directory
compilation terminated.
arm-none-eabi-gcc: fatal error: cannot execute 'd:/robocon/stm32cubeide_1.10.1/stm32cubeide/plugins/com.st.stm32cube.ide.mcu.externaltools.gnu-tools-for-stm32.10.3-2021.10.win32_1.0.0.202111181127/tools/bin/../lib/gcc/arm-none-eabi/10.3.1/../../../../arm-none-eabi/bin/as.exe': CreateProcess: No such file or directory
compilation terminated.
arm-none-eabi-gcc: fatal error: cannot execute 'd:/robocon/stm32cubeide_1.10.1/stm32cubeide/plugins/com.st.stm32cube.ide.mcu.externaltools.gnu-tools-for-stm32.10.3-2021.10.win32_1.0.0.202111181127/tools/bin/../lib/gcc/arm-none-eabi/10.3.1/../../../../arm-none-eabi/bin/as.exe': CreateProcess: No such file or directory
compilation terminated.
arm-none-eabi-gcc: fatal error: cannot execute 'd:/robocon/stm32cubeide_1.10.1/stm32cubeide/plugins/com.st.stm32cube.ide.mcu.externaltools.gnu-tools-for-stm32.10.3-2021.10.win32_1.0.0.202111181127/tools/bin/../lib/gcc/arm-none-eabi/10.3.1/../../../../arm-none-eabi/bin/as.exe': CreateProcess: No such file or directory
compilation terminated.
make: *** [Drivers/STM32F4xx_HAL_Driver/Src/subdir.mk:61: Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal.o] Error 1
make: *** Waiting for unfinished jobs....
make: *** [Drivers/STM32F4xx_HAL_Driver/Src/subdir.mk:61: Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_cortex.o] Error 1
make: *** [Drivers/STM32F4xx_HAL_Driver/Src/subdir.mk:61: Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_dma_ex.o] Error 1
make: *** [Drivers/STM32F4xx_HAL_Driver/Src/subdir.mk:61: Drivers/STM32F4xx_HAL_Driver/Src/stm32f4xx_hal_dma.o] Error 1
&quot;make -j4 all&quot; terminated with exit code 2. Build might be incomplete.
15:51:00 Build Failed. 5 errors, 0 warnings. (took 718ms)</p>
</blockquote>
<p>What does that error code mean and is there any applicable solution that I can use?</p>
<p>I expected my code to built, but it turns out the error keeps coming and I don't know what to do.</p>
",7/18/2023 15:34,,440,0,0,0,,20469939,,11/10/2022 15:19,4,,,,,,,,Error
222,4632,76756325,ModuleNotFoundError: No module named 'cv_bridge.boost.cv_bridge_boost',|python|ros|robotics|ros2|rosbag|,"<p>I have already installed <code>ros noetic</code> distribution and the <code>sudo apt-get install ros-noetic-cv-bridge</code> and <code>sudo apt-get install ros-noetic-cv-bridge</code> successfully. However, when I am trying to run the following code in pycharm, I am getting the following error:</p>
<pre><code>import os
import argparse
import pdb
import cv2
import rosbag
from sensor_msgs.msg import Image
from cv_bridge import CvBridge

bag_file = './bag_files/20230707_152832.bag'
output_dir= './frames/rgb_bag_output'


image_topic = '/device_0/sensor_1/Color_0/image/data'
# image_topic ='sensor_msgs/Image'

bag = rosbag.Bag(bag_file, &quot;r&quot;)
bridge = CvBridge()

#gg =bag.read_messages(topics = '/device_0/sensor_1/Color_0/image/data')
# bag.get_message_count()

count = 0
for topic, msg, t in bag.read_messages(topics=image_topic):
    cv_img = bridge.imgmsg_to_cv2(msg, desired_encoding= &quot;rgb8&quot;)#&quot;passthrough&quot;)
    cv2.imwrite(os.path.join(output_dir, &quot;frame%06i.png&quot; % count), cv_img)
    print(&quot;Wrote image %i&quot; % count)
    count += 1

bag.close() 
</code></pre>
<p>the error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/es/PycharmProjects/2-Process-RGBD/extract_bag_frame.py&quot;, line 32, in &lt;module&gt;
    cv_img = bridge.imgmsg_to_cv2(msg, desired_encoding= &quot;rgb8&quot;)#&quot;passthrough&quot;)
  File &quot;/home/es/anaconda3/envs/hsi-env/lib/python3.8/site-packages/cv_bridge/core.py&quot;, line 163, in imgmsg_to_cv2
    dtype, n_channels = self.encoding_to_dtype_with_channels(img_msg.encoding)
  File &quot;/home/es/anaconda3/envs/hsi-env/lib/python3.8/site-packages/cv_bridge/core.py&quot;, line 99, in encoding_to_dtype_with_channels
    return self.cvtype2_to_dtype_with_channels(self.encoding_to_cvtype2(encoding))
  File &quot;/home/es/anaconda3/envs/hsi-env/lib/python3.8/site-packages/cv_bridge/core.py&quot;, line 91, in encoding_to_cvtype2
    from cv_bridge.boost.cv_bridge_boost import getCvType
ModuleNotFoundError: No module named 'cv_bridge.boost.cv_bridge_boost'
</code></pre>
<p>How can I resolve this issue?</p>
",7/24/2023 16:17,,587,0,0,1,,6494707,,6/21/2016 14:33,543,,,,,,,,Error
223,4644,76830384,How to transfer latest state between threads in rust without using a mutex,|multithreading|rust|real-time|robotics|rust-tokio|,"<p>I am working on a Rust project where I have two distinct threads that serve specific purposes. One is a real-time thread which interfaces directly with a piece of hardware and maintains strict timing requirements, while the other is a web server thread that provides an API for querying the current state of said hardware.</p>
<p>I need a mechanism through which the web server thread can acquire the most recent state of the hardware from the real-time thread. The challenge here is to avoid the use of mutexes, as the real-time thread cannot afford to be blocked waiting for the mutex.</p>
<p>Here are a few solutions I have considered and their corresponding challenges:</p>
<ol>
<li><code>rwlock</code>: But the writer thread (real-time thread) would need to wait if any reader has a lock on it.</li>
<li>Double Buffering: The thread that does the swap needs a mutable reference to the entire struct, which causes borrow checker problems.</li>
<li><code>mpsc Channel</code>: If there are no web requests being made, the channel could fill up quickly, leading to wasted memory.</li>
</ol>
",8/3/2023 17:32,,110,2,2,3,,13163184,Canada,3/31/2020 7:10,2,76830453,"<p>You can use the <a href=""https://docs.rs/arc-swap"" rel=""nofollow noreferrer""><code>arc-swap</code></a> crate, that provides the <a href=""https://docs.rs/arc-swap/latest/arc_swap/type.ArcSwap.html"" rel=""nofollow noreferrer""><code>ArcSwap</code></a> type (and related types): an <code>Arc</code> that can be replaced and loaded in a lock-free fashion.</p>
",7884305,2,1,135452127,"@KevinReid It's a struct representing all the states aggregated from all parts of a robot, about 10 or so f32s and some booleans some enums",Timing
224,4663,77101051,Integrating Raspberry Camera with ROS2,|camera|ubuntu-20.04|raspberry-pi4|robotics|ros2|,"<p>I am trying to integrate raspberrypi camera into my robotic project. I have tried modules 2 and 3.  same result. I'm using ROS-FOXY( ubuntu 20.04) and ROS HUMBLE(ubuntu 22).
I installed  the driver using</p>
<pre><code>sudo apt install libraspberrypi-bin v4l-utils ros-foxy-v4l2-camera ros-foxy-image-transport-plugins
</code></pre>
<p>vcgencmd get_camera shows <em><strong>&quot; 0 detected 0 supported &quot;</strong></em> both in ros-foxy and ros-humble.</p>
<p>The cameras were detected when with raspberrypi OS, but not with Ubuntu.
<em><strong>using v4l2-ctl --list-devices</strong></em> shows</p>
<pre><code>Cannot open device /dev/video0, exiting.
</code></pre>
<p>Anybody with a clue as to what is missing?</p>
",9/13/2023 23:43,,311,0,0,0,,6351900,"Ontario, Canada",5/18/2016 15:31,40,,,,,,,,Connections
225,4667,77147437,Smooth movement of Pick and drop robot with stepper motor,|c++|robotics|firmware|stepper|nucleo|,"<p>I am building a pick and drop robot which has 3 stepper motors. I am using NUCLEO-F746ZG microcontroller and C++ programming to control it.</p>
<p>Now the problem is the arm moves with jerks in start and end (inertia) and I want to smooth the movement. For that I want to accelerate while the motor starts and decelerate when the motor stops. I tried different methods but none was working, the motor was moving in constant speed only. Kindly help me with this problem to make stepper motor move smoothly.</p>
<pre class=""lang-c++ prettyprint-override""><code>#include &quot;stepperMotor.h&quot;
#include &quot;mbed.h&quot;

int motorSpeed; // stepper speed
const float acc = 100;
int mspeed;

sMotor::sMotor(PinName A0, PinName A1, PinName A2, PinName A3) : _A0(A0), _A1(A1), _A2(A2), _A3(A3)
{ // Defenition of motor pins
    _A0 = 0;
    _A1 = 0;
    _A2 = 0;
    _A3 = 0;
}

void sMotor::anticlockwise(int mspeed)
{ // rotate the motor 1 step anticlockwise 
     for (int i = 0; i &lt; 8; i++) {
        switch (i)
        { // activate the ports A0, A2, A3, A3 in a binary sequence for steps
            case 0:
            {
                _A0 = 0;
                _A1 = 0;
                _A2 = 1;
                _A3 = 0;
            }
            break;

            case 1:
            {
                _A0 = 0;
                _A1 = 0;
                _A2 = 0;
                _A3 = 0;
            }
            break;

            case 2:
            {
                _A0 = 0;
                _A1 = 0;
                _A2 = 0;
                _A3 = 1;
            }
            break;

            case 3:
            {
                _A0 = 0;
                _A1 = 1;
                _A2 = 0;
                _A3 = 1;
            }
            break;

            case 4:
            {
                _A0 = 0;
                _A1 = 1;
                _A2 = 1;
                _A3 = 1;
            }
            break;

            case 5:
            {
                _A0 = 1;
                _A1 = 1;
                _A2 = 1;
                _A3 = 1;
            }
            break;

            case 6:
            {
                _A0 = 1;
                _A1 = 0;
                _A2 = 1;
                _A3 = 1;
            }
            break;

            case 7:
            {
                _A0 = 1;
                _A1 = 0;
                _A2 = 1;
                _A3 = 0;
            }
            break;
        }

        wait_us(mspeed);
      // wait_us(motorSpeed); // wait time defines the speed 
        
    }
}

void sMotor::clockwise(int mspeed)
{ // rotate the motor 1 step clockwise 
    for (int i = 7; i &gt;= 0; i--) {
        switch (i)
        {
            case 0:
            {
                _A0 = 0;
                _A1 = 0;
                _A2 = 0;
                _A3 = 1;
            }
            break;

            case 1:
            {
                _A0 = 0;
                _A1 = 0;
                _A2 = 1;
                _A3 = 1;
            }
            break;

            case 2:
            {
                _A0 = 0;
                _A1 = 0;
                _A2 = 1;
                _A3 = 0;
            }
            break;

            case 3:
            {
                _A0 = 0;
                _A1 = 1;
                _A2 = 1;
                _A3 = 0;
            }
            break;

            case 4:
            {
                _A0 = 0;
                _A1 = 1;
                _A2 = 0;
                _A3 = 0;
            }
            break;

            case 5:
            {
                _A0 = 1;
                _A1 = 1;
                _A2 = 0;
                _A3 = 0;
            }
            break;

            case 6:
            {
                _A0 = 1;
                _A1 = 0;
                _A2 = 0;
                _A3 = 0;
            }
            break;

            case 7:
            {
                _A0 = 1;
                _A1 = 0;
                _A2 = 0;
                _A3 = 1;
            }
            break;
        }
        wait_us(mspeed);
    }
}

void sMotor::step(int num_steps, int direction, int speed)
{// steper function: number of steps, direction (0- right, 1- left), speed (default 1200)
    int count = 0; // initalize step count
    float stepTime = 1.0 / speed;  //aTry
    int w = num_steps;
    mspeed = 250;
    int speedMultiplier = 10;
    
    motorSpeed = speed; //set motor speed
    if (direction == 0) // turn clockwise
        do
        {
            clockwise(mspeed);
            count++;

            for (int i = 0; i &lt; (w/4); i++)
            {
                while (mspeed != speed)
                {
                    mspeed = mspeed - speedMultiplier;
                } 
            }
            for (int i = w/4; i &lt; 3 * (w / 4); i++)
            {
                mspeed = speed; 
            }            
            for (int i = 3 * (w / 4); i &lt; w; i++)
            {
                while (mspeed != 400)
                {
                    mspeed = mspeed + speedMultiplier;
                }
            }
        } while (count &lt; num_steps); // turn number of steps applied 
        
    else if (direction == 1) // turn anticlockwise
    { 
        count = 0;
        do
        {
            anticlockwise(mspeed);
            count++;      

            for (int i = 0; i &lt; (w / 4); i++)
            {
                while (mspeed != speed)
                {
                    mspeed = mspeed - speedMultiplier;
                } 
            }
            for (int i = w/4; i &lt; 3 * (w / 4); i++)
            {
                mspeed = speed; 
            }            
            for (int i = 3 * (w / 4); i &lt; w; i++)
            {
                while (mspeed != 400)
                {
                    mspeed = mspeed + speedMultiplier;
                }
            }
        } while (count &lt; num_steps);// turn number of steps applied 
    }
}
</code></pre>
",9/21/2023 5:40,,144,1,1,2,,20749673,,12/11/2022 15:52,4,77147682,"<p>You have a few possible improvements in your code: you have a field <code>mspeed</code> and you have a parameter <code>mspeed</code> in two methods - the parameter overrides the field. But as you implemented it now, the parameter wouldn't be necessary at all, because the methods <code>clockwise</code> and <code>anticlockwise</code> can access the field <code>mspeed</code>.</p>
<p>Also you assign the field <code>motorspeed</code> but you never use it.</p>
<p>The name of the field and parameters <code>mspeed</code> is misleading, usually a movement is faster with a higher speed, but you use it as waiting time, so <code>delay</code> would be a better name.</p>
<p>For the <code>clockwise</code> and <code>anticlockwise</code> methods: are you sure that you use the correct values and the correct order to activate the coils of the stepper motor? To my understanding it should be possible that you have a list of values for a full step and you can &quot;play&quot; it in one direction for clockwise and in the opposite direction for counter-clockwise motion. This way you would not have to use a <code>switch</code> statement and also only one method for both directions.</p>
<p>In the method <code>step</code> you should use a better name for <code>w</code>, maybe <code>position</code>?
Also in that method you have a huge code block for clockwise and anticlockwise movement that is completely equal, just the call to the <code>clockwise</code> and <code>anticlockwise</code> methods differ. To prevent code duplication errors, you should move that outer <code>if</code> further inside so that it just decides the actual call to one of the two methods. This is the most important improvement as it has impact on the fix if your problem!</p>
<p>To your actual problem:
In your <code>step</code> method you use three <code>for</code> loops to decrease the delay (speed) from a position of 0 to 1/4 of the target position, the keep it still from 1/4 to the 3/4 of the target position and finally increase the delay again from 3/4 to the complete target position.
Inside the acceleration and deceleration <code>for</code> loops you have a <code>while</code> loop.
And all of these nested loops are contained in one outer <code>do</code>-<code>while</code> loop for the stepping.</p>
<p><strong>BUT</strong>: you only need the outer <code>do</code>-<code>while</code> loop. The inner loops are run for each step, so the delay is accelerated, kept constant and decelerated for every step of the motor.
What you should have instead is a nested <code>if</code> inside that outermost loop that decides for each cycle:</p>
<ul>
<li>if <code>count</code> is in the first quarter of the target range (<code>num_steps</code>), do the acceleration</li>
<li>else if <code>count</code> is in the last quarter of the target range, do the deceleration</li>
<li>else use the delay that was given into the <code>step</code> method.</li>
</ul>
<p>Please note that for an even smoother movement, you have to process acceleration and deceleration also inside your method(s) to move the motor one full step, because actually you execute the same delay 8 times for a step, but you could vary it there as well!</p>
<p>Here some code for the <code>step</code> method - please note that my C++ knowledge was unused for several years and I also wasn't able to test this code, so I don't know if it even compiles. Feel free to fix it where necessary ;-)</p>
<pre class=""lang-c++ prettyprint-override""><code>void sMotor::step(int num_steps, int direction, int delay)
{// stepper function: number of steps, direction (0- right, 1- left), waiting time (default 1200)
    int speedMultiplier = 10; 

    // ensure that input delay param is used for the largest part.
    mspeed = delay + speedMultiplier * num_steps / 4;

    for (int step = 0; step &lt; num_steps; ++step)
    {
        if (step &lt; (num_steps / 4))
        {
            mspeed -= speedMultiplier;
        }
        else if (step &gt; (3 * (num_steps / 4)))
        {
            mspeed += speedMultiplier;
        }
        else
        {
            mspeed = delay;
        }

        if (direction == 0)
        {
            clockwise(mspeed);
        }
        else if (direction == 1)
        {
            anticlockwise(mspeed);
        }
    }
}
</code></pre>
<p>This code doesn't use acceleration and deceleration &quot;inbetween&quot; the steps, so that optimization is left up to you.</p>
",2846138,1,2,136037236,Just as a side note - the usual way to calculate a smooth trajectory is to use a power of 5 polynomial function. It's easy to find polynomial coefficients for such a function with constraints that velocity and acceleration are 0 at the end points of the trajectory and maximum in the middle point.,Actuator
226,4682,77274713,Change color of CoppeliaSim shapes with Python,|python|colors|simulator|robotics|,"<p>I am using the CoppeliaSim Regular API with Python.</p>
<p>To set a color to my primitive shapes I am using the method setShapeColor().
To set a shape green I have the following code.</p>
<h1>code to set a shape green</h1>
<p>sim.setShapeColor(capsuleHandle, None, sim.colorcomponent_emission, [0, 255, 0])</p>
<p>However I am not convinced with the result. Although it is green, it looks quite pale and very shiny/reflective.</p>
<p>But from the setShapeColor() there are 2 parameters that I do not understand well.
<a href=""https://www.coppeliarobotics.com/helpFiles/"" rel=""nofollow noreferrer"">https://www.coppeliarobotics.com/helpFiles/</a>
What should the string colorName contain? Are there already predefined color names? Or it is that a tag just for me? Or what is the use of that string?</p>
<p>Also I do not understand the color components.
<a href=""https://www.coppeliarobotics.com/helpFiles/en/apiConstants.htm#colorComponents"" rel=""nofollow noreferrer"">https://www.coppeliarobotics.com/helpFiles/en/apiConstants.htm#colorComponents</a>
The documentation only gives the names but they do not explain what is the purpose of each component or when to use each.</p>
<p>So can someone please:</p>
<ol>
<li>Explain me how to use the colorName string and the color components</li>
<li>How can I not only change the color of the shape, but also control the intensity of the color and if it shiny or dull.</li>
<li>I also need to control the transparency of the shape. How to do that?</li>
</ol>
<p>Thank you :)</p>
<ul>
<li>Read the Regular API documentation</li>
<li>Trying all the color components but the result was always the same</li>
<li>For the transparency I tried: Using the transparency component, in the RGB instead of sending a list send a single value for the transparency, send a list with just 1 value, send a list with 4 values (RGB + transparency).</li>
</ul>
",10/11/2023 16:09,,73,0,1,0,,22694257,,10/6/2023 8:42,4,,,,,,136234841,It is a proprietary software. Why not ask the vendor? See [the official forum](https://forum.coppeliarobotics.com/).,Incoming
227,4685,77276377,I'm almost there? 4 DOF robot arm forward kinematics,|c#|quaternions|robotics|inverse-kinematics|,"<p>I started a project to learn the forward kinematics of a custom 4 DOF robot arm I made. Essentially, I wanted to learn the basics to apply them to learning inverse kinematics <em>fingers crossed</em>. I cannot seem to grasp the rotation axis. It will <em>click</em> in my brain once I overcome whatever mental block stops me.</p>
<p>I set out to create a Joint class that would identify each joint and the rotation axis. Again, why can't my brain wrap my head around the rotation axis? Do you have any suggestions to overcome this mental blockage?</p>
<pre><code>  public class Joint {

    public double Length { get; }
    public Vector3 RotationAxis { get; }

    public Joint(double length, Vector3 rotationAxis) {

      Length = length;
      RotationAxis = Vector3.Normalize(rotationAxis);
    }
  }
</code></pre>
<p>The heart of the class is to loop through each joint and keep the rotation state while calculating the vector3 offset for the end effector. But I feel like I am misusing Quaternions. Either that or the Transform axis is incorrect. Most likely, the axis is incorrect because I can't get my head around it.</p>
<pre><code>using System;
using System.Collections.Generic;
using System.Numerics;

namespace InverseKinematicsMover {

  public class Joint {

    public double Length { get; }
    public Vector3 RotationAxis { get; }

    public Joint(double length, Vector3 rotationAxis) {

      Length = length;
      RotationAxis = Vector3.Normalize(rotationAxis);
    }
  }

  public class Forward2 {

    public List&lt;Joint&gt; Joints = new List&lt;Joint&gt;();

    public Forward2() {

      Joints = new List&lt;Joint&gt;();
    }

    float degreeToRadian(int angle) {

      // because 90 is the center for servos
      int angleDegrees = angle - 90;

      return (float)Math.PI * angleDegrees / 180.0f;
    }

    public Vector3 CalculateEndEffectorPosition(List&lt;int&gt; jointAngles) {

      Vector3 endEffectorPosition = Vector3.Zero;
      Quaternion cumulativeRotation = Quaternion.Identity;

      for (int i = 0; i &lt; Joints.Count; i++) {

        // continue to keep track of the rotation as we progress through joints
        // ie 45 degrees of the current joint is relative to 90 degrees of previous joint
        cumulativeRotation += Quaternion.CreateFromAxisAngle(Joints[i].RotationAxis, degreeToRadian(jointAngles[i]));
        
        // add to the end effector with the quaternion transformation of this joint's rotation axis
        endEffectorPosition += (float)Joints[i].Length * Vector3.Transform(Joints[i].RotationAxis, cumulativeRotation);
      }

      return endEffectorPosition;
    }
  }
}
</code></pre>
<p>Lastly, my test program uses each servo's valueo (1-180).</p>
<pre><code>      Forward2 robotArm = new Forward2();

      // Base joint rotates the arm
      robotArm.Joints.Add(new Joint(0, new Vector3(0, 1, 0)));

      // Joints extend the robot arm
      robotArm.Joints.Add(new Joint(128.0, new Vector3(1, 0, 0)));
      robotArm.Joints.Add(new Joint(148.0, new Vector3(1, 0, 0)));
      robotArm.Joints.Add(new Joint(146.0, new Vector3(1, 0, 0)));

      // Specify joint angles (in degrees) for each joint
      var  jointAngles = new List&lt;int&gt; {
        GetServoPosition(0),
        GetServoPosition(1),
        GetServoPosition(2),
        GetServoPosition(3),
      };

      // Calculate the end effector position
      Vector3 endEffectorPosition = robotArm.CalculateEndEffectorPosition(jointAngles);

      // Display the end effector position
      log($&quot;End Effector Position: X={endEffectorPosition.X}, Y={endEffectorPosition.Y}, Z={endEffectorPosition.Z}&quot;);
</code></pre>
<p>I get a strange behavior for the logged output. It appears the Y and Z axis are dependent on the X (base rotation) from not being zero. And the Y and Z axis values are heavily dependent on the X value as a multiplier for some reason.</p>
<p>I have tried configuring the Joint definitions for different Axis. The first Joint is a base rotation, as seen on common Robot Arms. The remaining joints all rotate the arm vertically (up and down).</p>
<p>I have also tried thinking the end effector accumulated value may need the length multiplied by the rotation axis during the transform.</p>
<pre><code>  endEffectorPosition += Vector3.Transform(Joints[i].RotationAxis * (float)Joints[i].Length, cumulativeRotation);
</code></pre>
<p>That provides similar results where the Y and Z values are heavily dependent on the X axis and also seem to be a multiplier of X.</p>
",10/11/2023 21:01,77288612,124,1,1,-1,,22725238,,10/11/2023 20:27,5,77288612,"<p>Ah, after some further research I figured out my issue with the axis. You see, the arm extends upward from the base when all joint values are 0. That means all joints are along the y axis relative to each other.</p>
<p>Also, the first joint was configured for the z axis rotation which is incorrect. It should be the Y axis because it rotates the arm around it.</p>
<p>The main issue was using the rotation axis in the vector transform. Because the arm extends from the y axis out of the base, it merely needs this code changed to…</p>
<pre><code>endEffectorPosition += (float)Joints[i].Length * Vector3.Transform(Vector3.UnitY, cumulativeRotation);
      }
</code></pre>
",22725238,0,0,136240769,"Welcome to Stackoverflow! I think your question can be better answered in the [StackExchange Robotics](https://robotics.stackexchange.com/), have you tried asking there?",Actuator
228,4707,77461417,Switch between different ROS (Robot Operating System) versions,|ubuntu|simulation|ros|robotics|,"<p>As a ROS (Robot Operating System) developer, how is it possible to switch between ROS versions within my projects? Different Ubuntu OS needed to be installed?</p>
<p>I have tried virtual machines and dual booting but I did not find that a good solution.</p>
",11/10/2023 16:14,,86,1,5,0,,22894370,,11/10/2023 15:56,1,77469197,"<p>One option is using the Ubuntu version that is compatible with both ROS1 and ROS2. Then, you can just switch ROS1&lt;-&gt;ROS2 when you are sourcing it.</p>
<p>Another option is using docker, which is more recommended.</p>
",18268514,0,5,136594760,"Actually I do not get it, currently I am using Ubuntu 18 LTS with ROS1 Melodic, while there is Robot in the laboratory which is programmed on ROS1 Noetic. I do not know what to do? It seems, I need to upgrade my Ubuntu system to ver 20.04 LTS and change the programs coded in Melodic. Is n't it?
(I am not familiar with docker to handle these problem.)",Other
229,4717,77601467,Position control in Mujoco,|simulation|robotics|mujoco|,"<p>Being new to Mujoco, I just created a dummy scene in mujoco for which I now try to control some of the objects in it. The scene contains a fixed table on which a movable book is places, which can be moved by controlling the actuated spatula frame:</p>
<pre><code>xml = &quot;&quot;&quot;
&lt;mujoco&gt;
  &lt;worldbody&gt;
    &lt;light name=&quot;top&quot; pos=&quot;0 0 1&quot;/&gt;
    
    &lt;body name=&quot;table&quot; pos=&quot;0 0 0.025&quot;&gt;
      &lt;geom name=&quot;plate&quot; type=&quot;box&quot; size=&quot;0.25 0.2 0.025&quot; rgba=&quot;.8 .8 .8 1&quot;/&gt;
      &lt;geom name=&quot;bound0&quot; type=&quot;box&quot; size=&quot;.25 .01 .05&quot; pos=&quot;0 -.19 .075&quot;/&gt;
      &lt;geom name=&quot;bound1&quot; type=&quot;box&quot; size=&quot;.01 .19 .05&quot; pos=&quot;-.24 .01 .075&quot;/&gt;
    &lt;/body&gt;

    &lt;body name=&quot;booklink&quot; pos=&quot;0 0 0.065&quot;&gt;
      &lt;freejoint/&gt;
      &lt;geom name=&quot;book&quot; type=&quot;box&quot; size=&quot;.1 .05 .0125&quot; rgba=&quot;0 1 0 1&quot; mass=&quot;.1&quot;/&gt;
    &lt;/body&gt;  

    &lt;body name=&quot;spatulalink&quot; pos=&quot;0 0 .2&quot;&gt;
        &lt;joint name=&quot;transX&quot; type=&quot;slide&quot; axis=&quot;1 0 0&quot; limited=&quot;true&quot; range=&quot;-.6 .6&quot;/&gt;
        &lt;joint name=&quot;transY&quot; type=&quot;slide&quot; axis=&quot;0 1 0&quot; limited=&quot;true&quot; range=&quot;-.6 .6&quot;/&gt;
        &lt;joint name=&quot;transZ&quot; type=&quot;slide&quot; axis=&quot;0 0 1&quot; limited=&quot;true&quot; range=&quot;-.6 .6&quot;/&gt;
        &lt;joint name=&quot;hingeX&quot; type=&quot;hinge&quot; axis=&quot;1 0 0&quot; limited=&quot;true&quot; range=&quot;-3.2 3.2&quot;/&gt;
        &lt;joint name=&quot;hingeY&quot; type=&quot;hinge&quot; axis=&quot;0 1 0&quot; limited=&quot;true&quot; range=&quot;-3.2 3.2&quot;/&gt;
        &lt;joint name=&quot;hingeZ&quot; type=&quot;hinge&quot; axis=&quot;0 0 1&quot; limited=&quot;true&quot; range=&quot;-3.2 3.2&quot;/&gt;
        &lt;geom name=&quot;spatula&quot; type=&quot;box&quot; size=&quot;.05 .05 5e-4&quot; rgba=&quot;1 1 0 1&quot; friction=&quot;.1 .1 .1&quot; mass=&quot;.1&quot;/&gt;
    &lt;/body&gt;
  &lt;/worldbody&gt;
  &lt;actuator&gt;
    &lt;position ctrllimited=&quot;true&quot; ctrlrange=&quot;-.6 .6&quot; joint=&quot;transX&quot;/&gt;
    &lt;position ctrllimited=&quot;true&quot; ctrlrange=&quot;-.6 .6&quot; joint=&quot;transY&quot;/&gt;
    &lt;position ctrllimited=&quot;true&quot; ctrlrange=&quot;-.6 .6&quot; joint=&quot;transZ&quot;/&gt;
    &lt;position ctrllimited=&quot;true&quot; ctrlrange=&quot;-3.2 3.2&quot; joint=&quot;hingeX&quot;/&gt;
    &lt;position ctrllimited=&quot;true&quot; ctrlrange=&quot;-3.2 3.2&quot; joint=&quot;hingeY&quot;/&gt;
    &lt;position ctrllimited=&quot;true&quot; ctrlrange=&quot;-3.2 3.2&quot; joint=&quot;hingeZ&quot;/&gt;
  &lt;/actuator&gt;
&lt;/mujoco&gt;
&quot;&quot;&quot;
model = mujoco.MjModel.from_xml_string(xml)
data = mujoco.MjData(model)
</code></pre>
<p>Now my question is how I can properly run a trajectory in this environment during which I control the robot. Naively, I tried to put some controls by directly writing into <code>data.qpos</code> and <code>data.qvel</code>. This creates the desired motions, but the objects tunnel through each other, i.e., there are collisions of the geometries, which I of course do not want. You can see a picture of this here (you see the yellow spatula object going through the boundary in collision):</p>
<p><a href=""https://i.stack.imgur.com/oD1YR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oD1YR.png"" alt=""sim error"" /></a></p>
<p>Now, after some reading, I now learned that the correct way of controlling actuators / joints in Mujoco is instead to set the controls directly, which in my case would be <code>data.crtl = some_position_vector</code>. I do this as follows:</p>
<pre><code># Construct a BSpline to interpolate the via-points
spline_ref = BSpline_reference(data.qpos[7:], data.qvel[6:], data.time)
spline_ref.append(path, times, data.time)

with mujoco.viewer.launch_passive(model, data) as viewer:
  # Close the viewer automatically after 30 wall-seconds.
  start = time.time()
  sim_steps = int(n_steps // tau)
  i = 0
  while viewer.is_running(): #and i &lt; sim_steps:
    # Get a target position from the reference spline using current sim state
    qreal = data.qpos[7:]
    qDot_real = data.qvel[6:]
    qref, qDotref = spline_ref.getReference(qreal, qDot_real, data.time)
    data.ctrl = q_ref
    mujoco.mj_step(model, data)

    # Pick up changes to the physics state, apply perturbations, update options from GUI.
    viewer.sync()

    # Slow down sim. for visualization
    time.sleep(1e-2)
    i += 1
</code></pre>
<p>Somehow, the movement I get from this is completely different than when I set <code>data.qpos</code> directly. Can someone explain to me how the controls can be set properly?</p>
",12/4/2023 17:35,77691739,613,2,0,0,,19249619,"Berlin, Deutschland",6/1/2022 15:06,3,77691739,"<p>What you have in your model are position actuators, which apply a torque or force of <code>kp * (q_desired - q) - kv * qdot</code>. So the position is not set magically as it would when you set qpos directly.</p>
<p>The <a href=""https://mujoco.readthedocs.io/en/stable/XMLreference.html#actuator-position"" rel=""nofollow noreferrer"">defaults</a> are <code>kp=&quot;1&quot;</code> and <code>kv=&quot;0&quot;</code>. <code>kp</code> might be too small for your system.</p>
<p>Similarly to how you would on a real robot, you should tune the gains on your position actuators so you get the responsiveness you want without introducing unwanted oscillations.</p>
<p>Note that the kv parameter will essentially create a reference velocity of zero. If you want to track a reference velocity, as well as position, you could add extra velocity actuators to your model, and set the ctrl entries for those to your qDotref.</p>
",155171,0,2,,,Actuator
230,4720,77607266,Implementing Kalman Filter for Improved Measurement Accuracy in Autonomous Two-Wheeled Robot with Encoders,|pid|robotics|kalman-filter|encoder|,"<p>So i am a beginner in the field of robotics, and I have successfully created an autonomous two-wheeled robot equipped with two encoders. I've implemented a PI regulation on speed, and while the robot performs reasonably well, I still encounter some measurement errors. To enhance its performance, I'm considering the implementation of a Kalman filter.</p>
<p>While I have a theoretical understanding of the Kalman filter, I'm a bit confused about the practical implementation. Should I apply a simple Kalman filter to each encoder independently to smooth the measurements, or is it more appropriate to use the filter on the x and y positions of the robot? . Any guidance on the best approach for implementing the Kalman filter in this context would be greatly appreciated.</p>
",12/5/2023 15:12,,54,0,1,1,,23046277,,12/5/2023 14:46,0,,,,,,136829548,"At the end of the day, is the (x, y) position of the robot what you care about? If so, then it probably makes sense to have those be the states of the filter. Are there any other measurements that you get about the position of the vehicle? If the encoders is all that you have, a Kalman Filter probably won't help you all that much. The dynamics in the filter would be driven by the encoder, but there are no measurements to correct the state estimate, so uncertainty would just keep growing over time.",Moving
231,4727,77649089,Scara Robot Inverse Kinematics,|c++|math|arduino|robotics|,"<p>I am working on a Scara Robot controlled by an Arduino and trying to figure out the inverse kinematics. I programmed a simple example sketch to test my function but it doesn't work because of a mathematical error. Can someone tell me what's wrong?</p>
<pre><code>const int L1 = 202.25;
const int L2 = 193.35;

int alpha = 0;
int beta = 0;

void calc_IK(int x, int y){
  beta = acos((x*x+y*y-L1*L1-L2*L2)/2*L1*L2); //I could also ise pow()
  alpha = atan((y/x)-atan(L2*sin(beta)/L1+L2*cos(beta)));
  Serial.println(alpha, beta);
}

void setup() {
  Serial.begin(9600);
}

void loop() {
  calc_IK(3, 5);
  delay(200);
}
</code></pre>
",12/12/2023 21:21,,79,1,4,-1,,17977602,,1/19/2022 19:23,6,77668814,"<p>I am not sure where the error was but I changed some variables to float and some to doubles. I also noticed that the result was in radians so I have to convert it to degrees. Here is my now functioning code:</p>
<pre><code>#include &lt;math.h&gt;

const double L1 = 202.25;
const double L2 = 193.35;

float alpha = 0; //radians
float beta = 0;

float beta_d; //degrees
float alpha_d;

void calc_IK(float x, float y){
  beta = acos((pow(x, 2)+pow(y, 2)-pow(L1, 2)-pow(L2, 2))/(2*pow(L1, 2)*pow(L2, 2)));
  alpha = atan((y/x)-atan((L2*sin(beta))/(L1+L2*cos(beta))));

  beta_d = beta * 180/M_PI;
  alpha_d = alpha * 180/M_PI;


  Serial.print(&quot;Beta: &quot;);
  Serial.print(beta_d);
  Serial.print(&quot; &quot;);
  Serial.print(&quot;Alpha: &quot;); 
  Serial.print(alpha_d);
  Serial.println(&quot; &quot;);

}

void setup() {
  Serial.begin(9600);
}

void loop() {
  calc_IK(3.0, 5.0);
}
</code></pre>
<p>It may not be the best but it works.</p>
",17977602,0,1,136894859,"the same goes for the angles `alfa,beta` and probably also coordiantes `x,y`... maybe `float` is enough as FPU is not a guarantee on arduino used MCUs ...",Actuator
232,4740,77738665,Calculating IMU random walk using allan variance,|ros|robotics|calibration|imu|,"<p>I am trying to calculate accel and gyro noise and random walk for Intel realsense using <a href=""https://github.com/ori-drs/allan_variance_ros"" rel=""nofollow noreferrer"">allan variance</a>. I quickly generated imu csv in the format specified <a href=""https://github.com/ethz-asl/kalibr/wiki/bag-format"" rel=""nofollow noreferrer"">here</a> using custom CPP script for just 3 seconds. I wanted to check if it will work or not before recording 20 hours of IMU data.</p>
<p>I created ros bag from csv by running:</p>
<pre><code>rosrun kalibr kalibr_bagcreater --folder ./data/imu_data --output-bag ./data/newbag.bag
</code></pre>
<p>Checking the bag created:</p>
<pre><code>root@e9408da6622e:/data# rosbag info /data/data/newbag.bag  
path:        /data/data/newbag.bag 
version:     2.0 
duration:    2.4s 
start:       Dec 30 2023 18:41:25.42 (1703961685.42) 
end:         Dec 30 2023 18:41:27.84 (1703961687.84) 
size:        312.7 KB 
messages:    830 
compression: none [1/1 chunks] 
types:       sensor_msgs/Imu [6a62c6daae103f4ff57a132d6f95cec2] 
topics:      /imu0   830 msgs    : sensor_msgs/Imu 
</code></pre>
<p>Then I converted it to allan variance compliant bag by running:</p>
<pre><code>rosrun allan_variance_ros cookbag.py --input /data/data/imu_data/newbag.bag --output /data/data/imu_data/cooked_bag.bag 
</code></pre>
<p>Checking the bag created:</p>
<pre><code>root@e9408da6622e:/catkin_ws# rosbag info /data/data/imu_data/cooked_bag.bag  
path:         /data/data/imu_data/cooked_bag.bag 
version:      2.0 
duration:     2.4s 
start:        Dec 30 2023 18:41:25.42 (1703961685.42) 
end:          Dec 30 2023 18:41:27.84 (1703961687.84) 
size:         41.7 KB 
messages:     830 
compression:  lz4 [1/1 chunks; 8.47%] 
uncompressed: 296.1 KB @ 122.5 KB/s 
compressed:    25.1 KB @  10.4 KB/s (8.47%) 
types:        sensor_msgs/Imu [6a62c6daae103f4ff57a132d6f95cec2] 
topics:       /imu0   830 msgs    : sensor_msgs/Imu
</code></pre>
<p>I created allan variance config file imu_confog.yaml:</p>
<pre><code>imu_topic: &quot;/imu0&quot;
imu_rate: 346
measure_rate: 346 # since bag contains 830 messages / 2.4 seconds = 345.8333 approx 346
sequence_time: 3 # since bag contains 2.4 seconds worth of data
</code></pre>
<p>(I also tried with both rates 400, no luck.)</p>
<p>Here is how my data folder looks like:</p>
<p><img src=""https://github.com/ori-drs/allan_variance_ros/assets/52235655/dbddd258-5c01-41b7-87d4-9e03aa431e3e"" alt=""image"" /></p>
<p>Then I run allan variance:</p>
<pre><code>root@e9408da6622e:/catkin_ws# rosrun allan_variance_ros allan_variance /data/data/imu_data /data/data/allan_variance/imu_config.yaml
</code></pre>
<p>And finally run Analyse.py:</p>
<pre><code>root@e9408da6622e:/catkin_ws# rosrun allan_variance_ros analysis.py --data /data/data/imu_data/allan_variance.csv --config /data/data/allan_variance/imu_config.yaml
/catkin_ws/src/allan_variance_ros/scripts/analysis.py:23: RuntimeWarning: divide by zero encountered in log
  logy = np.log(y)
Traceback (most recent call last):
  File &quot;/catkin_ws/src/allan_variance_ros/scripts/analysis.py&quot;, line 101, in &lt;module&gt;
    accel_wn_intercept_x, xfit_wn = get_intercept(period[0:white_noise_break_point], acceleration[0:white_noise_break_point,0], -0.5, 1.0)
  File &quot;/catkin_ws/src/allan_variance_ros/scripts/analysis.py&quot;, line 24, in get_intercept
    coeffs, _ = curve_fit(line_func, logx, logy, bounds=([m, -np.inf], [m + 0.001, np.inf]))
  File &quot;/usr/lib/python3/dist-packages/scipy/optimize/minpack.py&quot;, line 708, in curve_fit
    ydata = np.asarray_chkfinite(ydata, float)
  File &quot;/usr/lib/python3/dist-packages/numpy/lib/function_base.py&quot;, line 495, in asarray_chkfinite
    raise ValueError(
ValueError: array must not contain infs or NaNs
</code></pre>
<p>But as you can see it is giving error <code>array must not contain infs or NaNs</code>. Turns out there are some NaNs in allan_variance.csv file generated. What I am doing wrong here?</p>
<p>I have uploaded all bags and csv file on drive <a href=""https://drive.google.com/drive/folders/15M2P3NHtgvJvRPZMNqd5uMvz1R0GN-0S?usp=sharing"" rel=""nofollow noreferrer"">here</a>.</p>
",12/31/2023 8:56,,41,0,0,0,,6357916,,5/19/2016 19:14,480,,,,,,,,Actuator
233,4745,77829740,WPILIB (FRC) Gradlew Issue (Gradle builds fine but gradlew doesn't),|java|gradle|robotics|wpilib|,"<p>I recently installed wpilib on my windows laptop and start an project, but when I try to build the project using <code>gradlew</code> it doesnt work:</p>
<p>Here is my <code>build.gradle</code>:</p>
<pre><code>plugins {
    id &quot;java&quot;
    id &quot;edu.wpi.first.GradleRIO&quot; version &quot;2024.1.1&quot;
}

java {
    sourceCompatibility = JavaVersion.VERSION_17
    targetCompatibility = JavaVersion.VERSION_17
}

def ROBOT_MAIN_CLASS = &quot;frc.robot.Main&quot;

// Define my targets (RoboRIO) and artifacts (deployable files)
// This is added by GradleRIO's backing project DeployUtils.
deploy {
    targets {
        roborio(getTargetTypeClass('RoboRIO')) {
            // Team number is loaded either from the .wpilib/wpilib_preferences.json
            // or from command line. If not found an exception will be thrown.
            // You can use getTeamOrDefault(team) instead of getTeamNumber if you
            // want to store a team number in this file.
            team = project.frc.getTeamNumber()
            debug = project.frc.getDebugOrDefault(false)

            artifacts {
                // First part is artifact name, 2nd is artifact type
                // getTargetTypeClass is a shortcut to get the class type using a string

                frcJava(getArtifactTypeClass('FRCJavaArtifact')) {
                }

                // Static files artifact
                frcStaticFileDeploy(getArtifactTypeClass('FileTreeArtifact')) {
                    files = project.fileTree('src/main/deploy')
                    directory = '/home/lvuser/deploy'
                }
            }
        }
    }
}

def deployArtifact = deploy.targets.roborio.artifacts.frcJava

// Set to true to use debug for JNI.
wpi.java.debugJni = false

// Set this to true to enable desktop support.
def includeDesktopSupport = false

// Defining my dependencies. In this case, WPILib (+ friends), and vendor libraries.
// Also defines JUnit 5.
dependencies {
    implementation wpi.java.deps.wpilib()
    implementation wpi.java.vendor.java()

    roborioDebug wpi.java.deps.wpilibJniDebug(wpi.platforms.roborio)
    roborioDebug wpi.java.vendor.jniDebug(wpi.platforms.roborio)

    roborioRelease wpi.java.deps.wpilibJniRelease(wpi.platforms.roborio)
    roborioRelease wpi.java.vendor.jniRelease(wpi.platforms.roborio)

    nativeDebug wpi.java.deps.wpilibJniDebug(wpi.platforms.desktop)
    nativeDebug wpi.java.vendor.jniDebug(wpi.platforms.desktop)
    simulationDebug wpi.sim.enableDebug()

    nativeRelease wpi.java.deps.wpilibJniRelease(wpi.platforms.desktop)
    nativeRelease wpi.java.vendor.jniRelease(wpi.platforms.desktop)
    simulationRelease wpi.sim.enableRelease()

    testImplementation 'org.junit.jupiter:junit-jupiter:5.10.1'
    testRuntimeOnly 'org.junit.platform:junit-platform-launcher'
}

test {
    useJUnitPlatform()
    systemProperty 'junit.jupiter.extensions.autodetection.enabled', 'true'
}

// Simulation configuration (e.g. environment variables).
wpi.sim.addGui().defaultEnabled = true
wpi.sim.addDriverstation()

// Setting up my Jar File. In this case, adding all libraries into the main jar ('fat jar')
// in order to make them all available at runtime. Also adding the manifest so WPILib
// knows where to look for our Robot Class.
jar {
    from { configurations.runtimeClasspath.collect { it.isDirectory() ? it : zipTree(it) } }
    from sourceSets.main.allSource
    manifest edu.wpi.first.gradlerio.GradleRIOPlugin.javaManifest(ROBOT_MAIN_CLASS)
    duplicatesStrategy = DuplicatesStrategy.INCLUDE
}

// Configure jar and deploy tasks
deployArtifact.jarTask = jar
wpi.java.configureExecutableTasks(jar)
wpi.java.configureTestTasks(test)

// Configure string concat to always inline compile
tasks.withType(JavaCompile) {
    options.compilerArgs.add '-XDstringConcat=inline'
}
</code></pre>
<p>Now if I build with gradle, it builds perfectly fine</p>
<p>settings.gradle:</p>
<pre><code>import org.gradle.internal.os.OperatingSystem

pluginManagement {
    repositories {
        mavenLocal()
        gradlePluginPortal()
        String frcYear = '2024'
        File frcHome
        if (OperatingSystem.current().isWindows()) {
            String publicFolder = System.getenv('PUBLIC')
            if (publicFolder == null) {
                publicFolder = &quot;C:\\Users\\Public&quot;
            }
            def homeRoot = new File(publicFolder, &quot;wpilib&quot;)
            frcHome = new File(homeRoot, frcYear)
        } else {
            def userFolder = System.getProperty(&quot;user.home&quot;)
            def homeRoot = new File(userFolder, &quot;wpilib&quot;)
            frcHome = new File(homeRoot, frcYear)
        }
        def frcHomeMaven = new File(frcHome, 'maven')
        maven {
            name 'frcHome'
            url frcHomeMaven
        }
    }
}

Properties props = System.getProperties();
props.setProperty(&quot;org.gradle.internal.native.headers.unresolved.dependencies.ignore&quot;, &quot;true&quot;);
</code></pre>
<p>I expected that it would build with gradlew as well but it doesnt,</p>
<p>heres my gradle-wrapper.properties</p>
<pre><code>distributionBase=GRADLE_USER_HOME
distributionPath=permwrapper/dists
distributionUrl=https\://services.gradle.org/distributions/gradle-8.5-bin.zip
networkTimeout=10000
validateDistributionUrl=true
zipStoreBase=GRADLE_USER_HOME
zipStorePath=permwrapper/dists
</code></pre>
<p>instead of building it gives me this:</p>
<pre><code>&gt;gradlew
Exception in thread &quot;main&quot; java.lang.ClassNotFoundException: org.gradle.launcher.GradleMain
        at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
        at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)
        at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)
        at org.gradle.wrapper.GradleWrapperMain.main(SourceFile:70)
</code></pre>
<p>I have even tried regenerating the wrapper, <code>gradle wrapper</code></p>
<p><strong>ALSO: FOR SOME REASON <code>gradlew</code> doesnt listen to commands its only building no matter what</strong></p>
<p><strong>EDIT:</strong>
I have tried my code on github codespaces and my own linux (arch linux) but windows isn't working for me, i have reinstalled wpilib multiple times not sure why its doing this to me. I have tried using a vm of windows and it works it weird why my computer doesn't work.</p>
",1/17/2024 2:56,77843380,110,1,0,1,,15027302,,1/18/2021 3:40,5,77843380,"<p>The solution to this error was that you had to remove <code>.gradle</code> folder in your home directory <code>C:\Users\{username}\</code>. Now something must happened during my installation that caused this error to happen. Everything builds correctly now.</p>
<p>NOTE: You have to reboot your computer before removing it because it won't let you remove unless you reboot.</p>
",15027302,0,0,,,Error
234,4751,77928392,How to Visualize the Safety_Lidar/Depth camera data on RViz from Docker Container,|ros|docker-swarm|robotics|docker-container|,"<p>I am currently collaborating with a mobile robot, as depicted in Image 1.</p>
<p><a href=""https://i.stack.imgur.com/YlRKW.jpg"" rel=""nofollow noreferrer"">Image 1</a></p>
<p>The robot is equipped with two sick_safety scanners, positioned at the front and rear. The data nodes/topics are publishing from a Docker container. To access the robot's PC, I utilize SSH by entering 'ssh username@ip.' Once inside the container, I can see the topic, and echoing the topic displays the raw text data.</p>
<p>I attempted two different approaches to visualize the data on rviz:</p>
<p>Approach 1:
I opened rviz from within the container, enabling me to view the rostopic list. However, upon attempting to open rviz, an error message appeared: 'qt.qpa.xcb: could not connect to display :0' (refer to Image 2).</p>
<p><a href=""https://i.stack.imgur.com/8oKpd.jpg"" rel=""nofollow noreferrer"">Image 2</a></p>
<p>Approach 2:
I configured the IP addresses of both the robot PC and my PC. The robot was connected to a wifi-router via LAN, while my PC was connected via Wi-Fi on the same network (refer to Image 3).
<a href=""https://i.stack.imgur.com/jMgss.jpg"" rel=""nofollow noreferrer"">Image 3</a></p>
<p>Although I could see all the topics on my PC, running rviz did not display any data; there was no TF data, and the Fixed Frame option was not available (refer to Image 4).
<a href=""https://i.stack.imgur.com/Vo4Wi.jpg"" rel=""nofollow noreferrer"">Image 4</a></p>
<p>Looking ahead, if I plan to integrate a RealSense camera with the robot, how can I visualize the camera data on rviz, whether on the robot's pc (GUI Tool) or my PC's?</p>
<p>Thank you.</p>
",2/2/2024 16:10,,28,0,0,0,,23337932,,2/2/2024 15:54,0,,,,,,,,Incoming
235,4778,78212426,Arduino ESP-01 doesn't works,|arduino|robotics|,"<p>A fatal esptool.py error occurred: Failed to connect to ESP8266: Timed out waiting for packet header</p>
<p>I have this code and this components:
<a href=""https://i.stack.imgur.com/i4Sf8.png"" rel=""nofollow noreferrer"">enter image description here</a><a href=""https://i.stack.imgur.com/scuB4.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I don't know what I should do any suggestions?</p>
<p>I'm just trying to connect and upload this simple code to the esp via FTDI usb</p>
",3/23/2024 20:44,,19,0,1,0,,23759528,,3/23/2024 20:39,1,,,,,,137888784,"If I'm asking it's because I haven't gotten the answer on Google, It's common sense.",Connections
236,2631,41599283,Translation from Camera Coordinates System to Robotic-Arm Coordinates System,|python|opencv|coordinates|robotics|coordinate-transformation|,"<p>I am new in robotics and I am working on a project where I need to pass the coordinates from the camera to the robot.</p>

<p>So the <strong>robot</strong> is just an arm, it is then stable in a <strong>fixed position</strong>. I do not even need the 'z' axis because the board or the table where everything is going on have always the same 'z' coordinates.</p>

<p>The <strong>webcam</strong> as well is always in a <strong>fixed position</strong>, it is not part of the robot and it does not move.</p>

<p>The <strong>problem</strong> I am having is in the conversion from 2D camera coordinates to a 3D robotic arm coordinates (2D is enough because as stated before the 'z' axis is not needed as is always in a fixed position).</p>

<p>I'd like to know from you guys, which one is the <strong>best approach</strong> to face this kind of problems so I can start to research.</p>

<p>I've found lot of information on the web but I am getting a lot of confusion, I would really appreciate if someone could address me to the right way. </p>

<p>I don't know if this information are useful but I am using OpenCV3.2 with Python</p>

<p>Thank you in advance</p>
",42746.81528,,1631,1,3,0,,5577396,"Rome, Metropolitan City of Rome, Italy",42326.65625,102,41604196,"<p>Define your 2D coordinate on the board, create a mapping from the image coordinate (2D) to the 2D board, and also create a mapping from the board to robot coordinate (3D). Usually, robot controller has a function to define your own coordinate (the board).</p>
",1637710,0,0,70421380,2D coordinates are 3D with z = 0. Then you can be interested in a [forward 3d transfomation matrix](https://studywolf.wordpress.com/2013/08/21/robot-control-forward-transformation-matrices/),Actuator
237,4772,78120537,Drawing a circle in a 3D enviroment using a 3DOF arm,|arduino|robotics|,"<p>I am a second year robotics student and have been giving a task to draw a circle in a 3D enviroment so far I haven't had the best results I used the 2D equation for a circle in the X and Z axis and at the moment I have set the Y axis to 0 and somehow the Y axis is moving.</p>
<pre><code>#include &lt;Servo.h&gt;

Servo motor1;
Servo motor2;
Servo motor3;

//add variables for lengths - l1, l2 and l3;
float l1 = 105;
float l2 = 150;
float l3 = 150;

float radToDeg = (180 / 3.14);

void ik(float x, float y, float z) {

  // accounts for the vertical displacement along Z
  z = z - l1;

  float theta1, theta2, theta3;

  //calculate the angle of the base joint using just the x and y value
  // Add solution for Theta 1 (base)
  theta1 = atan2(y, x);

  // Calculate distance from the base to the end effector projection in the xy-plane
  // Add solution for the d variable
  float d = sqrt(sq(x) + sq(y));

  //  float a = d / 2;
  //
  //  // Calculate theta2 (shoulder)
  //  // Add solution for Theta 2
  //  float angle1 = acos(a/ l2);
  //  float angle2 = acos(x/ d);
  //  theta2 = angle1 + angle2;
  //
  //  // Add solution for Theta3 (arm)
  //  theta3 = 2*((3.14*1.5)-angle1);

  // Calculate theta3
  theta3 = acos((sq(d) + sq(z) - sq(l2) - sq(l3)) / (2 * l2 * l3));
  // Calculate theta2
  float b = atan2(l3 * sin(theta3), l2 + l3 * cos(theta3));
  theta2 = atan2(-z, d) - b;

  theta1 = theta1 * radToDeg;
  theta2 = theta2 * radToDeg;
  theta3 = theta3 * radToDeg;
  theta2 = theta2 + 90;

  theta3 = 180 - theta3;
  Serial.print(theta1); Serial.print(&quot;   &quot;);
  Serial.print(theta2); Serial.print(&quot;   &quot;);
  Serial.print(theta3); Serial.print(&quot;   &quot;);
  Serial.println();

  //plug theta values into the motor write function
  motorWrite(theta1, theta2, theta3);
}

void motorWrite(float t1, float t2, float t3) {
  //offset the angles to go from -90 and 90 to be between 0 and 180 instead
  t1 += 90;
  t2 += 90;
  motor1.write(t1);
  motor2.write(t2);
  motor3.write((90 + t3 - t2)); // make link 3 dependent on theta 2
}

void setup() {
  motor1.attach(2);
  motor2.attach(3);
  motor3.attach(4);
}

void loop() {
  ik(sin(millis()/100) * 200, 0, cos(millis()/100) * 200);
}
</code></pre>
<p>Here is the code feel free to change it or do anything to it.</p>
<p>PSA I have asked my lectures and they are okay with me uploading this and asking questions as it is a good learning experience. YOU ARE NOT DOING MY HOMEWORK.</p>
<p>I tried the code above and this is a video of what has happend.
Here is a youtube link to what is happening at the moment:
<a href=""https://youtube.com/shorts/uRWFePCdgaA?feature=share"" rel=""nofollow noreferrer"">https://youtube.com/shorts/uRWFePCdgaA?feature=share</a></p>
",45358.43403,,13,0,1,0,,23551898,,45358.41944,0,,,,,,137740697,"Please clarify your specific problem or provide additional details to highlight exactly what you need. As it's currently written, it's hard to tell exactly what you're asking.",Actuator
238,3642,60347644,Getting Robot to Point to Correct Direction,|python|opencv|arduino|path-finding|robotics|,"<p>I am commanding a robot from a base station with radio. Base station takes location/orientation information from an overhead camera using the AR tag on the robot (with openCV). Moreover, base calculates the path robot should take to reach the target from location information (A* with each grid being 30 by 30 pixels in camera). My robot can only turn left/right (on its central point) and go forward/backward. Robot includes an Arduino Uno with two Lego NXT motors.</p>

<p>I use the following code to get robot to point at right direction. However, when the robot gets close to the angle that it is supposed to travel to, instead of stopping an going forward it tries to fix its orientation infinitely.</p>

<pre><code>    def correctOrientation(self, rx: int, ry: int):
        #returns direction robot needs to point.
        direction = self.getDirection((self.sx, self.sy), (rx, ry))
        #method to stop robot.
        self.comms.stop()

        anglediff = (self.angle - direction + 180 + 360) % 360 - 180

        while not (abs(anglediff) &lt; 15):
            #Decides which way to turn.
            if self.isTurnLeft(self.angle, direction):
                self.comms.turnLeft()
            else:
                self.comms.turnRight()
            #Put sleeps because there is a delay in camera feed. Allows it to get the location right
            time.sleep(0.3)
            self.comms.stop()
            #Updates position
            self.getPos()
            time.sleep(1)
            #Calculates orientation of robot and updates it
            self.angle = self.calcOrientation()
            anglediff = (self.angle - direction + 180 + 360) % 360 - 180
            print(anglediff)
            time.sleep(1)

</code></pre>

<p>My helper function that are used. I calculate orientation of robot by using two points known on the robot and drawing a line in between those two point. Hence, line becomes parallel with th orientation.</p>

<pre><code>    def isTurnLeft(self, angle, touchAngle):
        diff = touchAngle - angle
        if diff &lt; 0:
            diff += 360
        if diff &lt; 180:
           return False
        else:
            return True

    def calcOrientation(self) -&gt; float:
        return self.getDirection(self.marker[0], self.marker[3])

    def getDirection(self, source: Tuple[int], target: Tuple[int]) -&gt; float :
        return (math.degrees(math.atan2(target[1] - source[1], target[0] - source[0]))+360)%360
</code></pre>

<p>I can't figure out if my code is problematic in logic. If so what can I do about it? If code is fine and the problem is the delay/setup of the system, what are the other ways I can control the robot?</p>

<p>Thank you for the help.</p>
",43882.99306,60365777,456,1,4,1,0,3970582,,41874.475,28,60365777,"<p>Solved the problem by changing the image library for AR tag recognition. We were using 2 tags for one robot. It is significantly slower and fail prone to detect two tags. Updated it to only have one tag. Moreover, switched from angle based calculations to vector based which is way simpler to understand.</p>
",3970582,2,0,106753274,"Does it move back and forth to try and fix it's location? If so, then maybe it's overshooting the 15 degrees difference. Try sleeping a shorter time of rotation. Better still: make the rotation time dependent of the anglediff. Larger anglediff, rotate longer.",Coordinates
239,4368,73519254,How to activate motors with python3,|python|automation|raspberry-pi|robotics|,"<p>I am thinking of a way to pack my bag for work in the morning, but I'm trying to automate it with python3, raspberry pi and motors, but I don't know how to program something to make them work. Does anyone know how to do this? I think it might include circuitpython.</p>
<p>Thanks for your time,
Gomenburu</p>
",44801.57708,73594428,38,1,2,0,,17985160,,44581.59444,10,73594428,"<p>I figured out the answer. It is explained <a href=""https://www.tomshardware.com/how-to/dc-motors-raspberry-pi-pico"" rel=""nofollow noreferrer"">here</a>. It uses a Raspberry Pi Pico and MicroPython to activate the motors.</p>
",17985160,0,0,129828224,"Ok, thanks. I might try this with motors instead then, because I heard that is easier",Actuator
240,3647,60448037,Sending ROS messages through a cv2 python script,|python|ros|opencv|robotics|markers|,"<p>I am trying to implement a robotic solution, where the user will click on a point through a camera feed of an area and the mobile 4 wheel robot will path its way to that location.
I have created the part of translating video pixel coordinates to ground coordinates through the use of the homograph transformation and would like to implement the part of sending the ground coordinates to RViz.
The part of calculating the ground coordinates is shown bellow:</p>

<pre><code>global h
font = cv2.FONT_HERSHEY_SIMPLEX #Loading neccessary fonts

with open(""save.h"", ""rb"") as f:
    h = load(f)


print ""Homographic matrix loaded successfully.""

def draw_circle2(event,x,y,flags,param):
    global h,mouseX,mouseY, num_of_points, complete,np_coord_on_screen,np_coord_on_ground, myImage, emptyFrame, coord_on_screen, coord_on_ground

    if event == cv2.EVENT_LBUTTONDBLCLK:

        a = np.array([[x, y]], dtype='float32')
        a = np.array([a])
        pointOut = cv2.perspectiveTransform(a, h) #Calculation of new point location

        loc_pointOut = tuple(pointOut)

        pointOut=(loc_pointOut[0][0][0],loc_pointOut[0][0][1]) #Point on ground

        print ""Current Location: ""+str(pointOut)
        cv2.imshow('Video',emptyFrame) #Showing emptyFrame
        cv2.circle(myImage,(x,y),4,(255,0,0),-1) #Draw a circle on myImage
        cv2.putText(myImage,(str((round(pointOut[0],2)))+"",""+str(round(pointOut[1],2))), (x-5,y-15),font, 0.4,(0,255,0)) #Draw the text
        cv2.imshow('Video',myImage) #Showing resulting myImage

        myImage = emptyFrame.copy()



# Initial code 
# Showing first frame on screen
raw_input(""Press any key..."")
clear_all()
cv2.namedWindow('Video') #Naming the window to use.
cv2.setMouseCallback('Video',draw_circle2) #Set mouse callBack function.
ret, frame = cap.read() #Get image from camera
if (ret): #If frame has image, show the image on screen
    global myImage, emptyFrame
    myImage = frame.copy()
    emptyFrame = frame.copy()
    cv2.imshow('Video',myImage) # Show the image on screen



while True:  # making a loop

    if cv2.waitKey(1) &amp; 0xFF == ord('q'):
        break
    if (cv2.waitKey(1) &amp; 0xFF == ord('c')):
        #Deleting points from image

        cv2.imshow('Video',emptyFrame) #Show the image again, deleting all graphical overlays like text and shapes
        coord_on_screen = []        #Resetting coordinate lists
        coord_on_ground=[]          #Resetting coordinate lists
    if (cv2.waitKey(1) &amp; 0xFF == ord('s')):
        cap.release()
        cv2.destroyAllWindows()
        init()


# When everything done, release the capture
cap.release()
cv2.destroyAllWindows()
</code></pre>

<p>So, pointOut is the point on the ground (example: (2,4) m)
I need some directions on how to implement the creation of a node and the transmission of the pointOut Marker message to a ROS topic in my code.</p>

<p>My configuration is ROS Kinetic Kame, Python 2.7 </p>
",43889.36319,,182,0,4,0,,12969878,,43887.9875,10,,,,,,106950925,"I have tried your suggestion and it turns out it is correct.
It seems it was very simple after all. For some reason though, the first time I tried I got several errors making me believe it couldn't be achieved in such a simple manner.",Remote
241,4112,69371534,What PointCloud library would be the best to start working with for a beginner?,|computer-vision|point-clouds|robotics|,"<p>I am looking to learn to work with a pointcloud library. As a beginner, what would be the best library to use? I have heard about PCL, CGAL, PDAL etc. but am not sure which one to use. My main interests are at the intersection of classical computer vision, deep learning and robotics. I am using a Windows machine without a GPU (I know that using a Linux Machine with a GPU would be ideal, but at the moment, this is what I have). Also, I have moderate but not expert experience with C++.</p>
",44468.27847,,251,0,2,1,,14897937,"Princeton, NJ, USA",44192.84514,37,,,,,,122962978,"Each of them is great. Like every tool, it depends on what you need and with what. For difficult level, PDAL -> PCL -> CGAL. PDAL is probably the easiest but is fewer C++. PCL compare to the other 2 is limited (More you can do with PDAL or CGAL) but don't get me wrong is still a great tool. I would say CGAL is the most advanced but has also a higher starting point and is harder to use than the other two.",Specifications
242,2398,33271330,How to make a robot to play a video game by Python in Mac OSX?,|python|python-2.7|python-3.x|machine-learning|robotics|,"<p>I am doing research on machine learning, and want to apply an algorithm to training a robot to play a video game. I may want to use Python to implement the agent(controller) so I need a way to take screen shots of the game from python, and send keyboard events from python to the game. My OS is Mac OS X 10.10.</p>

<p>I am new to Python. Is it possible for Python to do so? If so, how to do it?
Thanks. </p>
",42299.00625,,345,1,2,-3,,2261693,,41373.52222,60,33271897,"<p>I'm not an expert in this field but I think you can look at some of the heuristic search algorithms like Genetic Algorithm (GA).</p>

<p><a href=""https://en.wikipedia.org/wiki/Genetic_algorithm"" rel=""nofollow"">https://en.wikipedia.org/wiki/Genetic_algorithm</a></p>

<p>Also, every games are different for instance the super mario vs chess game. I'm not sure what game you are after but GA has been successfully implemented to play Super Mario smartly. </p>

<p>I think for a start you can develop an AI to play the game first, then meantime collect some data for analytics so that it can play better the next time? Again, I dont know what kind of game you are after so it is  difficult for me to contribute ideas to help you.</p>
",4014291,0,0,54344177,I think your in a bit over your head. try learning a little more about python first. And try looking at other program source code to learn how to make the robot play your game.,Other
243,2136,27412408,How to move a robot a certain distance and still manage sensors?,|c++|arduino|robotics|,"<p>I have a robot with encoder feedback, and i want to use functions that move a robot a certain distance and turn it a certain angle, so for example:</p>

<pre><code>void loop(){

  if (Obstacle==false){  // if no obstacles

      forward(1000);  // move forward 1000 mm
      backward(1000); // move backward 1000 mm

      //...  
  }
}
</code></pre>

<p>Forward Function:</p>

<pre><code>void forward(int distance){  // distance in mm

   int desiredRotationNumber= round(distance/circumference) ; 

   int desiredTicks = encoderResolution * desiredRotationNumber;

   if (counter &lt; desiredTicks) // variable counter counts the pulses from the encoder
   {  
       analogWrite(motor,255);
   } 
   else 
   {
       analogWrite (motor,0); 
   }

}
</code></pre>

<p>The problem is that if i use the condition ""if"" my forward function will execute only once and then the program jumps to the next function, but if i use the ""while loop"" my move functions will execute correctly but i won't be able to manage sensors or anything.  </p>
",41983.95556,27412489,1627,1,0,0,,2729596,,41515.58958,28,27412489,"<p>What you probably want is to cut your moves in increments, and check the sensors in between each of these increments:</p>

<pre><code>while (distance &gt; 0 &amp;&amp; !Obstacle){
    forward(step);
    distance-=step;
    check_sensors();
}
</code></pre>

<p>With multithreading, you could make those operations (moving and sensing) work asynchronously, and use some kind of event posting to warn each thread of a change. Here we're simulating that functionality by interwinding the tasks (you could also look into coroutines for a similar, yet much more effective idea).</p>
",1769720,2,2,,,Incoming
244,2137,27432813,How to make a robot navigate a maze?,|python|robotics|maze|robot|myro|,"<p>I'm using the Myro library with the Python language.  I've had some weird results.</p>

<p>My idea was to call the getObstacle sensors.  </p>

<pre><code>left = getObstacle(0)
center = getObstacle(1)
right = getObstacle(2)
</code></pre>

<p>I want the robot to move forward as long as the center obstacle sensor is less than or equal to 4500.</p>

<p>If the right obstacle sensor on the robot has a higher reading than the left obstacle sensor, I want it to turn left.</p>

<p>Otherwise turn right.</p>

<p>Here are my attempts on youtube</p>

<p><a href=""https://www.youtube.com/watch?v=U5_sppAMe_8"" rel=""nofollow"">Attempt 1</a></p>

<p><a href=""https://www.youtube.com/watch?v=h1pWAmf_7xA&amp;feature=youtu.be"" rel=""nofollow"">Attempt 2</a></p>

<p>I'm going to submit 3 different variations of my code</p>

<pre><code>def main():
    setIRPower(135)
    while True:
        left = getObstacle(0)
        center = getObstacle(1)
        right = getObstacle(2)
        # 2 feet per 1 second at 1 speed
        if (center &lt;= 4500):
            forward(0.5, 0.2)   
            wait(0.4)   
        elif (right &gt; left):
            turnLeft(1, .45) 
        else:
            turnRight(1, .45)



def main():
    setIRPower(135)
    while True:
        left = getObstacle(0)
        center = getObstacle(1)
        right = getObstacle(2)
        # 2 feet per 1 second at 1 speed
        if (center &lt;= 4500):
            forward(0.5, 0.2)   
            wait(0.3)   

        elif(right &gt; center and left):
            turnLeft(1, .45) 
        elif(left &gt; center and right):
            turnRight(1, .45)
</code></pre>

<p>The latest one I'm working with</p>

<pre><code>def main():
    setForwardness(1)
    setIRPower(135)
    while True: 
        left = getObstacle(0)
        center = getObstacle(1)
        right = getObstacle(2)
        if (center &lt;= 5000 and left &lt;= 5000 and right &lt;= 5000):
            forward(0.5, 0.2)
            wait(.3)
        elif(right&gt;left):
            turnLeft(1, 0.45)
        else:
            turnRight(1, 0.45)
</code></pre>

<p>Is there any way I can improve my code? I want it to turn left and right at the correct times. </p>

<p>Should I be using different logic altogether?  Any help would be appreciated. </p>
",41984.89931,,1471,0,3,2,,3577397,,41756.09375,141,,,,,,43306882,"lol.. I just realized I said ""Are you kidding me?!""  In the first vid.  I'm a tad frustrated lol :P  This type of programming is like pulling teeth for me, and I'm fairly new to both programming and python in general.",Moving
245,4139,69676420,Detect when 2 buttons are being pushed simultaneously without reacting to when the first button is pushed,|c++|robotics|,"<p>I'm programming a robot's controller logic. On the controller there is 2 buttons. There is 3 different actions tied to 2 buttons, one occurs when only the first button is being pushed, the second when only the second is pushed, and the third when both are being pushed.</p>
<p>Normally when the user means to hit both buttons they would hit one after another. This has the consequence of executing a incorrect action.</p>
<p>Here is part of the code.</p>
<pre class=""lang-cpp prettyprint-override""><code>while (true)
{
    conveyor_mtr.setVelocity(22, pct);

    if (Controller1.ButtonL2.pressing() &amp;&amp; Controller1.ButtonL1.pressing())
    {
      conveyor_mtr.spin(fwd); // action 1
    }
    else if (Controller1.ButtonL2.pressing())
    {
      backGoalLift.setAngle(3); // action 2
    }
    
    else if (Controller1.ButtonL1.pressing())
    {
      backGoalLift.setAngle(55); // action 3
    }
    else
    {
      conveyor_mtr.stop(hold);
    }
    task::sleep(20); //ms
}
</code></pre>
",44491.49375,69676660,600,1,2,1,,12650035,,43834.08403,19,69676660,"<p>You could use a short timer, which is restarted every time a button press is triggered. Every time the timer expires, you check all currently pressed buttons. Of course, you will need to select a good timer duration to make it possible to press two buttons &quot;simultaneously&quot; while keeping your application feel responsive.</p>
<p>You can implement a simple timer using a counter in your loop. However, at some point you will be happier with an event based architecture.</p>
",17219185,1,0,123158313,"The term you're looking for is ""debouncing""",Remote
246,3284,54385059,Open-loop or Closed-loop (reactive) Path planning?,|robotics|planning|mpc|,"<p>When we do path planning for collision avoidance, we can realize it open-loop or closed-loop.
The open-loop method is to use an inherent simplified model, say, Bicycle model, for example, and propagate the system forward with an optimal input by designing a controller (MPC, or others). However, the states of the simplified model may surely diverge from the real ones due to modelling error as time goes by, hence we need to re-initialize the states of the path planner with the real system states (obtained via measurement or estimation). In this way, we have closed-loop planning. The question is that what frequency of this kind of re-initialization occurs? High re-initialization frequency makes the planning more accurate, but in the mean-time, it may cause zigzag sew-shape reference for the lower-level controller.   </p>
",43492.18125,54447882,755,1,0,0,,9438864,,43162.68472,35,54447882,"<p>The answer to this is very system dependent. You are correct in saying that the open loop system is non-realizable. Planning/control is usually done in two stages.</p>

<p>1) Trajectory Generation: This is usually done predictivly or in open loop (the P in MPC). Depending on the ability of the lower level control, this need not be done too frequently. For example, if trajectory execution deviates from your planned beyond some threshold (or beyond stability guarantees) you would then have to re-plan.</p>

<p>2) Trajectory following/execution: Given an nominal trajectory (including nominal open loop controls), a lower level controller attempts to follow this as closely as possible. This would include a stabilizing controller such as LQR or something similar. </p>

<p>The key to understanding what ""too fast"" is for re-planning is how much your system drifts over time and what kind of safety guarantees you want to produce. For example, if you allow for a 5cm buffer around obstacles in your open loop plan then an appropriate time to re-plan would be when the robot deviates from the trajectory (in R3 for example) by some threshold less than 5cm. If you re-plan any later than that, you cannot guarantee that your robot will not collide with the static obstacles in the environment. </p>

<p>Clearly, this is driven by the accuracy of your model as well as how good a job your low level control does in following that trajectory. Ideally, if your model is reasonably accurate and if your low level control is very good, then no re-planning is necessary (assuming a static environment). </p>
",3194808,1,1,,,Moving
247,2882,45941162,How would one connect an SQL db securely to an external client?,|python|database|web|iot|robotics|,"<p>I'm attempting to connect a database, located on a web server, to a robot but I do not know how to connect the database to the robot. I would like the robot to run SELECT and UPDATE queries from the robot. The other issue is that I do not intend on using C-languages or Java; I plan on using python in the main control system.</p>

<p>I do know:
PHP
VBScript
Batch
Python</p>

<p>If anyone knows how to connect the DB to a bot it would be a great help.</p>
",42976.58472,,353,1,0,-1,,8517235,,42972.62153,7,45941312,"<p>So basically how to connect to an SQL DB in python? I'm working on a virtual bot right now doing the same thing. Look into the module , SQL-connector!<br> <a href=""http://www.mysqltutorial.org/python-connecting-mysql-databases/"" rel=""nofollow noreferrer"">http://www.mysqltutorial.org/python-connecting-mysql-databases/</a><br>
You would start with creating a config.ini with your credentials<br></p>

<pre><code>[mysql]
host = localhost
database = python_mysql
user = root
password =
</code></pre>

<p>Read Config.ini and return a dictionary<br></p>

<pre><code>from configparser import ConfigParser 
def read_db_config(filename='config.ini', section='mysql'):
    """""" Read database configuration file and return a dictionary object
    :param filename: name of the configuration file
    :param section: section of database configuration
    :return: a dictionary of database parameters
    """"""
    # create parser and read ini configuration file
    parser = ConfigParser()
    parser.read(filename)

    # get section, default to mysql
    db = {}
    if parser.has_section(section):
        items = parser.items(section)
        for item in items:
            db[item[0]] = item[1]
    else:
        raise Exception('{0} not found in the {1} file'.format(section, filename))

    return db
</code></pre>

<p>and connect to MYSQL database<br></p>

<pre><code>from mysql.connector import MySQLConnection, Error
from python_mysql_dbconfig import read_db_config


def connect():
    """""" Connect to MySQL database """"""

    db_config = read_db_config()

    try:
        print('Connecting to MySQL database...')
        conn = MySQLConnection(**db_config)

        if conn.is_connected():
            print('connection established.')
        else:
            print('connection failed.')

    except Error as error:
        print(error)

    finally:
        conn.close()
        print('Connection closed.')


if __name__ == '__main__':
    connect()
</code></pre>

<p>and update statement would look like the following<br></p>

<pre><code>def update_book(book_id, title):
    # read database configuration
    db_config = read_db_config()

    # prepare query and data
    query = """""" UPDATE books
                SET title = %s
                WHERE id = %s """"""

    data = (title, book_id)

    try:
        conn = MySQLConnection(**db_config)

        # update book title
        cursor = conn.cursor()
        cursor.execute(query, data)

        # accept the changes
        conn.commit()

    except Error as error:
        print(error)

    finally:
        cursor.close()
        conn.close()


if __name__ == '__main__':
    update_book(37, 'The Giant on the Hill *** TEST ***')
</code></pre>
",4965901,0,1,,,Remote
248,4784,78227765,I want to use 3.10 python to take input from leap motion sensor. Is there any useful module or library for this purpose?,|python-3.x|arduino|sensors|robotics|leap-motion|,"<p>I have been trying to use python for using leap motion sensor. But i am getting the libraries but they are outdated not compatible with the latest python. Is there any python library or arduino library for this purpose?</p>
<p>I want libraries to get input from leap motion sensor and control my soft robot through it.</p>
",45377.81597,,16,0,0,-1,,23824099,,45377.80972,0,,,,,,,,Specifications
249,3933,65313003,Differentiable Signed Distance Function for Isosceles Triangle (Field of View),|math|optimization|graphics|computer-vision|robotics|,"<p>I'm trying to find a signed distance function (SDF) for a simple 2D isoscele triangle, which represents the Field of View of a robot / camera. Ideally the SDF is also differentiable.</p>
<p>So far I have tried the basic 2D SDF commonly found in shaders, but this is not applicable to my problem, as it involves calculation of non-differentiable functions.</p>
<p>Particularly I'm looking for the following (compare <a href=""https://goldberg.berkeley.edu/pubs/Patil-ICRA2014-BSP-Sensing-Discontinuity-final.pdf"" rel=""nofollow noreferrer"">https://goldberg.berkeley.edu/pubs/Patil-ICRA2014-BSP-Sensing-Discontinuity-final.pdf</a>):</p>
<p><a href=""https://i.stack.imgur.com/i8Q9c.png"" rel=""nofollow noreferrer"">SDF</a></p>
<blockquote>
<p>Relationship between measurements and signed distance to valid sensing region: (a) The expected position p^xt of the robot or object lies outside the field of view Π (shown in yellow) of a sensor, corresponding to a positive signed distance, where no measurements are obtained. The normal vector n indicates the direction of closest approach.</p>
</blockquote>
<p>Does anyone have any pointers on how to compute this?</p>
",44180.84167,,140,0,0,2,,14832680,,44180.83333,2,,,,,,,,Incoming
250,2628,41556912,How to publish a `geometry_msgs/PoseArray` from the command line?,|ros|robotics|,"<p>Can anybody give me an example of geometry_msgs/PoseArray message using rostopic pub? I keep on getting errors when i try and interpret the syntax from the ROS documentation, a solid example would be really helpful.</p>
",42744.87847,,5211,1,3,2,,6669999,"Cumbria, United Kingdom",42585.01319,58,41590025,"<p>Do you mean something like this:</p>

<pre><code>rostopic pub /my_topic geometry_msgs/PoseArray ""{header: {frame_id: 'base_frame'}, poses: [{position: {x: 1.0, y: 0.0, z: 0.0}, orientation: {x: 0.0, y: 0.0, z: 0.0, w: 1.0}}, {position: {x: 1.1, y: 0.0, z: 0.0}, orientation: {x: 0.0, y: 0.0, z: 0.0, w: 1.0}}]}""
</code></pre>

<p>This will publish a <code>PoseArray</code> message containing two poses to the topic <code>my_topic</code>. Furthermore, if you are using bash I believe you can auto-complete the message by hitting tab. </p>
",4788274,1,0,70354829,Clearly you know nothing about ROS. This is not a debugging question. Why don't you pipe down and look for verification points on another thread?,Error
251,2379,32941468,horizontal acceleration measurement in self-balancing 2-wheel vehicles?,|robotics|kalman-filter|inertial-navigation|,"<p>it's now the standard practices to fuse the measurements from accelerometers and gyro through Kalman filter, for applications like self-balancing 2-wheel carts:   for example: <a href=""http://www.mouser.com/applications/sensor_solutions_mems/"" rel=""nofollow"">http://www.mouser.com/applications/sensor_solutions_mems/</a></p>

<p>accelerometer gives a reading of the tilt angle through arctan(a_x/a_y).  it's very confusing to use the term ""acceleration"" here, since what it really means is the projection of gravity along the devices axis (though I understand that , physically, gravity is really just acceleration ).  </p>

<p>here is the big problem: when the cart is trying to move, the motor drives the cart and creates a non-trivial acceleration in horizontal direction, this would make the a_x no longer a just projection of gravity along the device x-axis. in fact it would make the measured tilt angle appear larger. how is this handled? I guess given the maturity of Segway, there must be some existing ways to handle it.  anybody has some pointers?</p>

<p>thanks
Yang</p>
",42282.20208,32950746,93,1,0,0,,933882,,40794.06806,203,32950746,"<p>You are absolutely right. You can estimate pitch and roll angles using projection of gravity vector. You can obtain gravity vector by utilizing motionless accelerometer, but if accelerometer moves, then it measures gravity + linear component of acceleration and the main problem here is to sequester gravity component from linear accelerations. The best way to do it is to pass the accelerometer signal through Low-Pass filter.
Please refer to 
<a href=""http://www.kircherelectronics.com/blog/index.php/11-android/sensors/8-low-pass-filter-the-basics"" rel=""nofollow"">Low-Pass Filter: The Basics</a> or 
<a href=""http://Android%20Accelerometer:%20Low-Pass%20Filter%20Estimated%20Linear%20Acceleration"" rel=""nofollow"">Android Accelerometer: Low-Pass Filter Estimated Linear Acceleration</a>
 to learn more about Low-Pass filter.</p>

<p><a href=""http://www.codeproject.com/Articles/729759/Android-Sensor-Fusion-Tutorial"" rel=""nofollow"">Sensor fusion algorithm</a> should be interesting for you as well.</p>
",3642042,0,0,,,Actuator
252,1625,14512520,"Making a half-elliptical path in MATLAB from three input points through 3D space, to be fed into Visual Studio and ultimately a robotic arm",|matlab|math|geometry|robotics|coordinate-systems|,"<p>First, a little bit about my set up: I have a robotic arm into which points and movement patterns can be programmed into. The goal is to get it to move in certain ways based on the inputs that we put into it.</p>

<p>On the same table as the robotic arm is another arm that moves under human power and can sense where it is in space. The two arms share a coordinate system already, but I am having trouble with a particular calcuation that is giving me a headache.</p>

<p>The current goal is specifically to take three points with the sensing arm and then translate that into a half-ellipse arc that travels through the three of them. This arc should start at the first point, reach apex at the second, and finish on the third, traveling through all three dimensions to do so if necessary. The three points feed through Visual Studio, then are put into MATLAB and turned into an array of 99 xyz coordinates.</p>

<p>We have every step working except for the MATLAB function. The points are nowhere near the actual coordinates, though the relationship between them seems okay. Can anyone tell me what is wrong with the code? </p>

<p>Here is what we have so far:</p>

<pre><code>function P = getEllipticalPath(h0,hl,hr)
%define center of ellipse
center = (hl+hr)/2;

%want everything centered at (0,0,0)
h0 = h0 - center;
hl = hl - center;
hr = hr - center;

%xz plane direction between h0 and center
d = [h0(1),0,0]/49;

%now get the major/minor axis of the ellipse
%minor axis(along z axis)
a = h0(3);
b = hr(2);%arbitrary with hr

%set increment of orbit
incr = (pi)/99;

%by symmetry, only need to compute first half of orbit
%allocation
Pf = zeros(99,3);
for i=1:99
    if(i &lt; 50)
       newpt = [0, b*cos(i*incr), a*sin(i*incr)] + (i*d);
    else 
        newpt = [0, b*cos(i*incr), a*sin(i*incr)] + (99 - i)*d;
    end 

    Pf(i,:) = [newpt(1), newpt(2), newpt(3)];
end
P = addOffset(Pf,-h0);

end

%simply adds a fixed translational offset to the given list of vectors
%(n*3 matrix). Assumes a matrix that is longer than 3.

function P = addOffset(points,offset)
newpoints = zeros(length(points),3);
for i=1:length(points);

    newpoints(i,:) = points(i,:) + offset;
end
P = newpoints;
end
</code></pre>

<p>EDIT: Forgot input-output information; here is an example:</p>

<p>Input:</p>

<pre><code>&gt;&gt; h0 = [-10.06   14.17   0.53 ]

h0 =

  -10.0600   14.1700    0.5300

&gt;&gt; hl = [-45.49   7.87   1.07 ]

hl =

  -45.4900    7.8700    1.0700

&gt;&gt; hr = [-4.52   -20.73   1.02 ]

hr =

   -4.5200  -20.7300    1.0200

&gt;&gt; P = getEllipticalPath(h0,hl,hr)
</code></pre>

<p>Output:</p>

<p><img src=""https://i.stack.imgur.com/f8Unw.png"" alt=""This is very different from ecpectations. We would expect it to start on hl, move in increments through h0 and end on hr, but instead we get this.""></p>
",41298.96319,,377,1,5,4,,2009238,,41298.94653,6,14512847,"<p>I think you complicated things for yourself with all the offsetting and stuff. I think what you want to do is this:</p>

<ul>
<li><p>Get the center vertex C of the ellipse as (hl+hr)/2</p></li>
<li><p>Get the major axis vector B as (center-hl)</p></li>
<li><p>Get the minor axis vector A as (h0-center)</p></li>
<li><p>Get the angular delta float INCR as (pi)/99</p></li>
<li><p>Now you can calculate each vertex in PF from i = 1 to 99 as <code>center *(vertex)* + B*cos(i*incr) *(vector)* + A*sin(i*incr) *(vector)*</code></p></li>
</ul>

<p>Basically you're starting at the center, moving towards hr or hl according to the cos of the angle, and moving towards h0 as the sin of the angle, using those vectors to control how far and in which direction you move.</p>
",497106,0,0,20231773,"Can you give us some sample input, expected and actual output, please?",Coordinates
253,423,1811366,"Infinite timeouts or ""fail fast"" in custom network protocol?",|network-protocols|robotics|peripherals|,"<p>Consider custom network protocol. This custom protocol could be used to control robotic peripherals over LAN from central .NET based workstation. (If it is important, the robot is busy moving fabs in chip production environment).</p>

<ul>
<li>there are only 2 parties in conversation: .NET station and robotic peripheral board</li>
<li>the robotic side can only receive requests and send responses</li>
<li>the .NET side can only initiate requests and receive responses</li>
<li>there always should be exactly one response per request</li>
<li>the consequent requests can follow immediately one after another without waiting for response, but never exceed the fixed limit of simultaneously served requests (for example 5)</li>
</ul>

<p>I had exhaustive discussion with my friend (who owns the design, I have discussed the thing as a bystander) about all nice details and ideas. At the end of discussion we had strong disagreement about missing timeouts. My friend's argument is that software on both sides should wait indefinitely. My argument was that timeouts are always needed by any network protocol. We simply could never agree. </p>

<p>One of my reasoning is that in case of any failure you should ""fail fast"" whatever cost, because if failure already occurred anyway, cost of recovery continues to grow proportionally to time spent to receive an info about failure. Say after 1 minute on LAN you definitely should stop waiting and just invoke some alarm.</p>

<p>But his argument was that recovery should include exactly the repairing of what failed (in this case recovery of network connection) and even if it takes to spend hours to figure out that network was lost and fixed, the software should just continue transparently running, immediately after reconnecting the LAN cables.</p>

<p>I would never seriously think about timeless protocols, until this discussion. </p>

<p><strong>Which side of argument is right ? The ""fail fast"" or ""never fail"" ?</strong></p>

<p>Edit: Example of failure is loss of communication, normally detected by TCP layer. This part was also discussed. In case of TCP layer returning error, the higher custom protocol layer will retry sends and there is no argument about it. The question is: for how long to allow the lower level to keep trying ?</p>

<p>Edit for accepted answer:
Answer is more complex than 2 choices: ""<em>The most common approach is never give up connection until actual attempt to send fails with solid confirmation that connection is long lost. To calculate that connection is long lost use heartbeats, but keep age of loss for this confirmation only, not for immediate alarm</em>"".</p>

<p>Example: When having telnet session, you can keep your terminal up forever and you never know if in between hitting Enter there were failures detectable by lower level routines.</p>
",40145.11111,1811461,305,2,0,2,0,,,,,1811431,"<p>In the scenario where ...</p>

<ul>
<li>Controller has sent a request</li>
<li>Robot hasn't received the request</li>
<li>Network fails</li>
</ul>

<p>... then the request has been sent, but has been lost and will never arrive.</p>

<p>Therefore, when the network is restored, the controller must resend the request: the controller cannot simply wait forever for the response.</p>
",49942,1,2,,,Remote
254,4747,77869639,Example of using TRAC_IK library to move the Fetch robot arm (without using MoveIt),|python|ros|robot|moveit|,"<p>I am having trouble using the TRAC_IK solver to move Fetch robot arm without using MoveIt.
Previously I used MoveIt, which has all the examples and code I need to run. However, I found out that TRAC_IK is more accurate with Fetch robot.</p>
<p>Does anyone have any example of using TRAC_IK in moving the arm? Also, how to publish the joint states directly to the robot?</p>
<p>I have this code here, but it doesn't work.</p>
<pre><code>#!/usr/bin/env python

import rospy
from trac_ik_python.trac_ik import IK
from sensor_msgs.msg import JointState

def calculate_joint_angles(ik_solver, pose):
    # Assuming the pose is [x, y, z, qx, qy, qz, qw]
    x, y, z, qx, qy, qz, qw = pose
    seed_state = [0] * ik_solver.number_of_joints
    return ik_solver.get_ik(seed_state, x, y, z, qx, qy, qz, qw)

if __name__ == '__main__':
    rospy.init_node(&quot;simple_disco&quot;)

    base_link = &quot;base_link&quot;
    end_link = &quot;wrist_roll_joint&quot;
    ik_solver = IK(base_link, end_link)

    # Publisher for joint states
    pub = rospy.Publisher('/joint_states', JointState, queue_size=10)

    joint_names = [&quot;torso_lift_joint&quot;, &quot;shoulder_pan_joint&quot;,
                   &quot;shoulder_lift_joint&quot;, &quot;upperarm_roll_joint&quot;,
                   &quot;elbow_flex_joint&quot;, &quot;forearm_roll_joint&quot;,
                   &quot;wrist_flex_joint&quot;, &quot;wrist_roll_joint&quot;]

    # Define your end-effector poses here
    # These need to be defined as [x, y, z, qx, qy, qz, qw]
    disco_poses = [[0.5, 0.5, 0.5, 0, 0, 0, 1],
                   [0.5, 1, 0.5, 0, 0, 0, 1],
                   [0.5, 0.5, 0.5, 0, 0, 0, 1],
                   [0.5, 1, 0.5, 0, 0, 0, 1]]  # Define your poses here

    rate = rospy.Rate(1)  # Adjust the rate as needed

    for pose in disco_poses:
        if rospy.is_shutdown():
            break

        joint_angles = calculate_joint_angles(ik_solver, pose)

        if joint_angles:
            joint_state = JointState()
            joint_state.header.stamp = rospy.Time.now()
            # joint_state.name = ik_solver.joint_names
            joint_state.name = joint_names
            joint_state.position = joint_angles

            pub.publish(joint_state)
            rospy.loginfo(&quot;Moving to pose&quot;)
        else:
            rospy.logerr(&quot;No IK solution found for the given pose&quot;)

        rate.sleep()

    rospy.loginfo(&quot;Disco sequence complete&quot;)

</code></pre>
",45314.94236,,16,0,0,0,,9596491,,43194.48611,1,,,,,,,,Actuator
255,540,2761748,Rotation Matrix calculates by column not by row,|matlab|matrix|rotation|linear-algebra|robotics|,"<p>I have a class called forest and a property called fixedPositions that stores 100 points (x,y) and they are stored 250x2 (rows x columns) in MatLab.  When I select 'fixedPositions', I can click scatter and it will plot the points.  </p>

<p>Now, I want to rotate the plotted points and I have a rotation matrix that will allow me to do that.</p>

<p>The below code should work:</p>

<p>theta = obj.heading * pi/180;
apparent = [cos(theta)  -sin(theta) ; sin(theta)  cos(theta)] * obj.fixedPositions;</p>

<p>But it wont.  I get this error.</p>

<p>??? Error using ==> mtimes
Inner matrix dimensions must agree.</p>

<p>Error in ==> landmarks>landmarks.get.apparentPositions at 22
            apparent = [cos(theta)  -sin(theta) ; sin(theta)  cos(theta)] * obj.fixedPositions;</p>

<p>When I alter forest.fixedPositions to store the variables 2x250 instead of 250x2, the above code will work, but it wont plot.  I'm going to be plotting fixedPositions constantly in a simulation, so I'd prefer to leave it as it, and make the rotation work instead.</p>

<p>Any ideas?</p>

<p>Also, fixed positions, is the position of the xy points as if you were looking straight ahead.  i.e. heading = 0.  heading is set to 45, meaning I want to rotate points clockwise 45 degrees.  </p>

<p>Here is my code:</p>

<pre><code>classdef landmarks
  properties
    fixedPositions   %# positions in a fixed coordinate system. [x, y]
    heading = 45;     %# direction in which the robot is facing
  end
  properties (Dependent)
    apparentPositions
  end
  methods
    function obj = landmarks(numberOfTrees)
        %# randomly generates numberOfTrees amount of x,y coordinates and set 
        %the array or matrix (not sure which) to fixedPositions
        obj.fixedPositions = 100 * rand([numberOfTrees,2]) .* sign(rand([numberOfTrees,2]) - 0.5);
    end
    function apparent = get.apparentPositions(obj)
        %# rotate obj.positions using obj.facing to generate the output
        theta = obj.heading * pi/180;
        apparent = [cos(theta)  -sin(theta) ; sin(theta)  cos(theta)] * obj.fixedPositions;
    end
  end
end
</code></pre>

<p>P.S. If you change one line to this: obj.fixedPositions = 100 * rand([2,numberOfTrees]) .* sign(rand([2,numberOfTrees]) - 0.5);</p>

<p>Everything will work fine... it just wont plot. </p>

<p>ans = obj.fixedPositions; ans';  will flip it to what I need to plot, but there has to be a way to avoid this? </p>
",40301.95417,2761889,1441,2,0,2,0,327502,"Cambridge, MA",40296.22708,207,2761889,"<p>I think you want to transpose the matrix before and after multiplying by the rotation. If the matrix is real numbers, you can do:</p>

<pre><code>apparent = ([cos(theta)  -sin(theta) ; sin(theta)  cos(theta)] * (obj.fixedPositions)')';
</code></pre>
",93910,3,1,,,Coordinates
256,3288,54652810,is there any method to create mapping for nao robot by python?,|python|robot|nao-robot|choregraphe|,"<p>I connect the Nao robot to my Python shell with <code>naoqi</code>, it works for some codes, now I want to do mapping for a room by Python, how can I do it without ROS, I do not have any idea.</p>
",43508.62014,,128,1,0,-2,,11002744,,43497.79583,9,54664981,"<p>Have a look at <a href=""http://doc.aldebaran.com/2-5/naoqi/motion/alnavigation.html"" rel=""nofollow noreferrer"">ALNavigation</a> it offers the method <a href=""http://doc.aldebaran.com/2-5/naoqi/motion/exploration-api.html?highlight=exploration#ALNavigationProxy::explore__float"" rel=""nofollow noreferrer"">explore</a>.</p>

<p>You can find Python script code examples for exploring and localizing <a href=""http://doc.aldebaran.com/2-5/naoqi/motion/exploration-api.html#code-samples"" rel=""nofollow noreferrer"">here</a>.</p>
",3049394,0,0,,,Moving
257,745,3372168,Where to begin with programming for robotics?,|java|.net|c|microcontroller|robotics|,"<p>Ok so i've been interested in robotics for a while and had a project in mind. Building a small remote controlled vehicle-robot/ unmanned vehicle-robot. Hopefully with the ability to read in data from sensory devices(gps,thermometer etc) and write the data to some kind of device. The idea(s) had been on the backburner for a while until i just read the following <a href=""http://www.engadget.com/2010/07/29/ask-engadget-best-robot-platform-for-under-400/#comments"" rel=""noreferrer"">article</a>.</p>

<p>So my question is this. Where should I begin. I have absolutely no experience in this at all other than a few google searches and my project idea. I would like to play around with programming the micro controller boards. I know some java .net languages and some C.</p>

<p>Any help on where to begin? </p>

<p>How do you design the robot, what steps do you go through from start to finish.</p>

<p>Thanks.</p>
",40389.58264,,33582,10,2,33,0,406777,,40389.57639,110,3372193,"<p>Microsoft have <a href=""http://www.microsoft.com/robotics/"" rel=""nofollow noreferrer"">Robotocs Developer Studio</a>.</p>
",208062,2,0,3504370,There's some crucial information missing: 1) what's your budget? 2) are you more interested in the low-level or high-level aspects of making a robot (e.g. building and programming your own motor drivers vs doing high-level stuff like [SLAM](http://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping))? 3) in what shape are your electronics skills?,Other
258,3221,52939173,RosAria node doesn’t receive messages,|message|ros|robot|,"<h1>Resources</h1>
<p>Macbook Air with Ubuntu virtual machine via VMWare<br>
Pioneer3 AT robot<br>
ROS<br>
RosAria</p>
<h1>Context</h1>
<p>I have at least most of the setup for Ros+RosAria already working:</p>
<ol>
<li>The master ROS node running on a laptop at <code>192.168.1.112</code> via <code>roscore</code></li>
<li>The RosAria node running on the robot’s built-in computer at <code>192.168.1.108</code> via <code>rosrun rosaria RosAria</code>, the port set accordingly to access the robot's motors and sensors via <code>rosparam set...</code>, <code>ROS_MASTER_URI=192.168.1.112:11311</code>, and <code>ROS_IP=192.168.1.112</code>.</li>
<li>Another terminal window on the laptop running <code>rostopic pub /RosAria/cmd_vel geometry_msgs/Twist “linear:... angular:...”</code></li>
</ol>
<p>The RosAria node running on the robot confirms it’s able to connect to the master, and if I run the command <code>rostopic list</code> on the laptop I can see <code>/RosAria/cmd_vel</code> as one of the available topics.</p>
<p>In the terminal window where I try to publish to <code>/RosAria/cmd_vel</code> I’m told that the message is sending but the robot shows no signs of having received it, neither console message nor movement of the wheels.</p>
<p>However, if I quit the RosAria node on the robot and restart it while leaving the <code>rostopic pub</code> command running on my laptop, the robot registers the message once upon startup, moves accordingly for a bit, and then does nothing.</p>
<p>I’ve also tried to <code>rostopic echo</code> other topics like battery state, motors status and pose, but I don’t receive any messages from the robot.</p>
<p>I already tried changing the parameters <code>/RosAria/TicksMM</code> and <code>/RosAria/RevCount</code> according to <a href=""https://answers.ros.org/question/91466/how-to-move-pioneer-3-at-with-rosaria/"" rel=""nofollow noreferrer"">this case</a>.</p>
<h1>Question</h1>
<p><em>Why is the Pioneer robot with RosAria running not receiving the <code>cmd_vel</code> commands from my client if it’s apparently able to communicate with the master node successfully?</em></p>
<h1>Disclaimer</h1>
<p>The project I was working on where this problem came up isn’t using the same equipment anymore, so now this question is not so urgent for me. I’m also not able to test proposed solutions in the near future since I no longer have access to that Pioneer robot. However, I’ll keep this topic unanswered in case someone else encounters this problem too.</p>
",43395.99236,,355,1,0,0,,10200417,,43320.98958,31,52973552,"<p>You need To Set ROS_IP On Robot side too to let the ROS System Knows What IP To Use as It's Network Interface</p>

<p>I myself Set This Environments Inside <code>.bashrc</code> Like This:</p>

<p><code>export ROS_IP=192.168.1.108</code></p>

<p>and then source <code>.bashrc</code>.</p>

<p>I think You can use <code>rosparam</code> Too  But  I never tested.</p>
",7350738,0,2,,,Connections
259,2402,33411393,Ratio of wheel diameter and wheel to wheel distance has any effect on alignment and desired heading of vehicle?,|algorithm|automation|robotics|encoder|,"<p>I am Using encoder to get the distance traveled and heading angle of vehicle. At turn it does not give precise angle, vehicle is turning with. In my algorithm i am using accumulation of all angle to find the total angle with respect to world(X-O-Y). </p>

<p>Does it have anything to do with Ratio of wheel diameter and wheel to wheel distance? </p>

<p>This question was raised in my mind because same algorithm worked with another hardware which has different dimension(diameter of wheel and wheel to wheel distance)and it did return precise turning angle too.</p>

<p>Will appreciate if some valuable suggestion is offered. </p>
",42306.42639,,608,1,1,0,,3431551,"Bangalore, India",41716.18542,43,33423016,"<p>The angle of a robot is dependent on the specific hardware, such as wheel size, encoder size, and vehicle width. Maybe you were lucky with the the other robot that the change in wheel diameter and wheel to wheel distance canceled each other out so the equations were the same, but normally you will not be so lucky</p>

<p><a href=""https://i.stack.imgur.com/aZgIf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aZgIf.png"" alt=""![enter image description here""></a></p>

<p>Assuming you have a two wheeled robot the same readings on two different setups can yield different results. In my simple illustration I have the axle of a two wheeled robot both bots have the same <code>wheel_diameter</code> and use the same rotary encoders. One bot has distance <code>R1</code> between the wheels the other has distance <code>R2</code> suppose both robots keep a wheel stationary and the other wheel moves 5 clicks forward on the wheel encoder (and both bots use the same encoder and same size wheel). Using the simple equation for circumference of a semi-circle we can find the distance the wheels moved. </p>

<p><code>dist_trav = (pi * wheel_diameter) * (#ticks / total ticks on encoder)</code> </p>

<p>Of course since one wheel is stationary, the bot is actually pivoting on another semicircle. We can calculate the new angle using </p>

<p><code>circumference = 2*pi*dist_between_wheels</code> dist_between_wheels is the radius of our circle </p>

<p><code>angle = % of circumference traveled * units = (dist_trav / circumference) * 360</code> we use 360 so the angle is in degrees, but you could use radians if desired</p>

<p>you will see that even in this example where the robots are identical except for the distance between the wheels the same number of ticks will mean very different angles. If <code>r2=2*r1</code> we can see that <code>dist_trav</code> is the same for both bots (since the wheels are the same diameter) but when we figure out the angles we get</p>

<p><strong>black bot</strong></p>

<p><code>angle_black = (dist_traveled / 2*pi*R1) * 360</code></p>

<p><strong>red bot</strong></p>

<pre><code>angle_black = (dist_traveled / 2*pi*2*R1 ) * 360
            = (dist_traveled / 4*pi  *R1 ) * 360
</code></pre>

<p>so for the same movement of the wheels the red bot will only have 1/2 the angle change as the black bot did. This is just a toy example, but you can easily see how different diameters of wheels and distance between them can make a huge difference.</p>
",2705382,1,2,54615610,"I'm not saying this is offtopic, but I think you might find a better StackExchange site for this question. Such as [Robotics SE](http://robotics.stackexchange.com/help/on-topic).",Coordinates
260,1400,11058724,C++ Inverse Kinematic Algorithm / library which includes method for IK when position of several nodes in chain known,|c++|graphics|computational-geometry|robotics|inverse-kinematics|,"<p>I am looking for ideally a c++ library / code (but if not at least an algorithm) that solves the IK problem for a given chain of n nodes, of which estimates for the position of k nodes (where k &lt; n) in the chain are known.</p>

<p>Any help much appreciated. </p>
",41075.91597,,1867,1,2,5,0,1106578,,40896.80208,156,11058846,"<p>This is possible using an iterative IK algorithm, such as Cyclic Coordinate Decent.</p>
",131345,1,0,27618481,"btw. for iteration algorithms you just need matrix arithmetics (i usually uses for IKs transformation matrices 4x4) needed operations are matrix*matrix, matrix*vector inverse matrix. for more detailed information google some OpenGL robotic arm demo/tutorial you will get there everything you need",Actuator
261,1809,18215873,How to represent N E S W orientation in OOP?,|oop|logic|orientation|robotics|,"<p>I'm trying to find a programming concept of representing the orientation of an object on a grid, but my logic is seriously failing me here. 
If I have Robot (R) and he's facing North, I want him to turn left and right and change orientation accordingly. Obviously this needs to be in a loop (possibly a circular linked list) with regards to if R is facing West but I turn right then R needs to be back facing North. </p>

<p>I've seen this answer <a href=""https://stackoverflow.com/questions/16637006/position-and-orientation-of-robot-in-a-grid"">Position and orientation of robot in a grid</a> and I have already done something similar using an array, but it doesn't seem right. There must be a better way of doing it. </p>

<p>Looking this up on Google just gives me oriented programming links or really complicated robotics design papers. </p>

<p>Thanks in advance!</p>
",41499.74236,19260828,122,2,2,2,,901486,England,40773.92639,662,18308530,"<p>Since you don't ask your question for a specific language I can suggest a oo-pseudocode version of what I would do:</p>

<pre><code>class Orientation2d
{
   int direction = 0;
   static Position2d[4] move = { {0,1}, {1,0}, {-1,0}, {-1,-1} };

   void left()
   {
       direction = (direction+4-1) % 4;
   }
   void right()
   {
       direction = (direction+1) % 4;
   }
   Pose2d forward()
   {
      return move[ direction ];
   }
}

class Pose2d
{
   Position2d pos;
   Orientation2d or;

   void moveForward()
   {
       pos += or.forward();
   }
   void turnLeft()
   {
       or.left();
   }
   void turnRight()
   {
       or.right();
   }
}
</code></pre>

<p>You should easily be able to convert this to C++ or Java. </p>
",672634,1,0,27131583,I'm not doing anything robotics specific. I'm doing a simple Mars Rover project - something similar to Winlogo. So I don't know if it would have any relevance on there. But thank you. I've never heard of that place.,Coordinates
262,4199,70643681,Can I use Instantaneous screw with Finite screw?,|math|robotics|kinematics|inverse-kinematics|,"<p>I'm trying to calculate angular velocity of each axis by movement of end-effector,</p>
<p>If S1 and S2 is finite screw, and S2 had infinitesimal movement from S1.
Also, let S1_ be (-)array of S1 and instantaneous screw of S1 is St1</p>
<p>So If I triangle product S2 and S1_ (S2△S1_=St1), It becomes almost instantaneous screw of S1 (I believe)</p>
<p>What I want to calculate is, if St1 is instantaneous screw, than can I calculate the angular velocity of each axis by using inverse jacobian with [ (J^-1)*St1 = answer ]?
(jacobian is from S1, if S1=S1_6△S1_5△S1_4△S1_3△S1_2△S1_1, (the robot has 6 axis),
jacobian matrix 'J' = [Su1_1, Su1_2, Su1_3, Su1_4, Su1_5, Su1_6], Su is for 'unit twist')</p>
",44570.71667,,49,1,2,0,,17766063,"Busan, 대한민국",44556.46528,2,70646595,"<p>When you look at the kinematics recursively from the base to the end effector you have</p>
<p><strong>v</strong><sub>i</sub> = <strong>v</strong><sub>i-1</sub> + <strong>s</strong><sub>i</sub> u<sub>i</sub></p>
<p>where <strong>v</strong><sub>i</sub> is the velocity screw of each link, <strong>v</strong><sub>i-1</sub> is the velocity screw of the previous link, <strong>s</strong><sub>i</sub> is the unit screw of the joint axis, and u<sub>i</sub> the joint speed.</p>
<p>So the end effector has a final velocity screw of</p>
<p><strong>v</strong><sub>6</sub> = <strong>s</strong><sub>1</sub> u<sub>1</sub> + <strong>s</strong><sub>2</sub> u<sub>2</sub> + <strong>s</strong><sub>3</sub> u<sub>3</sub> + <strong>s</strong><sub>4</sub> u<sub>4</sub> + <strong>s</strong><sub>5</sub> u<sub>5</sub> + <strong>s</strong><sub>6</sub> u<sub>6</sub></p>
<p>and I think you are asking on how to find the vector of joint speeds <strong>u</strong> = (u<sub>1</sub>, u<sub>2</sub>, u<sub>3</sub>, u<sub>4</sub>, u<sub>5</sub>, u<sub>6</sub>)</p>
<p>So you compose the 6×6 jacobian matrix <strong>J</strong>, by combining the individual joint axis unit screws in columns</p>
<p><strong>J</strong> = [ <strong>s</strong><sub>1</sub> <strong>s</strong><sub>2</sub> <strong>s</strong><sub>3</sub> <strong>s</strong><sub>4</sub> <strong>s</strong><sub>5</sub> <strong>s</strong><sub>6</sub>]</p>
<p>and invert the kinematics</p>
<p><strong>v</strong><sub>6</sub> = <strong>J</strong> * <strong>u</strong>  ⇒ <strong>u</strong> = <strong>J</strong><sup>-1</sup> <strong>v</strong><sub>6</sub></p>
",380384,0,3,124887268,"I have a MSME in Robotics and wrote my thesis on Screw theory. I still don't understand what you are asking. If this is indeed a programming question and not a [Mathematics.SE] or [Physics.SE] question then **a lot more details** are needed to effectively answer. You need to explain the kinematics framework used, any conventions you follow (what is the triangle product here) and what you want to calculate.",Actuator
263,2648,41657165,Gazebo / Ros: How to create a camera plugin with pixel-level segmentation?,|ros|robotics|,"<p>I am looking to create a camera plugin where, at each pixel of the image, I'm able to output what object it belongs to, if any. I've struggled to find a solution to this problem. Any suggestions regarding where to begin?</p>
",42750.09653,,1938,1,0,1,,3672582,,41783.89306,4,41665287,"<p>If u want to make a camera is Gazebo simulation than u have to use the sensor plugin or sensor element in ur robot sdf/urdf model like described <a href=""http://sdformat.org/spec?ver=1.6&amp;elem=sensor"" rel=""nofollow noreferrer"">here,</a>, U can find both type of camera their, depth and rgb. For example if u want a kinect sensor(camera) which contains both the rgb and depth image than u can use bolow sdf lines in ur robot model. Here when u run this code it will publish both rgb and depth datas as shown <a href=""https://www.youtube.com/watch?v=kkYRBFR3iqc"" rel=""nofollow noreferrer"">here</a>:, here i've used ray sensor. </p>

<pre><code> &lt;gazebo reference=""top""&gt;
    &lt;sensor name='camera1' type='depth'&gt;
      &lt;always_on&gt;1&lt;/always_on&gt;
      &lt;visualize&gt;1&lt;/visualize&gt;
      &lt;camera name='__default__'&gt;
        &lt;horizontal_fov&gt;1.047&lt;/horizontal_fov&gt;
        &lt;image&gt;
          &lt;width&gt;640&lt;/width&gt;
          &lt;height&gt;480&lt;/height&gt;
          &lt;format&gt;R8G8B8&lt;/format&gt;
        &lt;/image&gt;
        &lt;depth_camera&gt;
          &lt;output&gt;depths&lt;/output&gt;
        &lt;/depth_camera&gt;
        &lt;clip&gt;
          &lt;near&gt;0.1&lt;/near&gt;
          &lt;far&gt;100&lt;/far&gt;
        &lt;/clip&gt;
      &lt;/camera&gt;
      &lt;plugin name='camera_controller' filename='libgazebo_ros_openni_kinect.so'&gt;
        &lt;alwaysOn&gt;true&lt;/alwaysOn&gt;
        &lt;updateRate&gt;30.0&lt;/updateRate&gt;
        &lt;cameraName&gt;camera&lt;/cameraName&gt;
        &lt;frameName&gt;/camera_link&lt;/frameName&gt;
        &lt;imageTopicName&gt;rgb/image_raw&lt;/imageTopicName&gt;
        &lt;depthImageTopicName&gt;depth/image_raw&lt;/depthImageTopicName&gt;
        &lt;pointCloudTopicName&gt;depth/points&lt;/pointCloudTopicName&gt;
        &lt;cameraInfoTopicName&gt;rgb/camera_info&lt;/cameraInfoTopicName&gt;
        &lt;depthImageCameraInfoTopicName&gt;depth/camera_info&lt;/depthImageCameraInfoTopicName&gt;
        &lt;pointCloudCutoff&gt;0.4&lt;/pointCloudCutoff&gt;
        &lt;hackBaseline&gt;0.07&lt;/hackBaseline&gt;
        &lt;distortionK1&gt;0.0&lt;/distortionK1&gt;
        &lt;distortionK2&gt;0.0&lt;/distortionK2&gt;
        &lt;distortionK3&gt;0.0&lt;/distortionK3&gt;
        &lt;distortionT1&gt;0.0&lt;/distortionT1&gt;
        &lt;distortionT2&gt;0.0&lt;/distortionT2&gt;
        &lt;CxPrime&gt;0.0&lt;/CxPrime&gt;
        &lt;Cx&gt;0.0&lt;/Cx&gt;
        &lt;Cy&gt;0.0&lt;/Cy&gt;
        &lt;focalLength&gt;0.0&lt;/focalLength&gt;
      &lt;/plugin&gt;
    &lt;/sens

or&gt;
      &lt;/gazebo
</code></pre>
",,1,0,,,Incoming
264,1718,16252951,NXT - Tortoise and Hare - follow moving object - Theoretical,|robotics|lego-mindstorms|nxt|lejos-nxj|,"<p>I'm going through Robotics past papers as a revision before the exam, and I found one problem which seems very confusing. My department does not provide answers to past papers, so I can't check if I'm right.</p>
<p><img src=""https://i.stack.imgur.com/YTpo7.jpg"" alt=""partII"" /></p>
<pre><code>public class Question4i{

  public static main(){
    float d = 30;
    float k = 1; //If it's equal to 1, why do we need it at all?
    while(true){
      error= GetSonarDepth() - d;
      if(error&gt;100) error=100;
      setVelocity(k * error)
    }
  }

}
</code></pre>
<p>Then second part is where things are getting interesting:</p>
<p><img src=""https://i.stack.imgur.com/ZejCd.jpg"" alt=""partII"" /></p>
<p>This is my understanding:</p>
<ol>
<li>Robot and Hare are placed in the same position 0</li>
<li>Robot starts reversing, while hare travels forward at constant velocity (error is negative)</li>
<li>Robot fires a sonar</li>
<li>Sonar reading tells the distance is 30 (error is 0)</li>
<li>Robot stops (error is 0)</li>
<li>Hare travels constant distance during this adjustment</li>
<li>Robot fires sonar (error is positive)</li>
<li>Robot increases its speed to setVelocity(error)</li>
<li>Hare travels constant distance during this adjustment</li>
<li>Robot changes its speed based on &quot;old&quot; sonar reading, as during the speed change, hare will travel further</li>
<li>Therefore, robot will always be at least a little bit too far from desired distance</li>
</ol>
<p>Also I came to a conclusion that if hare speed is higher than that of robot, distance will be constantly increasing. There will be NO STEADY STATE - where steady refers to kept distance.</p>
<h3><strong>Question:</strong> I think in best case the robot will oscillate between 30 and 30+ distance, but how would you change the program to make it travel at constant 30cm distance? I also find it suspicious that k is 1 in part i, is that alright?</h3>
",41391.58194,16725327,521,2,0,0,,1087852,Imperial College London,40885.57292,632,16254512,"<p>With proportionate gain, the robot's forward velocity will be proportionate to its distance from the robot - 30 cm. When we reach a steady state, the robot will be matching the hare's forward velocity, at some distance such that (d - 30) * k == the hare's speed. I.e. at some constant distance > 30 cm.</p>

<p>As for how to modify the program, you might want to set the robot's speed not only proportionate to the error, but taking into account the rate of change of the error as well.</p>

<p>Recommended reading:</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/PID_controller"" rel=""nofollow"">https://en.wikipedia.org/wiki/PID_controller</a></li>
<li><a href=""http://lejos.sourceforge.net/nxt/nxj/api/lejos/util/PIDController.html"" rel=""nofollow"">http://lejos.sourceforge.net/nxt/nxj/api/lejos/util/PIDController.html</a></li>
</ul>

<p>Alternatively you could probably hack it to remember the speed when the distance ceases changing, and use that as a new base speed, with regular proportionate gain to keep the distance constant, but using PD control would be more robust :-).</p>
",2125397,2,4,,,Moving
265,3073,49725812,Robot Pepper Not able to run Android application on Real Pepper,|android|robotics|pepper|,"<ol>
<li><p>I have developed an app for pepper using android studio. the app run fine on emulator, but cannot run it on real pepper robot. I have connected real pepper robot through IP, but when I run through android studio cannot find the real pepper robot tablet in the list of devices.</p></li>
<li><p>From android.aldebaran.com site under connecting to real pepper its say 
<strong>Developer mode</strong> should be activated.but couldn't find any such option on real pepper</p></li>
</ol>

<p>kindly help with above querys folks 
<a href=""https://i.stack.imgur.com/01R8E.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/01R8E.jpg"" alt=""enter image description here""></a></p>
",43199.20625,,2266,3,5,3,0,5761320,Dubai - United Arab Emirates,42377.28194,26,49775649,"<p>You can't simply skip the activation of the developer mode. You need to set the developer mode in the settings of the real pepper tablet. Check out <a href=""https://android.aldebaran.com/sdk/doc/pepper-sdk/getting_started/running_application.html#on-a-real-robot"" rel=""nofollow noreferrer"">this</a> manual.</p>

<p>To show the settings on the tablet follow these instructions (<a href=""https://github.com/arlemi/setup-wifi-pepper#connect-peppers-tablet"" rel=""nofollow noreferrer"">Source</a>)</p>

<ul>
<li>Open a terminal</li>
<li>Connect to Pepper using SSH (ssh nao@PEPPER_IP, ...)</li>
<li>Run the followin command </li>
</ul>

<blockquote>
  <p>qicli call ALTabletService._openSettings</p>
</blockquote>

<p>-> The Settings menü should now be opened on the tablet.</p>

<p>There should be something like this:
<a href=""https://i.stack.imgur.com/6nEB5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6nEB5.png"" alt=""Pepper description""></a></p>

<p>If you can't do this, you can't go on...</p>
",2557919,2,2,108818260,Is your problem solved now?,Connections
266,4759,78038352,Lidar intensity value normalization,|python|c#|windows|robotics|lidar-data|,"<p>We have a file with X,Y,Z,I in binary format, We want to convert this to las file with intensity values, But the intensity values range from 1 to 11629. Las file supports the intensity value between 0 to 255, Could anyone suggest on how to create las file with these intensity values?</p>
<pre><code>var curIntensity = 525;
var maxInensity = 11629;
var fact = 255.0 / maxInensity;
var nIntensity = curIntensity * fact;

nIntensity = 11
</code></pre>
<p>tried to normalize with above formula, But results are not excepted.</p>
",45344.16806,,32,1,0,0,,23460169,,45344.14653,1,78038386,"<p>Your input is 1-based, while the output is 0-based, so you should normalize the base first:</p>
<pre><code>var curIntensity = 525;
var maxInensity = 11629;
var fact = 255.0 / (maxInensity - 1);
var nIntensity = (curIntensity - 1) * fact;
</code></pre>
",6890912,1,0,,,Programming
267,4329,72335499,wrong result in cv2.CalibrateHandEye,|python|robotics|calibration|,"<p>I am trying to calibrate a hand-to-eye robotic arm and camera system. I am using the cv2.CalibrateHandEye function to calculate the transform matrix between robot base and camera. But I only get wrong results.</p>
<p>The camera is calibrated, and I use the cv2.SolvePnp function to get the translation and rotation vector of the marker from images that took by camera, and use cv2.Rodrigues to transform the rotation vector into the rotation matrix.</p>
<p>The rotation matrix of end is generated using tfs.euler.euler2mat according to the rotation of end effector.</p>
<p>I checked both matrices many times, I think they are correct, but the cv2.CalibrateHandeye function just keep out put answers that not even close the true value.</p>
<p>here is some of the code.</p>
<p>I recorded about 20 sets of images and end pose, code below is how I extract matrix from each of them</p>
<pre><code>(success, rvec, tvec) = cv2.solvePnP(np.array(point_3d), np.array(corners_2d), mtx, dist,flags=cv2.SOLVEPNP_ITERATIVE)
R_board_in_camera = cv2.Rodrigues(rvec)[0]
T_board_in_camera = tvec
H_board_in_camera = np.zeros((4, 4), np.float)
H_board_in_camera[:3, :3] = R_board_in_camera
H_board_in_camera[:3, 3] = np.array(T_board_in_camera).flatten()
H_board_in_camera[3, 3] = 1

R_hand_in_base = tfs.euler.euler2mat(math.radians(angle_x), math.radians(angle_y), math.radians(angle_z), axes='rxyz')
T_hand_in_base = np.array([x,y,z])  # calculated in advance

H_hand_in_base = np.zeros((4, 4), np.float)
H_hand_in_base[:3, :3] = R_hand_in_base
H_hand_in_base[:3, 3] = T_hand_in_base.flatten()
H_hand_in_base[3, 3] = 1

</code></pre>
<p>After I got all matrices, I use calibrate funcion</p>
<pre><code>n = len(Ts_hand_to_base)
R_base_to_hand = []
T_base_to_hand = []
R_board_to_camera = []
T_board_to_camera = []

for i in range(n):
    Ts_base_to_hand = np.linalg.inv(Ts_hand_to_base[i])
    R_base_to_hand.append(np.array(Ts_base_to_hand[:3, :3]))
    T_base_to_hand.append(np.array(Ts_base_to_hand[:3, 3]))
    R_board_to_camera.append(np.array(Ts_board_to_camera[i][0, :3]))
    T_board_to_camera.append(np.array(Ts_board_to_camera[i][:3, 3]))

R_camera_to_base, T_camera_to_base = cv2.calibrateHandEye(R_base_to_hand, T_base_to_hand, R_board_to_camera,T_board_to_camera, method=cv2.CALIB_HAND_EYE_DANIILIDIS)
</code></pre>
<p>here are some of the results i get.</p>
<pre><code># method = cv2.CALIB_HAND_EYE_HORAUD
H_camera_to_base:
 [[  1.    -0.01  -0.04  13.54]
 [  0.    -0.96   0.29 141.48]
 [ -0.04  -0.29  -0.96   0.  ]
 [  0.     0.     0.     1.  ]]

method = cv2.CALIB_HAND_EYE_HORAUD
 [[  -0.22   -0.98   -0.01  186.04]
 [  -0.95    0.21    0.23 -187.49]
 [  -0.22    0.05   -0.97  782.4 ]
 [   0.      0.      0.      1.  ]]

method = cv2.CALIB_HAND_EYE_TSAI
 [[ 0.51 -0.3   0.8  64.88]
 [-0.54  0.62  0.57 69.03]
 [-0.67 -0.72  0.15  0.  ]
 [ 0.    0.    0.    1.  ]]

method = CALIB_HAND_EYE_ANDREFF
error (-7:Iterations do not converge) Rotation normalization issue: determinant(R) is null in function 'normalizeRotation'

method = CALIB_HAND_EYE_PARK
 [[nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]
 [ 0.  0.  0.  1.]]
</code></pre>
",44703.29653,,1128,0,0,0,,19167018,,44702.41528,7,,,,,,,,Incoming
268,1215,6619222,A Question on Omnidirectional Control,|math|language-agnostic|vector|robotics|,"<p>I am reading the follwoing paper,</p>

<p><a href=""http://robocup.mi.fu-berlin.de/buch/omnidrive.pdf"" rel=""nofollow"">http://robocup.mi.fu-berlin.de/buch/omnidrive.pdf</a></p>

<p>On page ten it introduces a formula to generate movement for n omniwheels,</p>

<pre>
<code>
                   −sin θ1 cos θ1 1 
(v1, v2, v3, v4)=   −sin θ2 cos θ2 1   (vx, vy, Rω)
                    ................
                   −sin θn cos θn 1
</code>
</pre>

<p>From what I understand, you take the vector of where you want to go say x direction no rotation that would be [1 0 0] and multiply that with sin cos matrix then you get how much each particular motor should be powered to generate motion in that direction.</p>

<p>In order to double check the results I calculated for velecoties for [1 0 0], that gives me [a b c d] 4 motor speeds and I have 4 θs, when I add each wheels sin and cos components I think I shoudl get back to [1 0 0] but I actualy get back something like [2.5 0 0]. What I was wondering is did I forget some property about matrix multipication? or  Do I have flow in my logic?</p>
",40732.08819,6619411,101,1,0,1,,89904,,39915.18542,3143,6619411,"<p>No, there's nothing wrong. Consider the simplest possible case, where a robot has two wheels across from each other on a single axle. For the robot to move forward at speed <code>x</code>, both wheels should be turning at speed <code>x</code>, not <code>x/2</code>.</p>
",,2,0,,,Actuator
269,3873,63952955,how to setup robotdyn arduino mega+esp8266,|html|robotics|arduino-esp8266|,"<p>I am making a web server with a robotdyn arduino mega + esp8266  to control a few bo motors I flashed the firmware for the esp8266 I have 2 questions, 1: when I give AT commands through the serial monitor It doesn't respond also when I open the serial monitor its saying, &quot;error opening COM4, port busy&quot; how can I solve this? the second one is how can I embed the html code into the arduino code? by the way I didn't use any code for the AT commands. Can you also tell how I should orient the switches (they're only on the robotdyn arduino mega/uno + esp8266 model not on normal esp8266 chips)on the mega while talking the the mega?</p>
",44092.39097,,103,0,5,0,,,,,,,,,,,113099806,there is no 'normal' firmware. https://blog.gabrielcsapo.com/arduino-web-server-mega-2560-r3-built-in-esp8266/,Connections
270,2446,35468253,SVG - Export of Points / Coordinates in Python,|python|svg|coordinates|robotics|cnc|,"<p>I am developing a robotic controlled drawing system and currently are looking for some input regarding the extraction of plain coordinates out of exisitng SVG-files. My question is very similar to <a href=""https://stackoverflow.com/questions/31557673/python-get-coordinates-of-the-vector-path-from-svg-file"">this one</a>, which did not get much feedback so far.</p>

<p>-> Is there some simple/established/prebuild way to extract point-coordinates out of existing SVG files within python ?</p>

<p>From my (limited) understanding, existing SVG libraries for python like <a href=""https://pypi.python.org/pypi/svg.path"" rel=""nofollow noreferrer"">svg.path</a> and <a href=""https://pypi.python.org/pypi/pysvg/0.2.2"" rel=""nofollow noreferrer"">pysvg</a> do not seem to include a function for extracting all points out of an existing file (as they seem to be build more towards writing SVG rather than reading SVG..)</p>

<p>thanks for answers.</p>
",42417.89931,,1693,0,3,1,0,5942645,,42417.87569,13,,,,,,58632801,"SVG is XML, use an XML parser.",Specifications
271,2403,33447171,Find pixel coordinates from real world using a Kinect,|matlab|computer-vision|kinect|robotics|,"<p>I am trying to do some basic level robotics with computer vision, and I have run into a problem with going from real world coordinates to pixels in order to read from the Kinect's depth map. I am doing this because I would like to be able to draw on a hand of non constant depth, which is located at a given x and y location in millimeters.</p>

<p>The problem I'm having is that the distance of the object from the camera needs to be known in order to convert between real world and pixels. How could I either find the depth at a location, or convert the depth map into metric coordinates instead of pixels?</p>
",42308.06528,,415,0,2,0,,3764063,,41812.18125,2,,,,,,54685938,"Thanks @ParagS.Chandakkar, I'll give it a shot.",Incoming
272,3272,54242283,How to program line following and vex claw bot to autonomously pick up pipe on RobotC?,|robot|robotc|,"<p>Our teacher gave us an assignment which is supposed to be self-taught via YouTube. I can't find a good tutorial, so can anyone explain to me how one would program a vex claw bot to move straight until it finds a line and begins to follow the line (line is black tape). Then we have a ultrasonic sensor which will detect pipes and cause the claw to pick up the pipes.</p>

<p><a href=""https://i.stack.imgur.com/EyH2e.png"" rel=""nofollow noreferrer"">Our claw bot looks like this</a></p>

<p>we have three sensors attached to the front</p>

<p><a href=""https://i.stack.imgur.com/ro9CQ.png"" rel=""nofollow noreferrer"">3 sensors attached like this</a></p>

<p>if any can help I will be really greatful, our teacher hasn't taught us anything.</p>
",43482.77708,54242383,262,1,0,0,,9909400,,43258.59097,4,54242383,"<p>This is a line following task. The approach should be that you move ahead, then use the IR sensors to detect if there is a black line (tape) detected due to the change in IR intensities and then start tracking that line with a normal line follower algorithm. </p>

<p>This link has a nice list of resources for line following tasks. <a href=""https://www.intorobotics.com/line-following-robot-tutorials-how-to-build-programming-code-and-resources/"" rel=""nofollow noreferrer"">https://www.intorobotics.com/line-following-robot-tutorials-how-to-build-programming-code-and-resources/</a> </p>
",4796617,0,0,,,Incoming
273,4497,75203121,drake bazel test failure in solvers:csdp_solver_test,|bazel|robotics|drake|,"<p>I followed these instrcutions:</p>
<pre><code>cd /path/to/drake
bazel build //...                 # Build the entire project.
bazel test //...                  # Build and test the entire project.
</code></pre>
<p>But, I got the following error:</p>
<pre><code>[----------] Global test environment tear-down
[==========] 31 tests from 17 test suites ran. (123 ms total)
[  PASSED  ] 30 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] TestSOS.MotzkinPolynomial

 1 FAILED TEST
================================================================================
INFO: Elapsed time: 1.431s, Critical Path: 0.32s
INFO: 3 processes: 1 internal, 1 linux-sandbox, 1 local.
INFO: Build completed, 1 test FAILED, 3 total actions
//solvers:csdp_solver_test                                               FAILED in 0.2s
  /home/aj/.cache/bazel/_bazel_aj/84dff73cad498d702e4464d2c661c905/execroot/drake/bazel-out/k8-opt/testlogs/solvers/csdp_solver_test/test.log

Executed 2 out of 6054 tests: 6053 tests pass and 1 fails locally.
INFO: Build completed, 1 test FAILED, 3 total actions
</code></pre>
<p>My OS is Ubuntu 20 and my complier is gcc 9.
Here is the <a href=""https://gist.github.com/ajaygunalan/caf0da28ee735b2047b41fea684f8fd1#file-test-log"" rel=""nofollow noreferrer"">link</a> to my test.log file.</p>
",44948.79722,,41,1,1,0,,9976843,"Bengaluru, Karnataka, India",43273.28889,1,75215499,"<p>It is solved by <code>sudo apt remove libatlas3-base</code></p>
<p>The detailed discussion is at this <a href=""https://github.com/RobotLocomotion/drake/issues/18635"" rel=""nofollow noreferrer"">link</a>.</p>
",9976843,0,0,132718780,"FYI for other readers, this question was also posted at https://github.com/RobotLocomotion/drake/issues/18635.  I'll reply there, instead of StackOverflow.",Error
274,4351,72839814,Using Each Thread As Separate Publisher in Ros2 Node,|c++|multithreading|ros|robotics|ros2|,"<p>I am trying to make a node with multiple publisher in it and each publisher will be work own separate thread.</p>
<p>When I try to basic idea implementation gives me this error;</p>
<pre><code>2022-07-02 13:14:16.757 [PARTICIPANT Error] Type with the same name already exists:rcl_interfaces::srv::dds_::GetParameters_Request_ -&gt; Function registerType 
2022-07-02 13:14:16.758 [PARTICIPANT Error] Type with the same name already exists:rcl_interfaces::srv::dds_::GetParameterTypes_Request_ -&gt; Function registerType 
2022-07-02 13:14:16.758 [PARTICIPANT Error] Type with the same name already exists:rcl_interfaces::srv::dds_::GetParameterTypes_Response_ -&gt; Function registerType 
2022-07-02 13:14:16.759 [PARTICIPANT Error] Type with the same name already exists:rcl_interfaces::srv::dds_::SetParameters_Request_ -&gt; Function registerType 2022-
07-02 13:14:16.759 [PARTICIPANT Error] Type with the same name already exists:rcl_interfaces::srv::dds_::SetParameters_Response_ -&gt; Function registerType 2022-
07-02 13:14:16.760 [PARTICIPANT Error] Type with the same name already exists:rcl_interfaces::srv::dds_::SetParametersAtomically_Request_ -&gt; Function registerType 
2022-07-02 13:14:16.762 [PARTICIPANT Error] Type with the same name already exists:rcl_interfaces::srv::dds_::ListParameters_Request_ -&gt; Function registerType 2022-
07-02 13:14:16.762 [PARTICIPANT Error] Type with the same name already exists:rcl_interfaces::srv::dds_::ListParameters_Response_ -&gt; Function registerType
</code></pre>
<p>My implementation;</p>
<p>Firstly making a basic Ros Node class to publishing data and then create 5 thread to make a publisher for each and their own stack. Using this Ros Node class. Then using rclcpp::spin(node) must be publish the data but it gives the above error.</p>
<pre><code>#include &quot;rclcpp/rclcpp.hpp&quot;
#include &quot;example_interfaces/msg/string.hpp&quot;
#include &lt;string&gt;
#include &lt;thread&gt;


class Publisher: public rclcpp::Node{

public:
    Publisher(std::string name, std::string _data) : Node(name), data(_data){
        publisher = this-&gt;create_publisher&lt;example_interfaces::msg::String&gt;(&quot;pub_topic&quot;, 10);
        timer = create_wall_timer(std::chrono::milliseconds(555), std::bind(&amp;Publisher::call_back, this));

    }

private:

    void call_back(){

        example_interfaces::msg::String msg;
        msg.data = data;
        publisher-&gt;publish(msg);
    }

    std::string data;
    std::shared_ptr&lt;rclcpp::Publisher&lt;example_interfaces::msg::String&gt;&gt; publisher;
    std::shared_ptr&lt;rclcpp::TimerBase&gt; timer; 
 };

void thread_main(std::string data){

    std::shared_ptr&lt;Publisher&gt; node = std::make_shared&lt;Publisher&gt;(data , data);
    rclcpp::spin(node);

}

int main(int argc, char* argv[]){

    rclcpp::init(argc, argv);

    std::vector&lt;std::string&gt; dataset{&quot;first&quot;, &quot;second&quot;, &quot;third&quot;, &quot;fourth&quot;, &quot;fifth&quot;};
    std::vector&lt;std::thread&gt; threads(5);

    for(size_t i=0; i&lt;dataset.size(); ++i){
        threads[i] = std::thread(thread_main, dataset.at(i));
    }

    for(int i=0; i&lt;10; ++i){
        threads[i].join();
    }

    rclcpp::shutdown();

    return 0;
 }
</code></pre>
",44744.60208,,1390,0,0,0,,14595867,,44142.54653,24,,,,,,,,Timing
275,2449,35598327,Using UDP to control robot via python,|python|udp|raspberry-pi|robot|udpclient|,"<p>I'm working on a robot using a raspberrypi. I was looking for a library that could help me with the networking stuff, like packets &amp;co. 
So i'm using this code to select the commands I receive:</p>

<pre><code>def selectPacket(x):
    if x == '00': 
        return '00'
    elif x == ""01"":
        Date = datetime.now()
        return str(Date.microsecond)
    elif x == ""02"":
        return '98'
    elif x == ""03"":
        return '98'
    elif x == ""04"":
        return '98'
    elif x == ""05"":
        return '98'
    else: 
        return '99'
</code></pre>

<p>I'm sure that there is a lib to make quick servers and clients using python, I want to use UDP because the connection I will use will be very unstable, so tcp is out of question. </p>
",42424.4,,1086,1,0,2,0,3722254,,41799.50069,43,35599384,"<p>Because controlling robot depends on complex parts such as how many legs they have, how many joints they have and so on, I think there is no library you want. And connection phase on python is too simple to make a library.</p>

<p>Here are simple UDP Client/Server code:</p>

<p>UDP Server</p>

<pre><code>import socket, traceback
s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
s.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)
s.bind(('', 5000))

print ""Listening for broadcasts...""

while 1:
    try:
        message, address = s.recvfrom(8192)
        print ""Got message from %s: %s"" % (address, message)
        s.sendto(""Hello from server"", address)
        print ""Listening for broadcasts...""
    except (KeyboardInterrupt, SystemExit):
        raise
    except:
        traceback.print_exc()
</code></pre>

<p>UDP Client</p>

<pre><code>import socket, sys
dest = ('&lt;broadcast&gt;', 5000)

s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
s.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)
s.sendto(""Hello from client"", dest)
print ""Listening for replies.""

while 1:
    (buf, address) = s.recvfrom(2048)
    if not len(buf):
        break
    print ""Received from %s: %s"" % (address, buf)
    s.sendto(""echo"", dest)
</code></pre>

<p>Also, if you have complex logic and need to separate into multiple parts that can be communication, control or I/O, refer Event-driven programming: <a href=""http://etutorials.org/Programming/Python+tutorial/Part+IV+Network+and+Web+Programming/Chapter+19.+Sockets+and+Server-Side+Network+Protocol+Modules/19.3+Event-Driven+Socket+Programs/"" rel=""nofollow"">http://etutorials.org/Programming/Python+tutorial/Part+IV+Network+and+Web+Programming/Chapter+19.+Sockets+and+Server-Side+Network+Protocol+Modules/19.3+Event-Driven+Socket+Programs/</a>)</p>
",5963434,0,1,,,Connections
276,297,1590073,Line Tracking Robot,|arduino|robotics|,"<p>Me and my friends are building a line tracking robot based on my previous question about how to track white line on a black surface. We settled on using photo resistors and a arduino board. Now all the reflectance sensors I've found are should be placed very close to the line 1 - 2 cm above the line. Now one of my team mates had a heated argument with the professor that there are reflectance sensors that can track 10cm or more but we could not find any.</p>

<p>Are there any type of sensor that would allow us to track the line farther away?</p>
",40105.73681,,1357,2,3,2,0,89904,,39915.18542,3143,1590138,"<p>Using an arduino, you are most likely going to use the pololu library for reflectance sensors. Even using an <a href=""http://www.pololu.com/catalog/product/961"" rel=""nofollow noreferrer"">array of sensors</a> of this type, you are looking at a maximum sensing distance of just UNDER a cm (9.5 mm.) I think your teammate was out by a factor of ten, you can score this one to the professor!</p>

<p>The lego light sensor is a good example of this type of sensor. If you can get your hands on an NXT kit, it is an alternative to the arduino. And who doesn't enjoy playing with lego!!</p>

<p>Kindness,</p>

<p>Dan</p>
",63627,0,0,1453373,"I'm interested in line tracking, but this questions seems to deal with finding some commodity hardware. I'm not sure that fits with SO.",Incoming
277,280,1322019,Associating s/w programming with h/w programming,|hardware|robotics|,"<p>I've been in s/w programming for years. Thru out the years i also had interest in h/w programming (circuits + robotics + etc).</p>

<p>Please advice from where i can start h/w programming. my aim is to combine both s/w &amp; h/w to work together. </p>
",40049.51181,1327292,205,4,1,5,0,147025,,40023.475,518,1322040,"<p>I'd recommend <a href=""http://www.arduino.cc"" rel=""nofollow noreferrer"">Arduinos</a> if you want to try some embedded programming. They're cheap, IDE works on multiple platforms and especially they are easy to start with.</p>
",89024,3,2,93084415,ask a stupid question: does h/w mean hardware?,Other
278,274,1252428,Hardware Programming - Hands-On Learning,|arduino|robotics|hardware|,"<p>Besides <a href=""http://www.arduino.cc/"" rel=""nofollow noreferrer"">Arduino</a>, what other ways are there to learn hardware programming in a hands-on way?  Are there any nifty kits available, either a pre-assembled robot, that you can program to move a certain way, or do certain things, or anything similar to that?</p>
",40034.91944,1252456,1164,6,0,4,0,83819,United States,39707.29653,905,1252441,"<p>There's the <a href=""http://www.microsoft.com/netmf/default.mspx"" rel=""nofollow noreferrer"">.NET Micro Framework.</a></p>

<p>It's incredibly simple to use/setup and there's lots of hardware being made to target this framework. </p>
",74183,0,0,,,Other
279,446,1967978,"Lego Mindstorm NXT, Cocoa, and HiTechnic Sensors",|objective-c|cocoa|robotics|lego-mindstorms|,"<p>I've taken the existing code from <a href=""http://code.google.com/p/legonxtremote/"" rel=""nofollow noreferrer"">this project</a>, and very happy with it so far.</p>

<p>I'm now however in a position where I need to make use of some third-party sensors which I've purchased from <a href=""http://www.hitechnic.com"" rel=""nofollow noreferrer"">hitechnic</a>, such as an accelerometer, a gyroscope, and a 3D compass - to mention a few.</p>

<p>I'm not sure where to start now, but what I need to do is add to my existing code base (which is based on the <a href=""http://code.google.com/p/legonxtremote/"" rel=""nofollow noreferrer"">this</a>), and effectively glue my framework to the new hardware.</p>

<p>Can anyone point me in the right direction?  I can't find any APIs from the device manufacturer, (but I have emailed them and asked - no reply yet).</p>

<p>I've also started to document my findings on <a href=""http://ai.autonomy.net.au/wiki/Project/Framework/Nimachine"" rel=""nofollow noreferrer"">this</a> page.</p>
",40175.19097,1973289,2192,2,0,4,0,109362,"Seattle, Washington",39952.55139,202,1972660,"<p>I heard back from HiTechnic today, and with their permission, I'm posting their response for everyone here.</p>

<pre><code>Hi Nima,

There are two types of sensors, digital and analog.  The Analog sensors you
can basically read like you would the LEGO light sensor.  If you have that
working then you can read the HiTechnic analog sensors.  These include the
EOPD, Gyro as well as the Touch Multiplexer.

For the TMUX there is [sample NXC code][1] on the product info page.
You should be able to use that as a basis if you want to support this device.

The other sensors are digital I2C sensors.  Most of these sensors have I2C
register information on their respective product information page and/or it
was included on a sheet that came with the sensor.  First of all, to make
these sensors work with your framework you need to have I2C communications
working.  After that it will be a matter of creating your own API that uses
the I2C interface with the sensors.  I recommend that you download and look
at Xander Soldaat's RobotC driver suite for the HiTechnic sensors.  You will
find this near the bottom of the HiTechnic downloads page.

Regards,
Gus
HiTechnic Support
</code></pre>

<p>References:</p>

<ul>
<li><a href=""http://www.hitechnic.com/cgi-bin/commerce.cgi?preadd=action&amp;key=NTX1060"" rel=""nofollow noreferrer"">Sample NXC code</a></li>
<li><a href=""http://mightor.wordpress.com/2009/11/08/robotc-i2c-howto-part-i/"" rel=""nofollow noreferrer"">Xander's Blog</a></li>
</ul>
",109362,2,1,,,Specifications
280,226,764202,Is Lego MindStorms a good choice for basic robotics development?,|robotics|lego-mindstorms|nxt|,"<p>I would like to learn how to write software for controlling robots.</p>

<p>Is Lego MindStorms a good choice for this? Are there better alternatives?</p>

<p>I'd prefer MindStorms, but after reading a couple of articles I get the impression that Lego has stopped research and development of MindStorms.</p>

<p>What are your suggestions?</p>
",39921.87847,764351,15585,9,0,28,0,169,"London, United Kingdom",39662.95208,717,764222,"<p>I can't give you a good side-by-side comparison vs other robotics kits (I know MS has one), but I've spent a lot of time with mindstorms (to the point where I gave a user group presentation) and I think that it makes the programming enjoyable and teaches you the basics of sensors, input and output that you'd need to know with any kit.</p>

<p>It gives you the foundation and makes it fun which is a great way to start. There are probably more sophisticated alternatives though...</p>
",3329,2,0,,,Specifications
281,907,4350199,"Does a wireless robot, or even robot sensors exist that I can control or read via command line or curl?",|wireless|robotics|,"<p>I see robots like Rovio out there, that claim to be ""wireless"" or ""wifi"", but it SEEMS like they expect you to use their included software to control the robots.  Is there any way to control an existing commercial robot (or even just read an available wireless sensor, such as light or motion) from a command line, or curl?  (For example, hitting the robots ip, and port it is listening on, and sending a web services, or soap, or even http message, or ANYTHING). Even just being able to listen to a sensor from a command line would be a help... </p>

<p>Basically, most ""programmable"" robots out there have lightweight languages on them, and you have to physically store the code on them, so you're pretty limited.  What I want to do (and SURELY this exists) is have a robot that is completely client-light and server heavy (i.e. all the intelligence and logic is stored on some machine that wirlessly commands the robot).  That way I could code in any language (and have arbitrarily long code base), so long as I could send dumb wireless commands to the robot (such as move forward, give me your sensor data, etc)</p>

<p>Does such a thing exist in any form?</p>
",40515.9125,4350523,176,1,1,0,0,99502,,39934.56319,620,4350523,"<p><a href=""http://www.ros.org/"" rel=""nofollow"">ROS</a>, which is quite popular these days, provides facilities to off-load code to a larger machine, featuring distributed nodes. As long as you remember that you need the communication stack, motor / manipulation control etc. right on the robot, you can pretty much implement the whole heavy-duty control anywhere you like.</p>
",407438,0,1,5489221,"Actually, looks like Lego NXT brick lets you send programmed bluetooth commands to it, which suits my purposes. Huzzah?",Remote
282,784,3687113,How to control multiple robots through PC using Serial communication?,|c|robotics|serial-communication|,"<p>I want to <strong>control multiple robots using my laptop</strong>. Robots do not have intelligence, they are sending sensor values to PC which computes the sensors values and sends back result to robots.(Centralized control of robots using PC ).</p>

<p>Robots are communicating with PC through serial communication using Zigbee mudule. </p>

<p>Problem: <strong>How to make &amp; send a structure</strong> (from robot) <strong>like {sen1, sen2,sen3..,robot id}</strong> where sen1, sen2..are sensors values and robot id is to recognize particular robot. 
After editing.....
The code I was using for sending sensors was like.</p>

<pre><code> void TxData(unsigned char tx_data)

{   SBUF = tx_data; //Transmit data that is passed to this function
     while(TI == 0); //wait while data is being transmitted
}
</code></pre>

<p>and then sending sensor values one by one</p>

<pre><code> TxData(left_whiteline_sensor);
 TI=0; // resetting transmit interrupt after each character
 TxData(middle_whiteline_sensor);
 TI=0;
 TxData(right_whiteline_sensor);
 TI=0;
 TxData(front_sharp_sensor);
 TI=0;
</code></pre>

<p>At PC end reading these values in buffer</p>

<pre><code>read(fd, buf1, sizeof(buf1));
.....
options.c_cc[VMIN]=4; // wait till not getting 4 values 
</code></pre>

<p>This was <strong>working fine</strong> when there was <strong>only one robot</strong>, now as we have <strong>multiple robots</strong> and each robot is sending data using above function, I am <strong>getting mixed sensor values</strong> of all robots of <strong>at PC end</strong>. One solution is to <strong>make a structure</strong> which I mentioned above and send it to PC. This is what I want to ask ""<strong>How to make and send such a structure</strong>""
Sorry for not framing question correctly before.</p>

<p>Thanks...</p>
",40431.75486,,1345,5,3,1,0,362250,"Bengaluru, Karnataka, India",40338.39028,134,3687353,"<p>I don't know what API you are using from the PC to communicate with the endpoints (robots), but I'm guessing that when you send data you specify either the short address, the long address (MAC), or some socket/file ID which you opened using one of the long or short addresses.  Also, I'm guessing that the robot id is the same as the short address -- if not you will need to create some code to map back and forth between the robot ID and the short address.  I'm also guessing that you are either using something like the <code>select</code> system call to wait for data from any of your robots or trying to read from each one.</p>

<p>If this is the case then you should be able to create a state machine for each robot and whenever you get data from that robot you feed that robot's state machine which processes it and generate a reply to that robot (or even sends data to the other robots' state machines).  The state machine will be almost like your single robot program, except that it will rely on an event loop getting data from the robots for it.  You may also want the event loop to be able to supply timer alarms for the state machines.  This is similar to the way you would write a http server that could handle multiple clients at one time.</p>

<p>If I was totally wrong about your API and you have those zigbee radios that just act as a serial port without wires then you are in a mess, because I think that you will have to reconfigure them in order to use more than one endpoint at a time -- and will have to change the API you use to communicate with the robots.</p>
",299301,0,0,3884562,"Yeah, what exactly are you asking?",Remote
283,636,3185471,Distance travelled by a robot using Optical Flow,|localization|opencv|robotics|opticalflow|,"<p>is there a way to find out the distance travelled by a robot using Optical Flow? For example, using OpenCV, I'm able to find out the velocity of each pixel between 2 images taken by a camera. However, I don't know where to go with this to find out the corresponding distance travelled by the robot. Can you suggest a way to do this?</p>

<p>My main aim is to do the localization of the robot and for that I need the distance travelled by it between 2 instances.</p>
",40365.44375,,570,2,2,0,,280454,India,40233.65139,573,5905756,"<p>No, not directly.  You can determine distance to objects, and then back calculate distance travelled from there, but it will likely be computationally expensive.</p>
",369609,0,0,3280502,"You have already asked this question: [Finding distance travelled by robot using Optical Flow](http://stackoverflow.com/questions/3069144/finding-distance-travelled-by-robot-using-optical-flow ""title"")",Other
284,778,3621244,Accounting for misalignment of wheels in diff drive odometry,|math|coordinates|robotics|,"<p>I have a differential drive robot, using odometery to infer its position.</p>

<p>I am using the standard equations:</p>

<pre><code>WheelBase = 35.5cm;
WheelRadius = 5cm;
WheelCircumference = (WheelRadius * 2 * Math.PI);
WheelCircumferencePerEncoderClick = WheelCircumference / 360;

DistanceLeft = WheelCircumferencePerEncoderClick * EncoderCountLeft
DistanceRight = WheelCircumferencePerEncoderClick * EncoderCountRight

DistanceTravelled = (DistanceRight + DistanceLeft) / 2
AngleChange (Theta) = (DistanceRight - DistanceLeft) / WheelBase
</code></pre>

<p>My (DIY) chassis has as a slight feature, where over the course of the wheel base (35.5cm), the wheels are misaligned, in that the left wheel is 6.39mm (I'm a software person not a hardware person!) more 'forwards' than the right wheel. (The wheels are the middle of the robot.)</p>

<p>I'm unsure how to calculate what I must add to my formulas to give me the correct values.. It doesn't affect the robot much, unless it does on the spot turns, and my values are way off, I think this is causing it.</p>

<p>My first thought was to plot the wheel locations on a grid, and calculate the slope of the line of their positions, and use that to multiply... something.</p>

<p>Am I on the right track? Can someone lend a hand? I've looked around for this error, and most people seem to ignore it (since they are using professional chassis). </p>
",40422.79097,3623793,489,2,0,3,,282090,,40188.52361,138,3622288,"<p>If I'm reading this right, the problem is that because the left wheel is ahead of the right, when they turn at different rates they cannot roll without slipping. The greater the difference in turning rates, the worse the problem gets, which is probably why it shows up while ""turning on a dime"", when the rotations are opposed.</p>

<p>I think the way to solve it is to consider a related problem: two wheels located correctly, but both skewed a little to the left (which is exactly the situation you're in, thinking of the diagonal between the wheels as the ""wheelbase""). The motion can then be broken into two components, the major forward-back component, which acts normally, and the minor sideways component which causes no angle change and depends only on the sum of the wheel rotations.</p>

<p>I'll see if I can come up with some math that makes sense...</p>
",128940,0,0,,,Actuator
285,799,3713385,"Writing Hardware Drivers, APIs, and Services",|wcf|hardware|soa|robotics|hardware-interface|,"<p>What books/blogs/podcasts etc... discuss patterns and best practices for designing software systems that interact with custom hardware and robotics? I am particularly interested in any books that would discuss strategies for writing a program that has to orchestrate and coordinate actions between several different custom hardware and robotics systems.     </p>
",40435.93681,3852334,136,1,0,0,,170347,,40064.76042,47,3852334,"<p>I'm not sure of any particular blog or podcast based resources for this type of material.</p>

<p>However, it sounds like what you are doing would be a perfect fit for something like ROS (Robotics Operating System), an open-source robotics framework with a heavy emphasis on software engineering ideas.  While it's not a book or blog, the framework along with the associated community is a great way to learn about the hardware/software interaction necessary for robotics systems.</p>

<p>You can find the website at <a href=""http://www.ros.org/"" rel=""nofollow"">ros.org</a></p>
",460065,1,0,,,Other
286,910,4557970,Best practices in Python for handling external exceptions (do I use raise?),|python|robotics|,"<p>I'm writing a procedure in Python which at it's fundamental level communicates with a motor controller. It is possible for the controller to throw flags indicating that an error has occurred. I'm trying to figure how to best handle these errors.</p>

<p>In the example below, there are three possible errors, a temperature fault, a current limit fault and a voltage fault. I've handled them differently. Is there a correct way or is it subjective?</p>

<pre><code>class motor_fault(Exception):
    def __init__(self,error):
        motor.move_at = 0  #Stop motor
        self.error = error
    def __str__(self):
        return repr(self.value)

motor.velocity_limit = motor.slow
motor.velocity_limit_enable = True
try:
    motor.move_to_absolute = motor.instrument_pos
    while motor.in_position == 0:
        if motor.current_limit == 1:
            motor.move_at = 0 #Stop motor
            print('Motor current error')
            break
        if motor.temp_fault == 1: raise motor_fault('Temperature Fault')
        if motor.voltage_fault == 1: raise voltage_fault:
        time.sleep(0.5)
    else:
        print('reached desired instrument position with no faults')
except motor_temp_fault as e:
    #Not sure what I'd do here...
    print('My exception occurred, value:', e.error)
    pass
except:
    motor.move_at = 0 #Stop motor just in case
    print(' some other fault, probably voltage')
else:
    print (' this is only printed if there were no errors')
finally:
    print ('this is printed regardless of how the try exits')
</code></pre>

<p>It seems a lot simpler to drop the whole <code>try:</code>. Just set a flag in the while loop and break. After the loop, look at the flag and see if the while loop exited successfully.</p>

<pre><code>fault = False
while motor.in_position == 0:
    if motor.current_limit == 1:
        fault = 'Motor current error'
        break
    if motor.temp_fault == 1:
        fault = 'Motor temperature error'
        break
    if motor.voltage_fault == 1:
        fault = 'Motor voltage error'
        break
    time.sleep(0.5)
else:
    print('reached waterline with no faults')
if fault:
    motor.move_at = 0 #Stop motor
    print(fault)
    # Now look at the fault string to determine the next course of action.
</code></pre>

<p>But that somehow seems wrong or non-pythonic to use a term I don't really understand. Is there really anything wrong with this?
Thanks and please keep in mind I'm not CS major and I haven't taken a programming class since 1982.</p>
",40541.89028,4558138,1387,4,4,2,0,141489,"Morehead City, NC",40014.66389,110,4558046,"<p>I'd go for exceptions with many except clauses for all the different exceptions you would like to handle at this point as these cases seem to be exceptional/failure scenarios.</p>

<p>I would not use flags to represent these scenarios as it would add more fields to motor which dont seem to be useful/relevant outside of this use case.</p>

<p>As far as knowing if it is the 'correct' way to handle this well, if both solution works they are both correct!</p>

<p>Hope I was clear enough... ;-)</p>
",73729,0,0,5005961,"...The motor controller driver (written in Java) sends status updates via JSON-RPC back to my python program whenever any values change. As you might suspect, there's a separate thread which listens for these messages and updates the motor's attributes.

The code I'm writing is a port from a simple version of BASIC which ran on a micro-controller. The code needs to be simple enough that someone with a somewhat limited programming knowledge can modify the behavior if not the functionality.",Other
287,535,2734897,C# Robotics / Hardware,|c#|robotics|,"<p>I'm aware of Phidgets, however, I'm looking for some other types of hardware that can be interfaced with C#.</p>

<p>Anyone got some good ones?</p>
",40297.21389,2741125,3410,5,1,9,0,88770,"Portland, OR, USA",39911.80694,317,2734931,"<p>Actually Lego Mindstorms kits are cheap and have a lot of different libraries to code in. 
<a href=""http://msdn.microsoft.com/robotics/"" rel=""nofollow noreferrer"">Microsoft Robotics</a> for example. More info can be pulled from <a href=""http://blogs.msdn.com/coding4fun/archive/2007/07/16/3902344.aspx"" rel=""nofollow noreferrer"">this article</a>. My experience with Lego Mindstorms was before the NXT versions and using C however it was a great and challenging time. I may even look into grabbing a kit now that this popped up..</p>
",36,2,0,2762956,There's excellent information about all types of physical computing (including robotics) at http://www.chiphacker.com.,Specifications
288,805,4032394,What languages/internet protocols for controlling robots/electronics remotely?,|java|c++|c|programming-languages|robotics|,"<p>I wonder what languages are used in robots and electronics. Is it low level languages like Java, C, C++ etc?</p>

<p>And if these robots and electronics could be controlled from another place, what protocol is used?</p>

<p>It couldn't be HTTP Rest, could it? :)</p>
",40478.45069,4032688,1005,8,2,2,,206446,,40125.92986,2201,4032432,"<p>I recent made a simple remote controlled robot programmed in Java with the help of this book</p>

<p><a href=""http://www.google.co.uk/products/catalog?q=build+java+robots&amp;hl=en&amp;cid=346434932749925759&amp;ei=WATITISGE5_g2ASm_tilCQ&amp;sa=title&amp;ved=0CAcQ8wIwADgA#p"" rel=""nofollow"">http://www.google.co.uk/products/catalog?q=build+java+robots&amp;hl=en&amp;cid=346434932749925759&amp;ei=WATITISGE5_g2ASm_tilCQ&amp;sa=title&amp;ved=0CAcQ8wIwADgA#p</a></p>

<p>This book showed me how to talk to the robot using bluetooth.</p>

<p>I've also read that BASIC is a good language to get started with, when build your first robot.</p>
",393908,0,2,4326237,Java is not a low level language by any definition.,Remote
289,592,3068658,What techniques exist for the software-driven locomotion of a bipedal robot?,|language-agnostic|machine-learning|robotics|robocup|,"<p>I'm programming a software agent to control a robot player in a simulated game of soccer.  Ultimately I hope to enter it in the RoboCup competition.</p>

<p>Amongst the various challenges involved in creating such an agent, the motion of it's body is one of the first I'm facing.   The simulation I'm targeting uses a Nao robot body with 22 hinge to control.  Six in each leg, four in each arm and two in the neck:</p>

<p><a href=""http://simspark.sourceforge.net/wiki/index.php/Soccer_Simulation"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vXdAU.png"" width=""200"" /></a><br>
<sub>(source: <a href=""http://simspark.sourceforge.net/wiki/images/b/b4/Models_NaoVirtual.png"" rel=""nofollow noreferrer"">sourceforge.net</a>)</sub>  </p>

<p>I have an interest in machine learning and believe there must be some techniques available to control this guy.</p>

<p>At any point in time, it is known:</p>

<ul>
<li>The angle of all 22 hinges</li>
<li>The X,Y,Z output of an accelerometer located in the robot's chest</li>
<li>The X,Y,Z output of a gyroscope located in the robot's chest</li>
<li>The location of certain landmarks (corners, goals) via a camera in the robot's head</li>
<li>A vector for the force applied to the bottom of each foot, along with a vector giving the position of the force on the foot's sole</li>
</ul>

<p>The types of tasks I'd like to achieve are:</p>

<ul>
<li>Running in a straight line as fast as possible</li>
<li>Moving at a defined speed (that is, one function that handles fast and slow walking depending upon an additional input)</li>
<li>Walking backwards</li>
<li>Turning on the spot</li>
<li>Running along a simple curve</li>
<li>Stepping sideways</li>
<li>Jumping as high as possible and landing without falling over</li>
<li>Kicking a ball that's in front of your feet</li>
<li>Making 'subconscious' stabilising movements when subjected to unexpected forces (hit by ball or another player), ideally in tandem with one of the above</li>
</ul>

<p>For each of these tasks I believe I could come up with a suitable fitness function, but not a set of training inputs with expected outputs.  That is, any machine learning approach would need to offer <a href=""https://en.wikipedia.org/wiki/Unsupervised_learning"" rel=""nofollow noreferrer"">unsupervised learning</a>.</p>

<p>I've seen some examples in open-source projects of circular functions (sine waves) wired into each hinge's angle with differing amplitudes and phases.  These seem to walk in straight lines ok, but they all look a bit clunky.  It's not an approach that would work for all of the tasks I mention above though.</p>

<p>Some teams apparently use inverse kinematics, though I don't know much about that.</p>

<p>So, what approaches are there for robot biped locomotion/ambulation?</p>

<hr>

<p>As an aside, I wrote and published <a href=""https://code.google.com/archive/p/tin-man"" rel=""nofollow noreferrer"">a .NET library called TinMan</a> that provides basic interaction with the soccer simulation server.  It has a simple programming model for the sensors and actuators of the robot's 22 hinges.</p>

<p>You can read more about RoboCup's 3D Simulated Soccer League:</p>

<ul>
<li><a href=""http://en.wikipedia.org/wiki/RoboCup_3D_Soccer_Simulation_League"" rel=""nofollow noreferrer"">http://en.wikipedia.org/wiki/RoboCup_3D_Soccer_Simulation_League</a></li>
<li><a href=""http://simspark.sourceforge.net/wiki/index.php/Main_Page"" rel=""nofollow noreferrer"">http://simspark.sourceforge.net/wiki/index.php/Main_Page</a></li>
<li><a href=""http://code.google.com/p/tin-man/"" rel=""nofollow noreferrer"">http://code.google.com/p/tin-man/</a></li>
</ul>
",40347.4125,,722,5,3,8,0,24874,"Melbourne, Australia",39724.62639,15318,3069718,"<p>I was working on a project not that dissimilar from this (making a robotic tuna) and one of the methods we were exploring was using a <a href=""http://en.wikipedia.org/wiki/Genetic_algorithm"" rel=""nofollow noreferrer"">genetic algorithm</a> to tune the performance of an artificial <a href=""http://en.wikipedia.org/wiki/Central_pattern_generator"" rel=""nofollow noreferrer"">central pattern generator</a> (in our case the pattern was a number of sine waves operating on each joint of the tail).  It might be worth giving a shot, Genetic Algorithms are another one of those tools that can be incredibly powerful, if you are careful about selecting a fitness function. </p>
",369609,2,1,3210880,"@John - I just tried again.  Only two.  Maybe because I'm in Australia or because I don't generally use Bing and haven't configured it somehow...  Anyway, care to share some of the better links?",Actuator
290,462,2199696,How to avoid that the robot gets trapped in local minimum?,|robotics|a-star|motion-planning|,"<p>I have some time occupying myself with motion planning for robots, and have for some time wanted to explore the possibility of improving the opportunities as ""potential field"" method offers. My challenge is to avoid that the robot gets trapped in ""local minimum"" when using the ""potential field"" method. Instead of using a ""random walk"" approach to avoid that the robot gets trapped I have thought about whether it is possible to implement a variation of A* which could act as a sort of guide for precisely to avoid that the robot gets trapped in ""local minimum"".</p>

<p>Is there some of the experiences of this kind, or can refer to literature, which avoids local minimum in a more effective way than the one used in the ""random walk"" approach.</p>
",40213.52986,,1605,2,0,5,0,163046,"Copenhagen, Denmark",40050.84097,46,2202108,"<p>A* and potential fields are all search strategies. The problem you are experiencing is that some search strategies are more ""greedy"" than others, and more often than not, algorithms that are too greedy get trapped in local minimum.</p>

<p>There are some alternatives where the tension between greediness (the main cause of getting trapped on local minimum) and diversity (trying new alternatives that don't seem to be a good choice in the short term) are parameterized.</p>

<p>A few years ago I've researched a bit about ant algorithms (search for Marco Dorigo, ACS, ACO) and they have a family of search algorithms that can be applied to pretty much anything, and they can control the greediness vs. exploration of your search space. In one of their papers, they even compared the search performance solving the TSP (the canonical traveling salesman problem) using genetic algorithms, simulated annealing and others. Ant won.</p>

<p>I've solved the TSP in the past using genetic algorithms and I still have the source code in delphi if you like.</p>
",229081,5,1,,,Moving
291,954,4973854,C or C++ for a Robot?,|c++|c|robotics|,"<p>Greetings,</p>

<p>I am trying to decide between C and C++ for my robot. I am a 5+ year veteran of Visual Basic.NET, however I'm going with Linux (Ubuntu) on this robot, and there is a compatibility problem between Linux and the .NET Framework. I want to stick with whichever language I choose for all of my projects, so I want to make sure that I choose the most appropriate one for the task.
For reference, I will describe my current robot in progress and what I am going to do with it. I am in the process of building a full-sized R4 Astromech (yep, I'm one of those guys). I have incorporated a PC motherboard with an Intel Core 2 2.1 GHz processor, 1 GB RAM. I will be using a scratch-built parallel interface card to control the drive motors, head motor, as well as a secondary parallel interface card (going to a second parallel port) which all of the sensors (IR, Ultrasonic Ranging, Visual Recognition via webcam, etc.) will be going to. Commands will be given using speech recognition (currently have a VB.NET scratch-built recognition program that I will be adapting to the new language).
Given the specifications and desired goals listed above, would I be better off with C or C++? I greatly appreciate any suggestions that you might have.
Thanks!
Thaskalas</p>
",40585.86458,,4910,7,6,17,0,613626,,40585.86458,12,4973940,"<p>What do you mean by a compatibility problem? Have you looked at <a href=""http://en.wikipedia.org/wiki/Mono_%28software%29"" rel=""nofollow"">Mono</a>? It's an open-source implementation of the .NET libraries. It's geared toward C# not VB.NET but if you're more comfortable in a .NET environment use that. Speed isn't really an issue here as a Core2Duo is plenty fast for what you need to do.</p>

<p>If Mono won't work for you, I'd recommend C++. There are a lot more libraries out there for C++ (or at least, I am familiar with more, e.g. <a href=""http://www.boost.org/"" rel=""nofollow"">Boost</a>), which can use most C libraries too. There's no real speed penalty for using C++. While using C wouldn't be bad per-se, C++ has some benefits and no drawbacks, so it's probably the better choice.</p>
",185936,8,3,5552965,@Hellfrost What's your problem with VB.net? While I prefer C# that's just a stylistic choice.,Specifications
292,1263,8091124,LeJOS NXT movement in centimeters,|java|robotics|lejos-nxj|,"<p>I just started learning LeJOS programming and have a small probelm. I understand that I can measure movement distance in seconds and degrees. Is it possible to measure distance in centimeters, for instance.
If yes, then how?</p>
",40858.32986,,565,2,0,0,,,,,,8091204,"<p>I am assuming that your robot uses wheels to enable it to move. If you can obtain the amount of degrees that your wheel turns, you can use the <a href=""http://math.about.com/od/formulas/ss/surfaceareavol_9.htm"" rel=""nofollow"">Arc Length Formula</a> to obtain the linear distance that your wheel moved. </p>
",243943,2,0,,,Other
293,1140,6007822,Finding path obstacles in a 2D image,|opencv|robotics|image-recognition|,"<p>what approach would you recommend for finding obstacles in a 2D image?</p>

<p>Here are some key points I came up with till now:</p>

<p>I doubt I can use object recognition based on ""database of obstacles"" search, since I don't know what might the obstruction look like.
I assume color recognition might be problematic if the path does not differ a lot from the object itself.</p>

<p>Possibly, adding one more camera and computing a 3D image (like a Kinect does) would work, but that would not run as smooth as I require.</p>

<p>To illustrate the problem; robot can ride either left or right side of the pavement. In the following picture, left side is the correct choice:
<img src=""https://i.stack.imgur.com/A95h0.jpg"" alt=""enter image description here""></p>
",40678.43542,6027601,3043,1,4,3,0,326257,"Prague, Czech Republic",40294.77708,1040,6027601,"<p>If you know what the path looks like, this is largely a <em>classification problem</em>. Acquire a bunch of images of path at different distances, illumination, etc. and manually label the ground in each image. Use this labeled data to train a classifier that classifies each pixel as either ""road"" or ""not road."" Depending upon the texture of the road, this could be as simple as classifying each pixels' RGB (or HSV) values or using OpenCv's built-in histogram back-projection (i.e. <code>cv::CalcBackProjectPatch()</code>).</p>

<p>I suggest beginning with manual thresholds, moving to histogram-based matching, and only using a full-fledged machine learning classifier (such as a Naive Bayes Classifier or a SVM) if the simpler techniques fail. Once the entire image is classified, all pixels that are identified as ""not road"" are obstacles. By classifying the <em>road instead of the obstacles</em>, we completely avoided building a ""database of objects"".</p>

<hr>

<p>Somewhat out of the scope of the question, the easiest solution is to add additional sensors (""throw more hardware at the problem!"") and directly measure the three-dimensional position of obstacles. In order of preference:</p>

<ol>
<li><strong>Microsoft Kinect</strong>: Cheap, easy, and effective. Due to ambient IR light, it only works indoors.</li>
<li><strong>Scanning Laser Rangefinder:</strong> Extremely accurate, easy to setup, and works outside. Also very expensive (~$1200-10,000 depending upon maximum range and sample rate).</li>
<li><strong>Stereo Camera:</strong> Not as good as a Kinect, but it works outside. If you cannot afford a pre-made stereo camera (~$1800), you can make a decent custom stereo camera using USB webcams.</li>
</ol>

<p>Note that professional stereo vision cameras can be <em>very fast</em> by using custom hardware (Stereo On-Chip, STOC). Software-based stereo is also reasonably fast (10-20 Hz) on a modern computer.</p>
",111426,2,1,6949549,"@Eric, thanks for your note. I think it's not necessary to get a Kinect-like setup but only to detect obstacles. Many cheap and simple robots do it using IR. Am I right?",Moving
294,1224,6793028,Can we control a robot using an Android phone?,|android|robot|,"<p>After stepping into the world of Android, I wondered if an Android phone can be used as a remote to control a basic pick and place robot . If just an SMS could be send to control the action of a robot like say ""pick object 1 at distance x"" would result in the bot performing the specified action.</p>

<p>Yes, It will involve Artificial Intelligence coupled with the basics of developing a robot but then I wanted to know whether it's possible to develop a machine like this ? If yes , how should one kickstart things ? Would Android ADK be helpful ?</p>

<p>Thanks  </p>
",40746.68333,6793208,3497,8,0,3,,841915,"Ahmadabad, India",40737.14028,1513,6793087,"<p>To be honest, I'd recommend knowing how to do it with a computer first. Once you know that, learn how to program Android (that's what we're here for) and get started. Hell, you could make an application to control it - that might be more impressive. </p>

<p>Edit: if you're controlling it via SMS, why are you limited to Android? All of the coding for that would be done on the side of the robot, and you'd have to assign a number for that. I'd recommend an application and communicate via WIFI or Bluetooth. </p>
",801858,1,3,,,Remote
295,1267,8348698,Mindstorm NXT Programming Loop Exit Conditions,|robotics|nxt|lego-mindstorms|,"<p>I am developing a robot for an engineering class.  For the purposes of the class I am required to use the NXT programming language.  To move, the robot needs to follow a solid black line.  </p>

<p>If the robot looses the line, I have it scan to the left for 1 second, or until it reaches a black line.  If no line is found it scans to the right for 2 seconds so the initial position is reached then 1 more second or rotation is achieved.  </p>

<p>I have the loop set up so that if the line has not been found, the robot continues to move.  That runs for a full 1 second time period.  If the line is found, motion stops, but the full second still has to complete.  Ultimately that means that my program works perfectly, but is really really slow.  </p>

<p><strong>tl;dr Is there a way to make loops with two exit condition in the LEGO Mindstorm programming environment?  Either after 1 second has elapsed, or a sensor gets the desired input?</strong></p>
",40878.90486,8563890,9956,3,1,3,,831913,United States,40730.64722,495,8356106,"<p>What you could do is make the timeout shorter (100 ms for instance) and stop if the line is found OR the loop ran 10 times.</p>

<p>I am no mindstorms expert, but I expect it to have an OR function.</p>
",47860,0,1,10321868,Have I retagged it correctly?,Incoming
296,1244,7541489,Obstacle avoidance using 2 fixed cameras on a robot,|graphics|opencv|computer-vision|robotics|,"<p>I will be start working on a robotics project which involves a mobile robot that has mounted 2 cameras (1.3 MP) fixed at a distance of 0.5m in between.I also have a few ultrasonic sensors, but they have only a 10 metter range and my enviroment is rather large (as an example, take a large warehouse with many pillars, boxes, walls .etc) .My main task is to identify obstacles and also find a roughly ""best"" route that the robot must take in order to navigate in a ""rough"" enviroment (the ground floor is not smooth at all). All the image processing is not made on the robot, but on a computer with NVIDIA GT425 2Gb Ram.   </p>

<p>My questions are :</p>

<ol>
<li><p>Should I mount the cameras on a rotative suport, so that they take pictures on a wider angle?</p></li>
<li><p>It is posible creating a reasonable 3D reconstruction based on only 2 views at such a small distance in between? If so, to what degree I can use this for obstacle avoidance and a best route construction?</p></li>
<li><p>If a roughly accurate 3D representation of the enviroment can be made, how can it be used as creating a map of the enviroment? (Consider the following example: the robot must sweep an fairly large area and it would be energy efficient if it would not go through the same place (or course) twice;however when a 3D reconstruction is made from one direction, how can it tell if it has already been there if it comes from the opposite direction  )</p></li>
</ol>

<p>I have found this <a href=""http://mkoval.org/downloads/projects/igvc/igvc_design.pdf"" rel=""nofollow"">response</a> on a similar question , but  I am still concerned with the accuracy of 3D reconstruction (for example a couple of boxes situated at 100m considering the small resolution and distance between the cameras).</p>

<p>I am just starting gathering information for this project, so if you haved worked on something similar please give me some guidelines (and some links:D) on how should I approach this specific task. </p>

<p>Thanks in advance,
Tamash</p>
",40810.82639,,1824,2,2,5,0,920202,,40785.71875,625,7572051,"<p>If you want to do obstacle avoidance, it is probably easiest to use the ultrasonic sensors. If the robot is moving at speeds suitable for a human environment then their range of 10m gives you ample time to stop the robot. Keep in mind that no system will guarantee that you don't accidentally hit something.</p>

<blockquote>
  <p>(2) It is posible creating a reasonable 3D reconstruction based on only 2 views at such a small distance in between? If so, to what degree I can use this for obstacle avoidance and a best route construction?</p>
</blockquote>

<p>Yes, this is possible. Have a look at ROS and their vSLAM. <a href=""http://www.ros.org/wiki/vslam"" rel=""nofollow"">http://www.ros.org/wiki/vslam</a> and <a href=""http://www.ros.org/wiki/slam_gmapping"" rel=""nofollow"">http://www.ros.org/wiki/slam_gmapping</a> would be two of many possible resources. </p>

<blockquote>
  <p>however when a 3D reconstruction is made from one direction, how can it tell if it has already been there if it comes from the opposite direction </p>
</blockquote>

<p>Well, you are trying to find your position given a measurement and a map. That should be possible, and it wouldn't matter from which direction the map was created. However, there is the loop closure problem. Because you are creating a 3D map at the same time as you are trying to find your way around, you don't know whether you are at a new place or at a place you have seen before. </p>

<p>CONCLUSION
This is a difficult task!</p>

<p>Actually, it's more than one. First you have simple obstacle avoidance (i.e. <code>Don't drive into things.</code>). Then you want to do simultaneous localisation and mapping (SLAM, read Wikipedia on that) and finally you want to do path planning (i.e. sweeping the floor without covering area twice).</p>

<p>I hope that helps?</p>
",461597,1,4,9180930,There's a reason DARPA has been doing grand challenges in this area.  It's not easy.,Moving
297,1146,6041316,iphone/ipad: create a remote control freature,|iphone|controller|robotics|,"<p>I am quite new to iOS development and just thought to take guidance from experts</p>

<p>Actually I have to do a project  in which I can use iPAd/iPhone to control some external device like camera movement or anything similar like that, some PIC programming or anything related to robotics, mechanics which can be controlled from iOS based device.</p>

<p>I am kind of lost goggling please guide me on this.</p>

<p>If you can help me  with these I can get some concrete redirections</p>

<p>1) Links to whitepapers / articles / blogs having relevant material</p>

<p>2) Links of third party libraries which can help me in this</p>

<p>3) Links of demo application which are already there</p>

<p>4) What stream should I focus on to get material regarding the same.</p>

<p>eg: something like survilance system</p>

<p>Thanks in advance</p>
",40681.31458,6260588,654,1,3,1,,195504,India,40109.74583,1161,6260588,"<p>So the practical ways to interface an iOS device to a robot are over WiFi, establishing either a UDP or TCP socket. Here are a few links:</p>

<p><a href=""http://www.iphonedevsdk.com/forum/iphone-sdk-development/2023-tcp-ip-udp-networking.html"" rel=""nofollow"">http://www.iphonedevsdk.com/forum/iphone-sdk-development/2023-tcp-ip-udp-networking.html</a>
http://www.youtube.com/watch?v=4XQeZE4nh6M
<a href=""http://www.youtube.com/watch?v=0ipAKzCwn4Y"" rel=""nofollow"">http://www.youtube.com/watch?v=0ipAKzCwn4Y</a></p>

<p>I would not recommend the Bluetooth path, as Apple considers bluetooth as an ""External Accessory"" and requires MFi certification (Made for iPhone)</p>
",229081,1,0,7303247,sure:http://www.youtube.com/watch?v=4XQeZE4nh6M,Remote
298,1018,5361791,Robot exploration algorithm,|algorithm|artificial-intelligence|robotics|,"<p>I'm trying to devise an algorithm for a robot trying to find the flag(positioned at unknown location), which is located in a world containing obstacles. Robot's mission is to capture the flag and bring it to his home base(which represents his starting position). Robot, at each step, sees only a limited neighbourhood (<strong>he does not know how the world looks in advance</strong>), but he has an unlimited memory to store already visited cells.  </p>

<p>I'm looking for any suggestions about how to do this in an efficient manner. Especially the first part; namely getting to the flag.</p>

<p><img src=""https://i.stack.imgur.com/cq17j.png"" alt=""enter image description here""></p>
",40621.47986,,9593,8,5,21,0,42803,Europe,39785.54792,199,5361847,"<p>With a simple <a href=""http://en.wikipedia.org/wiki/Depth-first_search"" rel=""nofollow"">DFS</a> search at least you will find the flag:)</p>
",222674,1,2,6057719,"@Lucasmus: I think it's hard to find some really smart algo, because the robot doesn't know anything about the world, beyond his limited neighbourhood.
@Mitch Wheat:
Well, so far I have only come up with a list of desired properties such algorithm would encompass:
- minimal repetition of visiting already visited cells
- global goal: finding the flag
- local goal: avoiding obstacles and infinite loops",Moving
299,1274,8463489,How to make a robot follow a line using its video camera,|computer-vision|navigation|robotics|,"<p>I'm trying to get a robot to identify a line on the ground and follow it.<br>
I've searched the internet and found many examples of line following robots, but all of them are using specialized sensors to detect the line.<br>
I'd like to use the camera on the robot for that purpose.</p>

<p>I'm new to the field of computer vision, so I'd like some advice on how to approach the problem.  Specifically, how can I detect the line and its angle/direction in relation to the robot? How can I detect turns?</p>

<p>Update following nikies comment:<br>
How the line looks is up to me, i was thinking to put some bright colored tape on the ground, but i can use whatever is easiest...<br>
The camera can take both color and b&amp;w image.<br>
Lighting and location may vary but i'll worry about it later, i just want to know what to look for to get started. Is there a ""common"" way to do it ?</p>
",40888.46667,8495755,7268,4,2,0,0,249878,,40191.60278,620,8495755,"<p>Here's one approach, suitable for refinement.</p>

<p>Through a combination of zooming, a pixellation filter, and thresholding, reduce the camera input to a 3 by 3 grid of white or black squares.  With suitable adjustment, the reduction should be able to blow up the line such that it takes up exactly three of the reduced pixels.  The robot's logic then consists of moving in one of eight directions to keep the center pixel black.</p>

<p>The image after reduction:</p>

<pre><code>☐ ■ ☐
☐ ■ ☐ ↑ move forward one unit
☐ ■ ☐
</code></pre>

<p>What a left turn looks like:</p>

<pre><code>☐ ☐ ☐
■ ■ ☐ ← turn 90 degrees left
☐ ■ ☐
</code></pre>

<p>This is a very simple scheme, and converting the video input to a clean 3 by 3 grid isn't a trivial task, but it should be enough to get you moving in the right direction.</p>
",85950,6,1,10515605,i updated the post to answer your questions,Incoming
300,1252,7804388,What is the right RTOS for a humanoid robot?,|real-time|robotics|rtos|freertos|,"<p>We're students developing a mid-size (~ 4.5 feet tall) humanoid robot as a sponsored research project in college. The major tasks that the robot should be able to perform include: moving around (forward, backward, sideways), running, picking up objects. We're considering using a <em>hard</em> real-time operating system to control the robot. However, since we're new to this field with practically no background in embedded systems or operating systems, and there are a wide range of options available, we're not sure which one would be a suitable choice. We've come across the following (in brackets is our current impression of them):</p>

<ol>
<li>RTLinux (now dead, kernel 2.4.x, gcc 2.95 (so difficult to build), little to no documentation)</li>
<li>FreeRTOS (good community and documentation, popular, ported to many architectures)</li>
<li>uc-OS II (small, clean core, light-weight)</li>
<li>RTAI (Linux-based)</li>
</ol>

<p>I have a number of questions:</p>

<ol>
<li>Which of the options would be better suited for this project? I know this sounds a little subjective, but <em>any</em> advice would be greatly appreciated. If you feel some important information is missing, please do point that out.</li>
<li>I've come across something called the <a href=""https://www.osadl.org/Realtime-Linux.projects-realtime-linux.0.html"" rel=""noreferrer"">CONFIG_PREEMPT_RT patch</a> for the Linux kernel, which grants hard real-time capabilities to the kernel. There are also pre-compiled kernels with this patch available for Debian-based distributions. Would that alone be sufficient for our requirements?</li>
<li>We have very little knowledge of operating systems in general. Is it necessary for us to learn about them first? If yes, what is a good, short primer to the subject?</li>
</ol>

<p><strong>UPDATE:</strong> Thank you for your incredibly detailed answers. It's clear that we're going about this the wrong way; diving in with no knowledge and vauge requirements certainly would be bad. We'll have to sit down and chalk out exactly what we need. As and when we're sufficiently ahead with that, we'll try to figure out a suitable OS. Let's see how that works out. I'm also going to read Chapter 2 of <a href=""https://rads.stackoverflow.com/amzn/click/com/1578201039"" rel=""noreferrer"" rel=""nofollow noreferrer"">MicroC OS II: The Real Time Kernel</a>.</p>
",40834.35208,7813685,8113,3,1,8,0,504611,"Hyderabad, Telangana, India",40493.59792,2017,7813685,"<p>Your choice of OS is unlikly to be determined by the ""humanoid robot"" constraint, there is no specific ""humanoid robot OS"", and certainly no OS would be determined by how tall such a robot is! ;-) !  The critical factors are </p>

<ul>
<li>real-time constraints (for motion control <a href=""http://en.wikipedia.org/wiki/PID_controller"" rel=""nofollow"">PID</a> update, sensor/actuator reaction time etc.)</li>
<li>processor architecture</li>
<li>system architecture (e.g. <a href=""http://distribured%20processing"" rel=""nofollow"">distributed processing</a>, <a href=""http://en.wikipedia.org/wiki/Symmetric_multiprocessing"" rel=""nofollow"">symetric-multiprocessing</a>)</li>
</ul>

<p>Other factors may be important such as:</p>

<ul>
<li>Communications and I/O requirements (e.g Ethernet, TCP/IP, USB, WiFi).</li>
<li>File system support</li>
</ul>

<p>although these need not necessarily be an intrinsic part of the OS in all cases, since third-party platform independent libraries are available in many cases, but where you need them, integration with the OS can be helpful since it avoids you having to deal with thread-safety and resource locking yourself.</p>

<p>Neither of the options you have suggested would likely make into my list.</p>

<p>Anything Linux based will require an MMU (unless using uCLinux or its derivitives, but MMU support is one of the few good reasons for using Linux in an embedded system).  Linux is not intended to be a real-time OS and any real-time support it has is pretty much an after-thought, and will seldom be as deterministic as a true RTOS.  Any Linux will also require significant memory resources just to boot-up, expect a minimum of 4Mb of RAM for anything usable, while RTOS kernels such as FreeRTOS and uC/OS-II require only about 4Kb - you are comparing chalk with cheese here.  That said they do not have the utility of a Linux based OS such as file-systems, or networking support (although those can be added as stand-alone libraries).</p>

<p>If you are going to be performing the motion-control and sensor/actuator functions on the same processor as your cognitive functions, then you certainly need a deterministic RTOS.  If however the platform will be a distributed system with separate processors dealing with motion-control and other real-time sensor/actuator I/O, then you may get away with a simple RTOS kernel or no OS at all in the I/O processors (which can also then be smaller, less powerful processors) and a GPOS in the cognitive (decision making and planning) processor.</p>

<p>I have evaluated FreeRTOS recently, it is minimalistic, simple and small, providing only the basic threading, timing and IPC mechanisms and little else.  It works, but so do many other more attractive options, both commercial and non-commercial.  I compared it with <a href=""http://www.keil.com/rl-arm/kernel.asp"" rel=""nofollow"">Keil's RTX kernel</a> (included with their MDK-ARM tool suite), and the commercial <a href=""http://www.segger.com/embos.html"" rel=""nofollow"">Segger embOS</a>.  It has significantly slower context switching time that the other two candidates (though still in the microseconds on a 72MHz Cortex-M3, and faster than anything you are likely to achieve with Linux).</p>

<p>uC/OS-II is well designed and documented (in Jean Labrosse's book), and is great if you aim were to see how an RTOS works.  Its biggest failing is its very restrictive priority scheduling scheme, which is efficient for very small targets, but possibly not as flexible as you might like.  Each thread must be assigned a distinct priority level so it has no support for round-robin scheduling, useful for non-real-time background tasks. uC/OS-III fixes that shortcoming, but again so do many other options.</p>

<p>If your target processor has an MMU I strongly suggest the use of an OS that supports it in such a way that each thread or process is protected from any other, the system will be far more robust and easy to debug, especially when developed as a team.  In such an OS an errant task that would otherwise stomp on some other thread's resources with non-deterministic and generally hard to debug results, will instead cause an exception and halt right where the error occurred, rather than perhaps sometime later when the corrupted data gets used.</p>

<p>You probably need not restrict yourself to a free or open-source RTOS, many vendors allow free use for education and evaluation.  I would strongly suggest that you consider <a href=""http://www.qnx.com/products/neutrino-rtos/index.html"" rel=""nofollow"">QNX Neutrino</a>, it is <a href=""http://www.qnx.com/products/evaluation/academic_faculty.html"" rel=""nofollow"">free for non-commercial and academic use</a>, and has the most robust intrinsic MMU support available in any RTOS, and all the development tools you need including the Eclipse based Momentics IDE are included.  It is more than just a mere scheduling kernel, including support for all the services you would expect of a complete OS. It runs on ARM, x86, SH-4 PowerPC and MIPS architectures.  Running on x86 is particularly useful since it means you can test and evaluate it, and even develop much of your code in a <a href=""http://www.qnx.com/products/evaluation/#target"" rel=""nofollow"">VM running on your desktop</a>.</p>

<p>Another alternative that is true hard-real-time, while supporting OS services beyond mere scheduling and IPC, is <a href=""http://ecos.sourceware.org/"" rel=""nofollow"">eCos</a>.  It has a native, POSIX and uITRON API, standard drivers for CAN, ADC, SPI, I2C, FLASH, PCI, Serial, Filesystems, USB and PCI and more, and includes TCP/IP networking support.  It is a complete OS in that sense, but unlike Linux is not monolithic; it is scalable and statically linked to your application code, so that features you do not use are simply not included in the runtime binary.  It runs on ARM, CalmRISC, Cortex-M, FR-V, FR30, H8, IA32, 68K/ColdFire, Matsushita AM3x, MIPS, NEC V8xx, PowerPC, SPARC, and SuperH.  Again in theory you could run the IA32 (x86) port it on a VM on a PC for test and development of high level code, though you'd have to get that working yourself unlike QNX's out of the box evaluation.</p>

<p><strong>Added regarding:</strong></p>

<blockquote>
  <p>We have very little knowledge of operating systems in general. Is it necessary for us to learn about them first? If yes, what is a good, short primer to the subject?</p>
</blockquote>

<p>This then is perhaps not the time to start learning Linux (although it has the advantages of wide familiarity and community support, it has a lot of stuff you will not need, and a lot of available support resources will not be familiar with real-time control systems applications.  </p>

<p>Chapter 2 of Labrosse's uC/OS-II book gives a general overview of RTOS concepts such as scheduling, synchronisation and IPC that are applicable to most RTOS not just uC/OS-II.  Similar material is presented in Jack Ganssle's recent <a href=""http://www.eetimes.com/electrical-engineers/education-training/courses/4213896/Fundamentals-of--Real-Time-Operating-Systems"" rel=""nofollow"">RTOS Fundamentals</a> course on EETimes (it is similar perhaps because it is sponsored by Mircium and uses uC/OS-II as a case study, but it is similarly general for the most part).</p>

<p>My solution to getting a quick start in any subject is to Google the subject with ""101"" after it (a common introductory course number in academia).  <a href=""http://www.google.com/search?q=rtos%20101"" rel=""nofollow"">""RTOS 101""</a> will get you some starting points of admittedly varying quality - check the reputability of the source, if it is a company, they may be peddling a specific product, if it is a hobbyist, they may have some insights, but perhaps a narrow view (often relating to specific favourite hardware), and may not have the rigour of an academic paper.</p>

<p><strong>Added regarding CONFIG_PREMPT_RT:</strong></p>

<p>It does not render Linux a hard real-time OS.  It may be suitable for some applications.  If you are doing PID motion control (rather than using a dedicated controller or separate processor), or any kind of closed loop feedback control for that matter, this patch will not enable it in my opinion, at least not reliably.  I found this: 
<a href=""http://archive.linuxgizmos.com/a-comparison-of-real-time-linux-approaches/"" rel=""nofollow"">A comparison of real-time Linux approaches</a>.  It discusses a number of approaches to using Linux in real-time applications, including CONFIG_PREMPT_RT.  It discusses these in detail in part C. </p>
",168986,21,6,9531638,The OS decision should not be the first decision you make. Design your *system solution* first and then look at it (or ask) to find an OS that meets the system requirements and constraints.  None of the OS requirements can be directly determined from your list of high level behaviours.,Specifications
301,1588,14022469,Guiding a Quadrotor Towards a Waypoint,|math|language-agnostic|vector|robotics|,"<p>I am working on a quadrotor, I know its position (a) I know where I would like to go (b) so what I do is calculate a vector c,</p>

<pre><code>c = b - a
c = normalize(c)
</code></pre>

<p>that gives me a unit vector that will take me to my target. But a quadrotor can move in any direction without rotation so what I have tried to do is rotate that vector c by the robots yaw angle then split it into its x y components and pass them to the robot as roll and pitch angles the problem is when the yaw is 0 degrees (-+ 5) this works but when the yaw is say +90 or -90 it fails and steers to wrong directions. My question is am I missing something obvious here?</p>
",41267.59375,,136,1,1,0,,1926790,,41267.57569,7,14191503,"<p>Not doing all the research to be 100% sure of this answer, but it sounds like you are getting a (kind of?) <a href=""http://en.wikipedia.org/wiki/Gimbal_lock"" rel=""nofollow"">Gimbal Lock</a>. A solution to this is to use <a href=""http://en.wikipedia.org/wiki/Quaternion"" rel=""nofollow"">Quaternions</a> as they are not succeptible to Gimbal Lock.</p>
",571138,0,0,19375126,"Do a search on ""Euler angle"" or ""quaternion"" or ""rotation matrix"" - three solutions to the same problem.  Each has its advantages.",Actuator
302,1350,10050554,Robot Motion in Python,|python|robotics|,"<p>I am trying to program a robot to move. The robot moves based on where currently is. There are four places where it can be:</p>

<pre><code>LOCATION1 Motion Plan is like so,
5 6
3 4
1 2
Initial positon is (x1,y1)
This gets coded as (x1,y1)-&gt;(x1+dx,y1)-&gt;(x1,y1+dy)-&gt;(x1+dx,y1+dy) ... and so on

LOCATION2 Motion Plan is like so,
5 3 1
6 4 2
The initial position is (x1,y1)
This gets coded as (x1,y1)-&gt;(x1,y1-dy)-&gt;(x1-dx,y1)-&gt;(x1-dx,y1-dy) ... and so on

LOCATION3 Motion Plan is like so,
6 5
4 3
2 1
Initial positon is (x1,y1)
This gets coded as (x1,y1)-&gt;(x1-dx,y1)-&gt;(x1,y1+dy)-&gt;(x1-dx,y1+dy) ... and so on

LOCATION4 Motion Plan is like so,
6 4 2
5 3 1
The initial position is (x1,y1)
This gets coded as (x1,y1)-&gt;(x1,y1+dy)-&gt;(x1-dx,y1)-&gt;(x1-dx,y1+dy) ... and so on
</code></pre>

<p>I am struggling to come up with a good pythonic way to code this. I am thinking on the lines of defining 4 different next move rules and then having a bunch of if statements that choose the correct rules</p>

<p>Has someone done something similar... Is there a better way</p>
",41006.00208,10050806,1279,3,1,5,0,1257953,San Francisco Bay Area,40976.85764,311,10050582,"<p>You could do that, what you would do is.</p>

<pre><code>def motion_loc1(x1,y1,dx,dy):
     # your operation


def motion_loc2(x1,y1,dx,dy):
     # your operation
</code></pre>

<p>And then in the main program, depending upon x1, y1 call the various motion methods.</p>
",18852,0,0,12863104,Use state machine design pattern,Moving
303,1562,13856211,Real time programming in robotics with c++,|c++|real-time|robotics|,"<p>I am working on a robotics project with C++ and <a href=""http://en.wikipedia.org/wiki/OpenCV"" rel=""nofollow"">OpenCV</a>. In this step, I have faced a problem which consists of:</p>

<p>I have two methods <code>moveRight()</code> and <code>moveLeft()</code> that I called successively in my code, but the problem is that the second one does not run, because the first one needs time (the time of robot movement), but when I put <code>Sleep(5000)</code> between them (I guessed that five seconds are enough for the movement), all is OK.</p>

<p>What is a programming solution that avoid the use of <code>Sleep</code> (because it makes some other problems)?</p>
",41256.37222,13856254,1664,3,3,5,,1807373,"Madinah, Saudi Arabia",41220.86736,793,13856254,"<p>You can try to add a <a href=""http://en.wikipedia.org/wiki/Indirection"" rel=""nofollow"">layer of indirection</a>. Add a queue of actions to perform, enqueue actions to moveLeft and moveRight, and somewhere else (different thread) execute actions from the queue correctly by waiting for previous action to complete before you do next action. Ideally you need a way to check if action is finished, so you can code it in a event based fashion.</p>
",1520364,6,7,19077952,"I created a robot control program for handling computer wafers. It could seamlessly change course in the middle of a move, based perhaps on data collected when the wafer passed through a laser light skirt. It is not a simple problem. I used a realtime operating system, with custom hardware that provided feedback to sync the controller with the robot. There is not nearly enough info in the question to begin to answer it. If this is a commercial project, I might be able to consult.",Timing
304,1320,9310212,How to write multi threaded testcases in robotium,|android|instrumentation|robotics|robotium|,"<p>How can I implement the <strong>Multithreading</strong> in test cases so that no test case will wait for a particular time in a test suit and can complete the test execution fast?</p>
",40955.46389,,241,0,0,2,0,1213638,,40955.45903,8,,,,,,,,Timing
305,1454,11810812,QMutexLocker() causes UI to freeze,|python|multithreading|pyqt|mutex|robotics|,"<p>I have a controller class which controls a robot (attached over serial interface). This controller is attached to a view. In addition to that I have a thread derived from <code>QThread</code> which periodically reads out the status of the robot.</p>

<p>Reading out the status must not colide with robot commands which get tiggered from the userinterface. Therefore I locked every robot acces with a mutex using <code>QMutexLocker</code> but this causes my userinterface to freeze if such a mutex block gets executed.</p>

<pre><code>class RobotControl(QObject):
    def __init__(self, view):
        super(RobotControl, self).__init__()
        self.view = view
        self.updatethread = UpdatePositionAndStatus(self.robot)
        self.mutex = QMutex()
        self.connect(self.updatethread, SIGNAL(""updateStatus( QString ) ""), self.print_error)
        self.updatethread.start()

@pyqtSlot()  
def init_robot(self):
    """"""
    Initializes the serial interface to the robot interface and checks if
    there is really a robot interface available.
    """"""
    with QMutexLocker(self.mutex):
        # Open interface
        try:
            index = self.view.robotcontrolui.get_selected_interface_index()
            interface = self.interfaces.item(index).text()
            self.robot = RobotController(interface)
        except DeviceError:
            self.view.error_dlg(self.tr(""Couldn't open interface {0}!"".format(interface)))
            self.robot = None
            return

        # Check if there is really a robot interface on the selected serial
        # interface with trying to read status byte
        try:
            self.robot.status()
        except DeviceError:
            # In case of failure release interface
            self.close_robot()
            self.view.error_dlg(self.tr(""Couldn't initialize robot interface!""))
            return

        self.view.robotcontrolui.bt_open_interface.setEnabled(False)
        self.view.robotcontrolui.bt_close_interface.setEnabled(True)

class UpdatePositionAndStatus(QThread):
    def __init__(self, robot, parent=None):
        QThread.__init__(self, parent) 
        self.robot = robot
        self.mutex = QMutex()
    def run(self):
        """""" 
        This function continously reads out the position and the status to for 
        updating it on the userinterface.
        """"""
        try:
            while True:
                if self.robot is not None:
                    # Do robot communication under a lock
                    self.mutex.lock()
                    (_, rel_pos) = self.robot.read_position()
                    status = self.robot.status()
                    self.mutex.unlock()

                    # Display position and status on userinterface
                    self.view.robotcontrolui.update_position_and_status(rel_pos, status)

                # Wait 1 seccond for next update
                QThread.sleep(1.0)
        except DeviceError:
            # Release lock on robot
            self.mutex.unlock()
            self.emit(SIGNAL(""updateStatus( QString )""), self.tr(""Error while updating current position and status!""))
</code></pre>

<p>After triggering the init method the userinterface freezes and the program crashes: Why is this so? How can I avoid that?</p>
",41125.74792,,1555,1,0,1,,967164,,40813.56875,554,11812398,"<p>It's hard to tell because your code sample is incomplete, but I see two fundamental problems with this code:</p>

<ol>
<li><p>You are locking two different QMutex objects. In order for mutual exclusion to work properly, both threads must be locking the <em>same</em> mutex object.</p></li>
<li><p>You appear to be directly interacting with the GUI from the update thread at this line:</p>

<pre><code>self.view.robotcontrolui.update_position_and_status(rel_pos, status)
</code></pre>

<p>Performing GUI operations may <em>only</em> be done from the GUI thread in Qt. It's a fair bet that this is causing your crash. See: <a href=""http://qt-project.org/doc/qt-4.8/threads.html"" rel=""nofollow"">http://qt-project.org/doc/qt-4.8/threads.html</a></p></li>
</ol>
",643629,3,7,,,Remote
306,1528,13482797,What is the purpose of calibrating in OpenCV?,|opencv|robotics|camera-calibration|calibration|,"<p>I am starting a new robotics project (with cameras), so i need to calibrate its two cameras.</p>

<p>My question is: why do i need to calibrate cameras?
Does it have any relation with defining object dimensions and distance to camera?</p>

<p>Any information could be helpful.</p>

<p>Thanks in advance</p>
",41233.90903,48476175,375,1,3,2,,1807373,"Madinah, Saudi Arabia",41220.86736,793,48476175,"<p>Camera calibration (and more generally) instrument calibration is an essential prior step for pretty much any application you want the device for.</p>

<p>For example suppose you have a single camera. In this case you would have to calibrate it (i.e. compute the values for) for its intrinsics parameters (focal length, principal point, skew coefficient). You would do that using a calibration board (e.g., <a href=""https://en.wikipedia.org/wiki/Chessboard_detection"" rel=""nofollow noreferrer"">Checkerboard</a>, <a href=""https://april.eecs.umich.edu/wiki/AprilTags"" rel=""nofollow noreferrer"">AprilTag</a>).</p>

<p>Now, you may say that these parameters are known and reported by the manufacturers of that camera and we can look them up either at the manual or at its corresponding documentation. However that's not always the case:</p>

<ul>
<li><p>Manufacturers might provide you with that calibration information. Even if they do though, those numbers would probably be rough so that they address the whole family of products. That is, they'll probably not going to calibrate every single camera. You could use these calibration parameters but if you want higher accuracy you are better off using them as a rough estimate and do the calibration yourself.</p></li>
<li><p>The device may be affected by the environment and the overall conditions it is operating in (temperature, humidity etc.). For example, say you have a stereo camera. In that case you would calibrate for the length between the two independent RGB cameras. But that length may change depending on the temperature (heating will make the rigid link between the cameras expand).</p></li>
<li><p>The specifications of the device may change depending on the its age and usage so it makes sense to recompute its calibration parameters every N number of years.</p></li>
</ul>

<p>All in all, by all means calibrate your cameras yourself. You'll be surprised at how significant this is.</p>

<p>Take a look at the following links for more information:</p>

<ul>
<li><p><a href=""https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Oth_Rolling_Shutter_Camera_2013_CVPR_paper.pdf"" rel=""nofollow noreferrer"">https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Oth_Rolling_Shutter_Camera_2013_CVPR_paper.pdf</a></p></li>
<li><p><a href=""https://uk.mathworks.com/help/vision/ug/camera-calibration.html"" rel=""nofollow noreferrer"">https://uk.mathworks.com/help/vision/ug/camera-calibration.html</a></p></li>
<li><p><a href=""https://www.measurement.govt.nz/services/temperature-and-humidity/humidity-calibration-service/calibration-certificate/purpose-of-a-"" rel=""nofollow noreferrer"">https://www.measurement.govt.nz/services/temperature-and-humidity/humidity-calibration-service/calibration-certificate/purpose-of-a-</a></p></li>
<li><p><a href=""http://www.npl.co.uk/reference/faqs/should-i-have-my-instrument-calibrated-and,-if-so,-how-often-(faq-force)"" rel=""nofollow noreferrer"">http://www.npl.co.uk/reference/faqs/should-i-have-my-instrument-calibrated-and,-if-so,-how-often-(faq-force)</a></p></li>
</ul>
",2843583,2,0,39968836,http://aishack.in/tutorials/two-major-physical-defects-in-cameras/ - Link in the first comment doesn't work.,Incoming
307,1447,11782221,iPhone - Any examples of communicating with an Arduino board using Bluetooth?,|iphone|objective-c|bluetooth|arduino|robotics|,"<p>I'm tinkering with an iPhone-controlled <a href=""http://en.wikipedia.org/wiki/Radio-controlled_car"" rel=""nofollow"">RC car</a> chassis that is the base of my <strong>robotics project</strong>. The chassis is controlled with a <a href=""http://wirc.dension.com/"" rel=""nofollow"">WiRC</a> Wi-Fi module. It has eight outputs to control electronic speed controllers and servos. </p>

<p>I'd like to improve my robot's ability to avoid obstacles using sensors. For this purpose, I have an Arduino board which I can interface with various inexpensive rangefinders and proximity sensors. I'm looking for examples or demo projects that would <strong>connect an iPhone to an Arduino board using Bluetooth to send commands to the board and receive data from the board.</strong> Is what I'm thinking of possible? </p>

<p>Thank you for any links to projects or hardware boards that may interact with an iPhone using Bluetooth. It's great if some of these boards have an SDK to simplify development.</p>
",41123.70417,11782737,5441,2,0,1,0,967484,"Chiang Mai, Thailand",40813.67292,2011,11782737,"<p>Unfortunately, standard Bluetooth communications with devices on iOS is restricted to devices within the MFi program, so you're not going to be able to use that with your Arduino board. However, the new Bluetooth 4.0 LE protocol that is supported in newer iOS devices (iPhone 4S, Retina iPad) is open and can be used to connect any LE device.</p>

<p>iOS 5.0 introduced a new framework for this in Core Bluetooth, and I highly recommend watching the two sessions from WWDC 2012 about this. They also have <a href=""https://developer.apple.com/library/ios/#samplecode/TemperatureSensor/Introduction/Intro.html"">some sample code</a> on the topic. I've been using this myself to connect to some sensors, and it works well for a low-bandwidth application like temperature, proximity, or heart rate sensing.</p>

<p>There are several BT LE modules out there, and it looks like Dr. Michael Kroll is about to start producing an <a href=""http://www.mkroll.mobi/?page_id=386"">Arduino shield for LE communication</a>, which would make it trivial to add this kind of capability onto an Arduino board.</p>
",19679,6,1,,,Remote
308,1366,10544630,"python and ctypes cdll, not getting expected return from function",|python|ctypes|robotics|,"<p>I'm in the process of working on interfacing a haptic robot and eye tracker. So, both come with their own programming requirements, namely that the eye tracker software is based in python and is the main language in which I'm programming. Our haptic robot has an API in C, so I've had to write a wrapper in C, compile it as a DLL, and use ctypes in python to load the functions.</p>

<p>I've tested my DLL with MATLAB, and everything works just fine. However, something about my implementation of ctypes in my python class is not giving me the expected return value when i query the robot's positional coordinates.</p>

<p>I'll post the code here, and a clearer explanation of the problem at the bottom.</p>

<p>C source code for DLL wrapper:</p>

<pre><code>#include &lt;QHHeadersGLUT.h&gt;
using namespace std;
class myHapClass{
public:
void InitOmni()
{
    DeviceSpace* Omni = new DeviceSpace; //Find a the default phantom
}
double GetCoord(int index)
{
    HDdouble hapPos[3];
    hdGetDoublev(HD_CURRENT_POSITION,hapPos);
    return hapPos[index];
}
};

extern ""C"" {
int index = 1;
__declspec(dllexport) myHapClass* myHap_new(){ return new myHapClass();}
__declspec(dllexport) void myHapInit(myHapClass* myHapObj){ myHapObj-&gt;InitOmni();}
__declspec(dllexport) double myHapCoord(myHapClass* myHapObj){ double nowCoord = myHapObj-&gt;GetCoord(index); return nowCoord;}
}
</code></pre>

<p>The theory for this code is simply to have 3 available C (not C++) calls that will be compatible with python/ctypes:</p>

<ol>
<li>myHap_new() returns a pointer to an instance of the class</li>
<li>myHapInit initializes the haptics device</li>
<li>myHapCoord returns a double for the current position, of the axis referenced by int Index.</li>
</ol>

<p>The python class follows here:</p>

<pre><code>import sreb
from ctypes import cdll
lib = cdll.LoadLibrary('C:\Documents and Settings\EyeLink\My Documents\Visual Studio 2010\Projects\myHapDLLSolution\Debug\myHapDLL.dll')

class CustomClassTemplate(sreb.EBObject):
def __init__(self):
    sreb.EBObject.__init__(self)
    self.pyMyHapObj = pyMyHap()
    self.coordval=2.0

def setCoordval(self,c):
    self.coordval = c
    pass

def getCoordval(self):
    return self.coordval

def initOmni(self):
    self.pyMyHapObj.pyHapInit()
    pass

def getCoord(self):
    self.coordval = self.pyMyHapObj.pyHapCoord()
    return self.coordval

class pyMyHap(object):
def __init__(self):
    self.obj = lib.myHap_new()
    self.coord = 1.0

def pyHapInit(self):
        lib.myHapInit(self.obj)

    def pyHapCoord(self):
    self.coord = lib.myHapCoord(self.obj)
    return self.coord
</code></pre>

<p>The theory of the python custom class is to instantiate an object (self.pyMyHapObj = pyMyHap()) of the loaded DLL class. Making a call to the function 'initOmni' successfully initializes the robot, however a call to 'getCoord' does not return the expected value. In fact, the result I get from 'getCoord' is 1 (and it is listed as 1, not 1.0, so I think it is returning an integer, not a double as it should).</p>

<p>In MATLAB, I have use the DLL library, and both the myHapInit and myHapCoord functions work, and I can initialize the robot and query the position coordinates successfully.</p>

<p>So what is it about my python class that is causing ctypes to not have the proper value returned from myHapCoord from my DLL?</p>

<p>Any help would be appreciated.
Thanks</p>

<p>edit: Python version 2.3, if it matters... I'm stuck to that version.</p>
",41040.10069,10544852,2838,1,0,3,0,1246135,,40970.95278,45,10544852,"<p>Return values default to <code>int</code>.  Use something like:</p>

<pre><code>lib.myHapCoord.restype = ctypes.c_double
</code></pre>

<p>before calling the function to interpret the return value properly.</p>
",235698,5,3,,,Coordinates
309,1325,9483615,parallel programming for robot control,|controls|parallel-processing|robotics|,"<p>I need to write a program which does 2 tasks at the same time for better efficiency &amp; high response. First task is, for example, get vision data from a camera &amp; process it. </p>

<p>Second task is, receive processed data from first task &amp; do sth else with this data (robot control strategy). However, while robot control task is being performed, the camera data receiving should still be working. </p>

<p>Is there a solution for such type of programming in C++/C#?? I'm learning TBB, is it the right choice? However, I'm reading things like ""loop parallelization"", am I going in the right direction??</p>

<p>This links to a very common style in control programming where the computer is used as a central unit to connect to electronic devices (sensors) &amp; actuators and all these devices are processed concurrently</p>
",40967.59722,9483712,535,2,0,0,0,1033713,Taipei,40854.51111,355,9483712,"<p>No, your example of loop paralleling is using parallel programming to speed up the result of a calculation for one set of data.</p>

<p>What you need is multitasking. You didn't mention any target architecture. Assuming this will be an embedded system, like a microprocessor, you have several options. There are embedded micro-OSes like VXworks and uC-OS that allow you to do just what you are asking. These allow you to set up multiple ""tasks"" that run virtually concurrently. Of course true concurrency is impossible with one CPU, but the scheduler in these OSes is designed to be very deterministic, for quasi-real-time systems like you describe.</p>
",119527,2,1,,,Timing
310,1452,11804800,Speech Precognition and Do some action with hardware,|c#|speech-recognition|robotics|,"<p>I am good programmer but I am new for Robotics. I want to make small project which will recognize my speech and my light on and off.</p>

<p>Please provide me some reference library for all things.</p>

<p>What kind of hardware I need for it.</p>
",41125.00625,,216,2,0,0,,1575399,,41125.00347,51,11804858,"<p>If you are new to Robotics I would diffiently recommend you to check out the Arduino device. The Arduino device is an easily programmable circut board. You can read more at <a href=""http://www.arduino.cc"" rel=""nofollow"">http://www.arduino.cc</a></p>

<p>Regarding Speech recogniztion you could use Microsoft Speech recogniztion software, which allows you to communicate with your app via. An library. Find out how to install it here: <a href=""http://support.microsoft.com/kb/306537"" rel=""nofollow"">http://support.microsoft.com/kb/306537</a></p>
",1331739,0,2,,,Specifications
311,1535,13677658,What algorithm should I implement to program a room cleaning robot?,|algorithm|theory|robot|,"<p>For this question assume that the following things are unknown:</p>

<ul>
<li>The size and shape of the room</li>
<li>The location of the robot</li>
<li>The presence of any obsticles</li>
</ul>

<p>Also assume that the following things are constant:</p>

<ul>
<li>The size and shape of the room</li>
<li>The number, shape and location of all (if any) obsticles</li>
</ul>

<p>And assume that the robot has the following properties:</p>

<ul>
<li>It can only move forward in increments of absolute units and turn in degrees. Also the operation that moves will return true if it succeeded or false if it failed to move due to an obstruction</li>
<li>A reasonably unlimited source of power (let's say it is a solar powered robot placed on a space station that faces the sun at all times with no ceiling)</li>
<li>Every movement and rotation is carried out with absolute precision every time (don't worry about unreliable data)</li>
</ul>

<p>I was asked a much simpler version of this question (room is a rectangle and there are no obstacles, how would you move over it guaranteeing you could over every part at least once) and after I started wondering how you would approach this if you couldn't guarantee the shape or the presence of obstacles. I've started looking at this with <a href=""http://en.wikipedia.org/wiki/Dijkstra%27s_algorithm"" rel=""nofollow"">Dijkstra's algorithm</a>, but I'm fascinated to hear how others approach this (or if there is a well accepted answer to this? (How does Roomba do it?)</p>
",41246.22847,13677758,1584,2,0,0,,16959,"Livermore, CA",39708.96389,7219,13677758,"<p>Take a look at SLAM <a href=""http://openslam.org/"" rel=""nofollow"">http://openslam.org/</a> and for more  <a href=""http://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping"" rel=""nofollow"">Wiki</a> </p>
",780095,2,1,,,Moving
312,1774,17126898,Problems with bttv camera mobile robot - initialisation noisy image,|linux|video-capture|robot|v4l2|,"<p>I am working with Ubuntu 12.04 LTS. We are having problems receiving a non-noisy image from an eye-in-hand camera on a mobile robot.</p>

<p>The camera image is collected with a BTTV PCI device, specifically the bt878  driver. The BTTV device is a PCI-104 card based on a Bt848 chip, and is supported under Linux by the bttv and associated kernel driver modules. The devices is a capture-only card - which means video is taken directly from a video source without the use of a tuner. The frame grabber is connected to the on-board computer Sensorary 311 (so bttv card=73)</p>

<p>The bttv driver installed is version 0.9.19. The webcam is present in dev/video0. </p>

<pre><code>$ dmesg |grep bttv
[    1.937779] bttv: driver version 0.9.19 loaded
[    1.937783] bttv: using 8 buffers with 2080k (520 pages) each for capture
[    1.937850] bttv: Bt8xx card found (0)
[    1.937873] bttv: 0: Bt878 (rev 17) at 0000:04:01.0, irq: 16, latency: 32, mmio: 0xdfdfe000
[    1.937888] bttv: 0: detected: Sensoray 311 [card=73], PCI subsystem ID is 6000:0311
[    1.937890] bttv: 0: using: Sensoray 311/611 [card=73,autodetected]
[    1.940185] bttv: 0: tuner absent
[    1.940313] bttv: 0: registered device video0
[    1.940591] bttv: 0: registered device vbi0
</code></pre>

<p>The modules loaded are as follows:</p>

<pre><code>$ lsmod | grep video

videodev              105518  2 bttv,v4l2_common
media                  20516  1 videodev
videobuf_dma_sg        18711  1 bttv
videobuf_core          25098  2 bttv,videobuf_dma_sg
video                  19117  1 i915

$ lsmod

Module                  Size  Used by
bt878                  13588  0 
rfcomm                 38104  0 
bnep                   17791  2 
bluetooth             189585  10 rfcomm,bnep
parport_pc             32115  0 
ppdev                  12850  0 
coretemp               13362  0 
kvm                   365588  0 
snd_hda_codec_idt      60238  1 
gpio_ich               13160  0 
snd_hda_intel          32983  3 
snd_hda_codec         116477  2 snd_hda_codec_idt,snd_hda_intel
snd_hwdep              13277  1 snd_hda_codec
snd_pcm                81124  2 snd_hda_intel,snd_hda_codec
microcode              18396  0 
snd_seq_midi           13133  0 
snd_rawmidi            25426  1 snd_seq_midi
psmouse                91381  0 
snd_seq_midi_event     14476  1 snd_seq_midi
snd_seq                51594  2 snd_seq_midi,snd_seq_midi_event
bttv                  116393  1 bt878
v4l2_common            20517  1 bttv
videodev              105518  2 bttv,v4l2_common
media                  20516  1 videodev
videobuf_dma_sg        18711  1 bttv
serio_raw              13032  0 
snd_timer              28932  2 snd_pcm,snd_seq
snd_seq_device         14138  3 snd_seq_midi,snd_rawmidi,snd_seq
videobuf_core          25098  2 bttv,videobuf_dma_sg
btcx_risc              13401  1 bttv
rc_core                21172  1 bttv
tveeprom               17010  1 bttv
mac_hid                13078  0 
snd                    62675  15 snd_hda_codec_idt,snd_hda_intel,snd_hda_codec,snd_hwdep,snd_pcm,snd_rawmidi,snd_seq,snd_timer,snd_seq_device
i915                  479158  2 
drm_kms_helper         47459  1 i915
lpc_ich                16993  0 
drm                   240232  3 i915,drm_kms_helper
i2c_algo_bit           13317  2 bttv,i915
soundcore              14636  1 snd
snd_page_alloc         14109  2 snd_hda_intel,snd_pcm
video                  19117  1 i915
lp                     17456  0 
parport                40931  3 parport_pc,ppdev,lp
e1000e                177679  0 
</code></pre>

<p>We believe that the bt878 driver is loaded correctly, and that the frame grabber is working properly. When the camera is powered off the frame-grabber (viewed in either camorama, vlc or gstreamer-properties programs) is blue, and when the camera is powered on, the image received from the camera is very noisy. No features can be detected but changes in light can be seen.</p>

<p>We have had the camera working before properly (implying that the drivers are ok), however it appears that it is almost random, and dependent on what viewing programs we use and in what order. Once the camera is working, it remains working until we power off the robot. We think this means that the camera is not initialised properly, and so is sending noisy data. Does anyone know of a good way to initialise a camera linked to a BTTV PCI device, besides just ensuring the camera is powered on?</p>

<p>Specifically the camera is part of a manipulator which is installed on a SeekurJr robot from Mobile Robotics. It is required that the manipulator is initialised (turning the camera on) before viewing the stream, which we do. </p>

<p>The camera is a RVision SEE camera.</p>

<p>Questions: How can we initialise the camera? Any other ideas on how to fix the noise?</p>
",41440.78611,17163118,269,1,0,1,,2177538,,41349.71736,145,17163118,"<p>i would guess you are having hardware problems, either with your camera or with your framegrabber, most likely with your camera.</p>

<ul>
<li><p>make sure that your camera has it's optics mounted</p></li>
<li><p>try connecting the camera to another display device (e.g. an old analog TV with composite in), and see whether you still have that noisy image</p></li>
<li><p>try connecting another camera to your framegrabber card, and see what the image looks like.</p></li>
</ul>

<p>from the bttv-side, the only configuration that could help, is to select the correct video norm, e.g. whether camera and grabber both agree that they are using <em>PAL</em> or <em>NTSC</em> or whatever (and of course the various subformats).</p>

<ul>
<li>try changing the norm with any viewer program, that allows that during playback, e.g. <code>xawtv</code></li>
</ul>

<p>PS: the fact that you get a nice blue image when the camera is powered off only means, that the framegrabber correctly detects whether there is <em>any</em> signal.</p>
",1169096,0,0,,,Incoming
313,1866,20433016,Teachable Robotic Arm coding error,|c++|arduino|robotics|,"<p>I am working on a robotic arm that has six servos which is being controlled by an arduino uno. The servos are analog servos from adafruit and they are special in the sense that you can get feedback from the servos, i.e their actual position after you told it where to go. What I am working on is trying to adapt this example code to include six servos instead of one. <a href=""https://github.com/adafruit/Feedback-Servo-Record-and-Play/blob/master/servo_recordplay.ino"" rel=""nofollow"">https://github.com/adafruit/Feedback-Servo-Record-and-Play/blob/master/servo_recordplay.ino</a></p>

<p>Here is my code so far and its errors </p>

<pre><code>// Example code for recording and playing back servo motion with a 
// analog feedback servo
// http://www.adafruit.com/products/1404


#include &lt;Servo.h&gt;
#include &lt;EEPROM.h&gt;

#define CALIB_MAX 512
#define CALIB_MIN 100
#define SAMPLE_DELAY 25 // in ms, 50ms seems good

uint8_t recordButtonPin = 12;
uint8_t playButtonPin = 7;
uint8_t servo1Pin = 9;
uint8_t servo2Pin = 10;
uint8_t servo1FeedbackPin = A0;
uint8_t servo2FeedbackPin = A1;
uint8_t servo3FeedbackPin = A2;
uint8_t servo4FeedbackPin = A3;
uint8_t servo5FeedbackPin = A4;
uint8_t servo6FeedbackPin = A5;
uint8_t ledPin = 13;

Servo servo1;
Servo servo2;
Servo servo3;
Servo servo4;
Servo servo5;
Servo servo6;

void setup() {
  Serial.begin(9600);
  pinMode(recordButtonPin, INPUT);
  digitalWrite(recordButtonPin, HIGH);
  pinMode(playButtonPin, INPUT);
  digitalWrite(playButtonPin, HIGH);
  pinMode(ledPin, OUTPUT);

  Serial.println(""Servo RecordPlay"");
}

void loop() {
 if (! digitalRead(recordButtonPin)) {
   delay(10);
   // wait for released
   while (! digitalRead(recordButtonPin));
   delay(20);
   // OK released!
   recordAllServos(servo1Pin, servo1FeedbackPin, recordButtonPin);
 }

  if (! digitalRead(playButtonPin)) {
   delay(10);
   // wait for released
   while (! digitalRead(playButtonPin));
   delay(20);
   // OK released!
   playAllServo(servo1Pin, playButtonPin);
 }
}

void playAllServo(uint8_t servoPin, uint8_t buttonPin) {
  uint16_t addr = 0;
  Serial.println(""Playing"");


  servo1.attach(servo1Pin);
  while (digitalRead(buttonPin)) {    
    uint8_t x = EEPROM.read(addr);
    Serial.print(""Read EE: ""); Serial.print(x);
    if (x == 255) break;
    // map to 0-180 degrees
    x = map(x, 0, 254, 0, 180);
    Serial.print("" -&gt; ""); Serial.println(x);
    servo1.write(x);
    delay(SAMPLE_DELAY);
    addr++;
    if (addr == 512) break;
  }
  Serial.println(""Done"");
  servo1.detach();
  delay(250);  
}

void recordAllServos(uint8_t servoPin, uint8_t buttonPin) {
  uint16_t addr = 0;

  Serial.println(""Recording"");
  digitalWrite(ledPin, HIGH);

  pinMode(servo1FeedbackPin, INPUT); 
  pinMode(servo2FeedbackPin, INPUT); 
  pinMode(servo3FeedbackPin, INPUT); 
  pinMode(servo4FeedbackPin, INPUT); 
  pinMode(servo5FeedbackPin, INPUT); 
  pinMode(servo6FeedbackPin, INPUT); 

  while (digitalRead(buttonPin)) 
  {
     readServo(servo1FeedbackPin);
     readServo(servo2FeedbackPin);
     readServo(servo3FeedbackPin);
     readServo(servo4FeedbackPin);
     readServo(servo5FeedbackPin);
     readServo(servo6FeedbackPin);

     if (addr &gt; 506) break;
     delay(SAMPLE_DELAY);
  }
  if (addr != 1024) EEPROM.write(addr, 255);

  digitalWrite(ledPin, LOW);

  Serial.println(""Done"");
  delay(250);
}

void readAllServo(uint8_t analogPin)
{
     uint16_t a = analogRead(analogPin);

     Serial.print(""Read analog pin ""); Serial.print(analogPin); Serial.print("": "");
     Serial.print(a);                  
     if (a &lt; CALIB_MIN) a = CALIB_MIN;
     if (a &gt; CALIB_MAX) a = CALIB_MAX;
     a = map(a, CALIB_MIN, CALIB_MAX, 0, 254);
     Serial.print("" -&gt; ""); Serial.println(a);
     EEPROM.write(addr, a);
     addr++;
}
</code></pre>

<p>The errors that I am getting from the arduino ide are</p>

<pre><code>Teachable_Arm_Mark.cpp: In function 'void loop()':
Teachable_Arm_Mark:10: error: too many arguments to function 'void recordAllServos(uint8_t, uint8_t)'
Teachable_Arm_Mark:49: error: at this point in file
Teachable_Arm_Mark.cpp: In function 'void recordAllServos(uint8_t, uint8_t)':
Teachable_Arm_Mark:100: error: 'readServo' was not declared in this scope
Teachable_Arm_Mark.cpp: In function 'void readAllServo(uint8_t)':
Teachable_Arm_Mark:127: error: 'addr' was not declared in this scope
</code></pre>

<p>Any help much appreciated 
Thanks</p>
",41614.85347,,377,1,1,1,,2970920,,41587.04375,3,20433317,"<p>In void loop you have</p>

<pre><code>  recordAllServos(servo1Pin, servo1FeedbackPin, recordButtonPin);
</code></pre>

<p>but recordAllServos is declared as</p>

<pre><code>void recordAllServos(uint8_t servoPin, uint8_t buttonPin)
</code></pre>

<p>Hence too many arguments 3 vs 2...</p>
",968969,0,1,30522763,Good question. Try to add what you have done to fixed these problems nad how what you did affected it.,Actuator
314,1793,17942862,Threading in C# and library that does not work,|c#|service|thread-safety|robotics|,"<p>I am implementing a DSS service for a robot we are developing. I have already made an interface for the robot, which has some functions like connect, rotate, move etc. This interface is basically a wrapper around some functions that i got in a library, that does the controlling of the robot.</p>

<p>The whole thing works, as long as i dont have any asynchronous actions within my application. I can move the robot and whatnot and there are no issues. However when i try to implement this in my DSS service, it does not work. I can read whatever i want from the robot and it's outputs/inputs, but i am unable to move the robot itself, which makes me believe this is an issue with asynchronous operations, which makes some functions in the library unable to work.</p>

<p>I have looked around but i am unable to find any way or form in which i can get this done, because i would love to get my service running.</p>

<p>It hangs at the Robot.Move(move) statement, which is the one i am using from the library, making me believe that that is the function that is the culprit.</p>
",41485.38333,,43,0,3,0,,2028781,Netherlands,41305.46042,52,,,,,,26242321,"Could you confirm that by `DSS service` you mean [Distributed Software Services](http://msdn.microsoft.com/en-us/library/bb483067.aspx) rather than [Data Services Server](http://stackoverflow.com/tags/dss/info) as the tag implies? Also, what robot and library are you using, some robotics libraries are inherently single threaded, so you have to make all calls from a single manager thread and let your application only talk through that manager.",Timing
315,1703,15764159,Trying to filter (tons of) noise from accelerometers and gyroscopes,|filtering|robotics|noise|kalman-filter|sensor-fusion|,"<p><strong>My project:</strong></p>

<p>I'm developing a slot car with 3-axis accelerometer and gyroscope, trying to estimate the car pose (x, y, z, yaw, pitch) but I have a big problem with my vibration noise (while the car is running, the gears induce vibration and the track also gets it worse) because the noise takes values between ±4[g] (where g = 9.81 [m/s^2]) for the accelerometers, for example.</p>

<p>I know (because I observe it), the noise is correlated for all of my sensors</p>

<p>In my first attempt, I tried to work it out with a Kalman filter, but it didn't work because values of my state vectors had a really big noise.</p>

<p>EDIT2: In my second attempt I tried a low pass filter before the Kalman filter, but it only slowed down my system and didn't filter the low components of the noise. At this point I realized this noise might be composed of low and high frecuency components.</p>

<p>I was learning about adaptive filters (LMS and RLS) but I realized I don't have a noise signal and if I use one accelerometer signal to filter other axis' accelerometer, I don't get absolute values, so It doesn't work.</p>

<p>EDIT: I'm having problems trying to find some example code for adaptive filters. If anyone knows about something similar, I will be very thankful.</p>

<p><strong>Here is my question:</strong></p>

<p>Does anyone know about a filter or have any idea about how I could fix it and filter my signals correctly?</p>

<p>Thank you so much in advance,</p>

<p>XNor</p>

<p>PD: I apologize for any mistake I could have, english is not my mother tongue</p>
",41366.52083,15870832,4061,3,0,5,0,2235928,,41366.49167,82,15766349,"<p>Have you tried a simple low-pass filter on the data?  I'd guess that the vibration frequency is much higher than the frequencies in normal car acceleration data.  At least in normal driving.  Crashes might be another story...</p>
",805659,0,1,,,Coordinates
316,1758,16954054,Should I use TCP or UDP packets to control a UAV?,|tcp|udp|robotics|,"<p>Should I use TCP or UDP connection to send control commands to an Unmanned Aerial Vehicle of a PC/base station?</p>

<p>The vehicle is small (approx. the size of a human nail) and needs continous control from a base station to stabilize it. </p>

<p>Here is what I am thinking: TCP is supposed to be reliable transmission but slow whereas UDP is does not provide a guarantee of packet transmission like TCP but is faster than TCP.<br>
Since I really care about getting the packets over to the UAV from the base station as quickly as possible I assume using UDP is the way to go. </p>

<p>Am I way off? Have I oversimplified this problem? </p>
",41431.21389,17005622,1902,2,0,1,,1068636,,40875.14028,317,17005622,"<p>In my oppinion i would say neither.</p>

<p>I would higly recommend that you have an internal control loop stabilising the UAV and only using the data connection for sending more behaviour oriented commands such as Fly west, Fly east etc. </p>

<p>I assume you are useing some kind of wireless transmitter for the connection. </p>

<p>If you use UDP you cannot be sure that the control packets reaches the UAV, which could cause it to become unstable and crash.</p>

<p>If you use TCP you cannot guarantee that the control packets reaches the UAV with regular time intervals, which might cause it become unstable and cause a crash.</p>

<p>If you really want to control everything from a base station i would recommend TCP as you can ensure that your control packets reaches the UAV. If you are using a standard wireless transmission you should have plenty of bandwith to retransmit packets that are lost.</p>

<p>If you wish to send large amounts of data eg. Video or sound without any direct impact on UAV stability i would definately go for UDP as you wouldn't care if you lost a frame or two.</p>

<p>I hope it makes sense.</p>

<p>Sigurd</p>
",1944249,2,1,,,Remote
317,1663,14935473,Good Pre-Built Stereo Camera and Robot Vehicle Suggestion,|opencv|camera|computer-vision|robotics|stereo-3d|,"<p>I am working on a hobby project related to autonomous navigation. I want to use stereo camera for obstacle detection and then control the movement of robot vehicle to avoid obstacle.</p>

<p>(1) I am trying to find a readymade robot vehicle that has Parallel/Serial/Ethernet port to interface with a computer. I should be able to send commands through PC to Vehicle to turn left/right, speed up/down, start/stop.</p>

<p>Are there any good,accurate,reliable,cheap robot vehicle available in market?</p>

<p>(2) I am thinking of building a stereo camera. But if a good,accurate,reliable,cheap stereo camera is available on market. I am ready to purchase it too.</p>

<p>Note: It is a small size pet project and hence looking for ""CHEAP yet RELIABLE"" stereo camera and robot vehicle that can be interfaced with PC.</p>

<p>Thanks in advance.</p>

<p>PS: Would be good if items are available in India.</p>
",41323.48681,14942685,1184,1,3,0,,1343922,,41018.50486,22,14942685,"<p>I have had some success with a Lego Mindstorm using the <a href=""http://lejos.sourceforge.net/"" rel=""nofollow"">Lejos</a> OS. This supports USB and Bluetooth control, so you can also use a cell phone. Although I have not used a camera with this, there are plenty of tutorials online which suggest it is quite simple to do so.</p>

<p>It comes with a selection of sensors and wheels (the kit I bought had an ultrasound rangefinder and a light sensor) so it's really easy to get something up and running if you're more interested in programming than electronics.</p>

<p>The stereo camera I have on my desk right now is a <a href=""http://nma.web.nitech.ac.jp/fukushima/minoru/minoru3D-e.html"" rel=""nofollow"">Novo Minoru</a>. It is a really cheap option but seems to work okay for simple vision tasks. It is also UVC compliant so seems to be pretty easy to interface with.</p>
",980866,1,5,20981748,"Do you have to use a stereo camera pair, or would you consider using a Microsoft Kinect or similar device?",Specifications
318,1780,17714284,tracking the robot from the overhead cam,|opencv|robotics|,"<p>I am thinking of creating a robot that can navigate using a map. It is controlled from a PC. An 8-bit controller performs low level tasks and the PC is doing the image processing. I plan to implement it in a single room where the robot is placed and the robot and environment are tracked by a camera from a height or from the ceiling of the room. First, the robot needs to be mapped, like this <a href=""http://www.societyofrobots.com/programming_wavefront.shtml"" rel=""nofollow"">http://www.societyofrobots.com/programming_wavefront.shtml</a></p>

<p>To do:</p>

<p>Track the robot from some height using camera Following the wavefont algorithim to locate robot and obstacles.</p>

<p>Procedure:(just my idea)</p>

<ul>
<li><p>The camera will give image of the robot surrounded by obstacles in
the random places. using some opencv technique draw some grind over
the image.</p></li>
<li><p>Locating the grid which contain robot(by having some colored symbol
over the robot) and locating the grids containing the obstacle.</p></li>
<li><p>Now the grids with obstacle is thought as wall and the remaining is
the free space for the robot to navigate.</p></li>
<li><p>robot is going to get the goal place which should be reached is given
from the pc(may be like point the place to reach in the image by
mouse click).</p></li>
</ul>

<p>firstly finding the obstacle from the video stream is to be done for that iam going to catch a image of the robot which is in the room now the image is  edit by manually in M.S paint by filling the grid with stationary obstacle by some color say red.</p>

<p>now the edited image is going to be the reference and it is compared with video stream to tell the free space available to robot.</p>

<p>each grid is given some value using that place value of the robot should be determined </p>

<p>now i should be able to give some grid value in the free space to make it as goal for the robot </p>

<p>now the Pc should calculate and tell the robot which grid should be traveled to reach the goal. the only thing the robot need to figure out itself is dynamic obstacle avoidance like cat walking across the robot </p>

<p>comparing the area of the room with the image of some dimension to find how far the robot moved and how far to go</p>

<p>is it possible to do? can anybody help me to do this? </p>

<p>\thanks in advance</p>
",41473.16597,19041798,1140,1,0,2,,2534242,,41454.35972,25,19041798,"<p>Not sure if this will help you, but I did some work related to your post.  Though I'd share.</p>

<p><a href=""http://letsmakerobots.com/node/38208"" rel=""nofollow"">http://letsmakerobots.com/node/38208</a></p>
",2108441,0,0,,,Moving
319,1726,16419356,Swig Python/C Pointer in struct to another struct,|python-2.7|swig|robotics|,"<p>I'm using Player/Stage SWIG generated code from C and now I'm trying to access a structure where a value points to another structure. I wonder how I can extract the array of structures in python.</p>

<p>The C code looks like this:</p>

<pre><code>/** @brief Localization device data. */
typedef struct
{
  /** Device info; must be at the start of all device structures. */
  playerc_device_t info;

  /** Map dimensions (cells). */
  int map_size_x, map_size_y;

  /** Map scale (m/cell). */
  double map_scale;

  /** Next map tile to read. */
  int map_tile_x, map_tile_y;

  /** Map data (empty = -1, unknown = 0, occupied = +1). */
  int8_t *map_cells;

  /** The number of pending (unprocessed) sensor readings. */
  int pending_count;

  /** The timestamp on the last reading processed. */
  double pending_time;

  /** List of possible poses. */
  int hypoth_count;
  player_localize_hypoth_t *hypoths;

  double mean[3];
  double variance;
  int num_particles;
  playerc_localize_particle_t *particles;

} playerc_localize_t;

/** @brief Hypothesis format.

Since the robot pose may be ambiguous (i.e., the robot may at any
of a number of widely spaced locations), the @p localize interface is
capable of returning more that one hypothesis. */
typedef struct player_localize_hypoth
{
  /** The mean value of the pose estimate (m, m, rad). */
  player_pose2d_t mean;
  /** The covariance matrix pose estimate (lower half, symmetric matrix) 
      (cov(xx) in m$^2$, cov(yy) in $^2$, cov(aa) in rad$^2$, 
       cov(xy), cov(ya), cov(xa) ). */
  double cov[6];
  /** The weight coefficient for linear combination (alpha) */
  double alpha;
} player_localize_hypoth_t;
</code></pre>

<p>The code for python is automatically generated. I'm trying to access the elements in hypoths variable, how can I do this in python? I can access most of the variables. Below is the generated swig code...</p>

<pre><code>class playerc_localize(_object):
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, playerc_localize, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, playerc_localize, name)
    __repr__ = _swig_repr
    __swig_setmethods__[""info""] = _playerc.playerc_localize_info_set
    __swig_getmethods__[""info""] = _playerc.playerc_localize_info_get
    if _newclass:info = _swig_property(_playerc.playerc_localize_info_get, _playerc.playerc_localize_info_set)
    __swig_setmethods__[""map_size_x""] = _playerc.playerc_localize_map_size_x_set
    __swig_getmethods__[""map_size_x""] = _playerc.playerc_localize_map_size_x_get
    if _newclass:map_size_x = _swig_property(_playerc.playerc_localize_map_size_x_get, _playerc.playerc_localize_map_size_x_set)
    __swig_setmethods__[""map_size_y""] = _playerc.playerc_localize_map_size_y_set
    __swig_getmethods__[""map_size_y""] = _playerc.playerc_localize_map_size_y_get
    if _newclass:map_size_y = _swig_property(_playerc.playerc_localize_map_size_y_get, _playerc.playerc_localize_map_size_y_set)
    __swig_setmethods__[""map_scale""] = _playerc.playerc_localize_map_scale_set
    __swig_getmethods__[""map_scale""] = _playerc.playerc_localize_map_scale_get
    if _newclass:map_scale = _swig_property(_playerc.playerc_localize_map_scale_get, _playerc.playerc_localize_map_scale_set)
    __swig_setmethods__[""map_tile_x""] = _playerc.playerc_localize_map_tile_x_set
    __swig_getmethods__[""map_tile_x""] = _playerc.playerc_localize_map_tile_x_get
    if _newclass:map_tile_x = _swig_property(_playerc.playerc_localize_map_tile_x_get, _playerc.playerc_localize_map_tile_x_set)
    __swig_setmethods__[""map_tile_y""] = _playerc.playerc_localize_map_tile_y_set
    __swig_getmethods__[""map_tile_y""] = _playerc.playerc_localize_map_tile_y_get
    if _newclass:map_tile_y = _swig_property(_playerc.playerc_localize_map_tile_y_get, _playerc.playerc_localize_map_tile_y_set)
    __swig_setmethods__[""map_cells""] = _playerc.playerc_localize_map_cells_set
    __swig_getmethods__[""map_cells""] = _playerc.playerc_localize_map_cells_get
    if _newclass:map_cells = _swig_property(_playerc.playerc_localize_map_cells_get, _playerc.playerc_localize_map_cells_set)
    __swig_setmethods__[""pending_count""] = _playerc.playerc_localize_pending_count_set
    __swig_getmethods__[""pending_count""] = _playerc.playerc_localize_pending_count_get
    if _newclass:pending_count = _swig_property(_playerc.playerc_localize_pending_count_get, _playerc.playerc_localize_pending_count_set)
    __swig_setmethods__[""pending_time""] = _playerc.playerc_localize_pending_time_set
    __swig_getmethods__[""pending_time""] = _playerc.playerc_localize_pending_time_get
    if _newclass:pending_time = _swig_property(_playerc.playerc_localize_pending_time_get, _playerc.playerc_localize_pending_time_set)
    __swig_setmethods__[""hypoth_count""] = _playerc.playerc_localize_hypoth_count_set
    __swig_getmethods__[""hypoth_count""] = _playerc.playerc_localize_hypoth_count_get
    if _newclass:hypoth_count = _swig_property(_playerc.playerc_localize_hypoth_count_get, _playerc.playerc_localize_hypoth_count_set)
    __swig_setmethods__[""hypoths""] = _playerc.playerc_localize_hypoths_set
    __swig_getmethods__[""hypoths""] = _playerc.playerc_localize_hypoths_get
    if _newclass:hypoths = _swig_property(_playerc.playerc_localize_hypoths_get, _playerc.playerc_localize_hypoths_set)
    __swig_setmethods__[""mean""] = _playerc.playerc_localize_mean_set
    __swig_getmethods__[""mean""] = _playerc.playerc_localize_mean_get
    if _newclass:mean = _swig_property(_playerc.playerc_localize_mean_get, _playerc.playerc_localize_mean_set)
    __swig_setmethods__[""variance""] = _playerc.playerc_localize_variance_set
    __swig_getmethods__[""variance""] = _playerc.playerc_localize_variance_get
    if _newclass:variance = _swig_property(_playerc.playerc_localize_variance_get, _playerc.playerc_localize_variance_set)
    __swig_setmethods__[""num_particles""] = _playerc.playerc_localize_num_particles_set
    __swig_getmethods__[""num_particles""] = _playerc.playerc_localize_num_particles_get
    if _newclass:num_particles = _swig_property(_playerc.playerc_localize_num_particles_get, _playerc.playerc_localize_num_particles_set)
    __swig_setmethods__[""particles""] = _playerc.playerc_localize_particles_set
    __swig_getmethods__[""particles""] = _playerc.playerc_localize_particles_get
    if _newclass:particles = _swig_property(_playerc.playerc_localize_particles_get, _playerc.playerc_localize_particles_set)
    def __init__(self, *args): 
        this = _playerc.new_playerc_localize(*args)
        try: self.this.append(this)
        except: self.this = this
    def destroy(self): return _playerc.playerc_localize_destroy(self)
    def subscribe(self, *args): return _playerc.playerc_localize_subscribe(self, *args)
    def unsubscribe(self): return _playerc.playerc_localize_unsubscribe(self)
    def set_pose(self, *args): return _playerc.playerc_localize_set_pose(self, *args)
    def get_particles(self): return _playerc.playerc_localize_get_particles(self)
    __swig_destroy__ = _playerc.delete_playerc_localize
    __del__ = lambda self : None;
playerc_localize_swigregister = _playerc.playerc_localize_swigregister
playerc_localize_swigregister(playerc_localize)
</code></pre>

<p>Everything compiles in SWIG, but now my python code gives errors. I know that the alpha line may be wrong, but I get the same error for both lines:</p>

<pre><code>LOC = playerc_localize(CON, 0)
if LOC.subscribe(PLAYERC_OPEN_MODE) != 0:
   raise playerc_error_str()

CON.read()
# This should work right? If I omit the index,
# then I get the first value from the array.
print LOC.get_hypoth(0).alpha
print LOC.get_hypoth(1)
</code></pre>

<p>I get the following errors in python:</p>

<pre><code>print LOC.get_hypoth(1).alpha
File ""/usr/local/lib/python2.7/dist-packages/playerc.py"", line 7413, in get_hypoth
return _playerc._playerc_localize_get_hypoth(self, index)
TypeError: in method '_playerc_localize_get_hypoth', argument 1 of type 'playerc_localize_t *'
</code></pre>

<p><strong>PART II</strong>
I've come around another problem, again I'm trying to access some values but it doesn't work and I can't figure out whats wrong. I'm trying to access waypoints in the planner device:</p>

<pre><code>PLN = playerc_planner(CON, 0)
  if PLN.subscribe(PLAYERC_OPEN_MODE) != 0:
     raise playerc_error_str()
# saves the waypoints in PLN.waypoints
PLN.get_waypoints()
# gives: &lt;Swig Object of type 'double (*)[3]' at 0x38105d0&gt;
# if i try accessing members of it using (0) or [0] or something I get errors
print PLN.waypoints
</code></pre>

<p>Now the related files and parts: in playerc.py</p>

<pre><code>class playerc_planner(_object):
...
__swig_setmethods__[""waypoints""] = _playerc.playerc_planner_waypoints_set
    __swig_getmethods__[""waypoints""] = _playerc.playerc_planner_waypoints_get
    if _newclass:waypoints = _swig_property(_playerc.playerc_planner_waypoints_get, _playerc.playerc_planner_waypoints_set)
</code></pre>

<p>The part in playerc_wrap.i:</p>

<pre><code>%header
%{
    #define new_playerc_planner playerc_planner_create
    #define del_playerc_planner playerc_planner_destroy
    typedef playerc_planner_t playerc_planner;
%}

typedef struct
{
  playerc_device info;
  int path_valid;
  int path_done;
  double px, py, pa;
  double gx, gy, ga;
  double wx, wy, wa;
  int curr_waypoint;
  int waypoint_count;
  double (*waypoints)[3];
    %extend
    {
        playerc_planner (playerc_client *client, int index);
        void destroy(void);
        int subscribe (int access);
        int unsubscribe (void);
        int set_cmd_pose (double gx, double gy, double ga);
        int get_waypoints (void);
        int enable (int state);
    }
} playerc_planner;
</code></pre>

<p>And the related part in playerc_wrap.h:</p>

<pre><code>typedef struct
{
  playerc_device_t info;
  int path_valid;
  int path_done;
  double px, py, pa;
  double gx, gy, ga;
  double wx, wy, wa;
  int curr_waypoint;
  int waypoint_count;
  double (*waypoints)[3];
} playerc_planner_t;

PLAYERC_EXPORT playerc_planner_t *playerc_planner_create(playerc_client_t *client, int index);
PLAYERC_EXPORT void playerc_planner_destroy(playerc_planner_t *device);
PLAYERC_EXPORT int playerc_planner_subscribe(playerc_planner_t *device, int access);
PLAYERC_EXPORT int playerc_planner_unsubscribe(playerc_planner_t *device);
PLAYERC_EXPORT int playerc_planner_set_cmd_pose(playerc_planner_t *device,
                                  double gx, double gy, double ga);
PLAYERC_EXPORT int playerc_planner_get_waypoints(playerc_planner_t *device);

PLAYERC_EXPORT int playerc_planner_enable(playerc_planner_t *device, int state);
</code></pre>

<p>So I'm again lost in how to access the waypoints in this double(*)[3] variable, I tried some stuff but it all fails to compile in SWIG. Again thanks!</p>
",41401.52917,16422401,789,1,0,0,,1226156,Belgium,40961.62153,158,16422401,"<p>It looks like those dynamically sized arrays are not properly wrapped in libcplayer's SWIG interface file. The <code>LOC.hypoths</code> accessor will probably just give you the first element in the array, not any of the others.</p>

<p>I think the easiest thing to do would be to add something like this to playerc.i and regenerate the SWIG bindings:</p>

<pre><code>%inline %{
  player_localize_hypoth_t *_playerc_localize_get_hypoth(playerc_localize *localize, int index)
  {
    return &amp;(localize-&gt;hypoths[index]);
  }
%}

%extend playerc_localize {
  %pythoncode %{
    def get_hypoth(self, index):
        return _playerc._playerc_localize_get_hypoth(self, index)
  %}
}
</code></pre>

<p>Then you should be able to access element number <code>n</code> in Python with <code>LOC.get_hypoth(n)</code>.</p>

<p>References:</p>

<ul>
<li><a href=""http://www.swig.org/Doc2.0/Python.html#Python_nn40"" rel=""nofollow"">SWIG 2.0 manual section 34.6</a></li>
<li><a href=""http://sourceforge.net/p/playerstage/svn/HEAD/tree/code/player/trunk/client_libs/libplayerc/bindings/python/playerc.i"" rel=""nofollow"">libplayerc/bindings/python/playerc.i</a></li>
<li><a href=""http://sourceforge.net/p/playerstage/svn/HEAD/tree/code/player/trunk/client_libs/libplayerc/playerc.h"" rel=""nofollow"">libplayerc/playerc.h</a></li>
<li><a href=""http://sourceforge.net/p/playerstage/svn/HEAD/tree/code/player/trunk/client_libs/libplayerc/dev_localize.c"" rel=""nofollow"">libplayerc/dev_localize.c</a></li>
<li><a href=""http://sourceforge.net/p/playerstage/svn/HEAD/tree/code/player/trunk/client_libs/libplayerc/bindings/python/playerc_swig_parse.py"" rel=""nofollow"">libplayerc/bindings/python/playerc_swig_parse.py</a></li>
</ul>

<p>Notes:</p>

<p>The name of the struct is <code>playerc_localize</code> instead of <code>playerc_localize_t</code> because of the regular expressions applied to the interface file by the <code>playerc_swig_parse.py</code> script.</p>

<h2>Part II</h2>

<p>A similar problem. Again, there might be a cleverer way to do this, but this simple way is nice and clean and there's no need to try and make it any more complicated than it already is:</p>

<pre><code>%inline %{
  double _playerc_planner_get_waypoint_coord(playerc_planner *planner, int index, int coord)
  {
    return planner-&gt;waypoints[index][coord];
  }
%}

%extend playerc_planner {
  %pythoncode %{
    def get_waypoint(self, index):
        x = _playerc._playerc_planner_get_waypoint_coord(self, index, 0)
        y = _playerc._playerc_planner_get_waypoint_coord(self, index, 1)
        z = _playerc._playerc_planner_get_waypoint_coord(self, index, 2)
        return (x, y, z)
  %}
}
</code></pre>

<p>Then you should be able to access waypoint number <code>n</code> in Python like this:</p>

<pre><code>(x, y, z) = PLN.get_waypoint(n)
</code></pre>
",1639256,1,13,,,Programming
320,1871,20863327,Unable to receive data from serial port,|c++|serial-port|robotics|,"<p>Currently I try to write a serial port communication in VC++ to transfer data from PC and robot via XBee transmitter. But after I wrote some commands to poll data from robot, I didn't receive anything from the robot (the output of filesize is 0 in the code.). Because my MATLAB interface works, so the problem should happen in the code not the hardware or communication. Would you please give me help? </p>

<p>01/03/2014 Updated: I have updated my codes. It still can not receive any data from my robot (the output of read is 0). When I use ""cout&lt;&lt;&amp;read"" in the while loop, I obtain ""0041F01C1"". I also don't know how to define the size of buffer, because I don't know the size of data I will receive. In the codes, I just give it a random size like 103. Please help me.</p>

<pre><code>// This is the main DLL file.
#include ""StdAfx.h""
#include &lt;iostream&gt;

#define WIN32_LEAN_AND_MEAN //for GetCommState command
#include ""Windows.h""
#include &lt;WinBase.h&gt;

using namespace std;

int main(){


  char init[]="""";

  HANDLE serialHandle;

  // Open serial port
  serialHandle = CreateFile(""\\\\.\\COM8"", GENERIC_READ | GENERIC_WRITE, 0, 0, OPEN_EXISTING, FILE_ATTRIBUTE_NORMAL, 0);

// Do some basic settings
  DCB serialParams;
  DWORD read, written;
  serialParams.DCBlength = sizeof(serialParams);

  if((GetCommState(serialHandle, &amp;serialParams)==0))
  {
    printf(""Get configuration port has a problem."");
    return FALSE;
   }

   GetCommState(serialHandle, &amp;serialParams);
   serialParams.BaudRate = CBR_57600;
   serialParams.ByteSize = 8;
   serialParams.StopBits = ONESTOPBIT;
   serialParams.Parity = NOPARITY;

   //set flow control=""hardware""
   serialParams.fOutX=false;
   serialParams.fInX=false;
   serialParams.fOutxCtsFlow=true;
   serialParams.fOutxDsrFlow=true;
   serialParams.fDsrSensitivity=true;
   serialParams.fRtsControl=RTS_CONTROL_HANDSHAKE;
   serialParams.fDtrControl=DTR_CONTROL_HANDSHAKE;

   if (!SetCommState(serialHandle, &amp;serialParams))
   {
       printf(""Set configuration port has a problem."");
       return FALSE;

   }


   GetCommState(serialHandle, &amp;serialParams);

   // Set timeouts
   COMMTIMEOUTS timeout = { 0 };
   timeout.ReadIntervalTimeout = 30;
   timeout.ReadTotalTimeoutConstant = 30;
   timeout.ReadTotalTimeoutMultiplier = 30;
   timeout.WriteTotalTimeoutConstant = 30;
   timeout.WriteTotalTimeoutMultiplier = 30;

   SetCommTimeouts(serialHandle, &amp;timeout);

   if (!SetCommTimeouts(serialHandle, &amp;timeout))
   {
       printf(""Set configuration port has a problem."");
       return FALSE;

   }



   //write packet to poll data from robot
   WriteFile(serialHandle,""&gt;*&gt;p4"",strlen(""&gt;*&gt;p4""),&amp;written,NULL);



   //check whether the data can be received
   char buffer[103];



   do {
  ReadFile (serialHandle,buffer,sizeof(buffer),&amp;read,NULL);
      cout &lt;&lt; read;
    } while (read!=0);

     //buffer[read]=""\0"";



   CloseHandle(serialHandle);
   return 0;
}
</code></pre>
",41639.89028,,1865,2,4,0,,3150235,,41639.88194,4,20863550,"<p>GetFileSize is documented not to be valid when used with a serial port  handle. Use the ReadFile function to receive serial port data.</p>
",2360695,1,6,31302313,@Lokno Thanks for reply. Currently I don't have suitable cable to directly connect robot and PC serial... It may take me a little time to answer your question.,Connections
321,1678,15320030,Real time programming in C++,|c++|real-time|robotics|,"<p>I have two C++ classes, <code>objecManip</code> and <code>updater</code>.  The <code>updater</code> class has a timer to check the status of the robot arm of my application.  </p>

<p>If it moving then do nothing, else getNextAction() from the actions queue.</p>

<p>The actions queue is populated with class <code>objectManip</code>.  I have a global variable: <code>current_status</code> of the robot arm that I need in <code>objectManip</code>.</p>

<p>The problem is that when filling in actions queue <code>current_status</code> is taken constantly not dynamically.</p>
",41343.31806,15320108,3101,1,4,-5,,1807373,"Madinah, Saudi Arabia",41220.86736,793,15320108,"<p>Question is very unclear, so this is really a stab in the dark, but you need to use atomic data types. With C++11, you have <code>std::atomic</code> (see <a href=""http://en.cppreference.com/w/cpp/atomic/atomic"" rel=""nofollow"">here</a> or <a href=""http://www.cplusplus.com/reference/atomic/atomic/atomic/"" rel=""nofollow"">here</a>. For an earlier version of C++, I think you need to use some library or compiler specific data type, which offers atomic data types.</p>

<p>If you make some assumptions about how multithreading works for your CPU and operating system, you <em>may</em> get away with just declaring shared variables <code>volatile</code> and reading value to temp variable when you use it. <code>volatile</code> is really meant for cases like reading hardware-mapped values, where value must be read from the memory every time, so many optimizations are not possible. It does not guarantee atomic updates in itself, because a thread modifying a value may be interrupted in the middle of update, and then another read may read invalid, partially updated value. For booleans this should be fairly safe. For integers which do not cross memory word boundary and which word size or less, this may be safe on many CPUs, which will not interrupt a thread in the middle of writing single memory word. Otherwise, it is data corruption waiting to happen. Some (today uncommon) CPUs also do not synchronize caches between multiple CPU cores, and in that case <code>volatile</code> will not help, different threads may see different cached value. So conclusion: use <code>volatile</code> as last resort hack!</p>
",1717300,1,0,21630201,Why did post the code seem a bad idea?,Timing
322,1797,18150100,SocketServer used to control PiBot remotely (python),|python|raspberry-pi|robot|socketserver|,"<p>this is my first question! (despite using the site to find most of the answers to programming questions i've ever had)</p>

<p>I have created a PiBotController which i plan to run on my laptop which i want to pass the controls (inputs from arrow keys) to the raspberry pi controlling my robot. Now the hardware side of this isn't the issue i have created a program that responds to arrow key inputs and i can control the motors on the pi through a ssh connection.</p>

<p>Searching online i found the following basic server and client code using socketserver which i can get to work sending a simple string.</p>

<p>Server:</p>

<pre><code>import socketserver

class MyTCPHandler(socketserver.BaseRequestHandler):
""""""
The RequestHandler class for our server.

It is instantiated once per connection to the server, and must
override the handle() method to implement communication to the
client.
""""""

    def handle(self):
        # self.request is the TCP socket connected to the client
        self.data = self.request.recv(1024).strip()
        print(""{} wrote:"".format(self.client_address[0]))
        print(self.data)
        # just send back the same data, but upper-cased
        self.request.sendall(self.data.upper())

if __name__ == ""__main__"":
    HOST, PORT = ""localhost"", 9999

# Create the server, binding to localhost on port 9999
server = socketserver.TCPServer((HOST, PORT), MyTCPHandler)

# Activate the server; this will keep running until you
# interrupt the program with Ctrl-C
server.serve_forever()
</code></pre>

<p>Client:</p>

<pre><code>import socket
import sys

HOST, PORT = ""192.168.2.12"", 9999
data = ""this here data wont send!! ""


# Create a socket (SOCK_STREAM means a TCP socket)
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

try:
    # Connect to server and send data
    sock.connect((HOST, PORT))
    sock.sendall(bytes(data + ""\n"", ""utf-8""))

    # Receive data from the server and shut down
    received = str(sock.recv(1024), ""utf-8"")
finally:
    sock.close()

print(""Sent:     {}"".format(data))
print(""Received: {}"".format(received))
</code></pre>

<p>Now this works fine and prints the results both on my Raspberry Pi (server) and on my laptop (client) however i have tried multiple times to combine it into a function that activates along with my key press' and releases' like i have in my controller.</p>

<p>PiBotController</p>

<pre><code>#import the tkinter module for the GUI and input control
try:
    # for Python2
    import Tkinter as tk
    from Tkinter import *
except ImportError:
    # for Python3
    import tkinter as tk
    from tkinter import *

import socket
import sys

#variables
Drive = 'idle'
Steering = 'idle'




#setting up the functions to deal with key presses
def KeyUp(event):
    Drive = 'forward'
    drivelabel.set(Drive)
    labeldown.grid_remove()
    labelup.grid(row=2, column=2)
def KeyDown(event):
    Drive = 'reverse'
    drivelabel.set(Drive)
    labelup.grid_remove()
    labeldown.grid(row=4, column=2)
def KeyLeft(event):
    Steering = 'left'
    steeringlabel.set(Steering)
    labelright.grid_remove()
    labelleft.grid(row=3, column=1)
def KeyRight(event):
    Steering = 'right'
    steeringlabel.set(Steering)
    labelleft.grid_remove()
    labelright.grid(row=3, column=3)
def key(event):
    if event.keysym == 'Escape':
        root.destroy()

#setting up the functions to deal with key releases
def KeyReleaseUp(event):
    Drive = 'idle'
    drivelabel.set(Drive)
    labelup.grid_remove()
def KeyReleaseDown(event):
    Drive = 'idle'
    drivelabel.set(Drive)
    labeldown.grid_remove()
def KeyReleaseLeft(event):
    Steering = 'idle'
    steeringlabel.set(Steering)
    labelleft.grid_remove()
def KeyReleaseRight(event):
    Steering = 'idle'
    steeringlabel.set(Steering)
    labelright.grid_remove()

#connection functions
def AttemptConnection():
    connectionmessagetempvar = connectionmessagevar.get()
    connectionmessagevar.set(connectionmessagetempvar + ""\n"" + ""Attempting to        connect..."") 

def transmit(event):
    HOST, PORT = ""192.168.2.12"", 9999
    data = ""this here data wont send!! ""


    # Create a socket (SOCK_STREAM means a TCP socket)
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

    try:
         # Connect to server and send data
         sock.connect((HOST, PORT))
         sock.sendall(bytes(data + ""\n"", ""utf-8""))

         # Receive data from the server and shut down
         received = str(sock.recv(1024), ""utf-8"")
    finally:
         sock.close()

    print(""Sent:     {}"".format(data))
    print(""Received: {}"".format(received))





#setting up GUI window        
root = tk.Tk()
root.minsize(300,140)
root.maxsize(300,140)
root.title('PiBot Control Centre')
root.grid_columnconfigure(0, minsize=50)
root.grid_columnconfigure(1, minsize=35)
root.grid_columnconfigure(2, minsize=35)
root.grid_columnconfigure(3, minsize=35)
root.grid_rowconfigure(2, minsize=35)
root.grid_rowconfigure(3, minsize=35)
root.grid_rowconfigure(4, minsize=35)
root.configure(background='white')
root.option_add(""*background"", ""white"")




#set up the labels to display the current drive states
drivelabel = StringVar()
Label(root, textvariable=drivelabel).grid(row=0, column=1, columnspan=2)
steeringlabel = StringVar()
Label(root, textvariable=steeringlabel).grid(row=1, column=1, columnspan=2)
Label(root, text=""Drive: "").grid(row=0, column=0, columnspan=1)
Label(root, text=""Steering: "").grid(row=1, column=0, columnspan=1)

#set up the buttons and message for connecting etc..
messages=tk.Frame(root, width=150, height=100)
messages.grid(row=1,column=4, columnspan=2, rowspan=4)


connectionbutton = Button(root, text=""Connect"", command=AttemptConnection)
connectionbutton.grid(row=0, column=4)
connectionmessagevar = StringVar()
connectionmessage = Message(messages, textvariable=connectionmessagevar, width=100, )
connectionmessage.grid(row=1, column=1, rowspan=1, columnspan=1)
disconnectionbutton = Button(root, text=""Disconnect"")
disconnectionbutton.grid(row=0, column=5)







#pictures
photodown = PhotoImage(file=""down.gif"")
labeldown = Label(root, image=photodown)
labeldown.photodown = photodown
#labeldown.grid(row=4, column=1)

photoup = PhotoImage(file=""up.gif"")
labelup = Label(root, image=photoup)
labelup.photoup = photoup
#labelup.grid(row=2, column=1)

photoleft = PhotoImage(file=""left.gif"")
labelleft = Label(root, image=photoleft)
labelleft.photoleft = photoleft
#labelleft.grid(row=3, column=0)

photoright = PhotoImage(file=""right.gif"")
labelright = Label(root, image=photoright)
labelright.photoright = photoright
#labelright.grid(row=3, column=2)

photoupleft = PhotoImage(file=""upleft.gif"")
labelupleft = Label(root, image=photoupleft)
labelupleft.photoupleft = photoupleft
#labelupleft.grid(row=2, column=0)

photodownleft = PhotoImage(file=""downleft.gif"")
labeldownleft = Label(root, image=photodownleft)
labeldownleft.photodownleft = photodownleft
#labeldownleft.grid(row=4, column=0)

photoupright = PhotoImage(file=""upright.gif"")
labelupright = Label(root, image=photoupright)
labelupright.photoupright = photoupright
#labelupright.grid(row=2, column=2)

photodownright = PhotoImage(file=""downright.gif"")
labeldownright = Label(root, image=photodownright)
labeldownright.photodownright = photodownright
#labeldownright.grid(row=4, column=2)




#bind all key presses and releases to the root window
root.bind_all('&lt;Key-Up&gt;', KeyUp)
root.bind_all('&lt;Key-Down&gt;', KeyDown)
root.bind_all('&lt;Key-Left&gt;', KeyLeft)
root.bind_all('&lt;Key-Right&gt;', KeyRight)

root.bind_all('&lt;KeyRelease-Up&gt;', KeyReleaseUp)
root.bind_all('&lt;KeyRelease-Down&gt;', KeyReleaseDown)
root.bind_all('&lt;KeyRelease-Left&gt;', KeyReleaseLeft)
root.bind_all('&lt;KeyRelease-Right&gt;', KeyReleaseRight)

root.bind_all('&lt;Key&gt;', key)
root.bind_all('&lt;Key&gt;', transmit)




#set the labels to an initial state
steeringlabel.set('idle')
drivelabel.set('idle')
connectionmessagevar.set ('PiBotController Initiated')

#initiate the root window main loop
root.mainloop()
</code></pre>

<p>this program compiles fine but then doesn't send any data to the server? (i'm aware its still just sending a string but i thought id start with something easy... and well evidently i got stuck so it is probably for the best)</p>

<p>Any suggestions to make it work just sending the string or sending the varibales drive and steering every time they change would be greatly appreciated.</p>

<p>Dave xx</p>

<p><em><strong>EDIT</em></strong></p>

<p>here is the transmit function i got to it work in the sense it sends data whenever i do a key press/release (like i wanted before) however it only sends the initial setting for the variables of 'idle'. Looking at the code now as well i think i should probably take the host and port info and creating a socket connection out of the function that runs every time? but im not sure so here is what i have right now anyway.</p>

<pre><code>def transmit():
    HOST, PORT = ""192.168.2.12"", 9999
    DriveSend = drivelabel.get
    SteeringSend = steeringlabel.get


    # Create a socket (SOCK_STREAM means a TCP socket)
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

    try:
        # Connect to server and send data
        sock.connect((HOST, PORT))
        sock.sendall(bytes(Drive + ""\n"", ""utf-8""))
        sock.sendall(bytes(Steering + ""\n"", ""utf-8""))

        # Receive data from the server and shut down
        received = str(sock.recv(1024), ""utf-8"")
    finally:
        sock.close()

    print(""Sent:     {}"".format(Steering))
    print(""Sent:     {}"".format(Drive))
    print(""Received: {}"".format(received))
</code></pre>
",41495.63611,18150274,4084,1,0,1,0,2668299,"Bristol, United Kingdom",41495.61111,47,18150274,"<p>The problem is that when Tkinter catches a key event, it triggers the more specific binding first (for example 'Key-Up'), and the event is never passed to the more general binding ('Key').  Therefore, when you press the 'up' key, KeyUp is called, but transmit is never called.</p>

<p>One way to solve this would be to just call transmit() within all the callback functions (KeyUp, KeyDown, etc).</p>

<p>For example, KeyUp would become</p>

<pre><code>def KeyUp(event):
    Drive = 'forward'
    drivelabel.set(Drive)
    labeldown.grid_remove()
    labelup.grid(row=2, column=2)
    transmit()
</code></pre>

<p>Then you can get rid of the event binding to 'Key'.</p>

<p>Another option would be to make ""Drive"" and ""Steering"" into Tkinter.StringVar objects, then bind to write events using ""trace"", like this:</p>

<pre><code>Drive = tk.StringVar()
Drive.set('idle')
Drive.trace('w', transmit)
</code></pre>

<p>Note that <code>trace</code> sends a bunch of arguments to the callback, so you'd have to edit <code>transmit</code> to accept them.</p>

<p><strong>EDIT</strong></p>

<p>Ok, I see the problems - there are three.  </p>

<p>1.  When you write </p>

<pre><code>Drive = 'forward'
</code></pre>

<p>in your callback functions, you're <strong>not</strong> setting the variable <code>Drive</code> in your module namespace, you're setting <code>Drive</code> in the local function namespace, so the module-namespace <code>Drive</code> never changes, so when <code>transmit</code> accesses it, it's always the same.</p>

<p>2.  In <code>transmit</code>, you write </p>

<pre><code>DriveSend = drivelabel.get
SteeringSend = steeringlabel.get
</code></pre>

<p>This is a good idea, but you're just referencing the functions, not calling them.  You need</p>

<pre><code>DriveSend = drivelabel.get()
SteeringSend = steeringlabel.get()
</code></pre>

<p>3.  In <code>transmit</code>, the values you send through the socket are the module-level variables <code>Drive</code> and <code>Steering</code> (which never change as per problem #1), rather than <code>DriveSend</code> and <code>SteeringSend</code>.</p>

<p><strong>Solution:</strong></p>

<p>I would recommend doing away with all the <code>Drive</code> and <code>Steering</code> variables entirely, and just using the <code>StringVars</code> 'drivelabel<code>and</code>steeringlabel`.  Thus your callbacks can become:</p>

<pre><code>def KeyUp(event):
#    Drive = 'forward'   (this doesn't actually do any harm, but to avoid confusion I'd just get rid of the Drive variables altogether)
    drivelabel.set('forward')
    labeldown.grid_remove()
    labelup.grid(row=2, column=2)
</code></pre>

<p>(and so on for the rest of the callbacks) and your transmit function will become</p>

<pre><code>def transmit():
    HOST, PORT = ""192.168.2.12"", 9999
    DriveSend = drivelabel.get()        # Note the ()
    SteeringSend = steeringlabel.get()

    # Create a socket (SOCK_STREAM means a TCP socket)
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

    try:
        # Connect to server and send data
        sock.connect((HOST, PORT))
        sock.sendall(bytes(DriveSend + ""\n"", ""utf-8""))    # Note Drive ==&gt; DriveSend
        sock.sendall(bytes(SteeringSend + ""\n"", ""utf-8"")) # Note Steering ==&gt; SteeringSend

        # Receive data from the server and shut down
        received = str(sock.recv(1024), ""utf-8"")
    finally:
        sock.close()

    print(""Sent:     {}"".format(SteeringSend))   # Note Steering ==&gt; SteeringSend
    print(""Sent:     {}"".format(DriveSend))      # Note Drive ==&gt; DriveSend
    print(""Received: {}"".format(received))
</code></pre>

<p><strong>modified solution (from OP):</strong></p>

<p>Since playing around with this method for a little while i found that constantly changing the variables every 100ms due to key being held down is troublesome and causes isues with the smoothness of the motor control when i am for example just driving forward. to fix this i used the following edit into each function</p>

<pre><code>def KeyUp(event):
    if drivelabel.get() == ""forward"":
        pass
    else:
        drivelabel.set(""forward"")
        labeldown.grid_remove()
        labelup.grid(row=2, column=2)
        transmit()
        print (drivelabel.get())
</code></pre>

<p>The code now checks if the varibale is already set to the relevent direction if it is it does nothing, otherwise it modifies it. the print line is just there for me to check it was working properly and could be removed or commented out.</p>
",1460057,2,5,,,Remote
323,1691,15559079,Python threading for wheel encoders on a Robot,|python|python-multithreading|robot|,"<p>I'm writing the code for a robot which my college is entering into a competition. I'm currently trying to build some wheel encoders using reflectance sensors. I realised a while back that I would probably need to use threading to achieve this, seeing as the robot needs to monitor both the left and right encoders at the same time. The code below is what I have so far: </p>

<pre><code>from __future__ import division
import threading
import time
from sr import *
R = Robot()

class Encoder(threading.Thread):
    def __init__(self, motor, pin, div=16):
        self.motor = motor
        self.pin = pin
        self.div = div
        self.count = 0
        threading.Thread.__init__(self)

    def run(self):
        while True: 
            wait_for(R.io[0].input[self.pin].query.d)
            self.count += 1

    def rotations(self, angle, start_speed=50):
        seg = 360/self.div
        startcount = self.count
        current_dist = angle #Distance away from target
        R.motors[self.motor].target = start_speed
        while current_dist &gt; 360:
            newcount = self.count - startcount
            current_dist = angle - newcount*seg
            R.motors[self.motor].target = 50
        while abs(current_dist) &gt; seg/2:  
            newcount = self.count - startcount
            current_dist = angle - newcount*seg
            current_speed = start_speed * current_dist / 360
            if current_speed &lt; 5:
                R.motors[self.motor].target = 5
            else:
                R.motors[self.motor].target = current_speed
        R.motors[self.motor].target = 0

WheelLeft = Encoder(0,0)
WheelLeft.start()
WheelRight = Encoder(1,3)
WheelRight.start()

WheelRight.rotations(720)
WheelLeft.rotations(720)
</code></pre>

<p>The sr module is provided by Southampton University, who are running the competition. It allows us to interact with the robot's hardware. </p>

<p>Now, the threads which get created seem to allow the two reflectance sensors to be monitored separately. This bit of code: <code>R.io[0].input[self.pin].query.d</code> works out whether the value coming from the reflectance sensor has changed. The 'rotations' method turns the wheel through a certain angle by constantly checking how many degrees the wheel has already turned through, and slowing it down as it reaches the end. I would like both wheels to start turning when I run the program, and then slow down and stop when they have gone through 2 rotations. Currently though, when I run the program, one wheel starts turning and slows down and stops, followed by the other wheel. It seems to me like the 'rotations' method is not running in a thread, like the 'run' method is. Is it only the code under the 'run' method that runs in a thread, or is it the whole class?</p>

<p>If it helps, I've been following this tutorial: <a href=""http://www.devshed.com/c/a/Python/Basic-Threading-in-Python/1/"" rel=""nofollow"">http://www.devshed.com/c/a/Python/Basic-Threading-in-Python/1/</a></p>

<p>Also, I would like to know why it is possible to start a thread only with <code>Encoder(0,0).start()</code>. Why do you not have to create an object using the class (e.g. <code>Thread = Encoder(0,0).start()</code> for a new thread to be created?</p>

<p>Sorry if the terminoligy I've used isn't up to scratch, as you can probably tell I'm quite new to threading, and programming in general. </p>
",41354.91944,15559137,1886,3,0,1,,2197015,,41354.88889,6,15559137,"<p><code>Encoder(0,0).start()</code> is a call to the method to start the thread. In turn, this method calls your <code>run</code> implementation, which doesn't use the <code>rotations</code> method. If you want to do so, then you have to call it in the while loop of <code>run</code>.</p>

<p>With <code>Thread = Encoder(0,0).start()</code> you store the value retrieved from that call (which is None), but to get it you need to start the new thread first anyway.</p>
",1019227,1,0,,,Timing
324,1720,16394748,Controlling robot makes a shaking movement,|c|coordinates|robotics|,"<p>Im trying to control a robot by sending positions with 100hz. It's making a shaking movement when sending so much positions. When I send 1 position that is like 50 mm from his start position it moves smoothly. When I use my sensor to steer,(so it send every position from 0 to 50mm) it is shaking. I'm probably sending like X0-X1-X2-X1-X2-X3-X4-X5-X4-X5 and this is the reason why it might shake. How can I solve this making the robot move smoothly when I use my mouse to steer it?</p>

<ul>
<li>Robot is asking 125hz</li>
<li>IR sensor is sending 100hz</li>
<li>Otherwise does the 25hz makes the diffrent?</li>
</ul>

<p>Here is my code.</p>

<pre><code>        while(true)
        // If sensor 1 is recording IR light.
        if (listen1.newdata = true)
        {

            coX1 = (int) listen1.get1X();           // 
            coY1 = (int) listen1.get1Y();       
            newdata = true;
        } else {
            coX1 = 450;
            coY1 = 300;
        }

        if (listen2.newdata = true)
        {       
            coX2 = (int) listen2.get1X();
            coY2 = (int) listen2.get1Y();
            newdata = true; 
        } else {
            coY2 = 150;
        }
        // If the sensor gets further then the workspace, it will automaticly correct it to these 
        // coordinates.

        if (newdata = true)
        {
            if (coX1&lt; 200 || coX1&gt; 680)
            {
                coX1 = 450;
            }
            if (coY1&lt;200 || coY1&gt; 680)
            {
                coY1 = 300;
            }
            if (coY2&lt;80 || coY2&gt; 300)
            {
                coY2 = 150;
            }
        }
        // This is the actually command send to a robot.
        Gcode = String.format( ""movej(p[0.%d,-0.%d, 0.%d, -0.5121, -3.08, 0.0005])""+ ""\n"", coX1, coY1, coY2);

        //sends message to server
        send(Gcode, out);     
            System.out.println(Gcode);
            newdata = false;

        }


}


private static void send(String movel, PrintWriter out) {
     try {


         out.println(movel); /*Writes to server*/
        // System.out.println(""Writing: ""+ movel);
        // Thread.sleep(250);
         }

         catch(Exception e) {
         System.out.print(""Error Connecting to Server\n"");
         } 
        }
}
</code></pre>

<p>@ Edit</p>

<p>I discovered on wich way I can do this. It is via min and max. So basicly what I think I have to do is: 
 * put every individual coordinate in a array( 12 coordinates)
 * Get the min and max out of this array 
 * Output the average of the min and max</p>
",41400.34861,,164,1,2,0,,2234928,,41366.28958,33,16395070,"<p>Without knowing more about your robot characteristics and how you could control it, here are some general considerations:</p>

<p>To have a smooth motion of your robot, you should control it in speed with a well designed PID controller algorithm.</p>

<p>If you can only control it in position, the best you can do is monitoring the position &amp; waiting for it to be ""near enough"" from the targetted position before sending the next position.</p>

<p>If you want a more detailed answer, please give more information on the command you send to the robot (<code>movej</code>), I suspect that you can do much more than just sending [x,y] coordinates. </p>
",2622924,1,1,23500928,This question should be moved to http://robotics.stackexchange.com,Actuator
325,1923,21999921,"distance measuring from ultrasonic sensor, variable overflow",|c|avr|robotics|,"<p>Hy, I have ultrasonic sensor measuring distance and no mather which type is my variable ""range""  (uint8_t, uint16_t, 32, 64)I always get overflow, and than sensor starts from 0 again..Is there a way that I can limit ""range"" variable or I must limit that on harder way with pulsewidth... Thanks</p>

<pre><code>SENSOR_DDR |= (1&lt;&lt;TRIGGER_PIN);  
SENSOR_DDR &amp;= ~(1&lt;&lt;ECHO_PIN) &amp; ~(1&lt;&lt;PB3) &amp; ~(1&lt;&lt;PB2) &amp; ~(1&lt;&lt;PB1) &amp; ~(1&lt;&lt;PB0); 
DDRD = DDRD | _BV(4); 
PORTD = PORTD | _BV(4);
ENGINE_DDR = 0xff; 
ENGINE_PORT = 0;

lcd_init(LCD_DISP_ON);
lcd_clrscr();
lcd_puts(""Something wrong..."");


while(1)
{

PORTB |= (1&lt;&lt;PB4); //Send Trigger
_delay_us(10);
PORTB &amp;= ~(1&lt;&lt;PB4); //Send trigger


timer0counter=0;
TCNT0=0; //Clear timer
while(bit_is_clear(PINB,5)); //Wait for rising edge
TCCR0 |= (1&lt;&lt;CS02); //Select prescalar 256
TIMSK |= (1&lt;&lt;TOIE0) | (1&lt;&lt;TOIE2); //Enable timer0 overflow interrupt

lcd_clrscr();

while(bit_is_set(PINB,5) &amp;&amp; timer0counter&lt;9) //wait for falling edge of echo
{
_delay_us(5);
}
TCCR0 &amp;= ~(1&lt;&lt;CS02); //Stop timer
TIMSK &amp;= ~(1&lt;&lt;TOIE0);
if(bit_is_set(PINB,5))
{
lcd_puts(""No OBSTACLE"");
}
else
{
range=(256*timer0counter+TCNT0)*32*0.017; //range conversion

lcd_clrscr();
lcd_puts(""Distance:"");
lcd_puts(itoa(range,buffer,10));
lcd_puts_P(""cm"");
}
if(range&lt;15){
...

ISR(TIMER0_OVF_vect) 
{
TIMSK &amp;= ~(1&lt;&lt;TOIE0);
TCNT0=0;
timer0counter++;

TIMSK |= (1&lt;&lt;TOIE0);

if(timer0counter&gt;8)
{
TCCR0 &amp;= ~(1&lt;&lt;CS02);
TIMSK &amp;= ~(1&lt;&lt;TOIE0);

}
</code></pre>
",41694.91389,22000520,437,1,2,0,,3214354,,41659.35972,6,22000520,"<p>The calculation </p>

<pre><code>256*timer0counter+TCNT0
</code></pre>

<p>saves temporary value in 'default' size int, which on AVR is 16b. so, every time timer0counter is higher than 256 it will overflow regardless of the final type of the variable.</p>

<p>instead of doing</p>

<pre><code>range=(256*timer0counter+TCNT0)*32*0.017;
</code></pre>

<p>try going with:</p>

<pre><code>double range_real = 256.0 * (double)timer0counter + (double)TCNT0 * 32.0 * 0.017;
range = (int) range_real;
</code></pre>

<p>Being explicit about types can really save your skin.</p>
",3010316,0,8,33344367,now I added some code I hope it helps you so you could help me :),Programming
326,2020,24998038,Path planning and collision avoidance for multiple autonomous robots in static environment.,|algorithm|path|robotics|motion-planning|,"<p>I am going to start my work on a robotic project. Before jumping into the question, let me first give a brief description of the set up of this project.</p>

<p>The set up consists of a facility where there is a rail system and there are multiple robots mounted on them. The environment is static with only mobile robots. As for now it can be 3 robots wagon on these rails. These robots are for pick-and-place tasks.There is, as such no communication between these robots but they are connected to the server, which gives the robots tasks.</p>

<p>Please have a look at the rough sketch (pardon me for this bad diagram) to have an idea of the set-up. <img src=""https://i.stack.imgur.com/ddM3X.jpg"" alt=""enter image description here""> </p>

<p>From the above diagram, R1 and R2 are robots on the rails. The server may assign a job to robot R1 for picking an object at ""A"" and dropping it at ""B"" and the robot has to move completely autonomous. 
Now, my queries are as follows:</p>

<ul>
<li>How the robot R1 moves to ""A"" and then to ""B"", taking the optimal path, concerning Path Planning of the robot?</li>
<li>How the robot avoid collision in a static map, with other mobile robots on the rails, concerning collision avoidance ? (I am thinking of using a camera to detect the other robot)</li>
</ul>

<p>I have looked into some literature and have a basic idea. I have also gone through some of the asked <a href=""https://stackoverflow.com/questions/19406147/path-planning-and-obstacle-avoidance-algorithms"">question</a> in here. But I dont have any concrete idea to start working. I am looking for some advice/ideas/algorithms/literature to go about the problem.
Please help me out. Thanks in advance !!</p>

<p>Note: I will be simulating the whole set-up in a 3D environment.</p>
",41848.62292,,671,1,0,1,,1800026,,41218.48681,65,24998363,"<p>For the first question, consider the entire rail network as a graph and use a shortest path algorithm to get the optimal path.</p>

<p>I do not know if you are allowed to move other robots when moving <code>R1</code> to <code>A</code> and <code>B</code>. If some of the robots cannot be moved, then remove those portions of the railway from the graph, and calculate the path.</p>

<p>To avoid collision, one method would be to allow motion in only one direction along the rails (a figure of 8 in this case).  Overall it should not be a problem since you are controlling the bots from a central server.</p>
",2963623,1,6,,,Moving
327,1942,23009549,"Roll, pitch, yaw calculation",|3d|robotics|,"<p>How can I calculate the roll, pitch and yaw angles associated with a homogeneous transformation matrix?</p>

<p>I am using the following formulas at the moment, but I am not sure whether they are correct or not.</p>

<pre><code>pitch = atan2( -r20, sqrt(r21*r21+r22*r22) );
yaw   = atan2(  r10, r00 );
roll  = atan2(  r21, r22 );
</code></pre>

<p>r10 means second row and first column.</p>
",41740.42083,23010193,71398,4,2,8,0,2983111,,41590.50417,53,23009933,"<p>[This might be better suited as a comment but it is to long for that ;) ]</p>

<p>When I compare your formula with the one on the german Wikipedia page about roll, pitch an yaw (<a href=""http://de.wikipedia.org/wiki/Roll-Nick-Gier-Winkel#Berechnung_aus_Rotationsmatrix"" rel=""nofollow"">see here</a>) there is a difference in the calculation of the pitch. According to Wikipedia your formula should look like this:</p>

<pre><code>pitch = atan2(-r20,(sqrt(pow(r21,2)+pow(r00,2))); // replaced r22 by r00
</code></pre>

<p>Note that on the wikipedia page they use a different indexing for the matrix elements (thex start with 1 and not with 0 for the first row/column). Furthermore they call pitch beta, yaw alpha and roll gamma. Also, they divide the coefficents for <code>atan2</code> in the yaw and roll calculation by the <code>cos(pitch)</code>, but that should cancel out.</p>

<p>Otherwise your formula looks fine to me.</p>
",1191041,2,2,35200721,"@Sh3ljohn no I am not satisfied because I know these things already. I found another way to find out roll pitch yaw, kindly see this link http://code.google.com/p/matlab-toolboxes-robotics-vision/source/browse/matlab/robot/tags/R9.4/tr2rpy.m?r=822 ..Do you know what should be input for this matlab function?",Coordinates
328,1903,21458336,bayes rule -> practical application with Sharp IR sensor,|python|robot|,"<p>I have a robot using IR and Sonar sensors for measuring distances. I build an occupancy map/grid.
So far I use a simple integer based system to ""calculate"" the probability of a cell being occupied. Something like +1 if sensor hit and -1 for all calls between 0 and sensor reading-1. If the number in the array for one cell if above the threshold the cell is counted as occupied and via versa for unoccupied. All between in uncertain. (a bit more complex but based on this idea)</p>

<p>I now wonder if it is worth to use a Bayes theorem based solution for this (first code snip below). As most people do it this way the answer is most likely yes :-).</p>

<p>What do p1 and p2 mean in this specific example - let's say for an IR distance sensor? I understand the examples when the theorem is explained. But somehow I can't translate them to the IR sensor situation. (my mind got a bit stuck here)
I have no clue what and how to estimate the values I should put in there and how to apply them to my array/map.</p>

<p>Would be nice if someone could enlighten me :-)
If somehow possible with some pseudo code.</p>

<p>Below also the class for my current map handling.</p>

<p>Thanks
Robert</p>

<p><br>
Bayes functions -- but how to apply?</p>

<pre><code>def pos(p0, p1, p2):
    return (p0 * p1)/(p0 * p1 + (1-p0) * (1-p2))

def neg(p0, p1, p2):
    return (p0 * (1-p1))/(p0 * (1-p1) + (1-p0) * p2)
</code></pre>

<p><br></p>

<p>My current Map class:</p>

<pre><code>templateData = {
    'MapWidth' : 800,
    'MapHeight': 600,
    'StartPosX' : 500,
    'StartPosY' : 300,
    'StartTheta' : 0,
    'Resolution' : 5,
    'mapThresholdFree' : 126,
    'mapThresholdOcc' : 130,
    'EmptyValue' : 128,
    'mapMaxOcc' : 255,
    'mapMaxFree' : 0,
    'ServoPos' : 0,
    'CurrentPosX' : 0,
    'CurrentPosY' : 0,
    'CurrentTheta' : 0
}

templateData[""MapHeight""] = templateData[""MapHeight""] / templateData[""Resolution""]
templateData[""MapWidth""] = templateData[""MapWidth""] / templateData[""Resolution""]
templateData[""StartPosX""] = templateData[""StartPosX""] / templateData[""Resolution""]
templateData[""StartPosY""] = templateData[""StartPosY""] / templateData[""Resolution""]

#map
map=robotmap.NewRobotMap(templateData[""MapWidth""],templateData[""MapHeight""], templateData[""Resolution""], templateData[""StartPosX""],templateData[""StartPosY""], templateData[""StartTheta""], templateData[""ServoPos""],templateData[""mapMaxOcc""],templateData[""mapMaxFree""],templateData[""EmptyValue""])
map.clear()

class NewRobotMap(object): 
    def __init__(self, sizeX, sizeY, Resolution, RobotPosX, RobotPosY, RobotTheta, ServoPos, mapMaxOcc, mapMaxFree, EmptyValue):
        self.sizeX = sizeX 
        self.sizeY = sizeY 
        self.RobotPosX = int(RobotPosX)
        self.RobotPosY = int(RobotPosY)
        self.mapResolution = int(Resolution)
        self.StartPosX = int(RobotPosX)
        self.StartPosY = int(RobotPosY)
        self.RobotTheta = float(RobotTheta)
        self.EmptyValue = EmptyValue
        self.ServoPos = ServoPos
        self.mapMaxOcc = mapMaxOcc
        self.mapMaxFree = mapMaxFree
    def clear(self):
        self.RobotMap = [[self.EmptyValue for i in xrange(self.sizeY)] for j in xrange(self.sizeX)]
    def updateMap(self ,x ,y , Val):
        oldval = self.RobotMap[x][y]
        self.RobotMap[x][y]=self.RobotMap[x][y] + Val
        if self.RobotMap[x][y] &gt; self.mapMaxOcc:
            self.RobotMap[x][y] = self.mapMaxOcc
        elif self.RobotMap[x][y] &lt; self.mapMaxFree:
            self.RobotMap[x][y] = self.mapMaxFree            
        return oldval, self.RobotMap[x][y]
    def updateRobot(self,theta,x,y):
        self.RobotTheta = float(theta)
        self.RobotPosX = int(round(self.StartPosX + float(int(x)/self.mapResolution), 0))
        self.RobotPosY = int(round(self.StartPosY - float(int(y)/self.mapResolution),0))
    def getRobotPos(self):
        return self.RobotPosX, self.RobotPosY
    def display(self):
        s = [[str(e) for e in row] for row in self.RobotMap]
        lens = [len(max(col, key=len)) for col in zip(*s)]
        fmt = '\t'.join('{{:{}}}'.format(x) for x in lens)
        table = [fmt.format(*row) for row in s]
        print '\n'.join(table)
    def updateServoPos(self, newServoPos):
        self.ServoPos = newServoPos
</code></pre>
",41669.57569,,248,1,6,0,,3173818,,41647.62778,21,21461033,"<p>In <code>pos</code>:</p>

<ul>
<li><code>p0</code> is the prior probability that an object is there. This would probably require knowing in advance the density of objects in the map.</li>
<li><code>p1</code> is the probability of a sensor hit given that an object is there (IOW a true positive)</li>
<li><code>p2</code> is the probability of no sensor hit given that an object is not there (a true negative)</li>
</ul>

<p>Now <code>1 - p2</code> is therefore the probability of a sensor hit given that an object is not there (a false positive). Additionally, <code>1 - p0</code> is the probability of no object being there. If we plug these values into Bayes' Rule, we get:</p>

<pre><code>Pr(Object|Hit) = Pr(Hit|Object)Pr(Object) / ( Pr(Hit|Object)Pr(Object) + Pr(No Hit|Object)Pr(No Object)

= p1 * p0 / ( p1 * p0 + (1 - p2) * (1 - p0) )
</code></pre>

<p>Which is the <code>pos()</code> function you gave.</p>

<p><em>Note:</em> In the event that you don't know the density of objects in the map beforehand, you can use <code>p0 = 0.5</code>, in which case the equation simplifies to:</p>

<pre><code>Pr(Hit|Object) / ( Pr(Hit|Object) + Pr(No Hit|Object) )
= p1 / (p1 + (1-p2))
</code></pre>
",1142167,0,2,32385249,"Yes, that's correct. But determining the current probability usually involves calculating frequency within a sample. If you have information about the underlying distribution through some other means, than that would work just fine.",Moving
329,2088,26327267,Myro programming - Making robot stop when it sees and obstacle,|python|robotics|myro|,"<p>I'm using a scribbler robot and writing code in Python.  I'm trying to get it to stop when it sees an obstacle</p>

<p>So I created variables for the left obstacle sensor, the center obstacle sensor and the right obstacle sensor</p>

<pre><code>    left = getObstacle(0)
    center = getObstacle(1)
    right = getObstacle(2)
</code></pre>

<p>Then an if statement</p>

<pre><code>if (left &lt; 6400 &amp; center &lt; 6400 &amp; right &lt; 6400):
        forward(1,1)
    else:
        stop()
</code></pre>

<p>Basically the idea is if the sensors read less than 6400, it should move forward, otherwise, it should stop.  When testing the scribbler with the <code>senses</code> function, I noticed when I put the robot close to an object, it would read around 6400.</p>

<p>Here's what I have for the <code>main()</code> code</p>

<pre><code>def main():
      while True: 
        left = getObstacle(0)
        center = getObstacle(1)
        right = getObstacle(2)
        lir = getIR(0)
        rir = getIR(1)
    if (left &lt; 6400 &amp; center &lt; 6400 &amp; right &lt; 6400):
        forward(1,1)
    else:
        stop()
</code></pre>

<p>Why isn't my robot responding?  The Python code doesn't show any errors when I put it into the shell, but nothing is happening with my robot.</p>

<p>EDIT:</p>

<p>Some code changes.  So far the robot will move, but it won't stop.  Are my if and else statements incorrect?</p>

<pre><code>center = getObstacle(1)
def main():


    if (center &lt; 5400):
        forward(0.5)
    else:
        stop()
</code></pre>
",41924.68194,26330019,2977,2,7,0,,3577397,,41756.09375,141,26327495,"<p><code>&amp;</code> is the <a href=""https://en.wikipedia.org/wiki/Bitwise_operation#AND"" rel=""nofollow"">Bitwise operator</a></p>

<p><code>and</code> is the <a href=""https://en.wikipedia.org/wiki/Logical_conjunction"" rel=""nofollow"">logical AND operator</a></p>

<p>So your condition should be :</p>

<pre><code>if (left &lt; 6400 and center &lt; 6400 and right &lt; 6400):
    forward(1,1)
else:
    stop()
</code></pre>
",4133467,0,1,41319578,I corrected the indentation in my editor.  But it still doesn't work.,Moving
330,1934,22680580,Robotics: Want MicroC Pro PIC Sample Code?,|pic|robotics|microc|,"<p>I'm new to robotics.
I have to develop a line following robot. <br>
I hope to use PIC18F452 microchip.
I am looking for alredy developed source code to start with. <br>
Thank you. </p>
",41725.29792,22680736,187,1,0,-1,,,,,,22680736,"<p><a href=""https://github.com/diunuge/SLIIT--Robofest-2012"">This</a> codebase is developed for a robot with line following and quite advanced other functionalities. This also includes some circuits, you may find useful.</p>
",3441894,1,0,,,Incoming
331,2103,26814872,Haskell for Robotics,|haskell|functional-programming|embedded|robotics|,"<p>I am looking up on web about Haskell and its applications. Basically i trying to learn functional programming language and i see Haskell is very famous among them. What i want to know is, is it possible to use Haskel as substitute for c in robotics? Can i use Haskell for embedded system programming and getting data from sensors, moving the motors, implementing mathematical model that is used to design the robot and its behaviour and if possible apply machine learning algorithms? </p>

<p>I am just starting off in this field so if the question is naive enough, please answer like you would answer any newbie.</p>

<p>Update: If the Question is too broad, i would like to know the specifics. Do people compile down the haskell to the embedded hardware or use haskell as a remote control in most of the cases? Which one is more approachable using haskell? What is the general way of using haskell in hardware embedded programming? If it is only used as a remote control, how to implement genetic algorithms and machine learning algorithms using haskell? I know its too broad but i would just like to know the general usage if my requirement is such.</p>
",41951.30972,,1086,0,8,5,0,902817,,40774.70278,337,,,,,,42313456,"I just want to know, how people use haskell on robotics like on quadrocopter, do they compile down the haskell and run it on embedded or use computer as a remote control for robots or quadrocopter. Is it even possible to do such thing using haskell??",Specifications
332,1887,21002816,real time object detection based on color in opencv and python,|python|opencv|robotics|,"<p>I am new to opencv. I saw many tutorials on color detection. My camera is rotating and I want it to stop rotation when it detects, say, a blue color. How can I check whether any blue object has come into my frame? I want it to be in the centre of the frame. Basically I want a feedback, maybe a flag bit, when a blue color is detected. I am using opencv2 and python. Please help</p>

<pre><code>    import cv2
    import numpy as np

    cap = cv2.VideoCapture(0)

    while(1):

    # Take each frame
    _, frame = cap.read()

    # Convert BGR to HSV
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)

    # define range of blue color in HSV
    lower_blue = np.array([110,50,50])
    upper_blue = np.array([130,255,255])

    # Threshold the HSV image to get only blue colors
    mask = cv2.inRange(hsv, lower_green, upper_green)

    # Bitwise-AND mask and original image
    res = cv2.bitwise_and(frame,frame, mask= mask)
</code></pre>

<p>I did this much. Now what should I check?? if <code>res</code> is a zero matrix??  That is to get an acknowledgement if a blue object is detected.</p>

<p>I want something like this: <a href=""http://www.youtube.com/watch?v=0PXnzAAKro8"" rel=""nofollow"">http://www.youtube.com/watch?v=0PXnzAAKro8</a>. So, how should I check whether a blue object has been detected.</p>
",41647.73889,,7072,0,4,2,0,3174434,,41647.73264,19,,,,,,31565931,please check https://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_imgproc/py_colorspaces/py_colorspaces.html#converting-colorspaces,Incoming
333,2117,26847636,"Newbie python script, odometry",|python|robotics|ros|,"<p>I was doing one online course where on page was special frame to run python script. 
My task in this exercise was to compute the odometry, velocities are given. </p>

<p>This script on page looks: <a href=""http://snag.gy/NTJGz.jpg"" rel=""nofollow"">http://snag.gy/NTJGz.jpg</a></p>

<p>Now I would like to do the same using ROS: 
there is nearly the same exercise but in ROS:</p>

<p>clear code looks: <a href=""https://github.com/tum-vision/autonavx_ardrone/blob/master/ardrone_python/src/example1_odometry.py"" rel=""nofollow"">https://github.com/tum-vision/autonavx_ardrone/blob/master/ardrone_python/src/example1_odometry.py</a></p>

<p>There is information that I should add code from this online_course version to function callback, I try, but it doesn't work.</p>

<p>My code: </p>

<pre><code>#!/usr/bin/env python

#ROS
import rospy
import roslib; roslib.load_manifest('ardrone_python')
from ardrone_autonomy.msg import Navdata
import numpy as np

def __init__(self):
    self.position = np.array([[0], [0]])


def rotation_to_world(self, yaw):
        from math import cos, sin
        return np.array([[cos(yaw), -sin(yaw)], [sin(yaw), cos(yaw)]])


def callback(self, t, dt, navdata):
        self.position = self.position + dt * np.dot(self.rotation_to_world(navdata.rotZ), np.array([[navdata.vx], [navdata.vy]]))      
        print(""received odometry message: vx=%f vy=%f z=%f yaw=%f""%(navdata.vx,navdata.vy,navdata.altd,navdata.rotZ))
        print(self.position)


if __name__ == '__main__':
    rospy.init_node('example_node', anonymous=True)

    # subscribe to navdata (receive from quadrotor)
    rospy.Subscriber(""/ardrone/navdata"", Navdata, callback(self, t, dt, navdata))

    rospy.spin()
</code></pre>

<p>Please correct me, I am totally newbie to python. </p>

<p>Now i got message: </p>

<blockquote>
  <p>Traceback (most recent call last):   File ""./example1_odometry.py"",
  line 28, in 
      rospy.Subscriber(""/ardrone/navdata"", Navdata, callback(self, t, dt, navdata)) NameError: name 'self' is not defined</p>
</blockquote>
",41953.66111,,1136,2,4,-1,,2898281,,41566.69722,52,26847702,"<p>The direct error you posted:</p>

<pre><code>Traceback (most recent call last): File ""./example1_odometry.py"", line 28, in rospy.Subscriber(""/ardrone/navdata"", Navdata, callback(self, t, dt, navdata)) NameError: name 'self' is not defined
</code></pre>

<p>is because in the line:</p>

<pre><code> rospy.Subscriber(""/ardrone/navdata"", Navdata, callback(self, t, dt, navdata))
</code></pre>

<p><code>self</code> is undefined. You might mean <code>rospy</code> ?</p>
",8002,0,2,42258129,"@hagubear you mean `class MyClass:`, right?",Coordinates
334,1926,22228890,Neural Network for a Robot,|neural-network|robotics|,"<p>I need to implement a Robot Brain, I used feedforward neural network as a Controller. The robot has 24 sonar sonsor, and only one ouput which is R=Right, L=Left, F=Forward, B=Back. I also have a large dataset which contain sonar data and the desired output. The FNN is trained using backpropagation algorithm.</p>
<p>I used neuroph Studio to construct the FNN and to do the trainnig. Here the network params:</p>
<p>Input layer: 24
Hidden Layer: 10
Output Layer: 1
LearnningRate: 0.5
Momentum: 0.7
GlobalError: 0.1</p>
<p>My problem is that during iteration the error drop slightly and seems to be static. I tried to change the parameter but I'm not getting any useful result!!</p>
",41704.64653,22233582,169,1,0,0,,3345867,,41694.36806,19,22233582,"<p>Use 1 of n encoding for the output. Use 4 output neurons, and set up your target (output) data like this:</p>

<pre><code>1 0 0 0 = right
0 1 0 0 = left
0 0 1 0 = forward
0 0 0 1 = back
</code></pre>

<p>Reduce the number of input sensors (and corresponding input neurons) to begin with, down to 3 or 5. This will simplify things so you can understand what's going on. Later you can build back up to 24 inputs.</p>

<p>Neural networks often get stuck in local minima during training, that could be why your error is static. Increasing the momentum can help avoid this.  </p>

<p>Your learning rate looks quite high. Try 0.1, but play around with these values. Every problem is different and there are no values guaranteed to work.</p>
",3387972,0,3,,,Other
335,1900,21211025,Trouble programming MindstormsNXT with RobotC,|c|robotics|lego-mindstorms|nxt|,"<p>I am having trouble making my robot controlling my Mindstorms NXT robot with RobotC. I want my robot to be able to move forward on a table and when reaching the end, the ultrasonic sensor, which is facing down, will determine if it is on the edge by seeing how far the ground is. When the ultrasonic sensor finds that it is on the edge, the robot will move back from the edge, turn around, and go the other way.</p>

<p>Here is my code:</p>

<pre><code>#pragma config(Sensor, S1,     ,               sensorSONAR)
//*!!Code automatically generated by 'ROBOTC' configuration wizard               !!*//

task main()

{

int Ultrasonic; 
Ultrasonic = SensorValue[S1];

while (true)
{
    if (Ultrasonic &gt;10)
{
motor[motorB] = -100;
motor[motorC] = -100;
wait1Msec(2000);

motor[motorB] = 100;
wait1Msec(2000);
}
if (Ultrasonic &lt;= 10)
{
    motor[motorB] = 50;
    motor[motorC] = 50;
    wait1Msec(5000);
}
}
}
</code></pre>
",41657.96181,,219,1,0,0,,3211005,,41657.96042,10,22058479,"<p>The problem with this program is that you really only read from the ultrasonic sensor once. Here's what happens when you run your program:</p>

<ol>
<li>The variable <code>Ultrasonic</code> is created, and the sensor value is assigned to it.</li>
<li>The program checks the value of the ultrasonic sensor.</li>
<li>The program does something based on what the ultrasonic sensor is read.</li>
<li>The program does something based on what the ultrasonic sensor is read.</li>
</ol>

<p>...</p>

<p>To fix this problem, all you need to do is to move the reading of the ultrasonic sensor into the <code>while</code> loop, so that the NXT continually checks the value of the sensor. Here's what a modified version would look like:</p>

<pre><code>task main()
{
    int Ultrasonic; 

    while (true)
    {
        Ultrasonic = SensorValue[S1];
        if (Ultrasonic &gt; 10)
        {
            // ... Do stuff here...
        }
        if (Ultrasonic &lt;= 10)
        {
            // ... Do more stuff here...
        }
    }
}
</code></pre>

<p>In fact, you could make this code even ""cleaner"" by combining the check for the value of the ultrasonic sensor by using an ""if... else..."" statement. This checks one value, and then makes a decision based on whether that value is true--or else. Just replace the line <code>if (Ultrasonic &lt;= 10)</code> with <code>else</code>.</p>
",1292093,1,0,,,Incoming
336,2023,25181253,Using objects (that require a parameter) and their methods within another class,|c++|beagleboneblack|robotics|,"<p>I'm trying to program an omni directional robot with the BeagleBone Black. I've found the BlackLib library works realy well for controlling pins, but I'm having a little trouble integrating it into the larger scheme of things. If I simply create objects associated with pins and set them high or low (or somewhere in between) I am able to control the motor as desired. But when I try to create an object that contains BlackLib ojects, I run into difficulty. Seems to me, a good way to go about this would be to create a motor object that contains a GPIO and a PWM BlackLib object. Then I can create a function to easily set the power and direction with</p>

<pre><code>rightMotor(50); //Should set the power to 50% in one direction
</code></pre>

<p>I've got some of it working, but am having trouble accessing the functions within the objects from BlackLib that are within my own Motors class.</p>

<p>Here's the code I'm working with now.</p>

<pre><code>#include ""BlackLib/BlackLib.h""

struct Motors
{
  BlackPWM* pwm;
  BlackGPIO* gpio;

  void drive(int power)
  {
    if(power &gt;= 0)
    {
      gpio.setValue(low);
      //gpio.setValue(low); generates the error described below. I'm not familiar enough with the intricacies of pointers to know how to handle this
      //motors.cpp: In member function ‘void Motors::drive(int)’:
      //motors.cpp:15:10: error: request for member ‘setValue’ in ‘((Motors*)this)-&gt;Motors::gpio’, which is of non-class type ‘BlackGPIO*’
//      pwm.setDutyPercent(power);
    }
    else
    {
//      gpio.setValue(high);
//      pwm.setDutyPercent(100+power);
    }
  }
};

int main()
{
  struct Motors rightMotor;
  rightMotor.pwm = new BlackPWM(P9_16);
  rightMotor.gpio = new BlackGPIO(GPIO_49,output);

  //Give the BeagleBone a little time to create the objects
  usleep(10000);

  rightMotor.pwm.setPeriodTime(500000);
  //I will eventually need to set the period time but I'm not sure how. I'm guessing this is the incorrect syntax
  //Ideally, I would create a function so that I could simply write rightMotor.setPeriodTime(500000);

  rightMotor.drive(50);
  sleep(1);
  rightMotor.drive(0);
}
</code></pre>

<p>If I'm totally off base and there's a much better way to do this, please let me know. My end goal is to be able to easily control multiple motors. Eventually I would like to create functions and classes such as</p>

<pre><code>robot.turn(30);
</code></pre>

<p>or</p>

<pre><code>robot.forward(100);
</code></pre>
",41858.47431,,453,1,1,0,,3865173,,41842.61875,6,25213129,"<p>Got the following solution from a professor of computer science at the University of Alaska Fairbanks. Works as desired now.</p>

<pre><code>struct Motors
{
        BlackPWM pwm;
        BlackGPIO gpio;

        Motors(pwm_pin_name pwmPin,gpio_name gpioPin) //motor constructor parameters
                : pwm(pwmPin),gpio(gpioPin,output) //member initializer list
        {
                pwm.setPeriodTime(500000);
        }

        void drive(int power)
        {
                if(power &gt;= 0)
                {
                        gpio.setValue(low);
                        pwm.setDutyPercent(power);
                }
                else
                {
                        gpio.setValue(high);
                        pwm.setDutyPercent(100+power);
                }
        }
};

int main()
{
        //User interface pins
        BlackGPIO button(GPIO_44,input);

        Motors rightMotor(P9_16,GPIO_49);

        while(!button.isHigh())
        {
                rightMotor.drive(0);
        }

        //The BeagleBone needs a little time to create the PWM object
        usleep(10000);

        rightMotor.drive(-50);

        sleep(1);

        rightMotor.drive(0);
}
</code></pre>
",3865173,0,0,39208871,"What is your experience with C++? You're doing some basic mistakes here (like accessing a member function of a pointer via `.` instead of `->`, or bad encapsulation). I'd strongly recommend to get some introduction for C++/OOP before tackling such a project.",Actuator
337,2021,25002980,"Python, Webiopi and Raspberry Pi",|python|raspberry-pi|robotics|webiopi|,"<p>I am trying to control my Raspberry Pi car via wireless and webiopi.  The basic function works fine - have the interface where I click fwd and the car will go forward and when I release the button it will stop.</p>

<p>I now want to integrate the Ultrasonic distance sensor so that when I drive forward, the car should stop when something is in front of it.  I have it going where when I click to fwd button, the car will drive and stop when something is in range but it will only stop when something is in range and not when I release the button. My while loop is somehow looping (stuck) and not reading the release button function from webiopi.</p>

<p>Can somebody please help - been at it for days now and not sure where I am going wrong :-( </p>

<p>Here is the loop from my python script:</p>

<pre><code>def go_forward(arg):
  global motor_stop, motor_forward, motor_backward, get_range
  print ""Testing""
  print mousefwd()
  while (arg) == ""fwd"":
      print (arg)
      direction = (arg)
      dist = get_range()
      print ""Distance %.1f "" % get_range()
      if direction == ""fwd"" and get_range() &lt; 30:
          motor_stop()
          return
      else:
          motor_forward()
</code></pre>

<p>And here is the code from my webiopi function call:</p>

<pre><code> function go_forward() {
              var args = ""fwd""
              webiopi().callMacro(""go_forward"", args);
      }
 function stop1() {
              var args = ""stop""
              webiopi().callMacro(""go_forward"", args);
              webiopi().callMacro(""stop"");
    }
</code></pre>

<p>This is how I have it now but still not working (I am a total noob :-) ) :</p>

<pre><code> def go_forward(arg):
  global motor_stop, motor_forward, motor_backward, get_range
  print ""Testing""
  direction = arg
  while direction == ""fwd"":
      print arg
      dist = get_range()
      print ""Distance %.1f "" % get_range()
      if get_range() &lt; 30:
         motor_stop()
      elif direction == ""fwd"":
         motor_forward()
      else:
         motor_stop()
</code></pre>

<p>Maybe a slight step forward.  See that webipi uses its own 'loop' and I added the loop code to check what the status of the motor GPIO's are and the distance and if the motor is running and if the distance is too short then stop. Car moves now when I press the forward button and stops when I release it and when moving forward and distance is less then 30cm it will stop.  Only problem is that when the distance is too short and I press the forward button too quickly multiple times, I now get a ""GPIO.output(Echo,1)
_webiopi.GPIO.InvalidDirectionException: The GPIO channel is not an OUTPUT""  error :-(  .</p>

<p>The Code looks now like this:</p>

<pre><code> def go_forward(direction):
   motor_forward()

 def loop():
   if get_range() &lt; 30 and GPIO.digitalRead(M1) == GPIO.HIGH:
     stop()
     print ""stop""
   sleep(1)
</code></pre>
",41848.81736,25007454,2310,2,0,0,,3817890,"Windhoek, Namibia",41828.81181,20,25003178,"<p>Try this - you always stop on range but may want to stop on direction</p>

<pre><code>  if get_range() &lt; 30:
      motor_stop()
  elif direction == ""fwd"":
      motor_forward()
  else:
      motor_stop()
</code></pre>

<p>btw you dont need the (arg) only arg when you reference it</p>

<p>print arg </p>

<p>the parens in the call signature are there to show its a parm to a function</p>
",3704291,1,2,,,Incoming
338,1932,22547738,RobotC VEX / Lego Programming: How to make a robot run multiple reactions in parallel?,|java|robotics|lego-mindstorms|nxt|,"<p>I need to get my robot to be able to use the bump switches so that either one can be pressed and the motor corresponding to that bumpswitch will run for as long as the bump switch is pressed. The problem I'm having is getting the LEDs to light up correctly. While the bump switch code block is running, I need the LEDs to light up and go off seven times for one second every time the Light sensor value gets higher than 400. How do I do it? Please Help!
My Code is posted below:</p>

<pre><code>#pragma config(Sensor, in2,    lightSensor,    sensorReflection)
#pragma config(Sensor, dgtl3,  bumpSwitch,     sensorTouch)
#pragma config(Sensor, dgtl4,  bumpSwitch2,    sensorTouch)
#pragma config(Sensor, dgtl10, ledGreen,       sensorLEDtoVCC)
#pragma config(Sensor, dgtl11, ledRed,         sensorLEDtoVCC)
#pragma config(Motor,  port1,           leftMotor,     tmotorVex269, openLoop)
#pragma config(Motor,  port10,          rightMotor,    tmotorVex269, openLoop)

task main() {

while(true) {

    if (SensorValue(lightSensor) &gt; 400) {
        int count = 0;
        while (count &lt; 7) {
            turnLEDOn(ledGreen);
            turnLEDOn(ledRed);
            wait(1);
            turnLEDOff(ledGreen);
            turnLEDOff(ledRed);
            count++;
        }
    }

    while (SensorValue(bumpSwitch) == 0 &amp;&amp; SensorValue(bumpSwitch2) == 0) {
        stopMotor(rightMotor);
        stopMotor(leftMotor);
    }

    while (SensorValue(bumpSwitch2) == 1) {
        startMotor(rightMotor, 55);
        startMotor(leftMotor, 55);
    }

    while (SensorValue(bumpSwitch) == 1){
        startMotor(rightMotor, -55);
        startMotor(leftMotor, -55);
    }
}
}
</code></pre>
",41719.00764,,2707,2,0,0,,3376956,,41701.98194,11,22549198,"<p>Experience with robot programming (although I haven't used Robot C) shows that to make the robot run two or more sensor reactions in parallel, you have to structure the program very differently than for just one reaction. The Robot C tutorial does not cover this or even introduce the language and library features needed to do it.</p>

<p>In this case, you want the light sensor to trigger an LED on/off sequence that lasts for 1 second, while the bump sensors trigger motor actions that last until those sensors are no longer bumping.</p>

<p>The problem here is your motor control's inner <code>while</code> loops are blocking up the thread, mostly keeping it from getting around to running the LED control loop. The LED control loop likewise blocks up the thread for 7 seconds, delaying it from running the motor control loop.</p>

<p>In general, each iteration of a robot control loop should read sensors, send output commands, remember any state that it needs to process later, then loop again so all parts of the control loop get to run each cycle (as often as possible or at a controlled rate). Don't block the control flow by staying in an inner <code>while</code> loop until a sensor changes.</p>

<p>The first step is to unblock the motor control loop:</p>

<pre><code>while (true) {

  // Light sensor LED control code ...

  // Bumper/motor control
  if (SensorValue(bumpSwitch) == 1) {
    startMotor(rightMotor, -55);
    startMotor(leftMotor, -55);
  } else if (SensorValue(bumpSwitch2) == 1){
    startMotor(rightMotor, 55);
    startMotor(leftMotor, 55);
  } else {
    stopMotor(rightMotor);
    stopMotor(leftMotor);
  }
}
</code></pre>

<p>Now your current LED control code still calls <code>wait(1)</code> 7 times. According to <a href=""http://www.robotc.net/support/nxt/ROBOTC-for-Beginners/"" rel=""nofollow"">http://www.robotc.net/support/nxt/ROBOTC-for-Beginners/</a> <code>wait(1)</code> waits for 1.0 seconds, so this part of the code currently takes 7 seconds to run. That's a long time between checking the bump switches.</p>

<p>Furthermore, your code has no significant delay between turning the LEDs off and back on again, so you won't actually notice the LEDs turn off until the end of that sequence.</p>

<p>So the second step is to fix the LED control code from blocking the control loop. Basically there are two approaches (I don't know if Robot C supports the first choice, but it's simpler):</p>

<ol>
<li>When the light sensor is above-threshold <em>and</em> it was below-threshold in the previous iteration, i.e. it just transitioned, then start [or ""fork""] a thread (or task) to run a LED on/off loop. That thread should loop 7 times: {turn the LEDs on, <code>wait(1.0/14.0)</code> seconds, turn the LEDs off, then <code>wait(1.0/14.0)</code> seconds}. That way, 7 cycles around the loop will take 7 * (1/14 + 1/14) = 1.0 seconds. I wrote that as <code>1.0/14.0</code> rather than <code>1/14</code> because many programming languages compute <code>1/14</code> in integer math, yielding <code>0</code>, while <code>1.0/14.0</code> uses floating point math. I don't know about Robot C. Or, you could use <code>wait1Msec(71)</code> to wait 71 msec, which is about 1/14 second.</li>
<li>Use variables to track the sequencing through the flash-LED loop. Each cycle around the main loop, use <code>if</code> statements to check those variables, do the next step in the flash-LED cycle if it's time, and update those variables. The steps are: turn on the LEDs when it was in the initial state and the light sensor is above-threshold, turn off the LEDs when it's 1/14 of a second later, turn on the LEDs when it's 1/14 of a second later, ... turn off and set the tracking variables back to the initial state after 14 of those steps. The simplest tracking variable would be a counter from 0 (initial state) to 14 (doing the last LED ""OFF"" phase) and the value of the system clock from the last change, to let you detect when the clock is 71 msec later than that. Once it finishes step 14, go back to 0. There are alternatives for the tracking variables, such as 3 separate variables for whether it's currently flashing, which of the 7 flash cycles it's currently doing, whether it's in the ON or OFF phase of that cycle, and the clock reading.</li>
</ol>

<p>Approach #2 is trickier because doing two things at once is tricky. (People who think they can multitask by texting while driving are dangerously wrong.)</p>

<p>A much simpler approach works if the robot doesn't need to run the bumper/motor control loop while flashing the LEDs, that is, if it doesn't mind being unresponsive to the bumpers while flashing the LEDs for 1 second. If those bumper sensors are keeping the robot from running off the end of the table, being unresponsive for 1 second is bad news. But if it's OK for the robot to press against the bumper for 1 second while flashing the lights, then the light sensor LED part of the code could go something like this:</p>

<pre><code>while (true) {
  // Light sensor LED control code
  if (SensorValue(lightSensor) &gt; 400) {
    for (int i = 0; i &lt; 7; ++i) {
      turnLEDOn(ledGreen);
      turnLEDOn(ledRed);
      wait1Msec(71); // 1/14 second
      turnLEDOff(ledGreen);
      turnLEDOff(ledRed);
      wait1Msec(71);
    }
  }

  // Bumper/motor control code ...
}
</code></pre>

<p>This makes another simplifying assumption: That if the light sensor is still bright after finishing the 1 second LED flashing loop, it's OK to do another 1 second flashing loop next time around the main control loop, which occurs very soon. So as long as the light sensor is bright, the LEDs will keep flashing and the motors will only check the bumpers once per second. To fix that, use a variable to figure out when the light sensor goes from dark to bright.</p>
",1682419,0,2,,,Timing
339,2178,28865322,Raspberry Pi quadcopter thrashes at high speeds,|java|raspberry-pi|robotics|control-theory|pid-controller|,"<p>I am attempting to build a raspberry pi based quadcopter. So far I have succeeded in interfacing with all the hardware, and I have written a PID controller that is fairly stable at low throttle. The problem is that at higher throttle the quadcopter starts thrashing and jerking. I have not even been able to get it off the ground yet, all my testing has been done on a test bench. Is this a problem with my code, or perhaps a bad motor? any suggestions are greatly appreciated.</p>

<p>here is my code so far:</p>

<p>QuadServer.java:</p>

<pre><code>package com.zachary.quadserver;

import java.net.*;
import java.io.*;
import java.util.*;

import se.hirt.pi.adafruit.pwm.PWMDevice;
import se.hirt.pi.adafruit.pwm.PWMDevice.PWMChannel;

public class QuadServer {
    private static Sensor sensor = new Sensor();

    private final static int FREQUENCY = 490;

    private static double PX = 0;
    private static double PY = 0;

    private static double IX = 0;
    private static double IY = 0;

    private static double DX = 0;
    private static double DY = 0;

    private static double kP = 1.3;
    private static double kI = 2;
    private static double kD = 0;

    private static long time = System.currentTimeMillis();

    private static double last_errorX = 0;
    private static double last_errorY = 0;

    private static double outputX;
    private static double outputY;

    private static int val[] = new int[4];

    private static int throttle;

    static double setpointX = 0;
    static double setpointY = 0;

    static long receivedTime = System.currentTimeMillis();

    public static void main(String[] args) throws IOException, NullPointerException {

        PWMDevice device = new PWMDevice();
        device.setPWMFreqency(FREQUENCY);

        PWMChannel BR = device.getChannel(12);
        PWMChannel TR = device.getChannel(13);
        PWMChannel TL = device.getChannel(14);
        PWMChannel BL = device.getChannel(15);

        DatagramSocket serverSocket = new DatagramSocket(8080);


        Thread read = new Thread(){
                public void run(){
                    while(true) {
                    try {
                            byte receiveData[] = new byte[1024];
                            DatagramPacket receivePacket = new DatagramPacket(receiveData, receiveData.length);
                            serverSocket.receive(receivePacket);
                            String message = new String(receivePacket.getData());
                            throttle = (int)(Integer.parseInt((message.split(""\\s+"")[4]))*12.96)+733;
                            setpointX = Integer.parseInt((message.split(""\\s+"")[3]))-50;
                            setpointY = Integer.parseInt((message.split(""\\s+"")[3]))-50;

                        receivedTime = System.currentTimeMillis();

                        } catch (IOException e) {
                            e.printStackTrace();
                        }
                    }
                }
        };
        read.start();

        while(true)
        {
            Arrays.fill(val, calculatePulseWidth((double)throttle/1000, FREQUENCY));

            double errorX = -sensor.readGyro(0)-setpointX;
            double errorY = sensor.readGyro(1)-setpointY;

            double dt = (double)(System.currentTimeMillis()-time)/1000;

            double accelX = sensor.readAccel(0);
            double accelY = sensor.readAccel(1);
            double accelZ = sensor.readAccel(2);

            double hypotX = Math.sqrt(Math.pow(accelX, 2)+Math.pow(accelZ, 2));
            double hypotY = Math.sqrt(Math.pow(accelY, 2)+Math.pow(accelZ, 2));


            double accelAngleX = Math.toDegrees(Math.asin(accelY/hypotY));
            double accelAngleY = Math.toDegrees(Math.asin(accelX/hypotX));

            if(dt &gt; 0.01)
            {

                PX = errorX;
                PY = errorY;

                IX += errorX*dt;
                IY += errorY*dt;

                IX = 0.95*IX+0.05*accelAngleX;
                IY = 0.95*IY+0.05*accelAngleY;

                DX = (errorX - last_errorX)/dt;
                DY = (errorY - last_errorY)/dt;

                outputX = kP*PX+kI*IX+kD*DX;
                outputY = kP*PY+kI*IY+kD*DY;
                time = System.currentTimeMillis();
            }

            System.out.println(setpointX);

            add(-outputX+outputY, 0);
            add(-outputX-outputY, 1);
            add(outputX-outputY, 2);
            add(outputX+outputY, 3);

            //System.out.println(val[0]+"", ""+val[1]+"", ""+val[2]+"", ""+val[3]);
            if(System.currentTimeMillis()-receivedTime &lt; 1000)
            {
                BR.setPWM(0, val[0]);
                TR.setPWM(0, val[1]);
                TL.setPWM(0, val[2]);
                BL.setPWM(0, val[3]);
            } else 
            {
                BR.setPWM(0, 1471);
                TR.setPWM(0, 1471);
                TL.setPWM(0, 1471);
                BL.setPWM(0, 1471);
            }

        }
    }

    private static void add(double value, int i)
    {
        value = calculatePulseWidth(value/1000, FREQUENCY);
        if(val[i]+value &gt; 1471 &amp;&amp; val[i]+value &lt; 4071)
        {
            val[i] += value;
        }else if(val[i]+value &lt; 1471)
        {
            //System.out.println(""low"");
            val[i] = 1471;
        }else if(val[i]+value &gt; 4071)
        {
            //System.out.println(""low"");
            val[i] = 4071;
        }
    }

    private static int calculatePulseWidth(double millis, int frequency) {
        return (int) (Math.round(4096 * millis * frequency/1000));
    }
}
</code></pre>

<p>Sensor.java:</p>

<pre><code>package com.zachary.quadserver;

import com.pi4j.io.gpio.GpioController;
import com.pi4j.io.gpio.GpioFactory;
import com.pi4j.io.gpio.GpioPinDigitalOutput;
import com.pi4j.io.gpio.PinState;
import com.pi4j.io.gpio.RaspiPin;
import com.pi4j.io.i2c.*;
import com.pi4j.io.gpio.GpioController;
import com.pi4j.io.gpio.GpioFactory;
import com.pi4j.io.gpio.GpioPinDigitalOutput;
import com.pi4j.io.gpio.PinState;
import com.pi4j.io.gpio.RaspiPin;
import com.pi4j.io.i2c.*;

import java.net.*;
import java.io.*;

public class Sensor {
    static I2CDevice sensor;
    static I2CBus bus;
    static byte[] accelData, gyroData;
    static long accelCalib[] = new long[3];
    static long gyroCalib[] = new long[3];

    static double gyroX = 0;
    static double gyroY = 0;
    static double gyroZ = 0;

    static double accelX;
    static double accelY;
    static double accelZ;

    static double angleX;
    static double angleY;
    static double angleZ;

    public Sensor() {
        //System.out.println(""Hello, Raspberry Pi!"");
        try {
            bus = I2CFactory.getInstance(I2CBus.BUS_1);

            sensor = bus.getDevice(0x68);

            sensor.write(0x6B, (byte) 0x0);
            sensor.write(0x6C, (byte) 0x0);
            System.out.println(""Calibrating..."");

            calibrate();

            Thread sensors = new Thread(){
                    public void run(){
                        try {
                            readSensors();
                        } catch (IOException e) {
                        System.out.println(e.getMessage());
                    }
                    }
            };
            sensors.start();
        } catch (IOException e) {
            System.out.println(e.getMessage());
        }
    }

    private static void readSensors() throws IOException {
        long time = System.currentTimeMillis();
        long sendTime = System.currentTimeMillis();

        while (true) {
            accelData = new byte[6];
            gyroData = new byte[6];
            int r = sensor.read(0x3B, accelData, 0, 6);
            accelX = (((accelData[0] &lt;&lt; 8)+accelData[1]-accelCalib[0])/16384.0)*9.8;
            accelY = (((accelData[2] &lt;&lt; 8)+accelData[3]-accelCalib[1])/16384.0)*9.8;
            accelZ = ((((accelData[4] &lt;&lt; 8)+accelData[5]-accelCalib[2])/16384.0)*9.8)+9.8;
            accelZ = 9.8-Math.abs(accelZ-9.8);

            double hypotX = Math.sqrt(Math.pow(accelX, 2)+Math.pow(accelZ, 2));
            double hypotY = Math.sqrt(Math.pow(accelY, 2)+Math.pow(accelZ, 2));


            double accelAngleX = Math.toDegrees(Math.asin(accelY/hypotY));
            double accelAngleY = Math.toDegrees(Math.asin(accelX/hypotX));

            //System.out.println((int)gyroX+"", ""+(int)gyroY);

            //System.out.println(""accelX: "" + accelX+"" accelY: "" + accelY+"" accelZ: "" + accelZ);

            r = sensor.read(0x43, gyroData, 0, 6);
            if(System.currentTimeMillis()-time &gt;= 5)
            {
                gyroX = (((gyroData[0] &lt;&lt; 8)+gyroData[1]-gyroCalib[0])/131.0);
                gyroY = (((gyroData[2] &lt;&lt; 8)+gyroData[3]-gyroCalib[1])/131.0);
                gyroZ = (((gyroData[4] &lt;&lt; 8)+gyroData[5]-gyroCalib[2])/131.0);

                angleX += gyroX*(System.currentTimeMillis()-time)/1000;
                angleY += gyroY*(System.currentTimeMillis()-time)/1000;
                angleZ += gyroZ;

                angleX = 0.95*angleX + 0.05*accelAngleX;
                angleY = 0.95*angleY + 0.05*accelAngleY;

                time = System.currentTimeMillis();
            }
            //System.out.println((int)angleX+"", ""+(int)angleY);
            //System.out.println((int)accelAngleX+"", ""+(int)accelAngleY);
        }
    }

    public static void calibrate() throws IOException {
        int i;
        for(i = 0; i &lt; 3000; i++)
        {
            accelData = new byte[6];
            gyroData = new byte[6];
            int r = sensor.read(0x3B, accelData, 0, 6);
            accelCalib[0] += (accelData[0] &lt;&lt; 8)+accelData[1];
            accelCalib[1] += (accelData[2] &lt;&lt; 8)+accelData[3];
            accelCalib[2] += (accelData[4] &lt;&lt; 8)+accelData[5];

            r = sensor.read(0x43, gyroData, 0, 6);
            gyroCalib[0] += (gyroData[0] &lt;&lt; 8)+gyroData[1];
            gyroCalib[1] += (gyroData[2] &lt;&lt; 8)+gyroData[3];
            gyroCalib[2] += (gyroData[4] &lt;&lt; 8)+gyroData[5];
            try {
                Thread.sleep(1);
            } catch (Exception e){
                e.printStackTrace();
            }
        }
        gyroCalib[0] /= i;
        gyroCalib[1] /= i;
        gyroCalib[2] /= i;

        accelCalib[0] /= i;
        accelCalib[1] /= i;
        accelCalib[2] /= i;
        System.out.println(gyroCalib[0]+"", ""+gyroCalib[1]+"", ""+gyroCalib[2]);
    }

    public double readAngle(int axis)
    {
        switch (axis)
        {
            case 0:
                return angleX;
            case 1:
                return angleY;
            case 2:
                return angleZ;
        }

        return 0;
    }

    public double readGyro(int axis)
    {
        switch (axis)
        {
            case 0:
                return gyroX;
            case 1:
                return gyroY;
            case 2:
                return gyroZ;
        }

        return 0;
    }

    public double readAccel(int axis)
    {
        switch (axis)
        {
            case 0:
                return accelX;
            case 1:
                return accelY;
            case 2:
                return accelZ;
        }

        return 0;
    }
}
</code></pre>
",42067.89375,,412,1,3,0,,4634134,,42067.88681,24,40819766,"<p>You can try to setup different values for the gains of your controller based on different operating conditions. Then you should only be able to identify each condition of operation, change the gain of your PID accordingly, and validate the design. In your case, you could try to schedule the gain of the PID with throttle or other associated variable read from the available sensors.</p>

<p>Search for Gain Scheduling implementations:</p>

<p><a href=""https://www.mathworks.com/help/control/ug/gain-scheduled-control-systems.html"" rel=""nofollow noreferrer"">https://www.mathworks.com/help/control/ug/gain-scheduled-control-systems.html</a></p>

<p>This is a very useful technique which applies linear control design to non-linear systems with very satisfactory results.</p>
",7207220,1,0,45995723,Impossible to tell whether the problem is with the code or the motor unless you post the code and/or the motor.,Actuator
340,2208,29226752,Differential drive robots: converting wheel speeds to lin/ang velocities,|box2d|robotics|kinematics|,"<p>I am modeling a simple differential drive robot (such as the e-Puck, Khepera, etc.) with pyBox2D. This class of robots is usually controlled by indicating speeds for the right and left wheel in rad/sec.<br>
However, Box2D can only control a (kinematic) body through two parameters: linear velocity (in meters/sec, as a 2D vector) and angular velocity (in rad/sec). I need to convert my wheel speeds to linear + angular velocities.</p>

<p>Linear velocity is actually straightforward. Given a wheel radius r in meters, and current robot orientation theta in radians, the forward speed is simply the average of the two wheel speeds in meters/sec and reduced to a vector according to current orientation:</p>

<p>(1) forwardSpeed = ((rightSpeed * r) + (leftSpeed * r)) / 2</p>

<p>(2) linearVelocity = (forwardSpeed * cos(theta), forwardSpeed * sin(theta))  </p>

<p>I cannot quite figure out the correct formula for angular velocity, though. Intuitively, it should be the difference between the two speeds modulo the distance between the wheels:</p>

<p>(3) angularVelocity = (rightSpeed - leftSpeed) / wheelSeparation  </p>

<p>in the limit cases: when right = left, the robot spins in place, and when either rightSpeed = 0 or leftSpeed = 0, the robot spins (pivots) around the stationary wheel, i.e. in a circle with radius = to the separation between the wheels.</p>

<p>I do not get the expected behavior with formula (3), though. As a test, I set the left wheel speed to 0 and progressively increased the value of the right wheel 's speed. The expected behavior is that the robot's should spin around the left wheel with increased velocity.
Instead, the robot spins in circles of increasing radius, i.e it spirals outward, which suggests that the angular velocity is insufficient. </p>

<p>Notice that I am using a Box2D kinematic body for the robot, so friction does not play a role in my results. </p>
",42087.29514,,1772,1,0,0,,1212775,,40955.05069,108,29266214,"<p>Keep in mind that angular velocities in Box2D work around the origin (0,0) in local body coordinates, which makes it very awkward to accurately reproduce this type of movement. Presumably the origin of the robot is in the center of the body, in which case you can never get it to rotate around one wheel (that would need both a rotate and translate). It might actually be easier to model the robot as a dynamic body and apply forces where the wheels are. If you don't want to deal with friction losses etc, you could use a kinematic body for each wheel and connect them to the main (dynamic) body with revolute joints.</p>
",624593,0,3,,,Actuator
341,2364,32465462,"Python script in Abaqus raises ""TypeError: keyword error on mergeWire""",|python|robotics|abaqus|,"<p>I'm new at Python scripting on Abaqus. I want to do fiber reinforced actuators which is described <a href=""http://softroboticstoolkit.com/book/fr-abaqus-step-1b"" rel=""nofollow noreferrer"">here</a> but I couldn't run the script properly. It gives me:</p>

<blockquote>
  <p>TypeError: keyword error on mergeWire</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/i9DAA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i9DAA.png"" alt=""Screenshot of error message""></a></p>

<p>How can I fix this?  I'm using Abaqus Student Edition which is free.</p>
",42255.79861,,3911,0,5,0,0,5314278,Turkey,42255.79167,12,,,,,,88002637,"I believe there are two issues with that code: 1) abaqusConstants (which includes ON,OFF,..) is not imported; 2) WireSpline only has ""points"", ""mergeType"" and ""smoothClosedspline"" as arguments, so I'd investigate which type of merge you are after by looking at the documentation section 38.2.55 WireSpline()",Error
342,2305,32013704,How to send data to cloud from arduino uno?,|arduino|cloud|sensors|robotics|arduino-uno|,"<p>I have some sensors connected to my ardunio uno and get periodically data from that sensors now I wanted to send that data to cloud.
I dont have any idea about how should I connect my arduino to Internet using GSM. How should I solve this problem or any alternative is there.</p>
",42230.64861,,3503,1,0,0,,4834394,,42120.52569,6,32013799,"<p>Just get an appropriate shield (like <a href=""https://www.arduino.cc/en/Main/ArduinoGSMShield"" rel=""nofollow"">this one</a>) and follow the <a href=""https://www.arduino.cc/en/Guide/ArduinoGSMShield"" rel=""nofollow"">documents</a> included with it.  The linked shield includes some basic service plan as part of the purchase.  Once connected, it's just like using any other TCP/IP application.</p>
",1168588,0,1,,,Connections
343,2210,29323850,Send data to Arduino on keypress Raspberry Pi,|python|serial-port|arduino|raspberry-pi|robotics|,"<p>I'm trying to build a car (Wild Thumper) that i can drive from my Raspberry Pi. Currently i'm using my Raspberry Pi over SSH. It should send to data to my Arduino so it knows when it has to go forward or when to turn.</p>

<p>I've tried making scripts that get called by jQuery (apache on Pi) and send an integer over the serial port but it requires a delay and this is not ideal. (example forwardStart.py:)</p>

<pre><code>import serial
ser = serial.Serial('/dev/ttyACM0', 9600)
ser.open()
# here a delay is needed
ser.write('4') # go forward
ser.close()
</code></pre>

<p>To solve this i tried looking for a single python script that read my keyboard and send the correct integer. However, all keylisteners require display and can't be used over SSH.</p>

<p>Can anybody help me with the Python script or another idea that would works?</p>

<p>Thanks!</p>
",42091.97917,,1081,1,4,0,,4725211,,42091.97083,33,29376428,"<p>You should start reading from <a href=""https://stackoverflow.com/questions/983354/how-do-i-make-python-to-wait-for-a-pressed-key"">here</a>. The idea would be something like</p>

<pre><code>import serial
ser = serial.Serial('/dev/ttyACM0', 9600)
ser.open()
# here a delay is needed

try:
    while 1:
        try:
            key = sys.stdin.read(1) # wait user input
            actionKey = key2action(key) # translate key to action
            ser.write(actionKey) # go forward
        except IOError: pass
finally:
    ser.close()
</code></pre>

<p><strong>Note:</strong> this code will fail, it's more like pseudo-code to illustrate the idea.</p>
",3443596,0,0,46894575,"The delay shouldn't be a problem. You should open the serial port only once at startup to avoid this delay every time you want to send a command. Also, Arduino resets every time you establish a serial connection (and maybe that's why you need that delay)",Remote
344,2290,31605893,How do I create an Android app that allows me to control a robot fitted with raspberry pi?,|android|python|web-services|raspberry-pi|robotics|,"<p>I would like to create an Android App that allows me to remotely control a robot fitted with raspberry pi using Wi-Fi. I have searched various threads and one of the ways is to make my rspi a web server. I've come across this code that I should use on my Android app to make http request.</p>

<pre><code>    class RequestTask extends AsyncTask&lt;String, String, String&gt; {
@Override
protected String doInBackground(String... uri) {
    HttpClient httpclient = new DefaultHttpClient();
    HttpResponse response;
    String responseString = null;
    try {
        response = httpclient.execute(new HttpGet(uri[0]));
        StatusLine statusLine = response.getStatusLine();
        if(statusLine.getStatusCode() == HttpStatus.SC_OK){
            ByteArrayOutputStream out = new ByteArrayOutputStream();
            response.getEntity().writeTo(out);
            out.close();
            responseString = out.toString();
        } else{
            //Closes the connection.
            response.getEntity().getContent().close();
            throw new IOException(statusLine.getReasonPhrase());
        }
    } catch (ClientProtocolException e) {
        //TODO Handle problems..
    } catch (IOException e) {
        //TODO Handle problems..
    }
    return responseString;
}

@Override
protected void onPostExecute(String result) {
    super.onPostExecute(result);

    Toast.makeText(getApplicationContext(), result, 0).show();
}
}
</code></pre>

<p>To make the connection,I have to write this code:</p>

<pre><code>new RequestTask().execute(""http://192.168.1.145:80/3"");
</code></pre>

<p>However, my question is:
After connecting to my Raspi,how do I send python code from the android app to be executed on my Raspi?</p>

<p>Thanks.</p>
",42209.36111,,592,0,4,1,0,5151362,,42209.33958,132,,,,,,51162564,"On a side note HttpClient is deprecated, if you intend to use this over a WIFI [this](http://elinux.org/RPI-Wireless-Hotspot) may interest you. Check [this](https://www.raspberrypi.org/forums/viewtopic.php?f=63&t=31596) and [this](http://dishingtech.blogspot.jp/2012/01/realtek-wi-fi-direct-programming-guide.html)",Remote
345,2168,28526200,How to program LEGO Mindstorms EV3 using C language?,|c|sensors|robotics|lego-mindstorms|lego-mindstorms-ev3|,"<p>First of all, I'm new for this and I need a little help!</p>

<p>I have a LEGO Mindstorms EV3 robot, I downloaded (LEGO Mindstorms EV3 Home Edition) to control the EV3. Unfortunately, I couldn't find the source code for the EV3 in the mentioned software. So, please if anybody could tell me the name of the software that enables you to program EV3! I would be most appreciated!</p>

<p>I also downloaded (Bricxcc) software but it was an old version. I couldn't find a newer version which contains EV3. </p>

<p>Can I use C language to program EV3 ? Or to add some features to the sensors?</p>

<p>Note: I ended with leJOS software to program the code with java it is much easier and there are a lot of resources for the EV3 brick in java. Wish you all the best!</p>
",42050.53403,,28761,2,1,12,0,,,,,28529537,"<p>You can find the EV3 source code here: <a href=""https://github.com/mindboards/ev3sources"">https://github.com/mindboards/ev3sources</a></p>

<p>The generated documentation from this source code is available <a href=""http://ev3.fantastic.computer/doxygen/index.html"">here</a> and <a href=""http://ev3.fantastic.computer/doxygen-all/"">here</a>.</p>

<p>Bricxcc has some experimental support for EV3 but it is not being actively developed (since Oct. 2013). You can find the latest test version <a href=""http://bricxcc.sourceforge.net/test_releases/"">here</a>. Searching the web for ""bricxcc ev3"" will come up with some tutorials (for example, the one at <a href=""http://www.robotnav.com"">http://www.robotnav.com</a> looks good).</p>

<p><a href=""http://www.robotc.net/"">ROBOTC</a> is a good alternative, although it is not free.</p>

<p>There is also <a href=""http://www.ev3dev.org"">ev3dev</a>. There is a C library for ev3dev <a href=""https://github.com/in4lio/ev3dev-c"">here</a> or you can write your own.</p>
",1976323,13,2,45370020,"Just a tip: 'robots.txt' is not the tag you want. It's not about robots, it's about web.",Specifications
346,2380,32978569,Foward/Inverse Kinematics Calculations 2-DOF python,|python|math|robotics|kinematics|inverse-kinematics|,"<p>The program calculates the points of an end-effector with forward kinematics using the equation, </p>

<p><code>x = d1cos(a1) + d2cos(a1+a2)</code> </p>

<p><code>y = d1sin(a1) + d2sin(a1+a2)</code></p>

<p>where <code>d1</code> is the length of the first joint, <code>d2</code> is the length of the second joint, <code>a1</code> is the angle of the first joint and <code>a2</code> is the angle of the second joint.</p>

<p>It calculates inverse kinematics by this equation</p>

<p><a href=""https://i.stack.imgur.com/e5cCm.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e5cCm.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/Oi6R4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Oi6R4.jpg"" alt=""enter image description here""></a></p>

<p>So, by entering the input required for forward kinematics I should get the points of the end effector. By entering in the same input and the points found in forward kinematics for inverse kinematics, I should get the angles I entered in as input for forward kinematics. But I do not get them back. 
Here is my code, </p>

<pre><code>'''
Created on Oct 5, 2015

@author: justin
'''
import math
def getOption():
    print('Select an option:\n')
    print('\t1) Forward Kinematics\n')
    print('\t2) Inverse Kinematics\n')
    option = input()
    try:
        option = int(option)
        if option == 1:
            fowardKinematics()
        elif option == 2:
            inverseKinematics()
        else:
            print('Not an option')
            return
    except ValueError:
        print('Not an integer/Point cannot be reached')
        return
def fowardKinematics():
    '''
    Ask user for input and computing points of end-effector 
    '''
    length1 = input('Enter length of joint 1 (a1):\n')  # Getting input from keyboard
    angle1 = input('Enter angle of joint 1 (theta1):\n')
    length2 = input('Enter length of joint 2 (a2):\n')
    angle2 = input(""Enter angle of join 2 (theta2)\n"")

    try:
        length1 = float(length1)  # Testing to see if user entered a number
        length2 = float(length2)  # Testing to see if user entered a number
        angle1 = float(angle1)  # Testing to see if user entered a number
        angle2 = float(angle2)  # Testing to see if user entered a number
    except ValueError:
        print('Invalid input, check your input again')
        return
    x = (length1 * math.cos(math.radians(angle1))) + (length2 * math.cos((math.radians(angle1 + angle2))))  # a1c1 + a2c12
    y = (length1 * math.sin(math.radians(angle1))) + (length2 * math.sin((math.radians(angle1 + angle2))))  # a1s1 + a2s12
    print('The position of the end-effector P(x,y) is:\n')
    print('X: ' + str(x))  # Convert x to string
    print('Y: ' + str(y))  # Convert y to string
def inverseKinematics():
    length1 = input('Enter length of joint 1 (a1):\n')
    length2 = input('Enter length of joint 2 (a2):\n')
    x = input('Enter position of X:\n')
    y = input('Enter position of Y:\n')
    try:
        length1 = float(length1)
        length2 = float(length2)
        x = float(x)
        y = float(y)
    except ValueError:
        print('Invalid input, check your input again')
        return
    # Computing angle 2 Elbow up/down
    numerator = ((length1 + length2)**2) - ((x**2) + (y**2))
    denominator = ((x**2) + (y**2)) - ((length1 - length2)**2)
    angle2UP = math.degrees(math.atan(math.sqrt(numerator/denominator)))
    angle2DOWN = angle2UP * -1

    # Angle 1 Elbow up
    numerator = (length2 * math.sin(math.radians(angle2UP)))
    denominator = ((length1 + length2) * math.cos(math.radians(angle2UP)))
    angle1UP = math.degrees(math.atan2(numerator, denominator))
    # Angle 1 Elbow down
    numerator = (length2 * math.sin(math.radians(angle2DOWN)))
    denominator = ((length1 + length2) * math.cos(math.radians(angle2DOWN)))
    angle1DOWN = math.degrees(math.atan2(numerator, denominator))
    print(""Angle 1 Elbow up: "" + str(angle1UP))
    print(""Angle 1 Elbow down: "" + str(angle1DOWN))
    print(""Angle 2 Elbow up: "" + str(angle2UP))
    print(""Angle 2 Elbow down: "" + str(angle2DOWN))
if __name__ == '__main__':
    getOption()
    pass
</code></pre>

<p>I think the problem is when the trig functions get introduced. The parameters for them are supposed to be in radians, they return the answer is degrees. Somewhere I am mixing up the two. I just don't know where. Thanks</p>
",42283.83056,,1885,1,0,1,,,,,,32995487,"<p>There is, I'm afraid, quite a bit wrong with this, either in your code or in the equations you're using. Your equation for theta2 doesn't make any sense to me if x and y are distances and a1 and a2 are angles (check your equation or give a source). Even if these should be d1 and d2, this equation involves subtracting two quantities with different dimensions (length^4 and length^2).</p>

<p>Then check your implementation of it, which does not evaluate the equation as given.</p>

<p>My advice about radians / degrees is to use radians throughout: accept the angles in degrees if you want, but then immediately convert to radians for the calculations, and convert angular results back to degrees for output.</p>

<p>Some more advice:</p>

<ul>
<li><p>you don't need to cast your floats to strings for output using print, just use <code>print('x: ', x)</code> and so on.</p></li>
<li><p>Give your variables the same names as the symbols they represent in your formula. This would make it easier to debug (well, it would if the  equations were correct).</p></li>
</ul>

<p>Hope that helps.</p>
",293594,1,0,,,Actuator
347,2312,32057612,How to connect usb webcam to ev3?,|image-processing|labview|robotics|lego-mindstorms|,"<p>I would like to connect an USB webcam to my EV3.
is it possible to do that?</p>

<p>and how can I do it and how can I reach the cam from my code?</p>

<p>I use Labview for programming</p>

<p>and thanks in advance :)</p>
",42233.78542,32063010,3206,2,2,2,0,4750112,,42098.82847,35,32063010,"<p>Both <a href=""http://lejos.org"" rel=""nofollow"">leJOS</a> and <a href=""http://www.evdev.org"" rel=""nofollow"">ev3dev</a> support USB web cams connected to the EV3 brick, however, neither work with LabView.</p>

<p>To get this working on the official LEGO firmware, you need to compile proper kernel modules, sideload them on the EV3 and as suggested in the comments, sideload another program that converts the image data into something that can communicate with labview remotely.</p>
",1976323,2,0,52019555,[ev3dev](http://www.ev3dev.org/) looks like something you should look into. It should allow install a driver and use a programming language of your choice to stream the webcam image over udp to labview,Incoming
348,2198,29143763,Need some tips which localization method to use for my NAO robot (robotic),|localization|orientation|project-management|robotics|nao-robot|,"<p>for university I'm working on a project in which I have to teach a robot(Nao-robot) play nine men's morris. Unfortunately I'm fairly new to the area of robotics and I need some tips how to solve some problems. Currently I'm working on the localization/orientation of the robot and I'm wondering which approach of localization would fit best in my project.</p>

<p><strong>A short explanation of the project:</strong>
The robot has a fixed starting position and has to walk around on a boardwhich has a size of about 3x3 meter ( I will post a picture of the board when i reach 10 reputation). There are no obstacles on the field except the game tokens and the game lines are marked yellow on the board. For orientation I use the two camera devices the robot has.</p>

<p>I found some approaches like </p>

<p><strong>Monte Carlo Localization</strong></p>

<p><strong>SLAM</strong> (Simultaneous Localization and Mapping)</p>

<p>but these approaches seem to be quite complex for a beginner like me and I would really appreciate if some has some good ideas what would be a simpler way to solve this problem. Functionality has for me a far higher priority than performance.</p>
",42082.49097,29150211,256,1,0,-2,0,4685999,,42081.63819,5,29150211,"<p>I have vague knowledge about the nine men's morris game as such, but I will try to give you my simpler idea.</p>

<p>First thing first, you need to have a map of your board. This should be easy in your case, cause your environment is static. There are few <a href=""http://forums.trossenrobotics.com/tutorials/introduction-129/an-introduction-to-mapping-and-localization-3274/"" rel=""nofollow"">technique</a> to do this mapping from your board. For your case I would suggest to have a metric map, which is an occupancy grid. Assign coordinates to each cell in the grid. This will be helpful in robot navigation.</p>

<p>As you have mentioned, your robot starts from a fixed position. On start up, initialize your robot with this reference location and orientation (with respect to X-Y axes of the grid, may be you don't need the cameras, I am not sure!!). By initialization I mean, mark your position on the grid.</p>

<p>Use <a href=""http://en.wikipedia.org/wiki/Dead_reckoning"" rel=""nofollow"">Dead Reckoning</a> for localization and keep updating position and orientation of your robot as it move through the board. I would hope that your robot get some feedback from the servos, like number of rotations and so forth. Do that math and update the position coordinates of your robot as it move into different cell in the grid.</p>

<p>You can use <a href=""http://wiki.gamegardens.com/Path_Finding_Tutorial"" rel=""nofollow"">A-Star</a> algorithm to find a path for your robot. You need to do the path planning before you want to navigate. You also have to mark those game tokens on the grid, to avoid collisions in planning the path. </p>
",1800026,0,0,,,Coordinates
349,2388,33186486,Advanced Line Follower Robot,|arduino|robotics|robotics-studio|,"<p>I know about line follower mainly the grid solving robots i know the basics actually. Actually they have to trace the path of the grid in an arena and then reach back to starting point in a shortest distance. Here my doubt is regarding the line follower in this link I have attached.<a href=""https://www.youtube.com/watch?v=5At_u5rnh2U"" rel=""nofollow noreferrer"">Advanced line follower robot for grid solving and maze solving</a></p>
<p>My doubt is what are the procedures to do it? They have mapped the path and used Dijkstra algorithm to solve the path. But how do they transfer the code(i.e) where it has to turn which direction it has to turn. how are they generating the what function should be passed? Please explain i need the procedure alone. am going to try it with python.</p>
",42294.50764,33242153,1147,1,10,0,0,,,,,33242153,"<p>From the comments we exchanged, I feel more confident to assume your actual question is this:</p>

<blockquote>
  <p>What data structure could be used to store the structure (geometry, topology) of the map into the robot's memory?</p>
</blockquote>

<p>Well there should be many possible ways to do that. Basically, this is a connected graph where nodes sit on a rectangular grid. So, for a start, one could describe the nodes as a set of coordinate pairs:</p>

<pre><code>// just an example, this is not the actual map
// it doesn't need to be variables, could be array of arrays, or dictionary
var A = (0,0);
var B = (1,0);
var C = (2,1);
var D = (4,2);
// etc.
</code></pre>

<p>Then, you could describe the edges as pairs of points:</p>

<pre><code>var edges = [(A,B), (A,D), (B,C), ...];
</code></pre>

<p>With these, you surely could calculate a good path from a list of points, and also the location and direction of each node.</p>

<p>I'm not sure at all if this is the most efficient data structure, but it is already a start. You only need to know the location of each node, and the edges are defined simply by linking two nodes together.</p>
",401828,0,2,54280119,"The downvote wasn't mine. Anyway, I agree your question is broader than Arduino's scope. Honestly, I think it may be downvoted actually by being too broad. In your second paragraph, it is not quite clear what you are asking, in my opinion.",Incoming
350,2420,34522095,GUI Button hold down - tkinter,|python|tkinter|robotics|,"<p>I'm trying to do a GUI in python to control my robotic car. My question is how I do a function that determine a hold down button. I want to move the car when the button is pressed and held down and stop the car when the button is released.</p>

<pre><code>from Tkinter import * 

hold_down = False 
root = Tk()

def button_hold(event):
      hold_down=true
      while hold_down== True: 
               print('test statement')
               hold_down = root.bind('&lt;ButtonRelease-1&gt;',stop_motor)

def stop_motor(event):
       hold_down= False
       print('button released')

button = Button(root, text =""forward"")
button.pack(side=LEFT)
root.bind('&lt;Button-1&gt;',button_forward)
root.mainloop()
</code></pre>

<p>I'm trying to simulate what I found in this <a href=""https://stackoverflow.com/a/2440643"">answer</a></p>

<p>I try to do it in a <code>while</code> loop with a boolean. When the user presses the button the boolean changes to <code>True</code> and code enters the while loop. When user releases the button the boolean changes to <code>False</code> and code exits from loop but in this code the boolean stay always true no matter if I released the button or not. </p>

<p>Edit: I want a function to be called until a condition occurs.The function to be called is hold_down() and the condition to check  is the button is released.</p>

<p>Update: I found a way to make it work.</p>
",42368.09722,,25805,5,1,3,0,5729115,,42368.12153,21,34522141,"<p>Try this...</p>

<pre><code>from Tkinter import *  
root = Tk()
global hold_down

def button_hold(event):
    hold_down = True
    while hold_down: 
        print('test statement')

def stop_motor(event):
    hold_down = False
    print('button released')

button = Button(root, text =""forward"")
button.pack(side=LEFT)
root.bind('&lt;Button-1&gt;',button_hold)
root.bind('&lt;ButtonRelease-1&gt;',stop_motor)
root.mainloop()
</code></pre>
",4655537,0,2,82612777,can you share your solution here? I have the same problem.,Remote
351,2306,32027233,How to multithread/multiprocess just one instance of a particular function in Python?,|python|multithreading|multiprocessing|robotics|,"<p>I'm running a Python script that controls a robot, but I'm a bit confounded as to how to multithread the motor control function.</p>

<p>The issue is that the hardware is designed such that the motors won't move unless there are several sleeps in the motor-control function, because the hardware requires time to send the electrical signals to the motors. Because of these sleeps in the motor-control function, the entire program halts and stops reading in sensor data.</p>

<p>What I would like to do is know how to multithread/multiprocess the motor-control function once it is called, but once the program hits the call again in the following iteration of the loop, it checks if the motor-control is still running (i.e. it isn't done with the sleep). If it is still running, it simply skips over the motor-control call and continues on with looping, reading sensor data, and then checking again if the motor-control function is still running. Of course, if the motor-control function is no longer running, I would like for it to once again be called.</p>

<p>Basically, the whole program only requires two threads: one that runs the main program, and one that branches off and continuously re-runs one instance of the motor-control function every time the motor-control function has completed its execution.</p>

<p>I had tried using the concurrent.futures import but got messages saying it was not supported and I couldn't find any usages particular to the way I intend to use it.</p>
",42231.72014,32034462,843,1,6,4,0,3293742,"Pensacola, Florida",41680.68819,368,32034462,"<p>I think you don't need threading, but I may misunderstand your requirements so I will present 2 alternatives.</p>

<ol>
<li>Without threading and sleeps</li>
</ol>

<p>Assuming your current program flows like that :</p>

<pre><code>while True:
    data = read_sensors()
    command = process(data)
    motor_do(command)
    time.sleep(delay)
</code></pre>

<p>Then you could just remove the sleep, and only call motor_do if the last call was at least <code>delay</code> seconds ago.</p>

<pre><code>last_call_time = -delay # to be sure that motor_do can be called the first time
# ""On Windows, [time.clock] returns wall-clock seconds elapsed since the first call to this
# function, as a floating point number, based on the Win32 function QueryPerformanceCounter()
# The resolution is typically better than one microsecond."" (python 2.7 doc)
# i.e. close to 0 on first call of time.clock()

while True:
    data = read_sensors()
    command = process(data)
    motor_try(command)

def motor_try(command):
    global last_call_time

    current_time = time.clock()
    # on win that works, on unix... you may want to take a look at the NB

    elapsed = current_time - last_call_time
    if elapsed &gt;= delay:
        motor_do(command)
        last_call_time = current_time
</code></pre>

<ol start=""2"">
<li>With threading (this is an example, I have no experience in threading/async with python 2.7 so there may be better ways to do this)</li>
</ol>

<p>Assuming your current program flows like that :</p>

<pre><code>while True:
    data = read_sensors()
    command = process(data)
    motor_do(command) // this function sleeps and you CANNOT change it
</code></pre>

<p>Then you have to launch 1 thread that will only push the commands to the motor, asynchronously.</p>

<pre><code>import thread

command = None
command_lock = thread.allocate_lock()

def main():
    thread.start_new_thread(motor_thread)

    global command
    while True:
        data = read_sensors()
        with command_lock:
            command = process(data)

def motor_thread():
    while True:
        while not command: # just to be sure
            time.sleep(0.01)
            # small delay here (for instance, the time consumed by read_sensors + process)
        with command_lock:
            motor_do(command)
            command = None
        time.sleep(delay)
</code></pre>

<p>NB : on Unix, <code>time.clock()</code> returns processor time (= without the idling time), so it would be better to use <code>time.time()</code>... unless the system clock is changed : ""While this function normally returns non-decreasing values, it can return a lower value than a previous call if the system clock has been set back between the two calls."" (python 2.7 doc)</p>

<p>I don't know how <code>time.sleep()</code> react to system clock changes.</p>

<p>see <a href=""https://stackoverflow.com/questions/1205722/how-do-i-get-monotonic-time-durations-in-python"">How do I get monotonic time durations in python?</a> for precise time delays on unix/py2.7 (and <a href=""https://stackoverflow.com/questions/25785243/understanding-time-perf-counter-and-time-process-time"">Understanding time.perf_counter() and time.process_time()</a> can be useful)</p>

<p>Python3 : just use <code>time.monotonic()</code>... or <code>time.perf_counter()</code>.</p>
",3125565,1,0,51957192,"You need to consider that you create a thread which should call your function, which itself should loop round forever. Different threads each call different functions.",Timing
352,2255,30604094,Arduino Making Spiral,|arduino|robotics|,"<p>I have a robot sweeper that I am creating it detects walls and turns when getting to a certain distance so it does not hit. However the movements are pretty random and I would like the device to start out making a small circle and then growing to a larger one. It seems to be getting stuck in just one size circle with maybe a little growth. I created multiple size circle functions however it does not seem to be taking hold. Thanks ahead of time. Any help no matter how small would be greatly appreciated. </p>

<pre><code>#include&lt;NewPing.h&gt;
#define MOTOR_A 0 
#define MOTOR_B 1
#define TRIGGER_PIN 5
#define ECHO_PIN 4
#define MAX_DISTANCE 200

#define CW 0 
#define CCW 1 

const byte PWMA = 3; 
const byte PWMB = 11; 
const byte DIRA = 12; 
const byte DIRB = 13; 

NewPing sonar (TRIGGER_PIN, ECHO_PIN, MAX_DISTANCE); 


void setup() {
  // put your setup code here, to run once:
setupArdumoto(); 
}

void loop() {
  delay(50); 
  unsigned int uS= sonar.ping();

  if(uS/US_ROUNDTRIP_CM&gt;50||uS/US_ROUNDTRIP_CM==0)
  {
    forward(); 
    curve();
    //stopArdumoto(MOTOR_A);
    // stopArdumoto(MOTOR_B);
  }
  else if(uS/US_ROUNDTRIP_CM&gt;=90)
  {
    smallerCurve();
  }

 else if(uS/US_ROUNDTRIP_CM&gt;=110)
 {
   smallestCurve();
 }
  else if(uS/US_ROUNDTRIP_CM&lt;20)
  { 
    //turnRight(100); 
    delay(500); 
  }
  // put your main code here, to run repeatedly:

}
void driveArdumoto(byte motor, byte dir, byte spd) 
{
  if(motor == MOTOR_A)
  {
    digitalWrite(DIRA, dir);
    analogWrite(PWMA, spd); 
  }
  else if(motor==MOTOR_B)
  {
    digitalWrite(DIRB,dir); 
    analogWrite(PWMB, spd); 
  }
}
void curve() 
{
 driveArdumoto(MOTOR_A, CW, 200);
 driveArdumoto(MOTOR_B, CW, 150);
}
void smallerCurve()
{
 driveArdumoto(MOTOR_A, CW, 200);
 driveArdumoto(MOTOR_B, CW, 120);
}
void smallestCurve()
{
 driveArdumoto(MOTOR_A, CW, 200);
 driveArdumoto(MOTOR_B, CW, 100);
}
void forward()
{
  driveArdumoto(MOTOR_A,CW,170);
  driveArdumoto(MOTOR_B,CW,170); 
}

void turnRight(byte spd)
{
  stopArdumoto(MOTOR_B); 
  driveArdumoto(MOTOR_A,CW,250); 
}
void turnLeft(byte spd) 
{
   stopArdumoto(MOTOR_A); 
   driveArdumoto(MOTOR_B,CW,250); 
}
void stopArdumoto(byte motor)
{
    driveArdumoto(motor, 0,0); 
}

void setupArdumoto()
{
   pinMode(PWMA,OUTPUT);
   pinMode(PWMB,OUTPUT);
   pinMode(DIRA,OUTPUT);
   pinMode(DIRB,OUTPUT); 

   digitalWrite(PWMA, LOW);
   digitalWrite(PWMB, LOW);
   digitalWrite(DIRA, LOW);
   digitalWrite(DIRB, LOW);
}
</code></pre>
",42157.78056,,590,1,0,0,,3006893,California,41597.02917,11,30631067,"<p>It's late but I think that your code rarely will execute <code>smallerCurve();</code> or <code>smallestCurve();</code> because if <code>uS/US_ROUNDTRIP_CM</code> is greater than 50, code in first <code>if</code> will execute and jump the code for <code>uS/US_ROUNDTRIP_CM &gt;= 90</code> and <code>uS/US_ROUNDTRIP_CM &gt;= 110</code> only when it was between 0 and 20 will execute different things. Take care of your <code>else-if</code>, once you have executed one block of code in an <code>if</code> you will jump over the <code>else</code>, and your first <code>else</code> take all the rest of your code.
In fact, a good compiler doesn't compile this <code>else</code>'s because they are unreacheable code.</p>

<p>And, moreover, you can improve your code with some simple things. Be careful with the function <code>drive Ardumoto</code> and don't use it so frequently, <code>forward</code> won't have time to move your robot. Use a delay between movements.</p>
",3861548,0,0,,,Moving
353,2256,30724864,"Graph-SLAM when it uses only odometry information, will it still run? and what is the outcome?",|robotics|kalman-filter|slam-algorithm|,"<p>This is a kind of difficult question.</p>

<p>I know about <strong>EKF-SLAM</strong>, which uses a state from previous time to estimate next state as an online filter, 
I also know about <strong>Graph-SLAM</strong>, which uses all states in past as full SLAM, and represents them as merely whole bunch of nodes and edges, and optimize structure of nodes and eges by minimizing error to estimate better states.</p>

<p>Now,
I know that there is no meaning in running EKF-SLAM with odometry info only, since what EKF does is estimating future state by balancing weight between Odometry info AND Observation of landmark info. so both are needed.</p>

<p>My question is, is it possible to run <strong>Graph-SLAM with only Odometry info</strong> and no landmark observation info whatsoever?
It seems like Graph-SLAM can run by gathering all Odometry info state upto current ones and converting them to nodes and edges  just like it does when both Odo and obs are provided, and it can optimize the structure of nodes and edges.
Is is possible?
What does output mean? ""Optimized"" Odometry?
Any thought to it?
Thank you in advance. :)</p>
",42164.28472,,211,1,0,0,,4989045,,42164.27083,0,30850314,"<p><strong>preface:</strong> I am not 100% certain, these are just my assumptions/opinions</p>

<p>the point of SLAM is <strong>S</strong>imultaneous <strong>L</strong>ocalization <strong>A</strong>nd <strong>M</strong>apping In order to do any mapping you need Observation of landmarks, or some other feature. Otherwise you are only performing localization. </p>

<p>Think if I dropped you a building you've never been in before and I said, create a map for me, BUT you can ONLY count your footsteps. You must not use any other senses (eyes closed, ears plugged, etc). You would quickly realize this is a nearly impossible task. If you use only odometry, something like a Kalman Filter, or EKF should work nicely, but again this is only doing localization, not mapping.</p>

<p>hope that helps</p>
",2705382,0,0,,,Moving
354,2347,32228629,IMU Orientation constantly changing,|orientation|ros|hardware|robotics|imu|,"<p>We have XSENS MTi IMU-Device and use the ROS-Framework (Ubuntu / Fuerte). 
We subscribe to the IMU-Data and all data looks good except <strong>orientation</strong>.</p>

<p>In Euler-Outputmode like in Quaternion-Outputmode the values are constantly changing. Not randomly, they increase or decrease slowly at a more or less constant rate, and sometimes I observed the change to flatten out and then change it's direction. </p>

<p>When the Value at Second X may be:</p>

<pre><code>x: 7.79210457616
y: -6.58661204898
z: 41.2841955308
</code></pre>

<p>the Z value changes in a range of about 10 within a few seconds (10-20 seconds I think). </p>

<p>What can cause this behaviour? Do we misinterpret the data or is there something wrong with the driver? The strange thing is, this also happend with 2 other drivers, and one other IMU device (we have 2). Same results in each combination. </p>

<p>Feel free to ask for more precise data or whatever you'd like to know that may be able to help us out. We are participating at the Spacebot-Cup in November, so it would be quite a relief to get the IMU done. :)</p>
",42242.57917,,2585,2,0,3,0,4315243,,41975.43264,32,32231388,"<p>If you have the IMU version, I assume that no signal processing has been done on the device. (but I don't know the product). So the data you get for the orientation should be only the integral of the gyroscope data.</p>

<p>The drift you can see is normal and can come from the integration of the noise, a bad zero rate calibration, or the bias of the gyroscope.</p>

<p>To correct this drift, we usually use an <code>AHRS</code> or a <code>VRU</code> algorithm (depending the need of a corrected yaw). It's a fusion sensor algorithm which take the gravity from the accelerometer and the magnetometer data (for AHRS) to correct this drift.</p>

<p>The algorithms often used are the <code>Kalman filter</code> and the <code>complementary filter</code> (Madgwick/Mahony).</p>

<p>It's not an easy job and request a bit of reading and experimenting on matlab/python to configure these filters :)</p>
",5269403,2,0,,,Coordinates
355,2495,36829189,How to receive data from Microsoft Kinect device?,|kinect|kinect-sdk|robotics|,"<p>I'm wondering are their any codes or application that I can use to receive data from Kinect device. </p>

<p>The idea is to use the kinect to send its signals to a surface and get back range of signal data. this could be set of numbers which will change according to the light, distance and angel 
Thanks.</p>
",42484.90139,36847102,1450,1,2,-1,,2948959,"Perth, Western Australia",41581.04514,82,36847102,"<p>Start with downloading the sdk from the link below (windows 8 or above is required for kinect to pc) and use their demo projects to see how to set up a program to read data in from the Kinect. It will have colour and depth cameras and you will be able to get the pixel data back and be able to use the depth feedback to tell how far away something is. As far as angle, the Kinect doesn't have a built in gyroscope so you'll have to use known points and trig to find angles.</p>

<p><a href=""https://www.microsoft.com/en-us/download/details.aspx?id=44561"" rel=""nofollow"">https://www.microsoft.com/en-us/download/details.aspx?id=44561</a></p>
",5758722,3,1,61254970,Are you using Kinect v1 or Kinect v2? If you are using Kinect v2 there are many examples included in the SDK!,Incoming
356,2557,38710748,Arduino radio frequency receiver does not work with motor shield,|c++|c|arduino|robotics|,"<p>Micro-controller : SainSmart Mega 2560<br>
Motor Shield: Osepp Motor Shield  V1.0<br>
I am trying to implement Radio Frequency communication on my wheeled robot however when the motors are running the radio frequency code will not receive messages.  </p>

<p>The motor shield uses pins 4,7,8,12<br>
I have setup the radio frequency to occur on pins 22,23 ,5 </p>

<p>I see this reference to 
<a href=""https://stackoverflow.com/questions/21069867/why-does-virtualwire-conflicts-with-pwm-signal-in-arduino-atmega328-pin-d10"">Why does VirtualWire conflicts with PWM signal in Arduino/ATmega328 pin D10?</a>
but am not sure if this applies to my situation.</p>

<p><strong>How do I get Radio Frequency receiver/transmitter to work while motor shield in use?</strong>  </p>

<p>code demonstrating the situation:</p>

<pre><code>  #include &lt;RH_ASK.h&gt;
  #include &lt;SPI.h&gt; // Not actually used but needed to compile
  RH_ASK driver(2000, 22, 23, 5,true); // ESP8266: do not use pin 11
  /// *************************
  //      MOTOR SETUP
  /// *************************
  // Arduino pins for the shift register
  #define MOTORLATCH 12
  #define MOTORCLK 4
  #define MOTORENABLE 7
  #define MOTORDATA 8

  // 8-bit bus after the 74HC595 shift register
  // (not Arduino pins)
  // These are used to set the direction of the bridge driver.
  #define MOTOR1_A 2
  #define MOTOR1_B 3
  #define MOTOR2_A 1
  #define MOTOR2_B 4
  #define MOTOR3_A 5
  #define MOTOR3_B 7
  #define MOTOR4_A 0
  #define MOTOR4_B 6

    // Arduino pins for the PWM signals.
    #define MOTOR1_PWM 11
    #define MOTOR2_PWM 3
    #define MOTOR3_PWM 6
    #define MOTOR4_PWM 5
    #define SERVO1_PWM 10
    #define SERVO2_PWM 9

    // Codes for the motor function.
    #define FORWARD 1
    #define BACKWARD 2
    #define BRAKE 3
    #define RELEASE 4

    void setup()
    {
        Serial.begin(9600); // Debugging only
        if (!driver.init())
             Serial.println(""init failed"");

      //comment out code below  to allow receiver to read radio frequency  communication
      //BEGIN
        motor(1, FORWARD, 255);
        motor(2, FORWARD, 255);
        motor(4, FORWARD, 255);
        motor(3, FORWARD, 255);
       //END

    }
    void loop()
    {
        uint8_t buf[RH_ASK_MAX_MESSAGE_LEN];
        uint8_t buflen = sizeof(buf);
        if (driver.recv(buf, &amp;buflen)) // Non-blocking
        {
            int i=0;
            // Message with a good checksum received, dump it.
            driver.printBuffer(""Got:"", buf, buflen);
            buf[5]= '\0';
            Serial.println((char*)buf);
        }
    }


    void motor(int nMotor, int command, int speed)
    {
      int motorA, motorB;

      if (nMotor &gt;= 1 &amp;&amp; nMotor &lt;= 4)
      {  
        switch (nMotor)
        {
        case 1:
          motorA   = MOTOR1_A;
          motorB   = MOTOR1_B;
          break;
        case 2:
          motorA   = MOTOR2_A;
          motorB   = MOTOR2_B;
          break;
        case 3:
          motorA   = MOTOR3_A;
          motorB   = MOTOR3_B;
          break;
        case 4:
          motorA   = MOTOR4_A;
          motorB   = MOTOR4_B;
          break;
        default:
          break;
        }

        switch (command)
        {
        case FORWARD:
          motor_output (motorA, HIGH, speed);
          motor_output (motorB, LOW, -1);     // -1: no PWM set
          break;
        case BACKWARD:
          motor_output (motorA, LOW, speed);
          motor_output (motorB, HIGH, -1);    // -1: no PWM set
          break;;
        case RELEASE:
          motor_output (motorA, LOW, 0);  // 0: output floating.
          motor_output (motorB, LOW, -1); // -1: no PWM set
          break;
        default:
          break;
        }
      }
    }

    void motor_output (int output, int high_low, int speed)
    {
      int motorPWM;

      switch (output)
      {
      case MOTOR1_A:
      case MOTOR1_B:
        motorPWM = MOTOR1_PWM;
        break;
      case MOTOR2_A:
      case MOTOR2_B:
        motorPWM = MOTOR2_PWM;
        break;
      case MOTOR3_A:
      case MOTOR3_B:
        motorPWM = MOTOR3_PWM;
        break;
      case MOTOR4_A:
      case MOTOR4_B:
        motorPWM = MOTOR4_PWM;
        break;
      default:
        speed = -3333;
        break;
      }

      if (speed != -3333)
      {
        shiftWrite(output, high_low);
        if (speed &gt;= 0 &amp;&amp; speed &lt;= 255)    
        {
          analogWrite(motorPWM, speed);
        }
      }
    }

    void shiftWrite(int output, int high_low)
    {
      static int latch_copy;
      static int shift_register_initialized = false;
      if (!shift_register_initialized)
      {
        // Set pins for shift register to output
        pinMode(MOTORLATCH, OUTPUT);
        pinMode(MOTORENABLE, OUTPUT);
        pinMode(MOTORDATA, OUTPUT);
        pinMode(MOTORCLK, OUTPUT);

        // Set pins for shift register to default value (low);
        digitalWrite(MOTORDATA, LOW);
        digitalWrite(MOTORLATCH, LOW);
        digitalWrite(MOTORCLK, LOW);
        // Enable the shift register, set Enable pin Low.
        digitalWrite(MOTORENABLE, LOW);
        // start with all outputs (of the shift register) low
        latch_copy = 0;
        shift_register_initialized = true;
      }
      bitWrite(latch_copy, output, high_low);
      shiftOut(MOTORDATA, MOTORCLK, MSBFIRST, latch_copy);
      delayMicroseconds(5);    // For safety, not really needed.
      digitalWrite(MOTORLATCH, HIGH);
      delayMicroseconds(5);    // For safety, not really needed.
      digitalWrite(MOTORLATCH, LOW);
    }
</code></pre>
",42584.08889,,468,2,0,2,,3140515,,41635.84028,2,38721025,"<p>It looks like the reference you give could be the problem. To try that fix just find the RH_ASK.cpp file and uncomment line 16 like this</p>

<pre><code>// RH_ASK on Arduino uses Timer 1 to generate interrupts 8 times per bit interval
// Define RH_ASK_ARDUINO_USE_TIMER2 if you want to use Timer 2 instead of Timer 1 on Arduino
// You may need this to work around other libraries that insist on using timer 1
#define RH_ASK_ARDUINO_USE_TIMER2
</code></pre>
",3604898,0,1,,,Remote
357,2440,35200665,Using OpenCV on raspberry pi for vision tracking FRC,|java|python|opencv|raspberry-pi|robotics|,"<p>I'm a senior in high school currently a programmer for the robotics team.  This year we plan on doing some vision processing/tracking to automatically find the goal and align ourselves with the goal.  We use java to program our robot and are apart of the FRC (First Robotics Competition).  We're having some trouble with the standard way of getting vision tracking to work, using RoboRealm, and I had a thought to use a Raspberry Pi as a co-processor solely for vision tracking purposes.  I've done a little bit of research as to what to use and it appears OpenCV is the best.  I have little experience in coding on the Raspberry Pi, but a basic understanding of python.  I was thinking of having the raspberry pi do all the tracking of the goal (has retro-reflective tape along the outer edge of the goal), and somehow sending that signal (through roborio -- on-board FRC standard processor) and to my java code, which will then tell our robot to either turn more left or more right depending on how far off from the target we are.  I'm just curious if this is in the realm of do-ability for a beginner programmer such as myself.  Any feedback would be great! </p>

<p>Thanks!</p>
",42404.50694,35212584,3003,1,0,1,,3259325,New Jersey,41671.02153,36,35212584,"<p>Everything you've said sounds very doable using <a href=""http://docs.opencv.org/master/dd/d49/tutorial_py_contour_features.html#gsc.tab=0"" rel=""nofollow"">contour features</a> You can use a bounding rectangle/circle,etc to extract center of mass (COM) coordinates of your goal. At this point you can do a simple thresholding like you said, if the COM is to the left, move left and vice versa.</p>

<p>The biggest issue would be reliably locating the goal, if you've never done CV before its easy to underestimate the difficulty of this task. My advice is to try to make the goal as apparent as possible. Since its reflective perhaps you can illuminate it to make it stand out more? Maybe shine IR(infra-red) from the robot and use an IR filter on the camera. You could also do this with any regular light in the visible spectrum. Once you've created adequate contrast between the goal and the background you could do simple <a href=""http://docs.opencv.org/master/d7/d4d/tutorial_py_thresholding.html#gsc.tab=0"" rel=""nofollow"">thresholding</a> or possibly do <a href=""http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_imgproc/py_template_matching/py_template_matching.html"" rel=""nofollow"">template matching</a> (though much slower, and it won't work if the goal is at an angle or skewed). </p>

<p>I hope I've given you some ideas, good luck.</p>

<p><strong>EDIT</strong><br>
In your comment you mentioned your target is green which can simplify your problem. I'm not sure how much you know about CV, but images come in RGB format. Each pixel has a red, green, and blue component. If you are looking for green it might be nice to <a href=""http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_core/py_basic_ops/py_basic_ops.html#splitting-and-merging-image-channels"" rel=""nofollow"">split the colors</a> and only the green channel of the image for <a href=""http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_imgproc/py_thresholding/py_thresholding.html#thresholding"" rel=""nofollow"">thresholding</a> THe open CV site has GREAT <a href=""http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_tutorials.html"" rel=""nofollow"">tutorials</a> for getting started. I would highly recommend you (and anyone else on your team) take a look at this. I'd recommend you read:</p>

<ol start=""2"">
<li>Gui Features in OpenCV<br>
a. Images<br>
b. videos</li>
<li>Core operations - Basic Operations on Images</li>
<li>Image Processing (this is the big one)<br>
c. Image Thresholding<br>
d. Smoothing (almost every image in every cv algorithm is smooth during pre processing)<br>
e. Morphological Transformations (may help clean the image after thresholding)<br>
i. Contours (this is where you get coordinates)  </li>
</ol>

<p>another tip is during algorithm development work with still images. Take a few different images of your goal from a perspective the robot would likely encounter. Do all your testing and development on those. Once you have a high confidence level then move to video. Even then I would start with <em>offline</em>
 video (a recording you capture, not real time). If you find issues its easy to reproduce them (just go back to that troublesome time-stamp in the vid and tweak your algorithm). Then finally do it with <em>online</em> (real-time) video.</p>

<p>Last piece of advice, even if your ultimate goal is to run on the RPi feel free to test your CV algorithm on any computer you have. If you use a laptop most of the time, throw opencv on that, the main difference in porting to Rpi will be the way you address the RPi camera module. But if you are still at early stages using stills and offline video this won't make a difference. But that's just my opinion, I know I myself have trouble dragging out my Pi to code when I'm on my windows laptop all day. I'm far more likely to work on code using my everyday PC.</p>
",2705382,1,2,,,Incoming
358,2594,40748032,Jinput Poll Data Never Changes,|java|input|controller|robotics|jinput|,"<p>I am attempting to create a simple test program to get familiar with the JInput Library for another project. I have tested my controller with the all of the provided test classes and it works as expected. However, when I attempt to poll the controller, all values remain unchanged regardless of my input. Here is the code I am working with:</p>

<p><code>
public class ControllerTest {</p>

<pre><code>public static void main(String[] args){
    //System.out.println(""Hello World"");

    Controller[] ca = ControllerEnvironment.getDefaultEnvironment().getControllers();
    Controller gamepad = null;
    Component[] components = null;
    EventQueue eventQueue;
    // Run through the list of available input devices and get the gamepad
    for(int i = 0; i &lt; ca.length; i ++){
        if(ca[i].getType().equals(Controller.Type.GAMEPAD)){
            gamepad = ca[i];
        }
    }
    // Print the name of the controller and its type
    if(gamepad != null){
        System.out.println(gamepad.getName() + "": "" + gamepad.getType());
        components = gamepad.getComponents();
        System.out.println(""\tComponents:"");
        for(int i = 0; i &lt; components.length; i ++){
            System.out.println(""\t\tComponent #"" + i + "": "" + components[i].getName() + ""\n\t\t\tIs Relative: ""
                    + components[i].isRelative());
        }
    }
    else{
        System.out.println(""No gamepad connected"");
    }

    while (true){
        // If we have no gamepad connected, exit
        if(gamepad == null){
            System.out.println(""No Gamepad detected, exiting..."");
            System.exit(0);
        }
        // Poll controller
        gamepad.poll();
        Component[] comp = gamepad.getComponents();

        for(int i = 0; i &lt; comp.length; i ++){
            StringBuffer buffer = new StringBuffer();
            buffer.append(comp[i].getName());
            buffer.append("", Value: "" + comp[i].getPollData());
            System.out.println(buffer.toString());
        }
        try{
            Thread.sleep(20);  // Sleep before polling again
        }
        catch(InterruptedException e){
            e.printStackTrace();
        }
    }
}
</code></pre>

<p>}
</code></p>

<p>I have been trying to find an answer online, but this library is not very well documented and seems to usually be wrapped in other libraries specific for making games. (The aforementioned project is robotic in nature) </p>
",42696.72569,,479,2,0,0,,3033582,"Somewhere, beyond the sea.",41603.77569,18,42728931,"<p>You have to use an EventQueue </p>

<pre><code>player.poll();
        EventQueue queue = player.getEventQueue();
        Event event = new Event();
        while (queue.getNextEvent(event)) {
            Component comp = event.getComponent();
            if (comp.getIdentifier() == Component.Identifier.Button._6){
                if (comp.getPollData() == 1){
                    example
                }
</code></pre>
",7553140,-1,1,,,Remote
359,2589,40419901,What is difference of occupancy grid map and elevation map,|navigation|robotics|elevation|,"<p>I want to use occupancy grid  map or elevation map for navigation. what is difference of occupancy grid  map and elevation map in navigation?
thanks</p>
",42678.42014,,390,1,0,0,,6812176,,42622.24861,29,40445523,"<p>Create a map of an environment around you [Say a 2D array where the dimensions are your position in x and y] - Lets call this a grid for this concept. This grid becomes a  binary occupancy grid map, if you denote the ""value"" of each point (i.e an xy coordinate) to be 1 if it is occupied, and 0 if it is not.
These are useful to model obstacles to avoid collisions.
We can make denser and more detailed occupancy maps by adding different features and representations of what we wish to map.</p>

<p>It becomes an elevation map, when the ""value"" that we spoke about above, indicates the height of the terrain or environment around you.</p>
",5477866,0,0,,,Moving
360,2442,35330232,How to program ESC to increase/decrease PWM in increments,|arduino|robotics|pwm|,"<p>I've coded with Python before however, I am in the process of learning C and from what I have been told Arduino is quite similar to C in some aspects (at least with coding). I noticed that my when I run the code on my robot it jolts due to the quick changes in PWM. So I'd like some guidance as to how to do an if statement on Arduino because I am trying to increase/decrease the PWM in increments.</p>

<pre><code>//On Roboclaw set switch 1 and 6 on. // &lt;-- what does this refer to?
//mode 2 option 4 // &lt;-- my note based on user manual pg 26


#include &lt;Servo.h&gt; 


Servo myservo1;  // create servo object to control a Roboclaw channel
Servo myservo2;  // create servo object to control a Roboclaw channel

//int pos = 0;    // variable to store the servo position  //&lt;-- left-over from arduino ide servo sweep example?

void setup() 
{ 
  myservo1.attach(9);  // attaches the RC signal on pin 5 to the servo object (Left Motor)
  myservo2.attach(11);  // attaches the RC signal on pin 6 to the servo object (Right Motor)
} 


void loop() 
{ 
  //forward
  myservo1.writeMicroseconds(1000);
  myservo2.writeMicroseconds(1000);
  delay(2000);

  //backward
  myservo1.writeMicroseconds(2000);
  myservo2.writeMicroseconds(2000);
  delay(2000);

  //left
  myservo1.writeMicroseconds(1500);
  myservo2.writeMicroseconds(1000);
  delay(2000);

  //right
  myservo1.writeMicroseconds(1000);
  myservo2.writeMicroseconds(1500);
  delay(2000);

}
</code></pre>
",42411.14167,,764,1,1,0,,5412716,,42283.26597,27,35337814,"<p>Ok, whenever you write a different value to the servo it will move to that position as fast as possible. So you will need to move your servo step by step.</p>

<p>For this task, however, you will not be able to use delays, since they block the processor. You will need to mimic the ""blink with no delay"" example (i.e. using the millis() function to do something when time passes by.</p>

<p>The acceleration control simply moves 1° every X milliseconds (in this case 6ms, which makes a full movement - 180° - last for about one second). Every X milliseconds, so, the uC moves 1° in the direction of a target position. In the other code you should just set the target to your desired position and you are done.</p>

<p>Here is the code I wrote. Let me know if it works for you</p>

<pre><code>#include &lt;Servo.h&gt; 

Servo myservo1;
Servo myservo2;
unsigned long prevMillisAccel = 0, prevMillisAction = 0;
uint8_t servo1_target;
uint8_t servo2_target;
uint8_t currAction;

#define TIME_FOR_ONE_DEGREE_MS 6

void setup()
{
    myservo1.attach(9)
    myservo2.attach(11);
    prevMillisAccel = millis();
    prevMillisAction = millis();
    servo1_target = 0;
    servo2_target = 0;
    myservo1.write(0);
    myservo2.write(0);
    currAction = 0;
}

void moveServoTowardsTarget(Servo *servo, uint8_t target)
{
    uint8_t pos = servo-&gt;read();
    if (pos &gt; target)
        servo-&gt;write(pos-1);
    else if (pos &lt; target)
        servo-&gt;write(pos+1);
}

void loop()
{
    unsigned long currMillis = millis();
    if (currMillis - prevMillisAccel &gt;= TIME_FOR_ONE_DEGREE_MS)
    {
        prevMillisAccel += TIME_FOR_ONE_DEGREE_MS;

        moveServoTowardsTarget(&amp;myservo1, servo1_target);
        moveServoTowardsTarget(&amp;myservo2, servo2_target);
    }

    if (currMillis - prevMillisAction &gt;= 2000)
    {
        prevMillisAction += 2000;
        currAction = (currAction + 1) % 4;
        switch(currAction)
        {
            case 0: // forward
                servo1_target = 0;
                servo2_target = 0;
                break;
            case 1: // backward
                servo1_target = 180;
                servo2_target = 180;
                break;
            case 2: // left
                servo1_target = 90;
                servo2_target = 0;
                break;
            case 3: // right
                servo1_target = 0;
                servo2_target = 90;
                break;
        }
    }
}
</code></pre>

<p>PS. I used the <code>write</code> function instead of the writeMicroseconds because you can <code>read</code> the value you wrote. If you really need to use <code>writeMicroseconds</code> (which is pretty useless in my opinion, since servo precision is less than 1°, so <code>write</code> is more than sufficient) just store the target as a <code>uint16_t</code> and store also the last set value (and use that instead of the <code>read</code> function)</p>
",3368201,0,0,58371171,The web is full of C/C++ tutorials on how to do If statements. So give it a shot and show the code if it doesn't work and let us see what we can do about it.,Actuator
361,2511,37007417,Inverse-Kinematics: How to calculate angles for servos of a robotic arm to reach all possible points in a canvas?,|trigonometry|robotics|inverse-kinematics|servo|,"<p>I have a robotic arm composed of 2 servo motors. I am trying to calculate inverse kinematics such that the arm is positioned in the middle of a canvas and can move to all possible points in both directions (left and right). This is an image of the system <a href=""https://i.stack.imgur.com/e5RX9.png"" rel=""nofollow"">Image</a>. The first servo moves  0-180 (Anti-clockwise). The second servo moves 0-180 (clockwise). </p>

<p>Here is my code:</p>

<pre><code>    int L1 = 170;
    int L2 = 230;
    Vector shoulderV;
    Vector targetV;
    shoulderV = new Vector(0,0);
    targetV = new Vector(0,400);


    Vector difference = Vector.Subtract(targetV, shoulderV);
    double L3 = difference.Length;
    if (L3 &gt; 400) { L3 = 400; }
    if (L3 &lt; 170) { L3 = 170; }

    // a + b is the equivelant of the shoulder angle
    double a = Math.Acos((L1 * L1 + L3 * L3 - L2 * L2) / (2 * L1 * L3));  
    double b = Math.Atan(difference.Y / difference.X);

   // S1 is the shoulder angle
   double S1 = a + b;
  // S2 is the elbow angle
  double S2 = Math.Acos((L1 * L1 + L2 * L2 - L3 * L3) / (2 * L1 * L2));

  int shoulderAngle = Convert.ToInt16(Math.Round(S1 * 180 / Math.PI));
  if (shoulderAngle &lt; 0) { shoulderAngle = 180 - shoulderAngle; }
  if (shoulderAngle &gt; 180) { shoulderAngle = 180; }
  int elbowAngle = Convert.ToInt16(Math.Round(S2 * 180 / Math.PI));

  elbowAngle = 180 - elbowAngle; 
</code></pre>

<p>Initially, when the system is first started, the arm is straightened with shoulder=90, elbow =0.
When I give positive x values I get correct results in the left side of the canvas. However, I want the arm to move in the right side as well. I do not get correct values when I enter negatives. What am I doing wrong? Do I need an extra servo to reach points in the right side?</p>

<p>Sorry if the explanation is not good. English is not my first language.</p>
",42493.61736,,976,1,2,0,0,5082242,,42190.47222,6,38584421,"<p>I suspect that you are losing a sign when you are using Math.Atan(). I don't know what programming language or environment this is, but try and see if you have something like this: </p>

<p>Instead of this line:</p>

<p><code>double b = Math.Atan(difference.Y / difference.X);</code></p>

<p>Use something like this: </p>

<p><code>double b = Math.Atan2(difference.Y, difference.X);</code></p>

<p>When <code>difference.Y</code> and <code>difference.X</code> have the same sign, dividing them results in a positive value. That prevents you from differentiating between the cases when they are both positive and both negative. In that case, you cannot differentiate between 30 and 210 degrees, for example. </p>
",679553,0,0,62953451,Have you tried using Denavit hartenberg convention to do the inverse kinematics calculation?,Actuator
362,2474,36738868,"Programming inverse kinematic solution provided only x,y,z of tool",|c++|robotics|inverse-kinematics|,"<p>I at moment trying to implement a inverse kinematic solution capable of finding all possible Q-states a robot can have given the position of the tool is x,y,z.. </p>

<p>I am choosen to do it using the least square approach, but something tells me that it won't provide all possible solution but just the one with smallest error, which in this case i am interested in all possible Q-states that fulfill the position of the tool. </p>

<p>My implementation looks as such. </p>

<pre><code>Eigen::MatrixXd jq(device_.get()-&gt;baseJend(state).e().cols(),device_.get()-&gt;baseJend(state).e().rows());
      jq = device_.get()-&gt;baseJend(state).e(); //Extract J(q) directly from robot


      //Least square solver - [AtA]⁻1AtB

      Eigen::MatrixXd A (6,6);
      A = jq.transpose()*(jq*jq.transpose()).inverse();



      Eigen::VectorXd du(6);
      du(0) = 0.1 - t_tool_base.P().e()[0];
      du(1) = 0 - t_tool_base.P().e()[1];
      du(2) = 0 - t_tool_base.P().e()[2];
      du(3) = 0;  // Should these be set to something if i don't want the tool position to rotate?
      du(4) = 0;
      du(5) = 0;

      ROS_ERROR(""What you want!"");
      Eigen::VectorXd q(6);
      q = A*du;


      cout &lt;&lt; q &lt;&lt; endl; // Least square solution - want a vector of solutions. 
</code></pre>

<p>first of all the inverse kin doesn't seem right as the Q-state doesn't move the robot to the desired position. I can't seem to see where my implementation is wrong?</p>
",42480.37847,36739131,762,1,7,0,,5585502,,42328.46528,287,36739131,"<p>Solving the inverse kinematics problem with numerical solution is not the best option. You should go for that option just in case you could not find the analytical solution. BTW, well designed robots should have a clear inverse kinematics model.</p>

<p>For your robot, here is the inverse kinematics model: <a href=""https://smartech.gatech.edu/bitstream/handle/1853/50782/ur_kin_tech_report_1.pdf"" rel=""nofollow"">https://smartech.gatech.edu/bitstream/handle/1853/50782/ur_kin_tech_report_1.pdf</a></p>
",4523099,0,1,61060329,"Analytical solution will provide you with the all solution without any epsilon error. If you are using a well-known robot, you should find a ready model on the internet or even in the manual of the robot.",Actuator
363,2439,34964928,CMUcam5 Pixy on Raspberry 2,|camera|raspbian|raspberry-pi2|robotics|,"<p>I just got my Raspberry B and a new CMUcam 5 from Pixy.
I follow this tutorial:
<a href=""http://cmucam.org/projects/cmucam5/wiki/Hooking_up_Pixy_to_a_Raspberry_Pi"" rel=""nofollow"">Hooking up Pixy to a Raspberry Pi</a></p>

<p>However, once I plug-in the camera to the RB the servos keep on doing some noise like trying to move even when they are at their top.
The LED light keeps on flashing.
The worst part is that for some reason the mouse and keyboard are no longer connected until I click camera button.
Once I have the keyboard back and I try to run the hello_pixy script I get an error:</p>

<pre><code>Hello Pixy:
libpixyusb: Version: 0.4
pixy_init(): USB ERROR: Target not found.
</code></pre>

<p>Am I missing something? Is something missing?
All suggestions are welcome</p>

<p>Thanks!</p>
",42392.62153,34998166,532,1,1,0,,2011375,,41299.62153,79,34998166,"<p>The issue is the power the servos need to move.
This was a hardware issue rather than a software one. 
Adding a simple powered-usb hub did the trick.</p>
",2011375,0,0,57724185,"Well, doing some test I found that the issue was not about the software but the hardware. Seems like the servos need extra power that is beyond the simple USB cable. I added a powered-uss hub and now is working just fine.",Incoming
364,2556,38659146,How can I use gyro or encoders for robot moving in straight line?,|c++|raspberry-pi2|gyroscope|robotics|encoder|,"<p>I've recently succeeded in building my Autonomous robot with DC motors, and it works well. However, it doesn't move in a straight line yet, when it should. I'm now studying which method should I implement to make the robot go straight. I pretty much understand how to use the encoders, but I'm not sure about the gyro. I had written a program for straight motion using encoder, but it's not moving straight exactly because of front brush speed, for further improvement I have decided to use gyro, If I use gyro possible to make straight motion ? or else any suggestion ?</p>
",42580.52778,,1112,1,1,0,,,,,,38661450,"<p>First, make sure you conceptually understand what it means for a robot to drive in a straight line. You will not get a robot that moves perfectly in a straight line. You may get one to move straight-enough (perhaps more perfect than humans can determine) but there will always be error.</p>

<p>A gyroscope may not be able to determine you're going off course if it's gradual enough and depending on the quality of your gyro it may have a buildup of drift, causing it to think it's turning ever-so-slightly when it's sitting perfectly still. The more expensive of a gyro you get, the less this will be, but still.</p>

<p>Even if you assume perfect sensors, there is still a difference between ""driving straight"" and ""keeping on the straight and narrow"". Imagine you had coded a robot to drive such that it tried to keep its bearing as consistent as possible (drive straight). If, while it was moving you knocked it a bit, it would swerve to the side some and then correct itself to the original angle. However, it would be on a different path. Sure, the path would be parallel to the original one, but it would not be the same path.</p>

<p>Then there's the option of having it try to figure out just how far off the beaten path it was pushed and try to get back on it. That'll either take a constant reference (as a line follower robot would do) or more sensors (like a 3D gyro and 3D accelerometer).</p>

<p>That second option sounds a bit more than what you're doing, so here's the first option done in no particular robotics framework:</p>

<pre><code>//initialize
double angle = gyro.get_heading_degrees();
//...
//logic code that may be looped or fired by events
{
  //this is our error; how far off we are from our target heading
  double error = angle - gyro.get_heading_degrees() - 360;

  drive_system.drive_arcade(error / 180, 1);
}
</code></pre>

<p>This assumes driving in an arcade fashion; you can adapt it to tank drive or swerve drive or mecanum drive or H-drive or...</p>

<p>The '1' is just a speed</p>

<p>The '-360' and '180' are values to reduce the angle to a value between -1 and 1. If your drive method uses a different range to determine angle, that'll have to be adapted.</p>

<p>Finally, this example isn't foolproof, but it should get you thinking about how to correct for errors when you've detected them.</p>
",5344725,0,0,64700281,Could you show anything that helps readers understand what you are trying to achieve and how?,Moving
365,2627,41411217,publishing trajectory_msgs/jointtrajectory msgs,|ros|robotics|,"<p>When i set the position and velocities of the joints in the trajectory msgs i got an error: \</p>

<pre><code>[state_publisher-2] process has died [pid 13362, exit code -11, cmd /home/rob/catkin_ws/devel/lib/r2d2/state_publisher __name:=state_publisher __log:=/home/rob/.ros/log/9980f352-cf74-11e6-8644-d4c9efe8bd37/state_publisher-2.log].
log file: /home/rob/.ros/log/9980f352-cf74-11e6-8644-d4c9efe8bd37/state_publisher-2*.log
</code></pre>

<p>My ros node to send geometry_msgs is:</p>

<pre><code>#include &lt;string&gt;
    #include &lt;ros/ros.h&gt;
    #include &lt;sensor_msgs/JointState.h&gt;
    #include &lt;tf/transform_broadcaster.h&gt;
    #include &lt;trajectory_msgs/JointTrajectory.h&gt;
    #include &lt;vector&gt;
    int main(int argc, char** argv) {
        ros::init(argc, argv, ""state_publisher"");
        ros::NodeHandle n;
        ros::Publisher joint_pub = n.advertise&lt;trajectory_ms

gs::JointTrajectory&gt;(""set_joint_trajectory"", 1);
   ros::Rate loop_rate(30);

   const double degree = M_PI/180;

   // robot state
   double tilt = 0, tinc = degree, swivel=0, angle=0, height=0, hinc=0.005;

   // message declarations
   trajectory_msgs::JointTrajectory joint_state;
   std::vector&lt;trajectory_msgs::JointTrajectoryPoint&gt; points_n(3);
   points_n[0].positions[0] = 1; points_n[0].velocities[0]=10;
   while (ros::ok()) {
       //update joint_state
       joint_state.header.stamp = ros::Time::now();
       joint_state.joint_names.resize(3);
       joint_state.points.resize(3);

       joint_state.joint_names[0] =""swivel"";
       joint_state.points[0] = points_n[0];
       joint_state.joint_names[1] =""tilt"";
       joint_state.points[1] = points_n[1];
       joint_state.joint_names[2] =""periscope"";
       joint_state.points[2] = points_n[2];


       joint_pub.publish(joint_state);



       // This will adjust as needed per iteration
       loop_rate.sleep();
   }


   return 0;
   }
</code></pre>

<p>Here when i donot set the position and velocity value it runs without error and when i run <code>rostopic echo /set_joint_trajectory</code> i can clearly see the outputs as all the parameters of points is 0. I also tried below program but it published nothing:</p>

<pre><code> #include &lt;string&gt;
    #include &lt;ros/ros.h&gt;
    #include &lt;sensor_msgs/JointState.h&gt;
    #include &lt;tf/transform_broadcaster.h&gt;
    #include &lt;trajectory_msgs/JointTrajectory.h&gt;
    #include &lt;vector&gt;
    int main(int argc, char** argv) {
        ros::init(argc, argv, ""state_publisher"");
        ros::NodeHandle n;
        ros::Publisher joint_pub = n.advertise&lt;trajectory_msgs::JointTrajectory&gt;(""set_joint_trajectory"", 1);

        trajectory_msgs::JointTrajectory joint_state;

           joint_state.header.stamp = ros::Time::now();
           joint_state.header.frame_id = ""camera_link"";
           joint_state.joint_names.resize(3);
           joint_state.points.resize(3);

           joint_state.joint_names[0] =""swivel"";
           joint_state.joint_names[1] =""tilt"";
           joint_state.joint_names[2] =""periscope"";

           size_t size = 2;
           for(size_t i=0;i&lt;=size;i++) {
              trajectory_msgs::JointTrajectoryPoint points_n;
              int j = i%3;
              points_n.positions.push_back(j);
              points_n.positions.push_back(j+1);
              points_n.positions.push_back(j*2);
              joint_state.points.push_back(points_n);
              joint_state.points[i].time_from_start = ros::Duration(0.01);
           }
           joint_pub.publish(joint_state);
           ros::spinOnce();
       return 0;
   }
</code></pre>
",42735.81597,41413111,3654,1,0,0,,,,,,41413111,"<p>You are accessing <code>points_n[0].positions[0]</code> and <code>points_n[0].velocities[0]</code> without allocating the memory for positions and velocities. Use</p>

<pre><code>...
// message declarations
trajectory_msgs::JointTrajectory joint_state;
std::vector&lt;trajectory_msgs::JointTrajectoryPoint&gt; points_n(3);
points_n[0].positions.resize(1);
points_n[0].velocities.resize(1);
...
</code></pre>

<p>then set the values or use <code>points_n[0].positions.push_back(...)</code> instead. The same applies to <code>points_n[1]</code> and <code>points_n[2]</code>.</p>

<p>In your second example it looks like your program terminates before anything is sent. Try to publish repeatedly in a while-loop with </p>

<pre><code>while(ros::ok()){ 
  ...
  ros::spinOnce();
}
</code></pre>
",1563315,0,0,,,Actuator
366,2580,39838761,Robotics library in Forth?,|robotics|forth|,"<p>I have read the documentation for the Roboforth environment from <a href=""http://www.strobotics.com/"" rel=""nofollow"">STrobotics</a> and recognized that this a nice way for programming a robot. What I missed is a sophisticated software library with predefined motion primitives. For example, for picking up a object, for regrasping or for changing a tool.</p>

<p>In other programming languages like Python or C++, a library is a convenient way for programming repetitive tasks and for storing expert knowledge into machine-readable files. Also a library is good way for not-so-talented programmers to get access on higher-level-functions. In my opinion Forth is the perfect language for implementing such an API, but I didn't find information about it. Where should I search? Are there any examples out there?</p>
",42646.79861,39915796,333,1,0,1,,6904362,,42643.46597,301,39915796,"<p>I am author of RoboForth, and you make a good point. I have approached the problem of starting off new users with videos on YouTube; see <a href=""https://www.youtube.com/playlist?list=PLBcXpOAbOrG3pnlRlJDISQmWOVJP9xX3L"" rel=""nofollow"">How to...</a> (playlist with 6 items, e.g ""ST Robotics How-to number 1 - getting started"") which is a playlist covering basics and indeed tool changing.</p>

<p>I never wrote any starter programs, because the physical positions (coordinates) would be different from one user to the next, however I think it can be done, and I will do it. Thanks for the heads up.</p>
",6936882,4,4,,,Specifications
367,2553,38101097,Disjoint Movement of Joints of Aldebaran Nao,|c++|robotics|nao-robot|servo|,"<p>I am working on a locomotion system for the Aldebaran Nao.  I noticed that my robot's motions are very disjoint as compared to those on other robots - a problem that I am pretty sure is code related.</p>

<p>I am updating my robots motions using code similar to Aldebaran's fast get set DCM.</p>

<p>(<a href=""http://doc.aldebaran.com/1-14/dev/cpp/examples/sensors/fastgetsetdcm/fastgetsetexample.html"" rel=""nofollow"">http://doc.aldebaran.com/1-14/dev/cpp/examples/sensors/fastgetsetdcm/fastgetsetexample.html</a>).</p>

<p>I am updating the joint angles every 10 ms (the fastest possible update rate).  However, it is clear that the motors move to the newly commanded angle very quickly and are motionless for the majority of the 10 ms.  Is there any way to control the velocity of the motors during this 10ms update period?</p>
",42550.5625,,136,1,1,-1,,5428956,,42286.79028,10,38392539,"<p>There's many way to send order to joints using DCM or ALMotion.</p>

<p>The easiest way is using ALMotion, with that you can use </p>

<ul>
<li>angleInterpolationWithSpeed, where you'll specify a ratio of speed.</li>
<li>angleInterpolation, where you'll specify a time. In this example 0.01 sec.</li>
</ul>

<p>Using the DCM, you just have to ask for the movement to finish 10ms later.</p>

<p>The ALMotion and DCM documentation are well wrotten, you should have a look...</p>
",1081418,1,0,94966882,"SO won't let me comment so I have to use ""answer"". You can't directly control the current inflow of Nao's motors. In my experience 10 ms is a very fine time step already - 100 frames of motion per second and should not appear ""disjoint"" to naked eyes given that your entire motion is smooth. Could you elaborate a little on what you are trying to achieve? Did you interpolate / smooth the actuator positions with respect to time?",Actuator
368,2577,39719830,Public Key is not available,|raspberry-pi|raspbian|ros|robotics|,"<p>I am trying to install ROS Kinetic on the Raspberry Pi 3 Model B running Raspbian GNU7Linux 8 (Jessie) following these <a href=""http://wiki.ros.org/ROSberryPi/Installing%20ROS%20Kinetic%20on%20the%20Raspberry%20Pi"" rel=""nofollow"">steps</a>.</p>

<p>Setting up the repositories I get this output:</p>

<pre><code>Executing: gpg --ignore-time-conflict --no-options --no-default-keyring --homedir /tmp/tmp.vAO4o1tMMY --no-auto-check-trustdb --trust-model always --keyring /etc/apt/trusted.gpg --primary-keyring /etc/apt/trusted.gpg --keyserver hkp://ha.pool.sks-keyservers.net:80 --recv-key 0xB01FA11
</code></pre>

<p>And when trying to run a sudo apt-get update I get this error:</p>

<pre><code>W: GPG error: http://packages.ros.org jessie InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 5523BAEEB01FA116
</code></pre>

<p>Anyone had this problem adding a public key?</p>
",42640.35486,,6019,2,0,4,0,6886568,Berlin,42640.34444,79,39721706,"<p>Solved it.</p>

<p>This manually adds the key:</p>

<pre><code>sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys &lt;key number&gt;    

sudo apt-get update
</code></pre>
",6886568,8,0,,,Other
369,2788,44123750,How can I calculate pitch value with gyroscope in MPU9250 of Navio2 (python2),|python|raspberry-pi|sensors|gyroscope|robotics|,"<p>I can obtain raw values of gyro sensor.</p>

<p>But, I don't know how to calculate the pitch value with these values.</p>

<p>I want to get pitch value between -180 and +180.</p>

<p>This is because I want to obtain a precise pitch value using a complementary filter with a pitch value obtained with an accelerometer and a pitch value obtained with a gyroscope.</p>

<p>Please help me.</p>

<hr>

<p><strong>My Code (obtain raw values of gyro sensor)</strong></p>

<pre><code>import spidev
import time
import argparse
import sys
import navio.mpu9250
import navio.util

navio.util.check_apm()

imu = navio.mpu9250.MPU9250()
imu.initialize()

m9a, m9g, m9m = acc.imu.getMotion9()

#These are raw values of gyroscope
gyro_x = m9g[0]
gyro_y = m9g[1]
gyro_z = m9g[2]
</code></pre>

<hr>

<p><strong>Detail(setting)</strong></p>

<p><em>gyro_scale = 2000DPS</em></p>

<p><em>gyro_divider = 16.4</em></p>

<p>'raw values of gyro sensor' = (PI/180)*data/gyro_divider</p>

<p><a href=""https://i.stack.imgur.com/IfzqR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IfzqR.png"" alt=""enter image description here""></a></p>

<hr>

<p><a href=""https://github.com/emlid/Navio2/blob/master/Python/navio/mpu9250.py#L350"" rel=""nofollow noreferrer""><b>Reference code.</b></a></p>

<pre><code>...

def getMotion9(self):
    self.read_all()
    m9a = self.accelerometer_data
    m9g = self.gyroscope_data
    m9m = self.magnetometer_data

    return m9a, m9g, m9m

...

def read_all(self):
    # Send I2C command at first
    # Set the I2C slave addres of AK8963 and set for read.
    self.WriteReg(self.__MPUREG_I2C_SLV0_ADDR, self.__AK8963_I2C_ADDR | self.__READ_FLAG)
    # I2C slave 0 register address from where ; //Read 7 bytes from the magnetometerto begin data transfer
    self.WriteReg(self.__MPUREG_I2C_SLV0_REG, self.__AK8963_HXL)
    # Read 7 bytes from the magnetometer
    self.WriteReg(self.__MPUREG_I2C_SLV0_CTRL, 0x87)
    # must start your read from AK8963A register 0x03 and read seven bytes so that upon read of ST2 register 0x09 the AK8963A will unlatch the data registers for the next measurement.

    # time.sleep(0.001)
    response = self.ReadRegs(self.__MPUREG_ACCEL_XOUT_H, 21);

    # Get Accelerometer values
    for i in range(0, 3):
        data = self.byte_to_float(response[i*2:i*2+2])
        self.accelerometer_data[i] = self.G_SI*data/self.acc_divider

    # Get temperature
    i = 3
    temp = self.byte_to_float(response[i*2:i*2+2])
    self.temperature = (temp/340.0)+36.53

    # Get gyroscope values
    for i in range(4, 7):
        data = self.byte_to_float(response[i*2:i*2+2])
        self.gyroscope_data[i-4] =(self.PI/180)*data/self.gyro_divider

    # Get magnetometer values
    for i in range(7, 10):
        data = self.byte_to_float_le(response[i*2:i*2+2])
        self.magnetometer_data[i-7] = data * self.magnetometer_ASA[i-7]
</code></pre>
",42877.99792,,1307,0,3,0,0,6694663,,42591.36111,11,,,,,,75271788,"Does that mean that the angle at which sampling starts should be zero degrees?
what if the actual sampling start angle is not 0° C ?",Coordinates
370,2634,41604310,What is wpilibj 2017 replacement method for initDigitalPort?,|java|robotics|,"<p>I've come into a little problem with some legacy code from the 2016 control system. I'm trying to control the adis16448 board with <a href=""https://github.com/juchong/ADIS16448-RoboRIO-Driver/blob/master/Java/com/analog/adis16448/frc/ADIS16448_IMU.java"" rel=""nofollow noreferrer"">this library</a>
which compiled fine in the 2016 wpilibj, but doesn't compile in the 2017 version. Now, I'd like to get this up and running quickly without having to wait for the dev to update, and there are actually only two errors.
Relevant code here:</p>

<pre><code>private static class InterruptSource extends DigitalSource {
    public InterruptSource(int channel) {
      initDigitalPort(channel, true);
    }
}
</code></pre>

<p>First is that the <code>InterruptSource</code> class has some unimplemented methods from the parent class. I just added empty definitions for these and that error obviously went away. Next is that the method <code>initDigitalPort</code> is not defined from the parent class. This is the part that I get stuck on. </p>

<p>Upon examination of the API Javadoc, the Source Code on github, and the context of this code, I still can't seem to figure out what this does or how to fix it. I'm guessing this has been depreciated in the 2017 wpilibj library. </p>

<p>My question is, what is the replacement method for initDigitalPort? </p>

<p>Forgive me for anything simple I've overlooked, we are a new FRC team so we have 0 experience with using wpilibj. </p>

<p>Also, it might help if I understood what the DigitalSource class actually does, it seems to involve encoders but that can't be right since this board has none. Could someone explain this to me?</p>

<p>Thanks, help is greatly apreciated!</p>
",42747.11597,41819137,43,2,0,0,,4484072,"Orange County, CA, United States",42026.82083,725,41819137,"<p>The library in question has now been updated as of <a href=""https://github.com/juchong/ADIS16448-RoboRIO-Driver/commit/077bb65af2c325d30e105bc4ffdc3d79322e8786"" rel=""nofollow noreferrer"">this commit</a>. The new class is called <code>DigitalInput</code> and the <code>initDigitalPort</code> method is called in the constructor of this class which is given the parameter for the port. </p>

<p><strong>Ex:</strong></p>

<pre><code>public InterruptSource(int channel) {
       initDigitalPort(channel, true);
}
</code></pre>

<p>can be subsituted with </p>

<pre><code>DigitalInput m_interrupt = new DigitalInput(10)
</code></pre>

<p>and will provide nearly the same functionality including class structure and methods. </p>
",4484072,0,0,,,Error
371,2763,43416538,How to increase the speed of transfer of data from android to arduino in bluetooth?,|android|bluetooth|arduino|robotics|arduino-ide|,"<p>I'm trying to use an android app to do the processing of a path finding algorithm for a robot using Bluetooth. But currently, it takes 1 or 2 seconds for the transfer to complete, so that there is an output in the Arduino. Is there a way to minimise this to make the transfer-output instant?</p>

<p>This kind of delay is causing problems such as stopping instantly when an obstacle is detected. Is there any better way of doing this?
Thanks in advance!</p>
",42839.73889,44488390,1677,3,0,0,,6552687,,42556.69444,29,43624927,"<p>Simple answer: You can't, bluetooth is laggy like that. If you instead had your path finding algorithm on the arduino board itself, you could avoid the issue. You can also try adding a delay to your arduino code, because it is possible that the arduino is sending messages repeatedly without taking into account the lag that bluetooth has.</p>
",7701649,0,4,,,Remote
372,2731,42674680,I need to run parallel functions that keep track of variables that are in a buffer.,|python|class|robotics|,"<p>Here's my dilemma. I've developed some code I want to modularize but I'm stuck on what should not be a difficult issue.</p>

<p>My code first does something like this:</p>

<pre><code>While True:
     value = getsensorvalue()
     values = deque(maxlen=10)

     # Get the mode value of the sensor readings
     Values = np.array(list(values))
     u, indices = np.unique(Values , return_inverse=True)
     mode = u[np.argmax(np.bincount(indices))]
     print mode      
</code></pre>

<p>Basically, I'm reading a sensor value continuously, I'm storing the values in a doubly linked list (import deque from collections). This is a simplified version of my program, but basically, it's returning the mode value of the queue we formed from our sensor readings.</p>

<p>What I want to do is create this into a class or function that can be used to read multiple sensors. So something like this:</p>

<pre><code>While True:
    GetSensorA()
    GetSensorB()
    GetSensorC()
    GetSensorD()
</code></pre>

<p>With all of the above having different queue lengths (buffer sizes) etc. </p>

<p>This has to be quite simple. I'm just a scientists who's self taught in programming. Not sure but I feel as if there is some basic programming principle I'm missing here.</p>
",42802.62361,,32,0,4,0,,6432363,,42527.87014,18,,,,,,72480466,"Sure, my apologies i should have elaborated.",Timing
373,2756,43223123,Kalman Filter Prediction Implementation,|algorithm|prediction|robotics|kalman-filter|,"<p>I am trying to implement a Kalman filter in order to localize a robot.
I am confused with the prediction step (excluding process noise) x = Fx + u</p>

<p>If x is a state estimation vector: [xLocation, xVelocity] and F is the state transition matrix [[1 1],[0 1]], then the new xLocation would be equal to xLocation + xVelocity + the corresponding component of the motion vector u.</p>

<p>Why is the equation not x = x + u?  Shouldn't the predicted location of the robot be the location + motion of the robot?</p>
",42830.25694,43223710,1292,1,3,0,0,7818382,"Cedar Park, TX, United States",42830.23819,5,43223710,"<p>Maybe there is some confusion with respect to what the matrices actually represent.</p>

<p>The ""control vector"", <em>u</em>, might be the acceleration externally applied to the system.</p>

<p>In this case, I would expect the equations to look like this:</p>

<p>x<sub>location</sub> = x<sub>location</sub> + x<sub>velocity</sub></p>

<p>x<sub>velocity</sub> = x<sub>velocity</sub> + u<sub>velocity</sub></p>

<p>These two equations assume that the update is applied every 1 second (otherwise some ""delta time"" factors would need to be applied and included the transition matrix and the control vector).</p>

<p>For the situation mentioned above, the matrices and vectors are:</p>

<ul>
<li><p>The state vector (column vector with 2 entries):</p>

<p>x<sub>location</sub></p>

<p>x<sub>velocity</sub></p></li>
</ul>

<hr>

<ul>
<li><p>The transition matrix (2 x 2 matrix):</p>

<p>1 1</p>

<p>0 1</p></li>
</ul>

<hr>

<ul>
<li><p>The control vector (column vector with 2 entries):</p>

<p>0</p>

<p>u<sub>velocity</sub></p></li>
</ul>

<hr>

<p><a href=""http://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/"" rel=""nofollow noreferrer"">This link</a> contains nice explanations and visualizations for the Kalman Filter.</p>
",6184684,1,0,73517391,"I think this question is better suited for http://math.stackexchange.com/ as you don't seem to have a problem implementing this algorithm, but you have a problem understanding the algorithm.",Coordinates
374,2787,44098251,How to get closest point on cubic spline for Robotics project?,|python|matplotlib|scipy|robotics|cubic-spline|,"<p>I've got a robot and I want to make this robot follow a predefined path in the form of an eight figure on a field outside. The robot is not easy to steer and there are some external factors such as wind which make it very likely that the robot will not follow the exact path so it will often be next to the path. </p>

<p>The path I want to make it follow was formed by defining five points and create a line over those points using a cubic spline (the code I used to define this path is below this message):</p>

<p><a href=""https://i.stack.imgur.com/G4is2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G4is2.png"" alt=""At any given moment I want to be able""></a> </p>

<p>I always want to be able to supply the robot a point to steer to which is on the cubic spline line. To be able to do this I thought the easiest way is to:</p>

<ol>
<li>Calculate the nearest point on the cubic spline from the current location of the robot</li>
<li>Advance <code>0.2</code> units along the cubic spline line to determine the new waypoint for the robot to aim at.</li>
</ol>

<p>For example, if the location of the robot in the grid above is <code>x=0.4, y=-0.5</code>, the nearest point is approximately <code>x=0.4, y=-0.28</code> and the new waypoint would be approximately <code>x=0.22, y=-0.18</code>:</p>

<p><a href=""https://i.stack.imgur.com/t1MQd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t1MQd.png"" alt=""enter image description here""></a></p>

<p>Now I've got three questions:</p>

<ol>
<li>How do I find the nearest point on the cubic spline?</li>
<li>How do I ""advance"" 0.2 units from the found point on the cubic spline?</li>
<li>How do I stay on the given path, even when the path crosses itself in the middle?</li>
</ol>

<p>All tips are welcome!</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
from scipy import interpolate

x = [-0.5, 0, 0.5, 0.5,  0, -0.5,  -0.5, 0, 0.5]
y = [0.25, 0, -0.25, 0.25, 0, -0.25, 0.25, 0, -0.25]

tck, u = interpolate.splprep([x, y], s=0)
unew = np.arange(0, 1.01, 0.01)
out = interpolate.splev(unew, tck)

plt.figure()
plt.plot(x, y, 'o', out[0], out[1])
plt.legend(['Predefined points', 'Cubic Spline'])
plt.axis([-0.75, 0.75, -0.75, 0.75])
plt.show()
</code></pre>
",42876.61944,,1083,1,0,1,,1650012,,41157.79722,2447,44104308,"<blockquote>
  <p>Now I've got three questions:</p>
  
  <p>How do I find the nearest point on the cubic spline? How do I
  ""advance"" 0.2 units from the found point on the cubic spline? How do I
  stay on the given path, even when the path crosses itself in the
  middle?</p>
</blockquote>

<p>I believe a help would be to take advantage of the spline array index as a navigaion dimension    </p>

<p>with no initial info, a crude but not that slow startup step would be to simply find the min distance to the whole spline x, y trace, <code>out</code> - many SE ans for finding closest</p>

<p>then use that point's index as a state variable, and along with direction (increasing or decreasing <code>out</code> index) find the next index you want to head for</p>

<p>if you have started up, are close, and know your last index position (or estimated target point's index), then you could just search for ""closest"" in a slice near (in whatever is the current ""forward"" direction) your internal state index, updating your internal out index as you step...</p>

<p>with state involved I would consider a OOP Robot Class with XY position, heading, move magnitude, out spline index estimate, index direction</p>

<p>and some fun to be had with actual programming</p>

<p><strong>[edits:]</strong><br>
1st pass at ""closest""    </p>

<pre><code># out is a list of arrays, covert to 2d array
aout=np.array(out)

tpoint = np.array([[0.5],[-0.7]])

diff = aout-tpoint

sd = diff[0]*diff[0] + diff[1]*diff[1]  # squared distance

np.min(sd)
cpi=np.where(sd&lt;=np.min(sd)+0.00001)[0]
plt.plot((aout[0, cpi],tpoint[0]),( aout[1, cpi], tpoint[1]), linewidth=2)
</code></pre>

<p>just checking if out index is srictly montonic/cyclic - and it is, but the linspace gives more than one exact cycle, I eyeballed ~ 75 points for a full cycle:
   # animate plotting of spline points by index</p>

<pre><code>#import numpy as np
#import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

fig1, ax = plt.subplots()
xdata, ydata = [], []
ln, = plt.plot([], [], 'ro', animated=True)

def update(frame):
    xdata.append(aout[0][frame])
    ydata.append(aout[1][frame])
    ln.set_data(xdata, ydata)
    return ln,


def init():
    ax.set_xlim(-1, 1)
    ax.set_ylim(-1, 1)
    return ln,

data = np.array(out) #np.random.rand(2, 25)
l, = plt.plot([], [], 'r-')
plt.xlim(0, 1)
plt.ylim(0, 1)
ani = FuncAnimation(fig, update,
                              frames=np.array(range(74)),
                               init_func=init, blit=True)
plt.show()
</code></pre>
",6876009,1,0,,,Moving
375,2919,46748212,What is the best and simplest file type to use to read and run a pick and place program c#?,|c#|csv|robotics|,"<p>I´m working on making a C# program to control a 3-axis pick and place machine and trying to figure out which type of file type I want to use to read and run the pick and place program.
The program would look something like this:</p>

<p>Move X to 999.9
Wait for input 1
Move Y to 1.0
Set output 1
And simple things like that.</p>

<p>So I figure that in a CSV file it would look like this:</p>

<pre><code>move,x,999.9
wait,in,1
move,y,1.0
set,out,1
</code></pre>

<p>And then I would have to check whats in the first column in the first line, if its move then check which axis in the second column then where to move in the last column.</p>

<p>Would that be the simplest way or should I look for something else?
Later I would also like to implement IF statements in the program.</p>
",43022.7875,,85,3,0,0,,8418682,,42951.72361,11,46748322,"<p>There surely are other ways to do this, but I would do it this way (just a  <strong>basic</strong> concept, you have to implement exception handling, unknown commands, illegal parameters and stuff like that...)</p>

<p>Command definition:  </p>

<pre><code>interface ICommand
{
    object Context { get; set; }
    bool IsResponsible(string line);
    void Execute(string line);
}
public class Move: ICommand
{
    public bool IsResponsible(string line)
    {
        return line.StartsWith(""move"");
    }
    public void Execute(string line)
    {
        var tmp = line.Split(',');
        // Validate and verify parameters here
        // code here to move the context to x/y 999.9
        // Context.Move(tmp[1], Convert.ToDouble(tmp[2]);
    }
}
public class Wait: ICommand
{
    public bool IsResponsible(string line)
    {
        return line.StartsWith(""wait"");
    }
    public void Execute(string line)
    {
        var tmp = line.Split(',');
        // code here to wait...
    }
}
</code></pre>

<p>Execution:  </p>

<pre><code>public void RunFile()
{
    var ctx = new YourContext();
    var commands = new List&lt;IMyCommand&gt;();
    // You could look for all ICommand implementations here with reflection or just hard-code all known classes...
    command.Add(new Move() { Context = ctx; });
    command.Add(new Wait() { Context = ctx; });

    var lines = File.ReadAllLines(""your-file.txt"");
    foreach(var line in lines)
    {
        var cmd = commands.FirstOrDefault(x =&gt; x.IsResponsible(line));
        if (cmd == null)
            throw new IOException(""unknown command!"");
        cmd.Execute(line);
    }
}
</code></pre>

<p><strong>not tested</strong></p>
",8400446,0,1,,,Actuator
376,2978,47970583,What is the future of filtering methods vs incremental-SFM in visual-SLAM,|robotics|kalman-filter|slam-algorithm|,"<p>In the Visual SLAM area, there's the well-known solution of <strong>EKF/UKF/Particle-SLAM</strong> , like the ""mono-slam"".</p>

<p>Recently , there was a direction to <strong>Local Bundle Adjustment</strong> methods , like <em>lsd-slam or orb-slam</em> ..</p>

<p>My question is : </p>

<blockquote>
  <p>Do the filtering ways still have a future or steady usage? in what applications? what are the pros/cons?</p>
</blockquote>

<p>I read these papers but, I failed to extract a final answer,(mostly out of misunderstanding):</p>

<ol>
<li><p><a href=""https://www.doc.ic.ac.uk/~ajd/Publications/strasdat_etal_ivc2012.pdf"" rel=""nofollow noreferrer"">Visual SLAM: why filter?</a></p></li>
<li><p><a href=""https://arxiv.org/pdf/1606.05830"" rel=""nofollow noreferrer"">Past, Present, and Future of Simultaneous Localization and Mapping </a></p></li>
</ol>

<p><strong>P. S.</strong>: I know the first is saying that Local BA is better somehow, and the second rarely mentioned filtering, so.., that's it.. , is it the end of the awesome Kalman filter in Visual-SLAM area?!!</p>
",43094.71458,47970866,842,2,0,4,0,6087307,Germany,42448.74583,166,47970692,"<p>No, the second paper does not describe the end of the Kalman filter in Visual-Slam.  The Kalman filter is a special case of the Maximum Likelihood Estimator for Gaussian noise.  I want to direct your attention to Page 4, paragraph 3 of the second paper.  There, the authors should clarify that the Kalman Filter and MAP are both extensions of Maximum Likelihood Estimation.  As written, that insight is only implicit.</p>
",,1,0,,,Moving
377,2766,43429195,Arduino IDE error: cannot declare variable <object> to be of abstract type <class>,|c++|arduino|accelerometer|sensors|robotics|,"<p>I can't figure out this error.  I initially copied from CurieIMU.h (which builds ok) to ashIMU.h...</p>

<p>Error is this:
ashIMU.h:17: error: cannot declare variable 'ashIMU' to be of abstract type 'ashIMUClass'</p>

<p>My sketch ARDUINO_LED_DEMO.ino .................</p>

<pre><code>#include ""ashIMU.h""
...
</code></pre>

<p>ashIMU.h..................</p>

<pre><code>        #ifndef ASH_IMU_API_H
        #define ASH_IMU_API_H

        #include ""ash_BMI160.h""

        class ashIMUClass : public ashBMI160Class {

            public:
                bool begin(void);

                void setAccelerometerRange(int range);
        };

    extern ashIMUClass ashIMU;

#endif // ASH_IMU_API_H
</code></pre>

<p>ash_BMI160.h .....................................
This is just BMI160.h but with class name changed from ""BMI160Class"" to ""ashBMI160Class"".</p>

<pre><code>...

class ashBMI160Class {
    public:

...

};
</code></pre>
",42840.73333,,1177,1,4,0,,746100,"Silicon Valley, California, USA",40411.54306,1922,43437712,"<p>If your <code>ashBMI160Class</code> is the same as the original <code>BMI160Class</code> from the CurieIMU library, then you are missing the implementation of pure virtual function:</p>

<p><code>virtual int serial_buffer_transfer(uint8_t *buf, unsigned tx_cnt, unsigned rx_cnt) = 0;</code></p>
",4760587,0,0,73932119,"If nothing fishy is going on, it simply means that your class has a pure virtual method. The class of which you'd like to create an instance needs to implement that method.",Programming
378,2781,43693218,Controlling Arduino Braccio arm using data from processing,|arduino|processing|robotics|,"<p>I am working on a project that uses processing to read data from a file and then sends these data to an arduino Uno board using serial, then supply these values to the arduino braccio Braccio.ServoMovment() function to move the robotic arm according to them. however whenever I run the code I keep getting wrong values into the function and the arm moves in a weird way, I have tested the same code and tried to light up an led and it worked fine for some reason.</p>



<pre class=""lang-java prettyprint-override""><code> import processing.serial.*;
 import java.io.*;
 int mySwitch=0;

 int counter=0;

 String [] subtext;

 Serial myPort;



 void setup(){

 //Create a switch that will control the frequency of text file reads.

 //When mySwitch=1, the program is setup to read the text file.

 //This is turned off when mySwitch = 0

 mySwitch=1;


 //Open the serial port for communication with the Arduino

 //Make sure the COM port is correct

 myPort = new Serial(this, ""COM5"", 9600);
  myPort.bufferUntil('\n');
 }

 void draw() {
  // print("" the switch "" + mySwitch);

  if (mySwitch&gt;0){
  /*The readData function can be found later in the code.
  This is the call to read a CSV file on the computer hard-drive. */
  readData(""C:/Users/Tareq/Desktop/data.txt"");

  /*The following switch prevents continuous reading of the text file,           until
  we are ready to read the file again. */
  mySwitch=0;
  }

  /*Only send new data. This IF statement will allow new data to be sent      to
  the arduino. */
  if(counter&lt;subtext.length){
  /* Write the next number to the Serial port and send it to the Arduino 
  There will be a delay of half a second before the command is
  sent to turn the LED off : myPort.write('0'); */
 // print(subtext[counter]);
  delay(5000);
  myPort.write(subtext[counter]);
  print(subtext[counter]);
  delay(2000);
  //Increment the counter so that the next number is sent to the arduino.
  counter++;
  } else{
  //If the text file has run out of numbers, then read the text file again      in 5 seconds.
  delay(5000);
  mySwitch=1;
  }
 } 


 /* The following function will read from a CSV or TXT file */
 void readData(String myFileName){

  File file=new File(myFileName);
  BufferedReader br=null;

  try{
  br=new BufferedReader(new FileReader(file));
  String text=null;

  /* keep reading each line until you get to the end of the file */
  while((text=br.readLine())!=null){

  /* Spilt each line up into bits and pieces using a comma as a separator */
  subtext = splitTokens(text,"","");

  }
  }catch(FileNotFoundException e){
  e.printStackTrace();
  }catch(IOException e){
  e.printStackTrace();
  }finally{
  try {
  if (br != null){
  br.close();
  }
  } catch (IOException e) {
  e.printStackTrace();
  }
  }
 }
</code></pre>

<p>Arduino Code</p>

<pre class=""lang-java prettyprint-override""><code>#include &lt;Braccio.h&gt;
#include &lt;Servo.h&gt;
#include&lt;SoftwareSerial.h&gt;

Servo base;
Servo shoulder;
Servo elbow;
Servo wrist_rot;
Servo wrist_ver;
Servo gripper;

String stringData[6] ;
int intData[6] ;

void setup() {
 Serial.begin(9600);
 Braccio.begin();
 pinMode(1, OUTPUT); // Set pin as OUTPUT
 pinMode(2, OUTPUT); // Set pin as OUTPUT
 pinMode(3, OUTPUT); // Set pin as OUTPUT
 pinMode(4, OUTPUT); // Set pin as OUTPUT
 pinMode(5, OUTPUT); // Set pin as OUTPUT
 pinMode(6, OUTPUT); // Set pin as OUTPUT
 pinMode(7, OUTPUT); // Set pin as OUTPUT
 pinMode(8, OUTPUT); // Set pin as OUTPUT
 pinMode(9, OUTPUT); // Set pin as OUTPUT
}

void loop() {




for(int y=0;y&lt;6;y++){

  while(Serial.available()== 0){}

stringData[y]= Serial.readString();

if(stringData[y]==""90"")
intData[y]=90;
else if(stringData[y]==""120"")
intData[y]=120;
else if(stringData[y]==""180"")
intData[y]=180;
else if(stringData[y]==""45"")
intData[y]=45;
else if(stringData[y]==""160"")
intData[y]=160;
else if(stringData[y]==""170"")
intData[y]=170;
else if(stringData[y]==""165"")
intData[y]=165;
else if(stringData[y]==""60"")
intData[y]=60;
else if(stringData[y]==""30"")
intData[y]=30;
else if(stringData[y]==""110"")
intData[y]=110;
else if(stringData[y]==""80"")
intData[y]=80;
else if(stringData[y]==""155"")
intData[y]=155;
}

Braccio.ServoMovement(20 ,         intData[0], intData[1], intData[2] , intData[3] ,intData[4] , intData[5]);

}
</code></pre>

<p>NOTE: The file im reading from is a notepad containing numbers separated with a "" , "" something like 150,30,60, etc for my code I used 6 for testing
someone might say its weird why send string then map it into an int instead of sending int directly, well I tried the int first and it didn't work then I tried this.</p>
",42854.32917,,783,1,1,0,,6613671,,42571.5125,4,43764473,"<p>I would try to isolate the issue among the following culprits:</p>

<p>Can you make the robot do repeatable things driving it from a static path in the arduino (ie hardcode the values in the arduino and check that you know how the robot is driven and it is working)</p>

<p>Are you cleanly passing your data between arduino and processing.  To test this create some objects/ arrays of numeric values and transfer them from processing to arduino and have arduino verify that it received what was expected.</p>

<p>Is your file read correct.  Read the file and write it to the processing console or something to verify you have what you think you have.</p>
",7298298,0,0,74570126,Have you verified that the values are being successfully passed and parsed on the arduino side?  (you could write some basic test files to push through a state machine on the arduino to demonstrate the accuracy of the values),Actuator
379,2671,42251735,I want to build a voice controlled robot please tell how should I proceed,|robotics|,"<p>I want to make a robot that follows my voice commands. So what sort of micro controller should I use? Should I go with Arduino or Should I use A Raspberry Pi. And also please tell me what language should I code in. Thank you!</p>
",42781.59167,,80,1,0,-1,,7569298,,42781.58542,8,42458308,"<p>You need two things here: a processor and a controller. You need a processor that can process data, do computations, run code etc. You need a controller for I/O. Raspberry pi has a strong(compared to Arduino) processor but the 40 GPIO pins are all digital i.e. you won't be able to use it alone for analog I/O (controlling the speed of the vehicle, reading sensor data etc.). Arduino on the other hand can handle both analog and digital I/O, but it has very low processing power. Generally people use both of them together. You will also need motor drivers, battery management circuit etc. Coming to the code, it should have two parts: one that processes your voice and understands the command, one that takes the command and tells the robot what to do.  Both could be done in C. For speech synthesis, you could use existing APIs and for controlling the robot, you can use Arduino libraries. I hope this answers your question.</p>
",6398022,0,0,,,Specifications
380,2745,43056359,matlab smooth vertical and horizontal rectangle,|matlab|computer-vision|robotics|image-morphology|,"<p>Could someone help me to find what morphological operations should I use in order to smoothen the vertical and horizontal rectangle on this image </p>

<p><a href=""https://i.stack.imgur.com/kmZsn.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kmZsn.jpg"" alt=""enter image description here""></a></p>

<p>More precisely what I would want is that the white rectangles become continuous. The final application of that would be to detect vertical and horizontal lines in the image, indeed this image is a map where white element represent obstacle and where i would want to detect the walls.</p>

<p>So the result i would want should be something like that: </p>

<p><img src=""https://i.stack.imgur.com/8JkBm.jpg"" alt=""enter image description here""></p>
",42821.87222,,67,0,4,0,,7776403,"Liège, Belgique",42821.86667,5,,,,,,73208131,could you please upload some visualization of the desired output?,Moving
381,2880,45845630,Integration of signal in embedded matlab function with variable timestep size,|matlab|signals|simulink|robotics|,"<p>I need to get a position from a velocity signal. I need to do it in SIMULINK and MATLAB and I need to do it to control a real robot. 
I can't use the standard continuous time integrator of SIMULINK because my time step changes at each iteration. </p>

<p>Therefore I was thinking to use a simple forward-euler method in the following embedded matlab function. </p>

<p>I am not sure this is the best solution and therefore I would like to know your opinion about it|</p>

<pre><code>function pCurr = fcn(vCurr, dt,p0)
%#codegen

persistent pOld
persistent vOld
if isempty(pOld)
    pOld = p0;
end

if isempty(vOld)
    vOld = vCurr;
end


pOld = pOld + trapz([vOld vCurr])*dt;
vOld = vCurr;
pCurr = pOld;
end
</code></pre>

<p>Thanks in advance </p>
",42970.71319,,190,0,2,0,,2761849,,41526.62083,630,,,,,,78661596,"If you're controlling a real robot, presumably with a real-time processor, how are using a variable time step?  That's very unusual.",Coordinates
382,2894,46154545,Bounding box orientation,|c++|computer-vision|point-cloud-library|robotics|,"<p>I have used a bounding box to get the pair of 4-points around an object after RANSAC and conditional outlier removal during the segmentation and filtration process.  After computing the 8 corner points of the bounding box, I have used 2 of them (front below) to localize the robot. for e.g following is a visual of the extracted object with the corner-points I intend to use </p>

<p><a href=""https://i.stack.imgur.com/WDka0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WDka0.png"" alt=""enter image description here""></a></p>

<p>The problem is if the object is straight like above the robot moves correctly almost in the middle but if the object is tilted the robot moves more on the right or left depending on which side it is tilted for example </p>

<p><a href=""https://i.stack.imgur.com/XmsK3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XmsK3.png"" alt=""enter image description here""></a></p>

<p>After analysis it seems that bounding box points are not exactly but with a offset on the object. What can be helpful in this situation ?  </p>

<p>-adding a manual offset on the corner points </p>

<p>-adjusting segmentation thresholds</p>

<p>-Possible to use surface normal or angle information from these points</p>

<p>anyother possibility </p>
",42989.47431,,1110,1,0,0,,7575638,Germany,42782.60833,31,46154833,"<p>I assume that when you tilt the object, the point of the object corner that closer to you gives more precise information than the other point. And if you can take angle information, find the surface normal and then calculate the angle between the vector or line you draw between two points that you got initially and the normal vector. And try change the point according to the result you got, in the area of your offset that the normal and our vector will be 90 to each other. </p>

<p>For example you know the normal vector and you have two points (x1,y1), (x2,y2). draw a vector between these two points, calculate the angle between this vector and normal. If the angle comes like 85 degrees. Manipulate (x2,y2) to get 90 degrees. It is like you hold a vector from it's start point and playing with it's direction. Because we assume that our starting point is precise.</p>
",,1,4,,,Coordinates
383,2835,45647371,Issue with finding Euler angles,|matlab|robotics|euler-angles|rotational-matrices|kinematics|,"<p>When i try to calculate roll, pitch, yaw values from a rotation matrix given below:</p>

<p><a href=""https://i.stack.imgur.com/iGEgp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iGEgp.png"" alt=""enter image description here""></a></p>

<p>i get math error in my calculator. But, matlab (using peter corke's robotics toolbox) gives me some values as result. </p>

<pre><code>%Rotation Matrix 5
R = [-0.9122 0.4098 0; -0.4098 -0.9122 0; 0 0 1];
tr2rpy(R,'deg')


Output:  0         0        -155.8083
</code></pre>

<p>Does that mean the rotation matrix is invalid ? Can i believe the matlab output ?</p>

<p>Thanks and regards !</p>
",42959.28194,,174,1,1,1,,3841374,,41835.61389,34,45647720,"<p>This is correct answer, so you have only roll, you can see it from rotation matrix, last row-column is [0,0,1] meaning that no change in z axis, meaning no pitch or yaw applied.
(In case of roll only, the roll angle is arccos(R(1,1)) )</p>
",2840531,2,0,78261193,Have you tried calculating the rotation matrix for a rotation of -155.8083 degrees about the z-axis and see if it comes out the same as `R`?,Coordinates
384,2663,42080006,Avoiding cycles in an directed graph efficiently,|algorithm|graph|robotics|graph-traversal|,"<p>I am trying to implement an algorithm called the 'rapidly exploring random belief tree'. The aim of this algorithm is to come up with a path for a robot, which, instead of connecting start and goal with a least distance metric or something similar, drives the robot into areas where it can get high accuracy measurements, and only then moves to the goal. </p>

<p>In the implementation of the algorithm, I start off by building a graph through random sampling of a given space. Every time a new point is sampled, a new edge is added to the graph. Initially, the error in the position of the robot will be high, as it won't be getting good measurements, but once the tree I am exploring through reaches the 'good' area, the robot's error suddenly drops, and with this knowledge of where the 'good' area is, I traverse backwards through the graph, pruning previously connected edges and connecting those vertices towards the good area. Given enough samples, in an optimal scenario, the graph would be propagating outward from the good area, hence connecting any two vertices would guarantee that you would pass through these points.  </p>

<p>Here's where I have a problem. Once I start the traversal to prune and update my edges, the algorithm shouldn't prune the edges that connect the good area to the starting point, which would result in an infinite loop. An example is shown in this picture:</p>

<p><a href=""https://i.stack.imgur.com/JQgJ2.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JQgJ2.jpg"" alt=""enter image description here""></a></p>

<ol>
<li><p>The algorithm finds vertices 1, 2, 3 and 4. The connections are 1 -> 2, 2 -> 3, 2 -> 4. </p></li>
<li><p>Now we find #5, which is a 'good' area for the robot. Visiting this vertex will magically enhance its accuracy in the future. I find neighboring vertices from 5: I find #3. I update the parent of #3 to #5 instead of #2. Now if there were more vertices surrounding #3, I would traverse those too, and update their parents to #3 etc., thus creating a path from #5 to all of those. (For more information, the traversal essentially computes the covariance of the robot if it were to move from #5 to #3 and whether that's more desirable than moving from #2 to #3). </p></li>
<li>But in no circumstances should I update the parent of #2 to #3: because the only way the robot got to #5 in the first place was through #2.</li>
</ol>

<p>To avoid this, a simple (but highly inefficient?) thing I did was to compute the shortest path to the vertex from where I began my traversal, and if any vertex I come across during traversal is part of this path, I don't update its parent. This works reasonably well, at the expense of a lot of computational cost because I need to do this every time I have to traverse backwards with an update, so I was wondering if there was a better, efficient way of doing this. </p>
",42773.04792,,897,0,3,1,,2588390,,41471.71667,162,,,,,,71336541,"you can reduce the cost of an edge from +infinity to 1 when you add it to the graph. For +infinity you can use any number you know is higher than the maximum possible distance from one point in the graph to another. Depending on the details of your minimum distance calculation you may not need to use +infinity explicitly. Your diagrams make it look like you are based on a grid, in which case I would have thought that Dijkstra distance finding and reducing edge costs would work quite well.",Moving
385,2649,41827191,How to turn any camera into a Depth Camera?,|computer-vision|virtual-machine|robotics|perspectivecamera|,"<p>I want to build a depth camera that finds out any image from particular distance. I have already read the following link.</p>

<ol>
<li><a href=""http://www.i-programmer.info/news/194-kinect/7641-microsoft-research-shows-how-to-turn-any-camera-into-a-depth-camera.html"" rel=""nofollow noreferrer"">http://www.i-programmer.info/news/194-kinect/7641-microsoft-research-shows-how-to-turn-any-camera-into-a-depth-camera.html</a></li>
<li><a href=""https://jahya.net/blog/how-depth-sensor-works-in-5-minutes/"" rel=""nofollow noreferrer"">https://jahya.net/blog/how-depth-sensor-works-in-5-minutes/</a></li>
</ol>

<p>But couldn't understand clearly which hardware requirements need &amp; how to integrated into all together?</p>

<p>Thanks</p>
",42759.48125,,3919,3,2,2,,7303836,"Rangpur, Rangpur Division, Bangladesh",42719.82639,8,41868969,"<p>Certainly, a depth sensor needs an IR sensor, just like in Kinect or Asus Xtion and other cameras available that provides the depth or range image. However, Microsoft came up with machine learning techniques and using algorithmic modification and research which you can find <a href=""https://techxplore.com/news/2014-08-microsoft-2d-camera-depth-sensor.html"" rel=""nofollow noreferrer"">here</a>. Also here is a video <a href=""https://youtu.be/hpCSebkYtUI"" rel=""nofollow noreferrer"">link</a> which shows the mobile camera that has been modified to get depth rendering. But some hardware changes might be necessary if you make a standalone 2D camera into a new performing device. So I would suggest you to see the hardware design of the existing market devices as well.</p>
",4750060,0,0,93310195,"I just wrote an answer for you but I probably could have just said ""google 'rgbd slam 2d'""",Incoming
386,2659,42030132,Inverse Kinematics programming C++,|c++|linux|macos|robotics|inverse-kinematics|,"<p>I want to write my own kinematics library for my project in C++. I do understand that there are a handful of libraries like RL (Robotics Library) and ROS with inverse kinematics solvers. But for my dismay, these libraries DO NOT support MacOS platform. I have already written the Forward Kinematics part, which was quite straight forward. But for the Inverse Kinematics part, I am quite skeptical since the solution to a IK problem involves solving sets of non-linear simultaneous equation. I found out the Eigen/Unsupported 3.3 module has a APIs to non-linear equations. But before I begin on this uncertain path, I want to get some insight from you guys on the plausibility and practicality of writing my IK library. My manipulator design is rather simple with 4 DoF and the library will not be used for other designs of manipulators. So what I am trying to achieve is taylor-made IK library for my particular manipulator design than a rather a 'Universal' library. </p>

<p>So,</p>

<ul>
<li>Am I simply trying to reinvent the wheel here by not exploring the already available libraries? If yes, please suggest examples of IK libraries for MacOS platform.</li>
<li>Has anyone written their own IK library? Is it a practical solution? Or is it rather complex problem which is not worth solving for a particular manipulator design?</li>
<li>Or should I just migrate all my project code (OpenCV) to a Linux environment and develop the code for IK in Linux using existing libraries?</li>
</ul>

<p>Thank you,</p>

<p>Vino</p>
",42769.74167,42030892,2707,1,4,-1,,4122660,"Sydney, New South Wales, Australia",41920.8,73,42030892,"<p>I have built some robots in the past and been required to solve kinematic equations.
As you stated ""manipulator design is rather simple with 4 DoF"" in my opinion you can write a fairly small function/module and you will not require the complexity of a general purpose library.
I used Maple to assist with creating the inverse equations, you may want to look for an alternative <a href=""http://alternativeto.net/software/maple/"" rel=""nofollow noreferrer"">Alternative</a>
On the other hand, the libraries you mention ROS and RL may not support your Mac at the highest level, but at the low level it is just c++ code, there is no reason you cannot use the libraries on your Mac and only use the low level functions.</p>
",7502032,1,1,71234720,Also: These kind of questions are explicitly _off-topic_ here. Read point #4 from this [help center article](http://stackoverflow.com/help/on-topic).,Actuator
387,2771,43580279,c++ 2D servos robot arm inverse kinematics,|c++|robotics|mbed|inverse-kinematics|,"<p>I am currently trying to write code to instruct a robot arm (2 servos motors, 3 sticks) to write out words and am having trouble getting it right. At the moment I am just trying to get it to move to a vector. I think that the inverse kinematics part is correct but all I'm getting is a twitch from one of the motors (if I'm lucky). A Nucleo- F411RE board is being used and I'm using the mbed developer. </p>

<pre><code>#include ""mbed.h""
#include ""Servo.h""
#include ""math.h""

Servo servo1(PA_8), servo2(PA_9);
    float len1 = 9.0;
    float len2 = 7.5;
    double pi = 3.1415926535897;

int lawOfCosines (float a, float b, float c )
{
    return acos((a*a + b*b - c*c) / (2 * a * b));
}
int distance(float x, float y) {
    return sqrt(x*x + y*y);
}

int deg(float rad) {
    return rad * 180 / pi;
}
int main() {

    //fmt.Println(""Lets do some tests. First move to (5,5):"");
    float x = 5.0;
    float y = 5.0;
    float dist = distance(x, y);
    float D1 = atan2(y, x);
    float D2 = lawOfCosines(dist, len1, len2);
    float A1 = D1 + D2;
    float A2 = lawOfCosines(len1, len2, dist);

   float  m1 = A1 * (100/90); 
   float  m2 = A2 * (100/90);
    for(int i=0; i&lt;m1; i++) {
              servo2 = i/100.0;

              wait(0.01);
          }
    for(int i=0; i&lt;m2; i++) {
               servo1 = i/100.0;              
              wait(0.01);
          }


    }
   `}
</code></pre>

<p>Any help to where I'm going wrong is greatly appreciated</p>
",42849.22014,,351,0,4,0,,7225744,,42703.57153,11,,,,,,74217792,"Thank you @sniper ! I have fixed the return types and changed the for loops to a ""servos.write(0)"" expression. However still not getting the desired response.",Actuator
388,2902,46530131,How to find change of direction in 3d space,|math|vector|coordinates|physics|robotics|,"<p>I am working in a project that takes data from moving sensor. we have converted sensor data in 3d coordinates. one of the sensor is planted on hand of the object.<br />
PROBLEM: When object moves his hand in backward direction and then play in forward direction. So i am looking for the frame number at which object hand ends moving back.</p>
<p>Just to clear the problem, this kind of points can come in 3D Here sample image data:</p>
<p><img src=""https://i.stack.imgur.com/kTjuJ.jpg"" alt=""img"" /></p>
<p>initially i implemented this, using change in y axis. With having some threshold value it's result is good. But only when  object hand moves back and comes forward with decreasing value of y axis. But in ideal case(image) hand can come forward in any way ie with increasing value of y or decreasing value of y. But i am sure that while hand moving in back direction it will always be y increasing (except some noise frame or outlier). using this i implemented this. `</p>
<pre class=""lang-cpp prettyprint-override""><code>std::pair&lt;std::pair&lt;int, int&gt;, bool&gt; calculateBacklift(std::vector&lt;glm::vec3&gt; points)
{
    std::pair&lt;std::pair&lt;int, int&gt;, bool&gt; backlift;
    backlift.second = false;
    std::vector&lt;glm::vec3&gt; distanceVec;
    for (int i = 0; i &lt; points.size() - 1; ++i)
    {

        // vector 
        glm::vec3 distance;
        distance.x = points[i + 1].x - points[i].x;
        distance.y = points[i + 1].y - points[i].y;
        distance.z = points[i + 1].z - points[i].z;
        distanceVec.push_back(distance);
    }
    writeTodisk(&quot;distanceVector.csv&quot;, distanceVec);
    for (int i = 0; i &lt; distanceVec.size(); ++i)
    {

        // y is major axis and if any of x or z hand changed then i am assuming there can be direction change. this comes from experiment.
        if (distanceVec[i].y &lt;= -0.09 &amp;&amp; (distanceVec[i].x &lt;= -0.1 || distanceVec[i].z &gt;= 0.9))
        {
            backlift.first = std::make_pair(1, i + 1);
            backlift.second = true;
            break;
        }
    }
    return backlift;
}`
</code></pre>
<p>But it doesn't work when hand comes forward in up direction. because change of y axis is positive.</p>
<p>Then i think of find change of direction using dot product. And cos value. But it also detect direction change on axis(X,Z). Or in simple way i can say i want to find corner point in moving sensor data in y axis(major) direction.  Can any one help me to resolve this issue.</p>
<p>Thanks.</p>
",43010.70556,46534082,614,1,6,0,,7664597,"New Delhi, Delhi, India",42800.25486,16,46534082,"<p>Assuming you have the entiry point trajectory, I'd just look for the point which has the largest distance to the line connecting the start to the end point. I don't know the vector library you are using, but using the <a href=""http://eigen.tuxfamily.org/"" rel=""nofollow noreferrer"">Eigen-library</a> this would be something like this:</p>

<pre class=""lang-cpp prettyprint-override""><code>std::vector&lt;Eigen::Vector3f&gt; points; // input set of points

typedef Eigen::ParametrizedLine&lt;float, 3&gt; Line3;
// Calculate line through first and last point of points:
Line3 start_to_end = Line3::Through(points[0], points.back());
float max_dist_squared=0.0f;
size_t max_idx=0;
for(size_t i=0; i&lt;points.size(); ++i) {
    // calculate squared (perpendicular) distance from points[i] to the line
    float dist2 = start_to_end.squaredDistance(points[i]);
    // check if distance is bigger than previous maximum:
    if(dist2 &gt; max_dist_squared) { max_dist_squared = dist2; max_idx = i; }
}
</code></pre>

<p>You should be able to implement something equivalent with the library of your choice.</p>

<p>If you need to handle outliers or noise, use some kind of moving median or moving average instead of single points.</p>
",6870253,0,5,80022510,"Do you want to find the ""corner point"" as soon as it occurs, or will you always have the entire point set?",Incoming
389,2834,45624243,PID Tuning Line Following Robot,|python|robotics|pid-controller|,"<p>I am working in a line following robot and I'm trying to implement a PID control loop, but i don't know how to tune it effectively.
I have two sensors, with the black line in the middle of them. I'm calculating the error with the intensity of sensor1 - the intensity of sensor2, and then calculating the speed.
This is my code simplified:</p>

<pre><code>error= sensor1_value-sensor2_value
error_sum+=error
dif_speed=error*kp+error_sum*ki+(error-last_error)*kd
run(speed-dif_speed+speed+dif_speed)
last_error=error
</code></pre>
",42957.92847,,196,0,0,1,,8350403,,42938.65417,3,,,,,,,,Incoming
390,2783,43857864,Robotics Cape Library not working on Beaglebone Blue,|c|beagleboneblack|robotics|,"<p>I have been trying to compile the example off of the <a href=""http://strawsondesign.com/files/BeagleBoneRobotics.pdf"" rel=""nofollow noreferrer"">BeagleBoneRobotics PDF</a>, but the compiler can't find the header.  </p>

<p>Here's the code I'm trying to compile:</p>

<pre><code>#include &lt;robotics_cape.h&gt;
#define PIN 67

int main (void){
 // export gpio pin for use
 if(gpio_export(PIN)){
  printf(""Unable to open export.\n"");
  return -1;
 }
 // set pin for output
 if(gpio_set_dir(PIN, OUTPUT_PIN)){
  printf(""Unable to open gpio67_direction.\n"");
  return -1;
 }
 // start blinking loop
 printf(""blinking LED\n"");
 int i = 0;
 while(i&lt;10){
  // turn pin on
  gpio_set_value(PIN, 1);
  printf(""ON\n"");
  sleep(1);
  // turn pin off
  gpio_set_value(PIN, 0);
  printf(""OFF\n"");
  i++; // increment counter
  sleep(1);
 }
 return 1;
}
</code></pre>

<p>Here's the error I'm getting:</p>

<p><a href=""https://i.stack.imgur.com/Aodqb.jpg"" rel=""nofollow noreferrer"">Error</a></p>

<pre class=""lang-none prettyprint-override""><code>root@beaglebone:/var/lib/cloud9# gcc Testing.c -lrobotics_cape -o Testing
Testing.c:1:27: fatal error: robotics_cape.h: No such file or directory
 #include &lt;robotics_cape.h&gt;
                           ^
compilation terminated.
</code></pre>

<p>I'm on the BeagleBone Blue with robotics cape version 0.3.4.</p>

<p>I checked the appropriate folders, and the header and library seem to be in place.  I've tried downloading the installer off of GitHub and making the library again, but still receive the same error.  I've tried to reinstall the the cape with the same result.  I've also dug through the source code to look for an error, but I can't find anything.  </p>

<p>Any help would be greatly appreciated</p>
",42863.91319,43882000,1481,3,0,2,0,7981942,,42863.70694,1,43858052,"<p>I would comment, but i cannot, until i get 50 reputation. </p>

<p>if the libriary is in the same directory as your program, then you use include ""</p>

<p><code>#include ""robotics_cape.h""</code> instead of <code>#include &lt;robotics_cape.h&gt;</code></p>

<p><code>#include &lt;name&gt;</code>  is used if the library is in the search path that your compiler used.</p>

<p>If the filename is quoted, searching for the file
typically begins where the source program was found.</p>

<p>If the library is not in the same directory as your file, then make sure it's in the gcc search path.  </p>

<p>this link will give you the gcc paths. </p>

<p><a href=""https://gcc.gnu.org/onlinedocs/gcc-3.3.3/cpp/Search-Path.html"" rel=""nofollow noreferrer"">https://gcc.gnu.org/onlinedocs/gcc-3.3.3/cpp/Search-Path.html</a></p>

<p>and</p>

<p><a href=""https://gcc.gnu.org/onlinedocs/cpp/Search-Path.html#Search-Path"" rel=""nofollow noreferrer"">https://gcc.gnu.org/onlinedocs/cpp/Search-Path.html#Search-Path</a></p>
",7892010,3,0,,,Error
391,2999,48299129,How can I get coordinates of a path through the remote API of v-rep?,|python|simulator|robotics|motion-planning|,"<p>Question in the title. Using the remote python API of <a href=""https://github.com/Troxid/vrep-api-python"" rel=""nofollow noreferrer"">v-rep</a>, I am able to get images and control motor properties of robots, but I cannot find any way to get the coordinates of a path object that was made with the path planning functionality in v-rep. I would like to get them as an array in my external python script.</p>
<p>I have found that there is no remote API functions dedicated to path objects, but there might be a more generic function that could be used for this.</p>
",43117.44514,,929,2,0,0,,5074515,Norway,42187.64583,20,48419671,"<p>A workaround provided by the developers on the v-rep forum as there does not seem to be a straightforward way to do this:
<a href=""http://www.forum.coppeliarobotics.com/viewtopic.php?f=9&amp;t=7095&amp;p=28040#p28040"" rel=""nofollow noreferrer"">http://www.forum.coppeliarobotics.com/viewtopic.php?f=9&amp;t=7095&amp;p=28040#p28040</a></p>

<p>First sample the path coordinates with simGetPathPosition, inside of a script function. Then call that script function from the python remote Api client, via simxCallScriptFunction.</p>
",5074515,0,0,,,Coordinates
392,3155,51161843,Create interrupt (ISR) to create smooth robotic arm motion,|python|multithreading|robotics|isr|,"<p>My school has a robotic arm (UR-10) that's hooked up to some buttons and I wanted to program it so the arm could move left and right smoothly when those buttons are clicked. </p>

<p>Currently, I'm finding the arm just moves then stops then moves and stops in a jerking fashion.</p>

<p>I wanted to implement an interrupt that could sense while a button is pressed down then the arm would continuously move by iterating through the loop fast enough so the arm would move without stopping. That is until the button is released.</p>

<p>If there is an easier way to implement this I would love to hear it.</p>

<p>Thanks!</p>

<p>Below is my code.</p>

<pre><code>import urx #UR-10 library
from threading import Thread

class UR10(object):
def __init__(self):

    # Establishes connection to UR-10 via IP address
    print(""establishing connection"")
    Robot = urx.Robot(""192.168.100.82"")
    self.r = Robot
    print(""Robot object is available as robot or r"")

    CheckingDigital_Inputs = Thread(target=self.ThreadChecker)
    CheckingDigital_Inputs.setDaemon(True)
    CheckingDigital_Inputs.start()

    MoveThread = Thread(target = self.MoveThread)
    MoveThread.setDaemon(True)
    MoveThread.start()

def ThreadChecker(self):
    while True:
        self.TestingInputs()

def TestingInputs(self):
    #Values = self.r.get_digital_in()

    self.In_1 = self.r.get_digital_in(1)
    self.In_2 = self.r.get_digital_in(2)

    #print(""Digital Input 1 is {},  Digital Input 2 is {}"".format(Values1, Values2))

def MoveThread(self):
    while True:
        self.LinearMovement()

def LinearMovement(self):
    #print(""linear move"")
    #need to run at 125khz
    while self.In_1:
        print(""move right linear"")

        l = 0.05
        v = 0.05
        a = 0.05

        pose = self.r.getl()
        print(pose)
        pose[0] += .01
        self.r.movel(pose, acc=a, vel=v)
</code></pre>
",43284.81944,51162299,176,1,0,1,,8524686,,42974.8625,84,51162299,"<p>The library code you are using says (in its description of a method):</p>

<blockquote>
  <p>This method is usefull since any new command from python to robot make the robot stop</p>
</blockquote>

<p><a href=""https://github.com/SintefRaufossManufacturing/python-urx/blob/193577c0efed8a24d00bd12b3b6e0c7ffefb9dd9/urx/urrobot.py#L356-L357"" rel=""nofollow noreferrer"">https://github.com/SintefRaufossManufacturing/python-urx/blob/193577c0efed8a24d00bd12b3b6e0c7ffefb9dd9/urx/urrobot.py#L356-L357</a></p>

<p>So it may not be possible to have the robot move continuously by issuing a bunch of commands, since it stops every time it gets a new command.</p>

<p>You need to figure out where the furthest place you want to move the robot to is, issue a command to move it there when the button is first pressed (with <code>wait=False</code>, so that your code doesn't sit around waiting for the robot to finish moving), and then use <code>self.r.stop()</code> to stop the robot whenever the button is released.</p>
",402891,1,0,,,Actuator
393,3013,48801772,Gazebo model does not stand,|ros|robotics|,"<p>Apologies for a very long post. I created the following xacro file, when I load in gazebo using the following launch file, the robot does not stand straight and falls down. I tried with different values of mass for different links, but no luck. It looks like I am missing something, can anyone help? </p>

<pre><code>&lt;?xml version=""1.0"" ?&gt;

&lt;robot xmlns:xacro=""http://www.ros.org/wiki/xacro"" 
    xmlns:sensor=""http://playerstage.sourceforge.net/gazebo/xmlschema/#sensor""
        xmlns:controller=""http://playerstage.sourceforge.net/gazebo/xmlschema/#controller""
        xmlns:interface=""http://playerstage.sourceforge.net/gazebo/xmlschema/#interface""
    name=""rosbot_v1""&gt;

&lt;!--Formula for calculation of mass moment of inertia of a cylinder
is given by the following formula:
Reference: http://www.amesweb.info/SectionalPropertiesTabs/Mass-Moment-of-Inertia-Cylinder.aspx
Mass moment of inertia about x axis     Ix  Ix= (m/12) * (3r^2+h^2)
Mass moment of inertia about y axis     Iy  Iy= (m/12) * (3r^2+h^2)
Mass moment of inertia about z axis     Iz  Iz= (mr2)/2--&gt;

&lt;xacro:macro name=""inertial_matrix_cylinder"" params=""mass arm_radius arm_length""&gt;
               &lt;inertial&gt;
                       &lt;mass value=""${mass}"" /&gt;
                       &lt;inertia ixx=""${mass*(3*arm_radius*arm_radius+arm_length*arm_length)/12}"" 
                                ixy = ""0"" ixz = ""0""
                                iyy=""${mass*(3*arm_radius*arm_radius+arm_length*arm_length)/12}"" iyz = ""0""
                                izz=""${mass*arm_radius*arm_radius/2}"" /&gt;
               &lt;/inertial&gt;
&lt;/xacro:macro&gt;

&lt;!--Physical attributes definition for base box--&gt;

&lt;xacro:property name=""base_box_length"" value=""1"" /&gt;
&lt;xacro:property name=""base_box_width"" value=""1"" /&gt;
&lt;xacro:property name=""base_box_height"" value=""0.6"" /&gt;
&lt;xacro:property name=""base_box_mass"" value=""4"" /&gt;

&lt;!--Physical attributes definition for the swivel arm--&gt;
&lt;xacro:property name=""swivel_arm_length"" value=""0.2"" /&gt;
&lt;xacro:property name=""swivel_arm_radius"" value=""0.2"" /&gt;
&lt;xacro:property name=""swivel_arm_mass"" value=""1"" /&gt;

&lt;!--Physical attributes definition for the arms--&gt;
&lt;xacro:property name=""arm_length"" value=""1"" /&gt;
&lt;xacro:property name=""arm_radius"" value=""0.1"" /&gt;
&lt;xacro:property name=""arm_mass"" value=""0.1"" /&gt;

&lt;!--Physical attributes definition for gripper box--&gt;
&lt;xacro:property name=""gripper_box_length"" value=""0.5"" /&gt;
&lt;xacro:property name=""gripper_box_width"" value=""0.4"" /&gt;
&lt;xacro:property name=""gripper_box_height"" value=""0.2"" /&gt;
&lt;xacro:property name=""gripper_box_mass"" value=""0.01"" /&gt;

&lt;!--Physical attributes definition for gripper fingers--&gt;
&lt;xacro:property name=""gripper_finger_length"" value=""0.12"" /&gt;
&lt;xacro:property name=""gripper_finger_width"" value=""0.4"" /&gt;
&lt;xacro:property name=""gripper_finger_height"" value=""0.12"" /&gt;
&lt;xacro:property name=""gripper_finger_mass"" value=""0.001"" /&gt;

&lt;!--Formula for calculation of mass moment of inertia of a cuboid
is given by the following formula: a=x(length); b=y(width)
Mass moment of inertia about x axis     Ix  Ix= (M/12) * a^2
Mass moment of inertia about y axis     Iy  Iy= (M/12) * b^2
Mass moment of inertia about z axis     Iz  Iz= (1/12)*M*(a^2+b^2)--&gt;

&lt;xacro:macro name=""inertial_matrix_cuboid"" params=""mass box_length box_width""&gt;
               &lt;inertial&gt;
                       &lt;mass value=""${mass}"" /&gt;
                       &lt;inertia ixx=""${mass/12*(box_length*box_length)}"" 
                                ixy = ""0"" ixz = ""0""
                                iyy=""${mass/12*(box_width*box_width)}"" iyz = ""0""
                                izz=""${mass/12*(box_length*box_length + box_width*box_width)}"" /&gt;
               &lt;/inertial&gt;
&lt;/xacro:macro&gt;

&lt;material name=""blue""&gt;
  &lt;color rgba=""0 0 0.8 1""/&gt;
&lt;/material&gt;

&lt;material name=""white""&gt;
  &lt;color rgba=""1 1 1 1""/&gt;
&lt;/material&gt;

&lt;material name=""green""&gt;
  &lt;color rgba=""0 1 0 1""/&gt;
&lt;/material&gt;

&lt;material name=""cyan""&gt;
  &lt;color rgba=""0 1 1 1""/&gt;
&lt;/material&gt;

&lt;!-- world link --&gt;
&lt;link name=""base_link""/&gt;

&lt;link name=""rosbot_base""&gt;
    &lt;xacro:inertial_matrix_cuboid mass=""${base_box_mass}"" box_length=""${base_box_length}"" box_width=""${base_box_width}""/&gt;
    &lt;collision name=""rosbot_collision""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""0  0  0""/&gt;
      &lt;geometry&gt;
        &lt;box size=""${base_box_length} ${base_box_width} ${base_box_height}""/&gt;
      &lt;/geometry&gt;
    &lt;/collision&gt;
    &lt;visual name=""rosbot_visual""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""0  0  0""/&gt;
      &lt;geometry&gt;
        &lt;box size=""${base_box_length} ${base_box_width} ${base_box_height}""/&gt;
      &lt;/geometry&gt;
      &lt;material name=""blue""/&gt;
    &lt;/visual&gt;
&lt;/link&gt;

&lt;!-- base_link and its fixed joint --&gt;
&lt;joint name=""joint_fix"" type=""fixed""&gt;
    &lt;parent link=""base_link""/&gt;
    &lt;child link=""rosbot_base""/&gt;
&lt;/joint&gt;

&lt;!-- A swiveling base on which next arm will sit --&gt;
&lt;link name=""rosbot_swivel_base""&gt;
    &lt;xacro:inertial_matrix_cylinder mass=""${swivel_arm_mass}"" arm_length=""${swivel_arm_length}"" arm_radius=""${swivel_arm_radius}""/&gt;
    &lt;collision name=""rosbot_collision""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""0   0  ${swivel_arm_length/2}""/&gt;
      &lt;geometry&gt;
        &lt;cylinder length=""${swivel_arm_length}"" radius=""${swivel_arm_radius}""/&gt;
      &lt;/geometry&gt;
    &lt;/collision&gt;
    &lt;visual name=""rosbot_visual""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""0   0  ${swivel_arm_length/2}""/&gt;
      &lt;geometry&gt;
        &lt;cylinder length=""${swivel_arm_length}"" radius=""${swivel_arm_radius}""/&gt;
      &lt;/geometry&gt;
      &lt;material name=""white""/&gt;
    &lt;/visual&gt;
&lt;/link&gt;

&lt;!-- The joint between swivel and base needs to be flush on the top face of rosbot_base  --&gt;
&lt;joint name=""rosbot_base_swivel_joint"" type=""revolute""&gt;
  &lt;parent link=""rosbot_base""/&gt;
  &lt;child link=""rosbot_swivel_base""/&gt;
  &lt;origin rpy=""0  0  0"" xyz=""0  0  ${base_box_height/2}""/&gt;
  &lt;axis xyz=""0  0  1""/&gt;
  &lt;limit effort=""100"" lower=""-1.57"" upper=""1.57"" velocity=""100""/&gt;
&lt;/joint&gt;

&lt;!-- A moving/manipulating arm1 --&gt;
&lt;link name=""rosbot_arm1""&gt;
    &lt;xacro:inertial_matrix_cylinder mass=""${arm_mass}"" arm_length=""${arm_length}""  arm_radius=""${arm_radius}""/&gt;
    &lt;collision name=""rosbot_collision""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""0   0   ${arm_length/2}""/&gt;
      &lt;geometry&gt;
        &lt;cylinder length=""${arm_length}"" radius=""${arm_radius}""/&gt; 
      &lt;/geometry&gt;
    &lt;/collision&gt;
    &lt;visual name=""rosbot_visual""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""0   0   ${arm_length/2}""/&gt;
      &lt;geometry&gt;
        &lt;cylinder length=""${arm_length}"" radius=""${arm_radius}""/&gt; 
      &lt;/geometry&gt;
      &lt;material name=""blue""/&gt;
    &lt;/visual&gt;
&lt;/link&gt;

&lt;!-- The joint between swivel and arm1 needs to be at the height of swivel link  --&gt;
&lt;joint name=""rosbot_swivel_arm1_joint"" type=""revolute""&gt;
  &lt;parent link=""rosbot_swivel_base""/&gt;
  &lt;child link=""rosbot_arm1""/&gt;
  &lt;origin rpy=""0  0  0"" xyz=""0  0  ${swivel_arm_length}""/&gt;
  &lt;axis xyz=""1  0  0""/&gt;
  &lt;limit effort=""100"" lower=""-1.57"" upper=""1.57"" velocity=""100""/&gt;
&lt;/joint&gt;

&lt;!-- A moving/manipulating arm2 --&gt;
&lt;link name=""rosbot_arm2""&gt;
    &lt;xacro:inertial_matrix_cylinder mass=""${arm_mass}"" arm_length=""${arm_length}""  arm_radius=""${arm_radius}""/&gt;
    &lt;collision name=""rosbot_collision""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""0   0   ${arm_length/2}""/&gt;
      &lt;geometry&gt;
        &lt;cylinder length=""${arm_length}"" radius=""${arm_radius}""/&gt; 
      &lt;/geometry&gt;
    &lt;/collision&gt;
    &lt;visual name=""rosbot_visual""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""0   0   ${arm_length/2}""/&gt;
      &lt;geometry&gt;
        &lt;cylinder length=""${arm_length}"" radius=""${arm_radius}""/&gt; 
      &lt;/geometry&gt;
      &lt;material name=""blue""/&gt;
    &lt;/visual&gt;
&lt;/link&gt;

&lt;!-- The joint between arm1 and arm2 needs to be at height of arm1  --&gt;
&lt;joint name=""rosbot_arm1_arm2_joint"" type=""revolute""&gt;
  &lt;parent link=""rosbot_arm1""/&gt;
  &lt;child link=""rosbot_arm2""/&gt;
  &lt;origin rpy=""0  0  0"" xyz=""0  0  ${arm_length}""/&gt;
  &lt;axis xyz=""1  0  0""/&gt;
  &lt;limit effort=""100"" lower=""-1.57"" upper=""1.57"" velocity=""100""/&gt;
&lt;/joint&gt;

&lt;!-- A gripper box, which holds the gripper joints --&gt;
&lt;link name=""rosbot_gripper_box""&gt;
    &lt;xacro:inertial_matrix_cuboid mass=""${gripper_box_mass}"" box_length=""${gripper_box_length}"" box_width=""${gripper_box_width}""/&gt;
    &lt;collision name=""rosbot_collision""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""0   0  ${gripper_box_height/2}""/&gt;
      &lt;geometry&gt;
        &lt;box size=""${gripper_box_length} ${gripper_box_width} ${gripper_box_height}""/&gt;
      &lt;/geometry&gt;
    &lt;/collision&gt;
    &lt;visual name=""rosbot_visual""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""0   0  ${gripper_box_height/2}""/&gt;
      &lt;geometry&gt;
        &lt;box size=""${gripper_box_length} ${gripper_box_width} ${gripper_box_height}""/&gt;
      &lt;/geometry&gt;
       &lt;material name=""cyan""/&gt;
    &lt;/visual&gt;
&lt;/link&gt;

&lt;!-- The joint between arm2 and gripper needs to be at height of arm2  --&gt;
&lt;joint name=""rosbot_arm2_gripper_joint"" type=""revolute""&gt;
  &lt;parent link=""rosbot_arm2""/&gt;
  &lt;child link=""rosbot_gripper_box""/&gt;
  &lt;origin rpy=""0  0  0"" xyz=""0  0  ${arm_length}""/&gt;
  &lt;axis xyz=""0  0  1""/&gt;
  &lt;limit effort=""100"" lower=""-1.57"" upper=""1.57"" velocity=""100""/&gt;
&lt;/joint&gt;

&lt;!-- The left gripper  --&gt;
&lt;link name=""rosbot_lgripper""&gt;
    &lt;xacro:inertial_matrix_cuboid mass=""${gripper_finger_mass}"" box_length=""${gripper_finger_length}"" box_width=""${gripper_finger_width}""/&gt;
    &lt;collision name=""rosbot_collision""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""-0.0  0.20  ${gripper_finger_height/2}""/&gt;
      &lt;geometry&gt;
        &lt;box size=""${gripper_finger_length} ${gripper_finger_width} ${gripper_finger_height}""/&gt;
      &lt;/geometry&gt;
    &lt;/collision&gt;
    &lt;visual name=""rosbot_visual""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""-0.0  0.20  ${gripper_finger_height/2}""/&gt;
      &lt;geometry&gt;
        &lt;box size=""${gripper_finger_length} ${gripper_finger_width} ${gripper_finger_height}""/&gt;
      &lt;/geometry&gt;
      &lt;material name=""green""/&gt;
    &lt;/visual&gt;
  &lt;/link&gt;

&lt;!-- The joint between gripper box and gripper needs to be at origin/slightly higher than origin of gripper box  --&gt;
&lt;joint name=""rosbot_lgripper_joint"" type=""prismatic""&gt;
    &lt;parent link=""rosbot_gripper_box""/&gt;
    &lt;child link=""rosbot_lgripper""/&gt;
    &lt;origin rpy=""0  0  0"" xyz=""-0.2  0.2  0.02""/&gt;
    &lt;axis xyz=""1  0  0""/&gt;
    &lt;limit effort=""100"" lower=""0"" upper=""0.14"" velocity=""100""/&gt;
&lt;/joint&gt;

&lt;!-- The right gripper  --&gt;
&lt;link name=""rosbot_rgripper""&gt;
    &lt;xacro:inertial_matrix_cuboid mass=""${gripper_finger_mass}"" box_length=""${gripper_finger_length}"" box_width=""${gripper_finger_width}""/&gt;
    &lt;collision name=""rosbot_collision""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""-0.0  0.20  ${gripper_finger_height/2}""/&gt;
      &lt;geometry&gt;
        &lt;box size=""${gripper_finger_length} ${gripper_finger_width} ${gripper_finger_height}""/&gt;
      &lt;/geometry&gt;
    &lt;/collision&gt;
    &lt;visual name=""rosbot_visual""&gt;
      &lt;origin rpy=""0  0  0"" xyz=""-0.0  0.20  ${gripper_finger_height/2}""/&gt;
      &lt;geometry&gt;
        &lt;box size=""${gripper_finger_length} ${gripper_finger_width} ${gripper_finger_height}""/&gt;
      &lt;/geometry&gt;
      &lt;material name=""green""/&gt;
    &lt;/visual&gt;
  &lt;/link&gt;

&lt;!-- The joint between gripper box and gripper needs to be at origin or slightly higher than origin of gripper box  --&gt;
&lt;joint name=""rosbot_rgripper_joint"" type=""prismatic""&gt;
    &lt;parent link=""rosbot_gripper_box""/&gt;
    &lt;child link=""rosbot_rgripper""/&gt;
    &lt;origin rpy=""0  0  0"" xyz=""0.2  0.2  0.02""/&gt;
    &lt;axis xyz=""1  0  0""/&gt;
    &lt;limit effort=""100"" lower=""-0.14"" upper=""0"" velocity=""100""/&gt;
&lt;/joint&gt;

&lt;/robot&gt;
</code></pre>

<p>When I edit the model in Gazebo and then toggle the static flag, it seems to be stable.</p>
",43146.30347,,1035,1,0,1,,9333030,India,43139.51181,91,48805293,"<p>I could solve this issue by adding the friction and damping elements to the URDF file.</p>

<pre><code>&lt;xacro:property name=""damping_value"" value=""10"" /&gt;
&lt;xacro:property name=""friction_value"" value=""0.1"" /&gt;
</code></pre>

<p>Sample usage of the property in joint is given below,</p>

<pre><code>&lt;!-- The joint between swivel and base needs to be flush on the top face of rosbot_base  --&gt;
&lt;joint name=""rosbot_base_swivel_joint"" type=""revolute""&gt;
  &lt;parent link=""rosbot_base""/&gt;
  &lt;child link=""rosbot_swivel_base""/&gt;
  &lt;origin rpy=""0  0  0"" xyz=""0  0  ${base_box_height/2}""/&gt;
  &lt;axis xyz=""0  0  1""/&gt;
  &lt;limit effort=""100"" lower=""-1.57"" upper=""1.57"" velocity=""100""/&gt;
  &lt;dynamics damping=""${damping_value}"" friction=""${friction_value}""/&gt;
&lt;/joint&gt;
</code></pre>
",9333030,0,0,,,Actuator
394,3211,52668674,"For a robot in a maze, infer path from detected edges of walls",|python|opencv|computer-vision|robotics|,"<p>I am working on Robot Vision for navigating in a maze. I am pretty new to OpenCV and so far I have managed to read a test image of a maze as seen from the robot's eye view, detect the edges using Canny edge detection, focus on a Region of Interest and using HoughLinesP Transform detect where the walls meet the floor and draw a blue line.</p>

<p><a href=""https://i.stack.imgur.com/ST8QW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ST8QW.jpg"" alt=""Blue lines inferred in the maze""></a></p>

<p>What I need to do now is calculate the center point between the two blue lines which are closer to the robot's camera and then do the same for the two lines which are more in the center of the image. Afterwards I want to connect the two center points to obtain a line.</p>

<p>Next step is to get the robot to follow this superimposed line.</p>

<p>Any help would be great :-)</p>

<p>Attached is an image created with my current script.</p>

<p>Here is my code:</p>

<pre><code># Original ideas abd code from 
# https://towardsdatascience.com/finding-driving-lane-line-live-with-opencv-f17c266f15db
# Testing edge detection for maze

import cv2
import numpy as np

image = cv2.imread(""/home/pi/opencv/maze_test_images/maze1.png"")
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
kernel_size = 5
blur_gray = cv2.GaussianBlur(gray,(kernel_size,kernel_size),0)
low_threshold = 50
high_threshold = 150

edges = cv2.Canny(blur_gray, low_threshold, high_threshold)

# create a mask of the edges image using cv2.filpoly()
mask = np.zeros_like(edges)
ignore_mask_color = 255

# define the Region of Interest (ROI) - source code sets as a trapezoid for roads
imshape = image.shape

vertices = np.array([[(0,imshape[0]),(100, 420), (1590, 420),(imshape[1],imshape[0])]], dtype=np.int32)

cv2.fillPoly(mask, vertices, ignore_mask_color)
masked_edges = cv2.bitwise_and(edges, mask)

# mybasic ROI bounded by a blue rectangle

#ROI = cv2.rectangle(image,(0,420),(1689,839),(0,255,0),3)

# define the Hough Transform parameters
rho = 2 # distance resolution in pixels of the Hough grid
theta = np.pi/180 # angular resolution in radians of the Hough grid
threshold = 15     # minimum number of votes (intersections in Hough grid cell)
min_line_length = 40 #minimum number of pixels making up a line
max_line_gap = 30    # maximum gap in pixels between connectable line segments

# make a blank the same size as the original image to draw on
line_image = np.copy(image)*0 

# run Hough on edge detected image
lines = cv2.HoughLinesP(masked_edges, rho, theta, threshold, np.array([]),min_line_length, max_line_gap)

for line in lines:
        for x1,y1,x2,y2 in line:
            cv2.line(line_image,(x1,y1),(x2,y2),(255,0,0),10)

# draw the line on the original image 
lines_edges = cv2.addWeighted(image, 0.8, line_image, 1, 0)
#return lines_edges

coord = np.where(np.all(lines_edges == (255,0,0), axis=-1))
print zip(coord[0], coord[1])


cv2.imshow(""original"", image)
cv2.waitKey(0)

cv2.imshow(""edges"", edges)
cv2.waitKey(0)

cv2.imshow(""detected"", lines_edges)
cv2.waitKey(0)

cv2.imwrite(""lanes_detected.jpg"", lines_edges)
cv2.destroyAllWindows()
</code></pre>
",43378.63403,,1234,0,3,3,,10462374,"England, UK",43378.62014,11,,,,,,92267931,I have added my code. What I am trying to do is be able to plot a line central to the walls of the maze (The lines now highlighted in blue) and get the robot to follow this central line. Just like a line follower but with a virtual line.,Incoming
395,3227,53307599,Difference between Evolutionary Strategies and Reinforcement Learning?,|deep-learning|reinforcement-learning|robotics|evolutionary-algorithm|,"<p>I am learning about the approach employed in Reinforcement Learning for robotics and I came across the concept of Evolutionary Strategies. But I couldn't understand how  RL and ES are different. Can anyone please explain?</p>
",43418.81667,53348627,6077,3,0,12,0,9761439,,43228.93403,67,53348627,"<p>To my understanding, I know of two main ones.</p>

<p><strong>1)</strong> Reinforcement learning uses the concept of one agent, and the agent learns by interacting with the environment in different ways.  In evolutionary algorithms, they usually start with many ""agents"" and only the ""strong ones survive"" (the agents with characteristics that yield the lowest loss).</p>

<p><strong>2)</strong> Reinforcement learning agent(s) learns both positive and negative actions, but evolutionary algorithms only learns the optimal, and the negative or suboptimal solution information are discarded and lost.  </p>

<p><strong><em>Example</em></strong></p>

<p>You want to build an algorithm to regulate the temperature in the room.</p>

<p>The room is 15 °C, and you want it to be 23 °C. </p>

<p>Using Reinforcement learning, the agent will try a bunch of different actions to increase and decrease the temperature.  Eventually, it learns that increasing the temperature yields a good reward.  But it also learns that reducing the temperature will yield a bad reward.</p>

<p>For evolutionary algorithms, it initiates with a bunch of random agents that all have a preprogrammed set of actions it is going to do.  Then the agents that has the ""increase temperature"" action survives, and moves onto the next generation.  Eventually, only agents that increase the temperature survive and are deemed the best solution.  However, the algorithm does not know what happens if you decrease the temperature.</p>

<p><strong>TL;DR:</strong> RL is usually one agent, trying different actions, and learning and remembering all info (positive or negative).  EM uses many agents that guess many actions, only the agents that have the optimal actions survive.  Basically a brute force way to solve a problem.</p>
",9191460,16,0,,,Other
396,3248,53683579,ROS Kinetic Installation Error on OSX Mojave,|python|installation|homebrew|ros|robotics|,"<p>During the process of installing ROS kinetic on my mac. I've been trying to resolve dependencies using the following commands:</p>

<pre><code>$ cd ~/ros_catkin_ws
$ rosinstall_generator ros_comm --rosdistro kinetic --deps --wet-only --tar &gt; kinetic-ros_comm-wet.rosinstall
$ wstool init -j8 src kinetic-ros_comm-wet.rosinstall

$ rosdep install --from-paths src --ignore-src --rosdistro kinetic -y # resolves dependancies
</code></pre>

<p>I get the following error:</p>

<pre><code>Error: No available formula with the name ""gtest"" 

ERROR: Rosdep experienced an internal error.
Please go to the rosdep page [1] and file a bug report with the message below.
[1] : http://www.ros.org/wiki/rosdep

rosdep version: 0.13.0

Bad installer [homebrew]: Error while parsing brew info for 'gtest'
 * Output of `brew info gtest --json=v1`:

 * Error while parsing:
 Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/site-packages/rosdep2/platforms/osx.py"", line 203, in is_installed
    pkg_info = json.loads(std_out)
  File ""/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/__init__.py"", line 339, in loads
    return _default_decoder.decode(s)
  File ""/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/decoder.py"", line 364, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/decoder.py"", line 382, in raw_decode
    raise ValueError(""No JSON object could be decoded"")
ValueError: No JSON object could be decoded
</code></pre>

<p>I've tried uninstalling and reinstalling several things, but I'm not sure what the actual issue is and how I can fix this.</p>
",43442.60972,,775,1,3,2,,7426971,,42751.78125,7,54100441,"<p>As also specified in the <a href=""http://wiki.ros.org/kinetic/Installation/OSX/Homebrew/Source"" rel=""nofollow noreferrer"">guide</a>, <em>Google Mock</em> is not compatible with OS X so you should skip it adding <code>--skip-keys google-mock</code> to <code>rosdep install</code></p>
",4922045,1,0,94290114,"Tried it and got: `Error: No available formula with the name ""gtest"" `",Other
397,3058,49612231,Collision avoidance system - image interpretation,|computer-vision|robotics|,"<p><br>
I am refining an algorithm for collision avoidance I wrote for an ASV using a monocular camera.<br></p>

<p>In order to have a simple yet effective system, the algorithm relies on edge detection using a <em>Canny</em> filter, after proper erosion/dilation to remove noise and shading spots.<br>
The final image is a black and white frame where, starting from the bottom, the first contour found and the upper part gets coloured in white while the rest is black: so black can be interpreted as ""free space"" whereas white as ""obstacle"".</p>

<p>The rover should use this reactive collision avoidance system to steer accordingly, avoiding close obstacles.</p>

<p>Now I am having some problems to decide how to actually find the way to go.</p>

<p>This is my approach so far.<br>
The frame is split in columns whose width is exactly 1° relative bearing, then columns are grouped in three super-columns (corresponding to the directions <em>turn left, go on, turn right</em>) and the robot steers on the direction where the most black space is found.</p>

<p>Do you have any other idea on how to find where to steer/proceed given the output of this algorithm? <br>Thank you</p>

<p><strong>Image processing example</strong></p>

<ol>
<li>initial frame</li>
</ol>

<p><a href=""https://i.stack.imgur.com/w41gc.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w41gc.jpg"" alt=""initial frame""></a></p>

<ol start=""2"">
<li>filtering</li>
</ol>

<p><a href=""https://i.stack.imgur.com/kMOyL.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kMOyL.jpg"" alt=""filtering""></a></p>

<ol start=""3"">
<li>occupancy frame</li>
</ol>

<p><a href=""https://i.stack.imgur.com/p1kVK.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p1kVK.jpg"" alt=""occupancy frame""></a></p>

<ol start=""4"">
<li>column divisions</li>
</ol>

<p><a href=""https://i.stack.imgur.com/vMbIA.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vMbIA.jpg"" alt=""column divisions""></a></p>

<ol start=""5"">
<li>command -> steer left</li>
</ol>
",43192.56042,,70,0,0,1,0,7123392,"Rome, Metropolitan City of Rome, Italy",42680.80069,48,,,,,,,,Moving
398,3149,51063970,Python fast calculating and slow Serial writing : Multithread or Multiprocess,|python|multithreading|multiprocessing|robotics|,"<p>I have a robotics project, basically a path tracking problem.
In PC, a reference generation algorithm is implemented in Python3.65. The algorithm takes Indoor GPS data and use these continuously-updated data to calculate the reference path for a robot-car. Of course, the algorithm runs in a 
<strong><em>while True:
   ....</em></strong>
framework.
The algorithm can work well only if the sampling frequency is quite high, say 0.001s. 
However, the problem is that, after calculating the reference path, the path info needs to be written to Serial port of the PC, byte by byte, via Serial.write(). 
This serial.write() function is still a for loop. And this writing process is very slow (more than 0.02s for 16 bytes). If the for loop is included in the while True: framework, something like：</p>

<hr>

<pre><code>while True:
  Data = Ref_generation()
  Bytes_Full = Float_2_Bytes_List(Data)
  for ele in Bytes_Full:
    Serial.write(ele)  # VERY SLOW!!!
  sleep(0.001)
</code></pre>

<hr>

<p>Then, the Data can not be calculated correctly since the cycle period is much longer than 0.001s. </p>

<p>In a nutshell, how can I separate the fast calculating algorithm from the slow serial.wtite()? I tried multithreads, but not works. </p>

<p>Anyhelp will be appreciated, thanks a lot!</p>
",43278.55833,51064156,1686,2,0,1,0,9438864,,43162.68472,35,51064062,"<p>You don't need to leverage multiple cpu cores, all you want is to <strong>wait</strong> for the serial port... Your CPU will be idle, <strong>waiting</strong>... Spawning new threads/processes to just <strong>wait</strong> is a waste...</p>

<p>That's why you should try to use some asynchronous IO solution.</p>

<p>Example, use <a href=""https://github.com/pyserial/pyserial-asyncio"" rel=""nofollow noreferrer"">https://github.com/pyserial/pyserial-asyncio</a> or <a href=""https://twistedmatrix.com/documents/16.1.0/api/twisted.internet.serialport.SerialPort.html"" rel=""nofollow noreferrer"">https://twistedmatrix.com/documents/16.1.0/api/twisted.internet.serialport.SerialPort.html</a> </p>

<p>These asynchronous frameworks allow you to register events and have your function called automatically when they finish, <strong>all in a single thread/process</strong>.</p>

<p>They also allow you to schedule events in the time you want. </p>
",17160,1,5,,,Timing
399,3152,51089252,Using serializing an object in python for use with an XBee,|python|serialization|deserialization|ros|robotics|,"<p>For a project I'm working on, I'm supposed to use XBee radio modules, which isn't super important, except that I have to read and write to their serial port in order to use them. I'm currently working with Python and ROS, so I'm attempting to send TransformStamped messages over the XBees.</p>

<p>My question is, unless I'm misunderstanding how Serial.read() and Serial.write() work, how can I tell how many bytes to read? I was planning on using Pickle to serialize the data into a string, and then sending that over the serial ports. Is there a better way that I've overlooked? Is there some sort of loop that would work to read data until the end of the pickled string is read?</p>
",43279.76875,51144344,193,1,0,1,,7901933,"Denver, CO, United States",42846.59097,15,51144344,"<p>The short answer is, serial.read() cannot tell you how many bytes to read. Either you have some prior knowledge as to how long the message is, or the data you send has some means of denoting the boundaries between messages. </p>

<p>Hint; knowing how long a message is is not enough, you also need to know whereabouts in the received byte stream a message has actually started. You don't know for sure that the bytes received are exactly aligned with the sent bytes: you may not have started the receiver before the transmitter, so they can be out of step. </p>

<p>With any serialisation one has to ask, is it self delimiting, or not? Google Protocol buffers are not. I don't think Pickle is either. ASN.1 BER is, at least to some extent. So is XML.</p>

<p>The point is that XBee modules are (assuming you're using the ones from Digi) just unreliable byte transports, so whatever you put through them has to be delimited in some way so that the receiving end knows when it has a complete message. Thus if you pickle or Google Protocol Buf your message, you need some other way of framing the serialised data so that the receiving end knows it has a complete message (i.e. it's seen the beginning <em>and</em> end). This can be as simple as some byte pattern (e.g. 0xffaaccee00112233) used to denote the end of one message and the beginning of the next, chosen so as to be unlikely to occur in the sent messages themselves. Your code at the receiving end would read and discard data until is saw that pattern, would then read subsequent data into a buffer until it saw that pattern again, and only then would it attempt to de-pickle / de-GPB the data back into an object. </p>

<p>With ASN.1 BER, the data stream itself incorporates effectively the same thing, saving you the effort. It uses tags, values and length fields to tell its decoders about the incoming data, and if the incoming data makes no sense to the decoder in comparison to the original schema, incorrectly framed data is easily ignored.</p>

<p>This kind of problem also exists on tcp sockets, though at least with those delivery is more or less guaranteed (the first bytes you receive are the first bytes sent). A Digimesh connection does not quite reach the same level of guaranteed delivery as a tcp socket, so something else is needed (like a framing byte pattern) for the receiving application to know that it is synchronised with the sender. </p>
",2147218,1,1,,,Connections
400,3122,50158870,How to get started with robotics having no idea what it is,|robotics|,"<p>I wanted to learn robotics, Actually i want to build a robot not very complex but atleast that makes sense of what it does. So basically im a cs student who knows only about programming and nothing about microcontroller or anything remotely close to that particular subject. I know little about electronics but never dug deep into that. </p>

<p>So i need an advice on how to start on my journey of learning robotics where i have no idea about the above mentioned fields </p>

<p>Ive seen in lot of article where u have to learn maths (obviously) and adruino. I've  seen them telling to use adruino ide and even use python language or c/c++
But atleast for now what would be the difderence between these two. Does adruino directly compile the code to machine level language or it is just that its easier to do so in adruino</p>

<p>Thanks for going through the post</p>
",43223.64792,50184942,259,2,0,-2,,9376844,"Bangalore, Karnataka, India",43149.54097,11,50184942,"<p>In order to start with Robotics, You should know the fundamentals of machine learning that are</p>

<ol>
<li><a href=""https://medium.com/@equipintelligence/computer-vision-giving-vision-to-machines-fd777c1c678c"" rel=""nofollow noreferrer"">Computer Vision </a></li>
<li><a href=""https://medium.com/@equipintelligence/bayes-text-classification-machine-learning-algorithms-915d320abcdb"" rel=""nofollow noreferrer"">Natural language processing </a></li>
<li><a href=""https://medium.com/@equipintelligence/artificial-neural-networks-mapping-the-human-brain-2e0bd4a93160"" rel=""nofollow noreferrer"">Artificial Neural networks </a></li>
<li>Deep understanding about learning       methods</li>
</ol>

<p>The above points refer to the software which controls the robot. </p>
",,0,1,,,Other
401,3232,53314997,solve a coupled ODE of euler angels Rotation vector of a body,|matlab|robotics|,"<p>i'm trying to solve coupled ODEs by using matlab ode45 function:</p>

<p><a href=""https://i.stack.imgur.com/gRVvu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gRVvu.png"" alt=""enter image description here""></a></p>

<p>Here is my function called 'Rot' to describe these ODE's for using matlab ode45.</p>

<pre><code>function omega= Rot(t,y)
omega(2,1)=(0.03*sin(3*t)*((cos(Y(1)))^2)+0.002*t^3*sin(y(1)))...
/-((cos(Y(1)))^2)+((sin(Y(1)))^2);
omega(1,1)=((0.002*t^2-omega(2,1)*sin(y(1)))...
/-cos(y(3))*sin(y(2)))*cos(y(2))+0.01*t^2+0.3*t;
omega(3,1)=(0.002*t^2-omega(2,1)*sin(y(1)))...
/-cos(y(3))*sin(y(2));
</code></pre>

<p>but I'm getting ""Not enough input arguments."" error.</p>
",43419.34375,,142,1,7,1,,10259947,"Tunis, Tunisia",43334.55764,9,53342309,"<p>OK, so by expressing <code>theta_dot</code> as a function of the other variables in equation (3) and injecting the result in equation (2), I get (pseudo-code):</p>

<p><code>phi_dot = (0.03*sin(psi)*sin(3*t) - 0.002*t^2 * cos(psi)) / (sin(theta)*(cos(psi))^2 + sin(theta) * sin(psi) * sin(phi))</code></p>

<p>So makes that the first equation of your ODE file as it only depends on time and the state vector.</p>

<p>Then your second equation in the ODE file is:</p>

<p><code>psi_dot = -phi_dot * cos(theta) + 0.01*t^2 + 0.3*t</code></p>

<p>which is OK because you've calcuated <code>phi_dot</code> in the previous equation.</p>

<p>And finally the last equation in your ODE file:</p>

<p><code>theta_dot = (-0.03*sin(3*t) + phi_dot * sin(theta) * sin(phi)) / cos(psi);</code></p>

<p>which is also OK because you have calculated <code>phi_dot</code> in your first equation.</p>

<p>You can then pass this to the ODE solver and it should work. (do check my maths though)</p>
",2257388,0,0,93517212,Yes. It is because of that.,Coordinates
402,3104,49842838,Multiple systems sharing resources on multiple SoC's,|raspberry-pi|mpi|distributed-computing|hpc|robotics|,"<p>I have some Raspberry Pi's from previous projects/learning and I would like to pool their resources to make a differential drive robot.</p>

<p>Two Pi's would have one camera each for a vision system, one connected to an Arduino to read analog sensors, one for driving motors, and the last pi is the ""control"" and hosting a user interface (web app).  Nothing really special here! But I would like to be able to share the resources of all the Pi's for improved performance...</p>

<p>My thoughts on sharing resources is one of two approaches:</p>

<p>1) Use distributed memcached as a RAM cluster and run each sub system on one CPU only to avoid data races.</p>

<p>or</p>

<p>2) Use a messaging layer to distribute processing on all CPU.</p>

<p>To avoid a lot of headache, I thought I could use MPI since it does a lot of heaving lifting when it comes to messaging. However I can't seem to find any examples of any robotics projcets using MPI.</p>

<p>It looks like MPI is <em>simplest</em> to design when it's for supervised learning, or genomics (same code and large data sets).</p>

<p>In my case, each sub system runs very different code from the other.  But for example, the vision system runs the same code on a stream of hundred/thousand images. So why not use MPI for the vision, and let the ""contorl"" schedule when its starts / stops.</p>

<p>Then use its output as input for the next system, which also runs the same code, so can be paralleled.</p>

<p>So my question is:</p>

<blockquote>
  <p>Is there a reason why MPI is not a common approach for things like
  this in Robotics? If so, why and what is a good alternative?</p>
</blockquote>

<p>There's a CUDA-MPI for GPU's so maybe this approach is not too far fetched?</p>
",43205.5875,,78,0,3,0,,1158977,"Boston, MA, USA",40927.69306,663,,,,,,86701939,"I agree! It's a course of action I am excited about, but that doesn't make it the right one",Other
403,3166,51346929,"How to convert 2D(x,y) coordinates to 3D(x,y,z) coordinates using MATLAB?",|matlab|image-processing|computer-vision|camera-calibration|robotics|,"<p>I am trying to automate an robotic arm using MATLAB. So the thing is, camera will be mounted on the base of robotic arm. It will capture the snapshots of the camera frames and do image processing on it and it will detect the target(object) and find the pixel coordinate for it. After it is done, this pixel coordinates should be <strong>mapped to real world x-y-z metric coordinates?</strong> and this real world coordinates(x,y,z) would serve as parameters for inverse kinematics function which would give the value of theta so that servos can move.</p>

<p>I'm stuck  here,  this pixel coordinates should be <strong>mapped to real world x-y-z metric coordinates?</strong> i dont know how to do it. nor getting any ideas, how to proceed it? 
Anyone having any lead, please share it!!</p>

<p>PS anyone of you guyz thinks that for automation of robotic arm should i use MATLAB or something else?. Coz this all code would be uploaded on raspberry pi 3 using ROS environment.</p>

<p>BEST REGARDS</p>

<p>hitesh kumar</p>
",43296.38125,,2439,1,5,0,0,8740644,India,43016.46806,17,51698370,"<p>To calculate 3D world point for the given pixel in an image, you need depth information (should use 3D camera like Kinect ..etc). One you have depth information and camera intrinsics and extrinsics you can convert 2D pixels to 3D world coordinates and vice versa.</p>

<p>Below equation for calcualting X,Y,Z in world coordinates.</p>

<p><a href=""https://i.stack.imgur.com/wPOhs.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wPOhs.gif"" alt=""enter image description here""></a></p>

<p>Technically <code>Perspective projection</code> is what camera does in converting 3D world to 2D and below equation represents this projection. </p>

<p><a href=""https://i.stack.imgur.com/2P1jS.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2P1jS.jpg"" alt=""enter image description here""></a></p>

<p>Above image is from <a href=""https://www.cc.gatech.edu/classes/AY2016/cs4476_fall/results/proj3/html/agartia3/index.html"" rel=""nofollow noreferrer"">here</a></p>
",1595504,1,0,89666909,"@seralouk I have seen them but some says that you need estimate camera intrinsics, extrinsics, and lens distortion parameters.",Moving
404,3047,49353430,running python code using single key presses,|python|raspberry-pi|robot|,"<p>Currently building Petrol-powered RC car controlled by a raspberry pi and 16ch adafruit servo controller Pi hat. Pretty novice query from a beginner but how can simple Python commands be carried out by a single key press. E.g. Holding the ""w"" key on a keyboard to run ""pwm.setPWM(0, 0, servoMax)"". (In order for the servo to push the throttle to move the vehicle forward). What follows is the code currently used:</p>

<pre><code>#!/usr/bin/python

from Adafruit_PWM_Servo_Driver import PWM
import time

pwm = PWM(0x40)

servoMin = 150
servoMax = 600

def setServoPulse(channel, pulse):
 pulseLength = 1000000
 pulseLength /= 60
 print ""%d us per period"" % pulseLength
 pulseLength /= 4096
 print ""%d us per bit"" % pulseLength
 pulse *= 1000
 pulse /= pulseLength
 pwm.setPWM(channel, 0, pulse)

pwm.setPWMFreq(60)
While (True): 
 pwm.setPWM(0, 0, servoMin)   #throttle servo set to off position -should be default 
 pwm.setPWM(0, 0, servoMAX)   #throttle servo set on -to be run by ""W"" key
 pwm.setPWM(1, 0, servoMin)   #steering servo left -by holding ""A"" key
 pwm.setPWM(1, 0, servoMax)   #steering servo right -by holding ""D"" key
</code></pre>

<p>I would assume the answer involves If and ElseIf commands, but I really would just like to run a program then input() keyboard presses to run the code.</p>
",43177.93264,,442,2,2,-1,0,9513500,,43177.90069,3,49407640,"<p>1)you can first of all make a infinite while loop.</p>

<p>2) after take input via row input</p>

<p>3) then after apply condition for which keyword is found then which function is called</p>

<p>4) now call the function if condition is true.</p>
",9219170,0,0,85707265,Have you seen this solution SO already ? [link](https://stackoverflow.com/questions/15855168/create-a-raw-input-with-commands-inside-a-python-script) also.. there are a few of these questions already answered asking for similar things and people seem to mention the python [cmd](https://docs.python.org/2/library/cmd.html). I’ve never used it before but it might be what your looking for.,Remote
405,2982,48110189,how to get wheel encoder count from turtlebot's /odom topic,|ros|robotics|,"<p>turtlebot's odom topic gives:</p>

<pre><code>header:    seq: 406289   stamp: 
    secs: 4392
    nsecs: 160000000   frame_id: odom child_frame_id: base_footprint pose:   pose: 
    position: 
      x: 1.56701645246e-05
      y: -9.82132735628e-06
      z: 0.0
    orientation: 
      x: 0.0
      y: 0.0
      z: -0.548275342929
      w: 0.836297882537   covariance: [0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1000000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1000000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1000000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05] twist:    twist: 
    linear: 
      x: -2.67171244095e-06
      y: 0.0
      z: 0.0
    angular: 
      x: 0.0
      y: 0.0
      z: -0.000185729678152   covariance: [0.0, 0.0, 0.0, 0.0, 0.0,
0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
</code></pre>

<p>I'd like to find the wheel encoder count from this data. How does turtlebot use wheel encoder values to compute positions? Where can I find the code that does this?</p>
",43105.37222,,288,0,0,1,,5192123,United States,42221.15139,418,,,,,,,,Coordinates
406,3193,52084828,Improving controls of Raspberry Pi RC robot,|python|raspberry-pi|robotics|,"<p>I'm currently working on a basic Raspberry pi project and I need help with improving the controls for it.</p>

<p>This is my current code:</p>

<pre><code>import RPi.GPIO as GPIO
import curses

GPIO.setmode(GPIO.BOARD)
GPIO.setup(7,GPIO.OUT)
GPIO.setup(11,GPIO.OUT)
GPIO.setup(13,GPIO.OUT)
GPIO.setup(15,GPIO.OUT)

screen = curses.initscr()
curses.noecho()
curses.cbreak()
screen.keypad(True)

try:
    while True:
        char = screen.getch()
        if char == curses.KEY_UP:
            GPIO.output(11,True)
            GPIO.output(13,True)
        elif char == ord('s'): 
            GPIO.output(7,False)
            GPIO.output(11,False)
            GPIO.output(13,False)
            GPIO.output(15,False)
            break
        elif char == curses.KEY_DOWN:
            GPIO.output(7,True)
            GPIO.output(15,True)
        elif char == curses.KEY_RIGHT:
            GPIO.output(7,True)
            GPIO.output(13,True)
        elif char == curses.KEY_LEFT:
            GPIO.output(11,True)
            GPIO.output(15,True)
        elif char == 10:
            GPIO.output(7,False)
            GPIO.output(11,False)
            GPIO.output(13,False)
            GPIO.output(15,False)
finally:
    curses.nocbreak()
    screen.keypad(False)
    curses.echo()
    curses.endwin()
    GPIO.cleanup()
</code></pre>

<p>The problem that I have is that it requires me to press the enter key to stop the motors and change direction and the way that I would like to have it set up is so that, for example, when the up key is <strong>pressed and held</strong> the motor moves forward and <strong>once the key is released the motors stop.</strong></p>
",43341.8125,,169,1,0,0,,10291731,,43341.79792,1,52414697,"<p>The problem with your code is it only changes the GPIO when a button is pushed. This is because <code>getch()</code> What you want is to </p>

<p>In order to get the behavior you want you need to have <code>getch()</code> return a value even when no button is pressed. There are two ways to do this</p>

<ol>
<li><strong>nodelay</strong>: this makes getch() return immediately, whether or not a key is pressed, if no key is pressed it will return <em>curses.ERR</em></li>
<li><strong>halfdelay</strong>: this makes getch() wait upto a set amount of time, if no key is pressed in that time period it returns <em>curses.ERR</em></li>
</ol>

<p>The similarity is both will return <em>curses.ERR</em> when no button is pushed, so you need to add that case to your if statement</p>

<pre><code>#this will shut off the GPIO when nothing is pushed
elif char == curses.ERR:
    GPIO.output(7,False)
    GPIO.output(11,False)
    GPIO.output(13,False)
    GPIO.output(15,False)
</code></pre>

<p>But that code won't work until you do the setup. Replace the <strong>cbreak</strong> mode with whichever mode we are using</p>

<h1>Method 1: nodelay</h1>

<p>The advantage of this method is that it is nearly instantaneous. As soon as you let go of a key the bot should stop. 
Disadvantage, this may consume a lot of processing power. Since it will execute the loop very quickly, set GPIOs very quickly, read key quickly, etc it will do many cycles while not achieving much real work</p>

<pre><code>screen = curses.initscr()
curses.noecho()
curses.nodelay(True)
screen.keypad(True)
</code></pre>

<h1>Method 2: halfdelay</h1>

<p>The advantage of this method is it should lower the processor usage compared to nodelay method. 
The disadvantage is there is a delay after you release a key. So after  you release the key the bot would continue executing the previous command for some time (in my sample code .7 seconds) before it stops</p>

<pre><code>#the delay units are tenths of seconds, or deciseconds
#this is a .7 second delay 7*0.10sec = 0.7 seconds
delay_in_deciseconds = 7 

screen = curses.initscr()
curses.noecho()
curses.halfdelay(delay_in_deciseconds)
screen.keypad(True)
</code></pre>
",2705382,0,0,,,Remote
407,2980,48109985,ros gazebo skipped loading plugin with error,|ros|robotics|,"<p>When I run my gazebo, I get the following error:</p>

<pre><code>[ERROR] [1515141508.242475977]: Skipped loading plugin with error: XML Document '/opt/ros/kinetic/share/gmapping/nodelet_plugins.xml' has no Root Element. This likely means the XML is malformed or missing..
[ERROR] [1515141508.249164933]: Skipped loading plugin with error: XML Document '/opt/ros/kinetic/share/gmapping/nodelet_plugins.xml' has no Root Element. This likely means the XML is malformed or missing..
</code></pre>

<p>Even with the errors, gazebo seems to work fine but I would like to fix it just to be safe. However, I'm not sure what the error messages mean and how I can fix it. </p>
",43105.36181,,2009,2,0,0,,5192123,United States,42221.15139,418,48337790,"<p>Can you verify if there is any ""nodelet_plugins.xml"" file is present in the /opt/ros/kinetic/share/gmapping directory?</p>
",9238248,0,0,,,Error
408,3604,58982694,Turning Issue With Vex Robot C++,|c++|robotics|,"<p>When we use the joystick with axises 1 and 2 to turn, when we turn left the robot turns right and when we turn right the robot turns left. We've tried switching different values to make some negative to reverse this but nothing seems to work. We've also tried making the left motor reversed instead of the right motor and it corrected the problem but the forward and backward are switched. </p>

<pre><code>  while (true) { 
      int rightSpeed= (Controller1.Axis3.position(vex::pct) + 
                              (Controller1.Axis1.position(vex::pct) + Controller1.Axis4.position(vex::pct)));

      int leftSpeed= (Controller1.Axis1.position(vex::pct) - 
                              (Controller1.Axis2.position(vex::pct) + Controller1.Axis3.position(vex::pct)));

      if (leftSpeed&gt;15||leftSpeed&lt;-15){
          RightMotor.spin(vex::directionType::fwd, leftSpeed, vex::velocityUnits::pct);   
      }

      if (rightSpeed&gt;15||rightSpeed&lt;-15){
           LeftMotor.spin(vex::directionType::fwd, rightSpeed, vex::velocityUnits::pct);
           RightMotor.spin(vex::directionType::fwd, -leftSpeed, vex::velocityUnits::pct);    
      }  
  }
</code></pre>
",43790.82431,,126,0,2,0,,11857443,,43676.49861,5,,,,,,104656027,The best option to get a good result would be to go to the vex forums (vexforums.com) and there is a wider range of people there who know the answers to the questions,Remote
409,3364,55360519,"Obtaining dimensions for Stewart Platform for arm, base and top plate",|robotics|servo|kinematics|,"<p>I am in a need to calculate the Stewart platform dimensions for an application where the Stewart platform top plate can be, consisting of a width of 19Inches maximum, and length 19Inches. Also, the height should not exceed, 25Inches. The Stewart platform should be able to withstand 100kg. The maximum tilt angle expected to achieve is 20Degrees. This Stewart platform operates using 6 servo motors at each leg. </p>

<p>The following website was referred to obtain the calculation so far.
<a href=""https://memememememememe.me/post/stewart-platform-math/"" rel=""nofollow noreferrer"">https://memememememememe.me/post/stewart-platform-math/</a></p>
",43550.63403,,449,0,2,0,,11261153,,43550.62292,1,,,,,,97445102,please show us what you did so far? maybe show us a [mcve] of what you are trying to achieve.,Specifications
410,3433,57008332,OpenCV: Wrong result in calibrateHandEye function,|c++|opencv|computer-vision|camera-calibration|robotics|,"<p>I am working in a robot application, in which I have a camera fixed to a robot gripper. In order to calculate the matrix transformation between the camera and the gripper Hcg I am using the calibrateHandEye new function provided in the OpenCV version 4.1.0 </p>

<p>I had taken 10 pictures of the chessboard from the camera mounted in the gripper and at the same time I recorded the robot position. </p>

<p>The code I am working on:</p>

<pre><code>// My_handeye.cpp : This file contains the 'main' function. Program execution begins and ends there.
//
#include &lt;iostream&gt;
#include &lt;sstream&gt;
#include &lt;string&gt;
#include &lt;ctime&gt;
#include &lt;cstdio&gt;
#include ""pch.h""

#include &lt;opencv2/opencv.hpp&gt;
#include &lt;opencv2/core.hpp&gt;
#include &lt;opencv2/core/utility.hpp&gt;
#include &lt;opencv2/imgproc.hpp&gt;
#include &lt;opencv2/calib3d.hpp&gt;
#include &lt;opencv2/imgcodecs.hpp&gt;
#include &lt;opencv2/videoio.hpp&gt;
#include &lt;opencv2/highgui.hpp&gt;

using namespace cv;
using namespace std;

Mat eulerAnglesToRotationMatrix(Vec3f &amp;theta);
Vec3f rotationMatrixToEulerAngles(Mat &amp;R);
float rad2deg(float radian);
float deg2rad(float degree);

int main()
{

    // Camera calibration information

    std::vector&lt;double&gt; distortionCoefficients(5);  // camera distortion
    distortionCoefficients[0] = 2.4472856611074989e-01;
    distortionCoefficients[1] = -8.1042032574246325e-01;
    distortionCoefficients[2] = 0;
    distortionCoefficients[3] = 0;
    distortionCoefficients[4] = 7.8769462320821060e-01;

    double f_x = 1.3624172121852105e+03; // Focal length in x axis
    double f_y = 1.3624172121852105e+03; // Focal length in y axis (usually the same?)
    double c_x = 960; // Camera primary point x
    double c_y = 540; // Camera primary point y

    cv::Mat cameraMatrix(3, 3, CV_32FC1);
    cameraMatrix.at&lt;float&gt;(0, 0) = f_x;
    cameraMatrix.at&lt;float&gt;(0, 1) = 0.0;
    cameraMatrix.at&lt;float&gt;(0, 2) = c_x;
    cameraMatrix.at&lt;float&gt;(1, 0) = 0.0;
    cameraMatrix.at&lt;float&gt;(1, 1) = f_y;
    cameraMatrix.at&lt;float&gt;(1, 2) = c_y;
    cameraMatrix.at&lt;float&gt;(2, 0) = 0.0;
    cameraMatrix.at&lt;float&gt;(2, 1) = 0.0;
    cameraMatrix.at&lt;float&gt;(2, 2) = 1.0;

    Mat rvec(3, 1, CV_32F), tvec(3, 1, CV_32F);
    //

    std::vector&lt;Mat&gt; R_gripper2base;
    std::vector&lt;Mat&gt; t_gripper2base;
    std::vector&lt;Mat&gt; R_target2cam;
    std::vector&lt;Mat&gt; t_target2cam;
    Mat R_cam2gripper = (Mat_&lt;float&gt;(3, 3));
    Mat t_cam2gripper = (Mat_&lt;float&gt;(3, 1));

    vector&lt;String&gt; fn;
    glob(""images/*.bmp"", fn, false);

    vector&lt;Mat&gt; images;
    size_t num_images = fn.size(); //number of bmp files in images folder
    Size patternsize(6, 8); //number of centers
    vector&lt;Point2f&gt; centers; //this will be filled by the detected centers
    float cell_size = 30;
    vector&lt;Point3f&gt; obj_points;

    R_gripper2base.reserve(num_images);
    t_gripper2base.reserve(num_images);
    R_target2cam.reserve(num_images);
    t_target2cam.reserve(num_images);

    for (int i = 0; i &lt; patternsize.height; ++i)
        for (int j = 0; j &lt; patternsize.width; ++j)
            obj_points.push_back(Point3f(float(j*cell_size),
                float(i*cell_size), 0.f));

    for (size_t i = 0; i &lt; num_images; i++)
        images.push_back(imread(fn[i]));

    Mat frame;

    for (size_t i = 0; i &lt; num_images; i++)
    {
        frame = imread(fn[i]); //source image
        bool patternfound = findChessboardCorners(frame, patternsize, centers);
        if (patternfound)
        {
            drawChessboardCorners(frame, patternsize, Mat(centers), patternfound);
            //imshow(""window"", frame);
            //int key = cv::waitKey(0) &amp; 0xff;
            solvePnP(Mat(obj_points), Mat(centers), cameraMatrix, distortionCoefficients, rvec, tvec);

            Mat R;
            Rodrigues(rvec, R); // R is 3x3
            R_target2cam.push_back(R);
            t_target2cam.push_back(tvec);
            Mat T = Mat::eye(4, 4, R.type()); // T is 4x4
            T(Range(0, 3), Range(0, 3)) = R * 1; // copies R into T
            T(Range(0, 3), Range(3, 4)) = tvec * 1; // copies tvec into T

            cout &lt;&lt; ""T = "" &lt;&lt; endl &lt;&lt; "" "" &lt;&lt; T &lt;&lt; endl &lt;&lt; endl;

        }
        cout &lt;&lt; patternfound &lt;&lt; endl;
    }

    Vec3f theta_01{ deg2rad(-153.61), deg2rad(8.3),   deg2rad(-91.91) };
    Vec3f theta_02{ deg2rad(-166.71), deg2rad(3.04),  deg2rad(-93.31) };
    Vec3f theta_03{ deg2rad(-170.04), deg2rad(24.92), deg2rad(-88.29) };
    Vec3f theta_04{ deg2rad(-165.71), deg2rad(24.68), deg2rad(-84.85) };
    Vec3f theta_05{ deg2rad(-160.18), deg2rad(-15.94),deg2rad(-56.24) };
    Vec3f theta_06{ deg2rad(175.68),  deg2rad(10.95), deg2rad(180) };
    Vec3f theta_07{ deg2rad(175.73),  deg2rad(45.78), deg2rad(-179.92) };
    Vec3f theta_08{ deg2rad(-165.34), deg2rad(47.37), deg2rad(-166.25) };
    Vec3f theta_09{ deg2rad(-165.62), deg2rad(17.95), deg2rad(-166.17) };
    Vec3f theta_10{ deg2rad(-151.99), deg2rad(-14.59),deg2rad(-94.19) };

    Mat robot_rot_01 = eulerAnglesToRotationMatrix(theta_01);
    Mat robot_rot_02 = eulerAnglesToRotationMatrix(theta_02);
    Mat robot_rot_03 = eulerAnglesToRotationMatrix(theta_03);
    Mat robot_rot_04 = eulerAnglesToRotationMatrix(theta_04);
    Mat robot_rot_05 = eulerAnglesToRotationMatrix(theta_05);
    Mat robot_rot_06 = eulerAnglesToRotationMatrix(theta_06);
    Mat robot_rot_07 = eulerAnglesToRotationMatrix(theta_07);
    Mat robot_rot_08 = eulerAnglesToRotationMatrix(theta_08);
    Mat robot_rot_09 = eulerAnglesToRotationMatrix(theta_09);
    Mat robot_rot_10 = eulerAnglesToRotationMatrix(theta_10);

    const Mat robot_tr_01 = (Mat_&lt;float&gt;(3, 1) &lt;&lt; 781.2, 338.59, 903.48);
    const Mat robot_tr_02 = (Mat_&lt;float&gt;(3, 1) &lt;&lt; 867.65, 382.52, 884.42);
    const Mat robot_tr_03 = (Mat_&lt;float&gt;(3, 1) &lt;&lt; 856.91, 172.99, 964.61);
    const Mat robot_tr_04 = (Mat_&lt;float&gt;(3, 1) &lt;&lt; 748.81, 146.75, 1043.29);
    const Mat robot_tr_05 = (Mat_&lt;float&gt;(3, 1) &lt;&lt; 627.66, 554.08, 920.85);
    const Mat robot_tr_06 = (Mat_&lt;float&gt;(3, 1) &lt;&lt; 715.06, 195.96, 889.38);
    const Mat robot_tr_07 = (Mat_&lt;float&gt;(3, 1) &lt;&lt; 790.9, 196.29, 1117.38);
    const Mat robot_tr_08 = (Mat_&lt;float&gt;(3, 1) &lt;&lt; 743.5, 283.93, 1131.92);
    const Mat robot_tr_09 = (Mat_&lt;float&gt;(3, 1) &lt;&lt; 748.9, 288.19, 910.58);
    const Mat robot_tr_10 = (Mat_&lt;float&gt;(3, 1) &lt;&lt; 813.18, 400.44, 917.16);

    R_gripper2base.push_back(robot_rot_01);
    R_gripper2base.push_back(robot_rot_02);
    R_gripper2base.push_back(robot_rot_03);
    R_gripper2base.push_back(robot_rot_04);
    R_gripper2base.push_back(robot_rot_05);
    R_gripper2base.push_back(robot_rot_06);
    R_gripper2base.push_back(robot_rot_07);
    R_gripper2base.push_back(robot_rot_08);
    R_gripper2base.push_back(robot_rot_09);
    R_gripper2base.push_back(robot_rot_10);

    t_gripper2base.push_back(robot_tr_01);
    t_gripper2base.push_back(robot_tr_02);
    t_gripper2base.push_back(robot_tr_03);
    t_gripper2base.push_back(robot_tr_04);
    t_gripper2base.push_back(robot_tr_05);
    t_gripper2base.push_back(robot_tr_06);
    t_gripper2base.push_back(robot_tr_07);
    t_gripper2base.push_back(robot_tr_08);
    t_gripper2base.push_back(robot_tr_09);
    t_gripper2base.push_back(robot_tr_10);

    calibrateHandEye(R_gripper2base, t_gripper2base, R_target2cam, t_target2cam, R_cam2gripper, t_cam2gripper, CALIB_HAND_EYE_TSAI);

    Vec3f R_cam2gripper_r = rotationMatrixToEulerAngles(R_cam2gripper);

    cout &lt;&lt; ""R_cam2gripper = "" &lt;&lt; endl &lt;&lt; "" "" &lt;&lt; R_cam2gripper &lt;&lt; endl &lt;&lt; endl;
    cout &lt;&lt; ""R_cam2gripper_r = "" &lt;&lt; endl &lt;&lt; "" "" &lt;&lt; R_cam2gripper_r &lt;&lt; endl &lt;&lt; endl;
    cout &lt;&lt; ""t_cam2gripper = "" &lt;&lt; endl &lt;&lt; "" "" &lt;&lt; t_cam2gripper &lt;&lt; endl &lt;&lt; endl;
}

Mat eulerAnglesToRotationMatrix(Vec3f &amp;theta)
{
    // Calculate rotation about x axis
    Mat R_x = (Mat_&lt;double&gt;(3, 3) &lt;&lt;
        1, 0, 0,
        0, cos(theta[0]), -sin(theta[0]),
        0, sin(theta[0]), cos(theta[0])
        );

    // Calculate rotation about y axis
    Mat R_y = (Mat_&lt;double&gt;(3, 3) &lt;&lt;
        cos(theta[1]), 0, sin(theta[1]),
        0, 1, 0,
        -sin(theta[1]), 0, cos(theta[1])
        );

    // Calculate rotation about z axis
    Mat R_z = (Mat_&lt;double&gt;(3, 3) &lt;&lt;
        cos(theta[2]), -sin(theta[2]), 0,
        sin(theta[2]), cos(theta[2]), 0,
        0, 0, 1);


    // Combined rotation matrix
    Mat R = R_z * R_y * R_x;

    return R;

}

float rad2deg(float radian) {
    double pi = 3.14159;
    return(radian * (180 / pi));
}

float deg2rad(float degree) {
    double pi = 3.14159;
    return(degree * (pi / 180));
}

// Checks if a matrix is a valid rotation matrix.
bool isRotationMatrix(Mat &amp;R)
{
    Mat Rt;
    transpose(R, Rt);
    Mat shouldBeIdentity = Rt * R;
    Mat I = Mat::eye(3, 3, shouldBeIdentity.type());

    return  norm(I, shouldBeIdentity) &lt; 1e-6;

}

// Calculates rotation matrix to euler angles
// The result is the same as MATLAB except the order
// of the euler angles ( x and z are swapped ).
Vec3f rotationMatrixToEulerAngles(Mat &amp;R)
{

    assert(isRotationMatrix(R));

    float sy = sqrt(R.at&lt;double&gt;(0, 0) * R.at&lt;double&gt;(0, 0) + R.at&lt;double&gt;(1, 0) * R.at&lt;double&gt;(1, 0));

    bool singular = sy &lt; 1e-6; // If

    float x, y, z;
    if (!singular)
    {
        x = atan2(R.at&lt;double&gt;(2, 1), R.at&lt;double&gt;(2, 2));
        y = atan2(-R.at&lt;double&gt;(2, 0), sy);
        z = atan2(R.at&lt;double&gt;(1, 0), R.at&lt;double&gt;(0, 0));
    }
    else
    {
        x = atan2(-R.at&lt;double&gt;(1, 2), R.at&lt;double&gt;(1, 1));
        y = atan2(-R.at&lt;double&gt;(2, 0), sy);
        z = 0;
    }
    return Vec3f(x, y, z);

}
</code></pre>

<p>The result the function is giving me is the next one:</p>

<pre><code>R_cam2gripper =
 [0.3099803593003124, -0.8923086952824562, -0.3281727733547833;
 0.7129271761196039, 0.4465219155360299, -0.5406967916458927;
 0.6290047840821058, -0.0663579028402444, 0.7745641421680119]

R_cam2gripper_r =
 [-0.0854626, -0.680272, 1.16065]

t_cam2gripper =
 [-35.02063730299775;
 -74.80633768251272;
 -307.6725851251873]
</code></pre>

<p>I am getting  'good' results provided by other software. With them, the robot got to the exact points I am pointing in the camera (I have a 3D camera, from which I am getting the x, y, z from the camera world) so they are certainly correct, but I am having troubles to repeat the same result with the OpenCV function. </p>

<p>Sorry for the long introduction to my problem. Any understanding of why the solutions are not what is supposed to be? My guess is, that I have a problem understanding the angles or converting them but I couldn't find any way to solve this. Any hint will be well welcome!</p>
",43658.57569,57383985,1417,1,0,0,,11775921,Chile,43658.57222,16,57383985,"<p>I actually managed to solve this problem. The general idea was correct, but:</p>

<ol>
<li>I was not understanding correctly the vector rotation notation the robot was giving. It was necessary to multiply the actual values by a factor.</li>
<li>I created a new program that extracts directly from the robot and the pictures the matrixes that the algorithm requires and writes these values to a YML file.</li>
<li>The CALIB_HAND_EYE_TSAI method wasn't giving me correct values. But with the four others, the values seem to converge to the actual values</li>
</ol>

<p>Anyway, thank you for your help. I am stuck to get more precision in the algorithm, but that's for another question.</p>
",11775921,0,0,,,Coordinates
411,3273,54358550,Pepper Robot Connected But Show Offline On Connected Device,|android|android-studio|windows-7|robot|pepper|,"<p>I try to connect to the Pepper robot using my Windows 7 laptop. It is connected and I am able to view the Robot Viewer using Android Studio, but it shows ""offline"" when I want to deploy the Pepper robot.</p>

<p>I have tried:</p>

<ol>
<li>uninstall Vysor</li>
<li>kill adb.exe on my task manager</li>
<li>Restart the robot and my PC</li>
</ol>

<p>Any other solutions? </p>

<p>Screenshot showing the problem:</p>

<p><img src=""https://i.stack.imgur.com/vOfto.png"" alt=""Screenshot""></p>
",43490.14236,,240,1,0,1,,6145865,,42461.66319,5,54997410,"<p>Are your SDK platform-tools up-to-date? Also, have you activated the debug option on the tablet? Are you asked for your password when you connect the robot?  </p>

<p>Try to run <code>adb kill-server</code> by command line and then try again:  </p>

<p>1- Disconnect the robot<br>
2- Connect again (make sure you connect to the <strong>robot</strong> ip and not the tablet ip)<br>
3- Try to select target</p>
",4949074,0,0,,,Remote
412,3530,58056874,How to create an occupancy grid map in python similar to Matlab occupancy grid,|python|grid|robotics|,"<p>I am aiming to create an occupancy grid as following in Matlab:</p>

<p>(<a href=""https://au.mathworks.com/help/robotics/ug/occupancy-grids.html"" rel=""nofollow noreferrer"">https://au.mathworks.com/help/robotics/ug/occupancy-grids.html</a>)</p>

<pre><code>map = binaryOccupancyMap(10,10,5);
setOccupancy(map,[5 5], 1);
</code></pre>

<p>I have googled and got overwhelmed with Python's robotics algorithms. 
Could anyone please help to instruct a simple way to do in python?</p>
",43731.27778,,2541,0,0,2,,10122673,Australia,43304.55139,9,,,,,,,,Moving
413,3318,54949446,Java: duration of movement of Finch Robot,|java|eclipse|robot|finch|,"<p>I'm new to programming. I have code for my Finch robot that simply loops a zigzag section after the user inputs how many times it should loop for, but how do I input another question that asks how long each zigzag section should be?</p>

<p>For example, the first question I ask is how many zigzag sections the user wants to loop for, but I also want to ask how long each zigzag segment should be (how long each line should be before it turns the other way).</p>

<p>Code:</p>

<pre class=""lang-java prettyprint-override""><code>Finch myFinch = new Finch();
Scanner sc = new Scanner(System. in );

System.out.println(""Welcome! Complete the following entries"");
System.out.println(""Number of zigzag sections:  "");

int noOfTimes = sc.nextInt();

do {
    myFinch.setLED(Color.green);
    myFinch.buzz(600, 2250);
    myFinch.setWheelVelocities(180, 0, 750);
    myFinch.setWheelVelocities(100, 100, 1500);
    myFinch.setLED(Color.red);
    myFinch.buzz(600, 2350);
    myFinch.setWheelVelocities(0, 180, 850);
    myFinch.setWheelVelocities(180, 180, 1500);
    noOfTimes--;

} while ( noOfTimes &gt; 0 );

myFinch.quit();
System.exit(0);
</code></pre>
",43525.72083,54951801,225,1,1,0,,11120121,"Leyton, London, UK",43522.63264,16,54951801,"<p>Check java <code>Scanner</code> doccumentation <a href=""https://docs.oracle.com/javase/7/docs/api/java/util/Scanner.html"" rel=""nofollow noreferrer"">here</a></p>

<p>Code sample to accept multiple inputs</p>

<pre><code>import java.util.Scanner;

class GetInputFromUser
{
   public static void main(String args[])
   {
      int a;
      float b;
      String s;

      Scanner in = new Scanner(System.in);

      System.out.println(""Enter an integer"");
      a = in.nextInt();
      System.out.println(""You entered integer "" + a);

      System.out.println(""Enter a float"");
      b = in.nextFloat();
      System.out.println(""You entered float "" + b);  

      System.out.println(""Enter a string"");
      s = in.nextLine();
      System.out.println(""You entered string "" + s);
   }
}
</code></pre>
",3065198,0,0,96664163,"What have you tried? Can you see where the other values are asked for and then scanned in? What happens if you try it yourself? Unfortunately, SO is not a great tutorial site, so you are encouraged to figure this out yourself with the many tutorials you can find online.",Moving
414,3382,55965675,From camera calibration to picking up colored cubes with robot arm,|python|opencv|camera-calibration|robotics|coordinate-transformation|,"<p>I want to pick up colored cubes with a robot arm that I detect with a camera and OpenCV, in Python. I managed to detect the different colored cubes and I calibrated the camera with the checkerboard process.</p>

<p><strong>The setup:</strong></p>

<p><a href=""https://i.stack.imgur.com/dt0rH.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dt0rH.jpg"" alt=""Setup front""></a>
<a href=""https://i.stack.imgur.com/rpWsl.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rpWsl.jpg"" alt=""Setup top""></a></p>

<p><strong>Cube detection:</strong>
<a href=""https://i.stack.imgur.com/wYvNL.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wYvNL.jpg"" alt=""Cubes detected""></a></p>

<p>The problem is that I don't understand the rest of the process of getting the coordinates from the object with the camera and translating them to the robot arm for pick up.</p>

<p><strong>The following steps have been completed:</strong></p>

<ol>
<li><p>Separating color boundaries with HSV and drawing bounding boxes. So I have the pixel x, y of the object.</p></li>
<li><p>Calibrating the camera resulting in the following camera matrix and distortion coefficients: </p>

<p>Camera Matrix: [[1.42715609e+03 0.00000000e+00 9.13700651e+02]  [0.00000000e+00 1.43275509e+03 5.58917609e+02]  [0.00000000e+00
0.00000000e+00 1.00000000e+00]]</p>

<p>Distortion:[[ 0.03924722 -0.30622971  0.00124042 -0.00303094  0.49458539]]</p></li>
<li><p>Trying to figure out the next steps in the documentation of OpenCV</p></li>
</ol>

<p><strong>The result</strong></p>

<p>I read through the API documentation on this page: <a href=""https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html"" rel=""nofollow noreferrer"">https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html</a></p>

<p>But with my skill level I can't seem the extract the practical steps to take, in order to attain my goal.</p>

<p>My questions:</p>

<ol>
<li>How do I use the camera matrix and distortion coefficients to get coordinates of the object in the image frame?</li>
<li>How do I translate the coordinates on the image frame to the robot end-effector coordinates?</li>
<li>If I keep the camera on a fixed position. Does this mean I only have to do the calibration ones?</li>
</ol>

<p>******* <strong>edit:</strong> *******</p>

<p>I went for a different approach. I managed to solve the rotation and translation between two coordinate systems through SVD. But I made an error in thinking I could use pixel coordinates to translate from the camera coordinate system to that of the robot. I think you need to translate the u, v values first. </p>

<p><strong>How can I translate the pixel uv to world coordinates, so that I can use the code below to get translation and rotation to robot arm?</strong></p>

<p>Here's my code:</p>

<pre><code>#######################################################################
# Step 1: Input camera and world coordinates, and calculate centroids #
#######################################################################
print("""")

# Camera and robot to world coordinates
Pc = np.matrix([[604,119,0],[473,351,0], [730,329,0]])
print(""Camera points matrix: "")
print(Pc)
print("""")

Pr = np.matrix([[177,-38,0],[264,-93,0], [258,4.7,0]])
print(""Robot points matrix: "")
print(Pr)
print("""")

# Calculate centroids
Cc = Pc.mean(axis=0)
Cr = Pr.mean(axis=0)

print(""Centroid camera: "")
print(Cc)
print("""")
print(""Centroid robot: "")
print(Cr)
print("""")

# Pc and Pr - centroids of Pc and Pr
Pc_minus_Cc = np.subtract(Pc, Cc)
print(""Pc - centroidC: "")
print(Pc_minus_Cc)
print("""")

Pr_minus_Cr = np.subtract(Pr, Cr)
print(""Pr - centroidR: "")
print(Pr_minus_Cr)
print("""")

############################################################################
# Step 2: Calculate H, perform SVD and get rotation and translation matrix #
############################################################################

# Get H
print(""(Pr - centroidR) transposed: "")
print(Pr_minus_Cr.T)
print("""")
H = np.matmul(Pc_minus_Cc, Pr_minus_Cr.T)
print(""H: "")
print(H)
print("""")

# Perform SVD
u, s, v = np.linalg.svd(H)
print(""SVD result: "")
print(""u: "")
print("""")
print(u)
print("""")
print(""s: "")
print(s)
print("""")
print(""v: "")
print(v)
print("""")

# Calculate rotation matrix
R = np.matmul(v,u.T)
print(""Rotation matrix: "")
print(R)

# Calculate t
t = -R * Cc.T + Cr.T
print(""t: "")
print(t)
</code></pre>
",43588.33819,,1696,1,3,0,0,4581914,,42054.02222,39,55968784,"<p>to 1) you have already code which draws boxes around detected objects. So you have already coordinates in your matrix. If not you could do something like that.</p>

<pre><code>        for c in contours:
        if cv2.contourArea(c) &lt; self.min_area:
            continue
        # detected
        (x, y, w, h) = cv2.boundingRect(c)
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
</code></pre>

<p>and than x + w / 2 is your x in the matrix</p>

<p>to 2 ) you cannot directly you would need someorientation point so that the Arm 
would know where (x+y distance) starts your matrix in the world of the arm</p>

<p>to 3) calibartion depends always on your light conditions right ? so as long they don't change your calibartion should be fine. However it turns out the calibration is needed from time to time e.g. with usb cameras</p>
",3732793,0,4,98582618,"The camera matrix is related to the camera per se (focal length, image size, etc) I suggest [this article](http://ksimek.github.io/2013/08/13/intrinsic/). So in other words, for question 3, If you have the same camera/resolution, the calibration (in this case the camera matrix) is the same. For question 1 and 2, you need depth, or know points which you can get the depth out of it. You can take a look to [this](https://www.pyimagesearch.com/2015/01/19/find-distance-camera-objectmarker-using-python-opencv/) and [this](https://docs.opencv.org/3.3.0/dc/d2c/tutorial_real_time_pose.html)",Moving
415,3542,58584411,Java: Instance of Class B in Class A where Class B uses variables declared in Class A,|java|class|robotics|,"<p>AutoRedBuilding class declares instance variables leftMotor, rightMotor and foundationServo. RobotMover class contains methods that use those instance variables and I would like to call the RobotMover methods in AutoRedBuilding. </p>

<p>Specifically, the instance variables declared in AutoRedBuilding are actuators like motors and servos. RobotMover is a class that contains methods which move the actuators in a certain way such as driving forward and turning. I would like to somehow call the RobotMover methods in AutoRedBuilding. </p>

<p>I've tried making the AutoRedBuilding variables public, then creating an instance of RobotMover in AutoRedBuilding but that didn't work (code below). Example error message (I get the same error message for all the actuator variables): </p>

<pre><code>RobotMover.java - cannot find symbol
symbol: leftMotor
</code></pre>

<pre><code>// Copyright (c) 2019 Terrace BroBots. All rights reserved.

package org.firstinspires.ftc.teamcode;

import com.qualcomm.robotcore.eventloop.opmode.Autonomous;
import com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;
import com.qualcomm.robotcore.hardware.DcMotor;
import com.qualcomm.robotcore.hardware.Servo;

@Autonomous(name=""Autonomous: Red Building Zone"", group=""SkyStone"")

public class AutoRedBuilding extends LinearOpMode {

    // declare hardware variables
    public DcMotor leftMotor;
    public DcMotor rightMotor;
    public Servo foundationServo;

    @Override
    public void runOpMode() {
        // initialise hardware variables
        leftMotor = hardwareMap.get(DcMotor.class, ""leftMotor"");
        rightMotor = hardwareMap.get(DcMotor.class, ""rightMotor"");
        foundationServo = hardwareMap.get(Servo.class, ""foundationServo"");

        // wait for game to start
        waitForStart();

        /* 
        THIS IS WHERE RobotMover CLASS METHODS ARE CALLED
        */
        RobotMover mover = new RobotMover();
        mover.driveForward(10, 1);
        mover.driveReverse(10, 1);
        mover.turnRight90();
        mover.turnLeft90();
        mover.clipFoundation();
        mover.unclipFoundation();
    }
}

</code></pre>

<pre><code>// Copyright (c) 2019 Terrace BroBots. All rights reserved.

import java.lang.Math; 

public class RobotMover {

    // unit conversion rates
    private double robotWidthCm = 0;
    private double wheelRadiusCm = 0;
    private double degree90ToCm = (2 * Math.PI * robotWidthCm) / 4;
    private double cmToTicks = 2240 / (2 * Math.PI * wheelRadiusCm);

    public void setMotorPowers(double leftPower, double rightPower) {
        leftMotor.setPower(leftPower);
        rightMotor.setPower(rightPower);
    }

    public void driveMotorDistances(double cmLeftDistance, double cmRightDistance, double power) {
        // reset encoders
        leftMotor.setMode(DcMotor.RunMode.STOP_AND_RESET_ENCODER);
        rightMotor.setMode(DcMotor.RunMode.STOP_AND_RESET_ENCODER);

        // convert cm to ticks and set target position
        int tickLeftDistance = (int) Math.round(cmLeftDistance * cmToTicks);
        int tickRightDistance = (int) Math.round(cmRightDistance * cmToTicks);
        leftMotor.setTargetPosition(tickLeftDistance);
        rightMotor.setTargetPosition(tickRightDistance);

        // drive until position is reached 
        setMotorPowers(power, power);
        while(leftMotor.isBusy() &amp;&amp; rightMotor.isBusy()) {}
        setMotorPowers(0, 0);
    }

    /*
    THESE ARE THE METHODS CALLED IN AutoRedBuilding CLASS
    */

    public void driveForward(double cmDistance, double power) {
        driveMotorDistances(cmDistance, cmDistance, power);
    }

    public void driveReverse(double cmDistance, double power) {
        driveMotorDistances(-cmDistance, -cmDistance, power);
    }

    public void turnRight90() {
        driveMotorDistances(degree90ToCm, 0, 0.8);
    }

    public void turnLeft90() {
        driveMotorDistances(0, degree90ToCm, 0.8);
    }

    public void clipFoundation() {
        foundationServo.setPosition(0.5);
    }

    public void unclipFoundation() {
        foundationServo.setPosition(0);
    }
}
</code></pre>
",43765.99861,,45,1,3,0,,9227818,,43117.24861,16,58584496,"<p>If you want <code>RobotMover</code> to know about these variables then they need to be passed to this class either in the constructor or as <code>setter</code> methods</p>

<p>e.g.</p>

<pre><code>RobotMover mover = new RobotMover(DCMotor left, DCMotor right); 
</code></pre>

<p>The <code>RobotMover</code> will need to have corresponding fields </p>
",2310289,0,0,103482962,"*but that didn't work* - why do you mean? Do you have some errors, if so please share.",Actuator
416,3336,55151675,My team's robot keeps spinning for a random amount of time and we don't know why,|java|methods|robotics|,"<p>We are using the modern robotics gyro.
We program in java.
We are using the base code provided by FTC for the First Tech Challenge.
Here is our gyro code as well as the turning code including the code that is supposed to stop the robot.</p>

<p>gyro = hardwareMap.get(ModernRoboticsI2cGyro.class, ""gyro"");</p>

<p>ModernRoboticsI2cGyro gyro;</p>

<p>private static final double GYRO_TOLERANCE = 4.00; // degrees
  private static final long GYRO_DELAY = 10; //ms</p>

<pre><code>public void turn(double angle, double power)
{

    Hw.frontLeftDrive.setMode(DcMotor.RunMode.RUN_USING_ENCODER);
    Hw.frontRightDrive.setMode(DcMotor.RunMode.RUN_USING_ENCODER);
    Hw.backLeftDrive.setMode(DcMotor.RunMode.RUN_USING_ENCODER);
    Hw.backRightDrive.setMode(DcMotor.RunMode.RUN_USING_ENCODER);

    // Sets angle above 0 and below 360 to prevent errors when plugging values in.
    angle = angle % 360;
    if(angle == 0 || power == 0) // if someone plugs in 0 for power or the angle, it just stops the statement and continues
        return;
    power = Math.abs(power);
    if(power &gt; 0.5) power = 0.5; // and this sets power to half power if anyone plugs in a higher value than half

    if(Math.abs(angle) &gt; 180.0) power = -power; //sets power sign (+ or -) to sign of angle

    //gets current angle and adds desired change to it
    float currentAngle = Hw.gyro.getHeading();
    float targetAngle = (float) (currentAngle + angle);


    // give robot some boundaries when checking gyro to prevent infinite checking of accuracy
    double tolerance = GYRO_TOLERANCE * Math.abs(power);
    long compensatedDelay = (long)(GYRO_DELAY / Math.abs(power));

    //make sure angle is over 0 or under 361
    if (targetAngle &gt; 360)
        targetAngle -= 360;
    else if (targetAngle &lt; 0)
        targetAngle += 360;

    double maxLimit = targetAngle + tolerance;
    double minLimit = targetAngle - tolerance;

    //set power states of motors
    Hw.frontLeftDrive.setPower(power);
    Hw.frontRightDrive.setPower(-power);
    Hw.backLeftDrive.setPower(power);
    Hw.backRightDrive.setPower(-power);

    // repeatedly check position for best accuracy until it falls into the set boundaries
    double current = Hw.gyro.getHeading();
    **while(current &gt; maxLimit || current &lt; minLimit){
        sleep(compensatedDelay);
        current = Hw.gyro.getHeading();
    }**
    //stops motors
    Hw.frontLeftDrive.setPower(0);
    Hw.frontRightDrive.setPower(0);
    Hw.backLeftDrive.setPower(0);
    Hw.backRightDrive.setPower(0);
}
</code></pre>
",43537.90694,,90,0,5,0,,,,,,,,,,,97044373,"hmm either it hops over the tolerance(during sleep) or adding tollerance lead you out of the gyro.getHeading() range(probably both). I see other parts that look confusing. Why don't adjust in the while the power. So you need some initial power(depending on the angle) and the closer you get to target, the slower you turn",Coordinates
417,3321,55081981,How to include a folder of libraries in C?,|c|include|libraries|clion|robotics|,"<p>I'm trying to include a folder that contains a combination of around 60 .h and .hpp files. This folder contains libraries for programming robots with a <a href=""https://www.kipr.org/kiss-institute-for-practical-robotics/hardware-software"" rel=""nofollow noreferrer"">Wallaby</a> (a mini-computer-like device) for Botball competition. <code>include</code> is located in the same place as <code>main.c</code> (inside <code>code</code>). Up until now, this is what my header for including libraries looks like:</p>

<pre><code>#include ""../code/include/accel.h""
</code></pre>

<p>Just like <code>accel.h</code>, I have 60 other .h and .hpp files inside <code>include</code>. So, coming to my question, do I need to type out all the 60 header lines? or is there a way to include the <code>include</code> folder. </p>

<p>I'm using Clion for this project, if I can't include the folder itself, does anyone know of a shortcut in Clion to include all the files in <code>include</code>.</p>

<p>I was also thinking of using some sort of placeholder for the folder name and only specify the file type. So, for example: <code>#include ""../code/include/(generic placeholder name).h""</code>. I have no clue if something like this exists.</p>

<p>I would also request you to keep in mind that I'm a beginner to programming, so please keep your answers simple.</p>

<p>This is just for some extra info:<br>
The Wallaby is a mini computer to which you would connect your sensors, motors, servos and cameras in order to control a robot for the Botball competition. Usually, one can connect to the Wallaby either via Wifi Direct or a cable and write programs on it directly through an online interface (not entirely sure of the word for it, but you just type in an IP address in your browser and it brings up an interface where you can make projects and code). All the code written in that interface saves directly onto the Wallaby. Here the default include statement is <code>#include &lt;kipr/botball.h&gt;</code>, so I'm assuming that <code>botball.h</code> (which is located on the Wallaby's storage) has all those 60 libraries consolidated in it. I got the include folder that I'm using from <a href=""https://github.com/kipr/libwallaby"" rel=""nofollow noreferrer"">GitHub</a>. This link was provided to me by one of the Botball organisers. So the main point in me trying to download the library is so that I can write and successfully compile code even when I'm not connected to the Wallaby. Hope this provides some relevant context. </p>

<p>Thank you for your answers!</p>
",43533.88194,,402,2,5,0,,11176928,,43533.81042,5,55082150,"<p>CLion is an IDE for Clang and GCC.  These compilers are instructed to search paths for include files by specifying <code>-I&lt;path&gt;</code> command line arguments.  Any number may be specified, and they are searched in the order given, and the first match found is the file that gets included.</p>

<p>I am not familiar with CLion specifically but no doubt it has a dialog somewhere where you can set header file search paths.</p>

<hr>

<p>Edit:  It seems that CLion may not make this so straightforward.  I understand that you have to add then via CMake: <a href=""https://cmake.org/cmake/help/v3.0/command/include_directories.html#command:include_directories"" rel=""nofollow noreferrer"">https://cmake.org/cmake/help/v3.0/command/include_directories.html#command:include_directories</a>, but after that, the IDE will not recognise the header in the editor and will warn you of unrecognised files and will not provide code comprehension features.  I believe it will build nonetheless.</p>
",168986,0,0,96909757,What do you want to achieve? Less typing? Faster something? Smaller something? More maintainability? More readability?,Error
418,3315,54935240,Robot reverses at high speed before following path given by ros nav stack,|localization|navigation|ros|robotics|motion-planning|,"<p>I am using ros nav stack in conjunction with google cartographer (Mapping and localization) to navigate the robot through a known map. Right now, the robot follows the path generated with acceptable accuracy. But,often, once the path has been generated, the robot reverses at the highest speed set in the params file (escape_velocity parameter), and then starts to move forward correctly on the genrated path.</p>

<p>I have attached images of all my param file: 1.<a href=""https://i.stack.imgur.com/fEzXV.png"" rel=""nofollow noreferrer"">Praram Files-1</a> 2. <a href=""https://i.stack.imgur.com/G3wKO.png"" rel=""nofollow noreferrer"">Param Files-2</a>. The name of each parameter file is mentioned at the top. But to avoid confusion, they are in the order:</p>

<p>A. Param Files-1:
   1. Global Costmap Params  2. Local Costmap Params  3. Common Costmap Params 
   4. Global Plnner Params  5. Local Planner Params</p>

<p>B. Param Files-2: Move Base Params</p>

<p>This is a link to a video of how it looks on rviz. <a href=""https://vimeo.com/320040685"" rel=""nofollow noreferrer"">https://vimeo.com/320040685</a></p>

<p>The thinner line in green is the plan generated by ros nav stack. The thicker line seen later in the video is the actual robot movement. You can see that the robot first reverses and then starts moving forward.</p>

<p>I am new to this forum so please let me know if I need to give anymore data for anyone to answer this</p>

<p>Has anyone else has this problem? Will appreciate any tips on fixing this! Thanks in advance!</p>

<p>P.S: I am using ROS Indigo on Ubuntu 14.04</p>
",43524.93403,,626,2,0,0,,11133212,"MI, USA",43524.92222,1,54947869,"<p>This is a list of observation that may help you to find the problem:</p>

<ol>
<li><p>Your robot seem not sure of is position at the beginning. Try to improve that to get a better navigation.</p></li>
<li><p>(I think it's your problem) Your robot start in the inflation layer of the local costmap. I don't see your local planner file but by default the robot will avoid to be in this place. 
Have a look at  : <a href=""http://wiki.ros.org/costmap_2d/hydro/inflation"" rel=""nofollow noreferrer"">Inflation Costmap Plugin</a></p></li>
<li><p>Check if your robot radius, obstacle avoidance and all that config match to your robot.</p></li>
<li><p>Navfn is quite old, it should works on your project but it might be nice to use global planner. Another local planner could also be tried.</p></li>
</ol>

<p>Hope this help.</p>

<p>Have a nice day! </p>
",8508485,0,1,,,Moving
419,3606,58997045,run background loop or callbacks concurrently with qt C++ application?,|c++|qt|callback|robotics|,"<p>I'm writing a qt5 application in C++ to control my robotic contraption. I'm using a Raspberry Pi running Ubuntu 18.04 to accomplish this. The program will never run on anything but Linux. I'm not too familiar with qt, but it seems like the common way to run a qt application is by returning </p>

<pre><code>int QApplication::exec()
</code></pre>

<p>in the main function. However, I need to run non-GUI-related code concurrently. For example, I need a callback to be executed every time my encoders change state. This callback has nothing to do with user input or the GUI. How can I make the main function receptive to the callback while it is running the qt application window at the same time?</p>

<p>The documentation says: </p>

<pre><code>To make your application perform idle processing, i.e., executing a special function whenever
there are no pending events, use a QTimer with 0 timeout. More advanced idle processing schemes 
can be achieved using processEvents().
</code></pre>

<p>I don't really understand how either of these methods are applicable to my situation. Should either of these schemes be used, or should I use another technique? A simple example would help me a lot.</p>

<p>Edit: I'm adding this due to the comments below:</p>

<p>I'd rather not sample in intervals, because it would most likely miss some changes in state completely. I'm reading the data output from an AM26C32 chip via input on my gpio pins of the Pi. Essentially, every change in state needs to execute a callback. Since there are 40,000 states per motor revolution, I cannot have periodic checks -- the callbacks need to be executed immediately. The encoders (sensors) are incremental and not absolute. Also, I'm using the pigpio library for gpio handling.</p>

<p>Edit 2: After a bit more reading, I think I'll need to use QThread with event loops. The Raspberry Pi 3B+ has 4 cores; if I run the GUI on one thread, and each (of 2) encoder on another thread, this may work. Does anyone have experience with this? Am I on the right track here?</p>
",43791.63403,58998285,941,1,4,0,,6928068,,42648.75833,14,58998285,"<p>It seems to me this is more about the design of the program to handle events, probably realtime asyou are talking about robotic control</p>

<p>As callback in qt application thread => Execution blocked by GUI events</p>

<p>In background thread => Mostly free from interruption from GUI events, as you can have control on the priority. See: <a href=""http://man7.org/linux/man-pages/man3/pthread_setschedparam.3.html"" rel=""nofollow noreferrer"">http://man7.org/linux/man-pages/man3/pthread_setschedparam.3.html</a></p>

<p>IMO handling in a background thread is a better options. Even if you are using Qt you can still use other threading / timer implementation.</p>

<p>The Frequency Counter 1 example from pigpio is still applicable</p>

<p>If you need communication between the callbacks and GUI, you can use a wait-free queue (e.g. <a href=""https://github.com/Taymindis/wfqueue"" rel=""nofollow noreferrer"">https://github.com/Taymindis/wfqueue</a> ) to pass message between them, while not being blocked by the progress of GUI thread</p>

<p>At the end you have to ensure the GPIO callback is can be completed in each interval.</p>
",2829853,0,8,104245202,"A `QTimer` (interval configurable in milliseconds) performs a periodic callback of a signal handler. If you have to read data periodically it might be sufficient but two additional requirements have to be matched: 1) Reading may not block. 2) Reading may not last too long. It's important that signal handlers return to event loop ASAP. (While a signal is handled the event loop is blocked.) For a blocking read, I see no alternative than to do it in an extra thread.",Timing
420,3283,54373318,"VSCode Java Project: Class file has wrong version 55.0, should be 52.0, Gradle Build Failing",|java|visual-studio-code|robotics|,"<p>I have a Java project in VSCode that is failing to build via Gradle and WPILib (FRC code).</p>

<p><a href=""https://i.stack.imgur.com/lPJZM.png"" rel=""nofollow noreferrer""> This is a screenshot of my terminal while trying to build.</a>
I've seen replies that say it's a problem with the JAVA_PATH variable, but that can't be it because this code is failing to build on multiple devices. It seems to be a problem with the software itself, has anyone had this problem before or have suggestions about how to fix it? I've checked my vendor libraries, all of them are up to date (the only one relevant here is REV Robotics, and that's the correct version.</p>
",43490.91389,,1170,1,1,1,0,9276670,,43127.59931,22,54373519,"<p>This errors say, that the classes inside SparkMax-java-1.0.27.jar were compiled with a newer Java compiler (Java 11) and your Gradle Build compiles with a Java 8 compiler. You have to set your Gradle build to compile with Java 11.</p>

<p>Should be build.gradle</p>

<pre><code>apply plugin: 'java'
sourceCompatibility = 11
targetCompatibility = 11
</code></pre>
",10519477,1,0,95561201,"Possible duplicate of [Class file has wrong version 52.0, should be 50.0](https://stackoverflow.com/questions/28180915/class-file-has-wrong-version-52-0-should-be-50-0)",Error
421,3267,54147301,Applying PID Controller for trajectory planning algorithm,|linear-algebra|robotics|pid-controller|,"<p>I am developing a path planning algorithm for a Segway robot. After running the algorithm path to the goal is obtained in form of <code>(x, y, theta)</code> coordinates. The hardware and sensor noises causing much trouble in following the path and error is accumulating. I thought of applying PID Control. I need to set the steering angle to the direction of movement by calculating the cross track error. </p>

<p>1) How to calculate the CTE with original co-ordinate <code>(x1, y1, theta1)</code> and current position <code>(x1', y1', theta1')</code>?</p>

<p>2) How to select values for <i>K<sub>p</sub></i>, <i>K<sub>i</sub></i> and <i>K<sub>d</sub></i> for this scenario?</p>

<p>Additional information: Development environment is Android studio.</p>
",43476.55208,54184184,577,1,2,0,,10579805,Germany,43403.43264,32,54184184,"<p>You can find the <i>K<sub>p</sub></i>, <i>K<sub>i</sub></i>, <i>K<sub>d</sub></i> values either experimentally or with a method like <a href=""https://en.wikipedia.org/wiki/Ziegler%E2%80%93Nichols_method"" rel=""nofollow noreferrer"">Ziegler-Nichols Method</a> which would be more accurate.</p>

<p>Experimentally, you can try:</p>

<ol>
<li>Set all gains to 0.</li>
<li>Increase <i>K<sub>d</sub></i> until the system oscillates.</li>
<li>Reduce <i>K<sub>d</sub></i> by a factor of 2-4.</li>
<li>Set <i>K<sub>p</sub></i> to about 1% of <i>K<sub>d</sub></i>.</li>
<li>Increase <i>K<sub>p</sub></i> until oscillations start.</li>
<li>Decrease <i>K<sub>p</sub></i> by a factor of 2-4.</li>
<li>Set <i>K<sub>i</sub></i> to about 1% of <i>K<sub>p</sub></i>.</li>
<li>Increase <i>K<sub>i</sub></i> until oscillations start.</li>
<li>Decrease <i>K<sub>i</sub></i> by a factor of 2-4.</li>
</ol>
",3636858,1,0,95392439,Cross track error,Moving
422,3739,63019490,Error with few arguments while running from ROS a python file,|python|pycharm|ros|robotics|,"<p>I am new to ROS and I am working with a Jaco 2 robotic arm (model 7 DOF)
Now I am trying to do some stuff to understand better how exactly it works</p>
<p>I found some code here:</p>
<p>[https://github.com/Kinovarobotics/kinova-ros/blob/master/kinova_demo/nodes/kinova_demo/pose_action_client.py][1]</p>
<p>to move the arm but I when I tried to run it from termninal with command : python JacoTutorial.py</p>
<p>I had a weird error that says:</p>
<p>**</p>
<pre><code>JacoTutorial.py [-h] [-r] [-v]
                       kinova_robotType [unit] [pose_value [pose_value ...]]
JacoTutorial.py: error: too few arguments
</code></pre>
<p>**</p>
<p>The problem is that because Pycharm doesnt recognize rospy and all ros libraries I cannot also debug it to see where the problem with the few argument exists...</p>
<p>I thought that maybe the code is only for 6 DOF Jaco but it doesnt seem to have different arguments for different models.</p>
<p>Does anybody face that error in the past ?</p>
",44033.71042,,150,1,0,0,,12737914,,43848.63611,7,63059513,"<p>It does not look like there is an error with the actual program, you need to pass arguments in the command line to the program to configure it correctly.</p>
<p>If you run <code>python JacoTutorial.py -h</code> (<code>-h</code> is an argument!), it will display the help prompt with all of the arguments you can pass on the command line. Looks like the only required argument is <code>kinova_robotType</code>.</p>
",8316278,0,0,,,Actuator
423,3727,62553569,problem while installing rock-robotics package in ubuntu 18.04 LTS,|installation|ubuntu-18.04|robotics|corba|,"<p>I've followed the instalation guide proposed at &quot;<a href=""https://www.rock-robotics.org/documentation/installation.html"" rel=""nofollow noreferrer"">https://www.rock-robotics.org/documentation/installation.html</a>&quot; .I'm having an error while installing &quot;rock-robotics&quot; I tried at first in my lighter distribution &quot;Lubuntu&quot; and now I'm trying in &quot;Ubuntu 18.04 LTS&quot; and it happens again, I'm getting crazy with this and the very problem is that I need this package to develop my degree final thesis. Please I need help on this matter, at the end you can see the full output when I run $sudo sh bootstrap.sh (I tried both of the bootstraps suggested and both of them give me the same problem) and here you have the <strong>output lines of the error message</strong>:</p>
<pre><code>configuring CMake for base/orogen/std   configured CMake for tools/pocolog_cpp   ERROR: got an error processing base/orogen/std, waiting for pending jobs to end   updated environment Command failed base/orogen/std(/home/emi/rock-robotics/base/orogen/std): failed in configure phase
    'cmake -DCMAKE_INSTALL_PREFIX=/home/emi/rock-robotics/install -DCMAKE_MODULE_PATH= -DCMAKE_PREFIX_PATH=/home/emi/rock-robotics/install;/home/emi/rock-robotics/tools/orogen
-DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DCMAKE_BUILD_TYPE=Release -DROCK_TEST_ENABLED=OFF /home/emi/rock-robotics/base/orogen/std' returned status 1
    see /home/emi/rock-robotics/install/log/base/orogen/std-configure.log for details
    last 10 lines are:
    -- Checking for module 'orocos-rtt-corba-gnulinux&gt;=2.1.0'
    --   No package 'orocos-rtt-corba-gnulinux' found
    CMake Error at .orogen/config/FindOrocosCORBA.cmake:8 (MESSAGE):
      RTT has not been built with CORBA support
    Call Stack (most recent call first):
      .orogen/typekit/transports/corba/CMakeLists.txt:4 (find_package)
   
   
    -- Configuring incomplete, errors occurred!
    See also &quot;/home/emi/rock-robotics/base/orogen/std/build/CMakeFiles/CMakeOutput.log&quot;.
</code></pre>
<p><strong>FULL-OUTPUT</strong></p>
<pre><code>--2020-06-24 12:02:16--  http://rock-robotics.org/master/autoproj_bootstrap
Resolviendo rock-robotics.org (rock-robotics.org)... 37.17.224.128
Conectando con rock-robotics.org (rock-robotics.org)[37.17.224.128]:80... conectado.
Petición HTTP enviada, esperando respuesta... 301 Moved Permanently
Ubicación: https://rock-robotics.org/master/autoproj_bootstrap [siguiente]
--2020-06-24 12:02:17--  https://rock-robotics.org/master/autoproj_bootstrap
Conectando con rock-robotics.org (rock-robotics.org)[37.17.224.128]:443... conectado.
Petición HTTP enviada, esperando respuesta... 301 Moved Permanently
Ubicación: https://www.rock-robotics.org/master/autoproj_bootstrap [siguiente]
--2020-06-24 12:02:17--  https://www.rock-robotics.org/master/autoproj_bootstrap
Resolviendo www.rock-robotics.org (www.rock-robotics.org)... 185.199.110.153, 185.199.111.153, 185.199.109.153, ...
Conectando con www.rock-robotics.org (www.rock-robotics.org)[185.199.110.153]:443... conectado.
Petición HTTP enviada, esperando respuesta... 200 OK
Longitud: 30078 (29K) [application/octet-stream]
Guardando como: “autoproj_bootstrap”

autoproj_bootstrap  100%[===================&gt;]  29,37K  --.-KB/s    en 0,03s   

2020-06-24 12:02:17 (906 KB/s) - “autoproj_bootstrap” guardado [30078/30078]

Which protocol do you want to use to access rock-core/buildconf.git on github.com? [git|ssh|http] (default: http) 
Detected 'gem' to be /usr/bin/gem2.5
Detected bundler at /home/emi/.local/share/autoproj/gems/ruby/2.5.0/bin/bundle
Installing autoproj in /home/emi/.local/share/autoproj/gems/ruby/2.5.0
Don't run Bundler as root. Bundler can ask for sudo if it is needed, and
installing your bundle as root will break this application for all non-root
users on this machine.
[DEPRECATED] The --binstubs option will be removed in favor of `bundle binstubs`
Fetching gem metadata from https://rubygems.org/......
Fetching gem metadata from https://rubygems.org/.
Resolving dependencies...
Using rake 12.3.3
Using equatable 0.5.0
Using tty-color 0.4.3
Using pastel 0.7.2
Using tty-cursor 0.5.0
Using necromancer 0.4.0
Using timers 4.3.0
Using tty-screen 0.6.5
Using wisper 2.0.1
Using tty-reader 0.2.0
Using tty-prompt 0.15.0
Using facets 3.1.0
Using utilrb 3.0.1
Using autobuild 1.20.0
Using backports 3.18.1
Using bundler 2.1.4
Using concurrent-ruby 1.0.5
Using ffi 1.13.1
Using rb-inotify 0.10.1
Using thor 0.20.3
Using tty-spinner 0.8.0
Using xdg 2.2.5
Using autoproj 2.12.1
Bundle complete! 2 Gemfile dependencies, 23 gems now installed.
Bundled gems are installed into `/home/emi/.local/share/autoproj/gems`
starting the newly installed autoproj for stage2 install
saving temporary env.sh and .autoproj/env.sh
running 'autoproj envsh' to generate a proper env.sh
[DEPRECATED] `Bundler.with_clean_env` has been deprecated in favor of `Bundler.with_unbundled_env`. If you instead want the environment before bundler was originally loaded, use `Bundler.with_original_env` (called at /home/emi/rock-robotics/.autoproj/bin/autoproj:8)
  Which prepackaged software (a.k.a. 'osdeps') should autoproj install automatically (all, none or a comma-separated list of: os gem pip) ?
    The software packages that autoproj will have to build may require other
    prepackaged softwares (a.k.a. OS dependencies) to be installed (RubyGems
    packages, packages from your operating system/distribution, ...). Autoproj
    is able to install those automatically for you.
    
    Advanced users may want to control this behaviour. Additionally, the
    installation of some packages require administration rights, which you may
    not have. This option is meant to allow you to control autoproj's behaviour
    while handling OS dependencies.
    
    * if you say &quot;all&quot;, it will install all packages automatically.
      This requires root access thru 'sudo'
    * if you say &quot;pip&quot;, only the Python packages will be installed.
      Installing these packages does not require root access.
    * if you say &quot;gem&quot;, only the Ruby packages will be installed.
      Installing these packages does not require root access.
    * if you say &quot;os&quot;, only the OS-provided packages will be installed.
      Installing these packages requires root access.
    * if you say &quot;none&quot;, autoproj will not do anything related to the
      OS dependencies.
    
    Finally, you can provide a comma-separated list of pip gem and os.
    
    As any configuration value, the mode can be changed anytime by calling
      autoproj reconfigure
    
    Finally, the &quot;autoproj osdeps&quot; command will give you the necessary information
    about the OS packages that you will need to install manually.
    
    So, what do you want ? (all, none or a comma-separated list of: os gem pip) [all] 
  Would you like autoproj to keep apt packages up-to-date? [yes] 
  updated environment
running 'autoproj osdeps' to re-install missing gems
[DEPRECATED] `Bundler.with_clean_env` has been deprecated in favor of `Bundler.with_unbundled_env`. If you instead want the environment before bundler was originally loaded, use `Bundler.with_original_env` (called at /home/emi/rock-robotics/.autoproj/bin/autoproj:8)
  updated environment
Command finished successfully at 2020-06-24 12:02:30 +0200 (took 1 sec)
The current directory is not empty, continue bootstrapping anyway ? [yes] 
  checked out autoproj main configuration


autoproj bootstrap successfully finished

To further use autoproj and the installed software, you
must add the following line at the bottom of your .bashrc:
source /home/emi/rock-robotics/env.sh

WARNING: autoproj will not work until your restart all
your consoles, or run the following in them:
$ source /home/emi/rock-robotics/env.sh

To import and build the packages, you can now run
aup
amake

The resulting software is installed in
/home/emi/rock-robotics/install

  How should I interact with github.com (git, http, ssh)
    If you give one value, it's going to be the method used for all access
    If you give multiple values, comma-separated, the first one will be
    used for pulling and the second one for pushing. An optional third value
    will be used to pull from private repositories (the same than pushing is
    used by default) [http,ssh] 
  operating system: ubuntu,debian - 18.04,18.04.4,lts,bionic,beaver
  updating bundler
  updating autoproj
  bundler: connected to https://rubygems.org/
  already up-to-date autoproj main configuration
  checking out git:https://github.com/rock-core/package_set.git interactive=false push_to=git@github.com:/rock-core/package_set.git repository_id=github:/rock-c  checked out git:https://github.com/rock-core/package_set.git interactive=false push_to=git@github.com:/rock-core/package_set.git repository_id=github:/rock-core/package_set.git retry_count=10
  Which flavor of Rock do you want to use ?
    Stay with the default ('master') if you want to use Rock on the most recent
    distributions (Ubuntu 16.04 and later). Use 'stable' only for 
    now officially unsupported distributions (Ubuntu 14.04) [master] 
  Do you want to activate python? [no] 
  checking out git:https://github.com/rock-core/rock-package_set.git interactive=false push_to=git@github.com:/rock-core/rock-package_set.git repository_id=gith  checked out git:https://github.com/rock-core/rock-package_set.git interactive=false push_to=git@github.com:/rock-core/rock-package_set.git repository_id=github:/rock-core/rock-package_set.git retry_count=10
  checking out git:https://github.com/rock-tutorials/tutorials-package_set.git interactive=false push_to=git@github.com:/rock-tutorials/tutorials-package_set.gi  checked out git:https://github.com/rock-tutorials/tutorials-package_set.git interactive=false push_to=git@github.com:/rock-tutorials/tutorials-package_set.git repository_id=github:/rock-tutorials/tutorials-package_set.git retry_count=10
  checking out git:https://github.com/orocos-toolchain/autoproj.git interactive=false push_to=git@github.com:/orocos-toolchain/autoproj.git repository_id=github  checked out git:https://github.com/orocos-toolchain/autoproj.git interactive=false push_to=git@github.com:/orocos-toolchain/autoproj.git repository_id=github:/orocos-toolchain/autoproj.git retry_count=10
  WARN: osdeps definition for cmake, previously defined in /home/emi/.local/share/autoproj/gems/ruby/2.5.0/gems/autoproj-2.12.1/lib/autoproj/default.osdeps overridden by /home/emi/rock-robotics/autoproj/remotes/rock.core/rock.osdeps:
  WARN:   resp. apt-dpkg: cmake
  WARN:         osdep: build-essential
  WARN:   and   apt-dpkg: cmake
  Do you need compatibility with OCL ? (yes or no)
    New Rock users that don't need backward compatibility with legacy Orocos components
    probably want to say 'no'. Otherwise, say 'yes'.
    Saying 'yes' will significantly impact compilation time and the size of the resulting binaries
    Please answer 'yes' or 'no' [no] 
  the target operating system for Orocos/RTT (gnulinux, xenomai, or macosx) [gnulinux] 
  which CORBA implementation should the RTT use ?
    Answer &quot;none&quot; to disable CORBA, otherwise pick either tao or omniorb [omniorb] &quot;none&quot;
invalid value: invalid value '&quot;none&quot;', accepted values are 'none', 'tao', 'omniorb' (without the quotes)
  which CORBA implementation should the RTT use ?
    Answer &quot;none&quot; to disable CORBA, otherwise pick either tao or omniorb [omniorb] none
  checked out base/templates/cmake_vizkit_widget
  checked out base/orogen/std
  checked out base/console_bridge
  checked out base/numeric
  checked out base/logging
  checked out base/templates/bundle
  checked out base/templates/vizkit3d_plugin
  checked out base/templates/ruby_lib
  checked out base/orogen/types
  checked out base/templates/cmake_lib
  checked out base/scripts
  checked out base/cmake
  checked out bundles/rock
  checked out drivers/orogen/aggregator
  checked out bundles/common_models
  checked out drivers/orogen/iodrivers_base
  checked out drivers/aggregator
  checked out drivers/iodrivers_base
  checked out drivers/orogen/transformer
  checked out drivers/transformer
  checked out base/types
  checked out perception/frame_helper
  checked out perception/jpeg_conversion
  checked out gui/osgviz
  checked out tools/log_tools
  checked out gui/rock_webapp
  checked out gui/vizkit3d
  checked out tools/logger
  tools/class_loader: checking out branch indigo-devel
  checked out gui/rock_widget_collection
  checked out tools/class_loader
  checked out tools/orogen_metadata
  checked out gui/vizkit
  checked out tools/pocolog2msgpack
  checked out tools/pocolog_cpp
  checked out tools/pocolog
  checked out tools/telemetry
  checked out base/templates/doc
  checked out tools/rest_api
  checked out rtt_typelib
  checked out utilrb
  checked out tools/orocos.rb
  checked out orogen
  checked out tools/metaruby
  checkout of tools/syskit failed, deleting the source directory /home/emi/rock-robotics/tools/syskit and retrying (1/10)
  checkout of tools/msgpack-c failed, deleting the source directory /home/emi/rock-robotics/tools/msgpack-c and retrying (1/10)
  checked out tools/syskit
  tools/msgpack-c: resetting branch master to 83a82e3eb512b18d4149cabb7eb43c7e8bc081af
  checked out tools/msgpack-c
  WARN: tools/msgpack-c from rock.core does not have a manifest
  checkout of tools/service_discovery failed, deleting the source directory /home/emi/rock-robotics/tools/service_discovery and retrying (1/10)
  checkout of typelib failed, deleting the source directory /home/emi/rock-robotics/tools/typelib and retrying (1/10)
  checkout of tools/roby failed, deleting the source directory /home/emi/rock-robotics/tools/roby and retrying (1/10)
  checkout of external/sisl failed, deleting the source directory /home/emi/rock-robotics/external/sisl and retrying (1/10)
  checked out typelib
  typelib: using the castxml importer
  checked out tools/service_discovery
  checked out external/sisl
  checked out tools/roby
  checkout of rtt failed, deleting the source directory /home/emi/rock-robotics/tools/rtt and retrying (1/10)
  checked out rtt
  building initial autoproj import log, this may take a while
  bundler: connected to https://rubygems.org/
  updated environment
Command finished successfully at 2020-06-24 12:08:47 +0200 (took 6 mins 9 secs)
  bundler: connected to https://rubygems.org/
  updated environment
Command finished successfully at 2020-06-24 12:08:52 +0200 (took 3 secs)
  operating system: ubuntu,debian - 18.04,18.04.4,lts,bionic,beaver
  WARN: osdeps definition for cmake, previously defined in /home/emi/.local/share/autoproj/gems/ruby/2.5.0/gems/autoproj-2.12.1/lib/autoproj/default.osdeps overridden by /home/emi/rock-robotics/autoproj/remotes/rock.core/rock.osdeps:
  WARN:   resp. apt-dpkg: cmake
  WARN:         osdep: build-essential
  WARN:   and   apt-dpkg: cmake
  WARN: tools/msgpack-c from rock.core does not have a manifest
  typelib: using the castxml importer
  configured CMake for tools/msgpack-c
  built tools/msgpack-c
  installed tools/msgpack-c
  configured CMake for rtt
  built rtt (10 warnings)
  set up Ruby package utilrb
  installed rtt
  configured CMake for external/sisl
  set up Ruby package tools/metaruby
  configured CMake for typelib
  set up Ruby package tools/roby
  built typelib (2 warnings)
  set up Ruby package base/scripts
  installed typelib
  built external/sisl (165 warnings)
  set up Ruby package tools/pocolog
  configured CMake for rtt_typelib
  installed external/sisl
  built rtt_typelib
  configured CMake for base/cmake
  installed rtt_typelib
  built base/cmake
  set up Ruby package orogen
  installed base/cmake
  set up Ruby package tools/log_tools
  configured CMake for tools/orogen_metadata
  configured CMake for gui/osgviz
  built tools/orogen_metadata
  configured CMake for base/logging
  built gui/osgviz (6 warnings)
  installed tools/orogen_metadata
  installed gui/osgviz
  built base/logging
  configured CMake for gui/vizkit3d
  built gui/vizkit3d (7 warnings)
  installed base/logging
  installed gui/vizkit3d
  configured CMake for tools/service_discovery
  configured CMake for base/console_bridge
  configured CMake for base/types
  built tools/service_discovery
  installed tools/service_discovery
  built base/types (3 warnings)
  built base/console_bridge
  generated oroGen base/orogen/std
  installed base/types
  installed base/console_bridge
  configuring CMake for base/orogen/std
  configured CMake for tools/pocolog_cpp
  ERROR: got an error processing base/orogen/std, waiting for pending jobs to end
  updated environment
Command failed
base/orogen/std(/home/emi/rock-robotics/base/orogen/std): failed in configure phase
    'cmake -DCMAKE_INSTALL_PREFIX=/home/emi/rock-robotics/install -DCMAKE_MODULE_PATH= -DCMAKE_PREFIX_PATH=/home/emi/rock-robotics/install;/home/emi/rock-robotics/tools/orogen -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DCMAKE_BUILD_TYPE=Release -DROCK_TEST_ENABLED=OFF /home/emi/rock-robotics/base/orogen/std' returned status 1
    see /home/emi/rock-robotics/install/log/base/orogen/std-configure.log for details
    last 10 lines are:

    -- Checking for module 'orocos-rtt-corba-gnulinux&gt;=2.1.0'
    --   No package 'orocos-rtt-corba-gnulinux' found
    CMake Error at .orogen/config/FindOrocosCORBA.cmake:8 (MESSAGE):
      RTT has not been built with CORBA support
    Call Stack (most recent call first):
      .orogen/typekit/transports/corba/CMakeLists.txt:4 (find_package)
    
    
    -- Configuring incomplete, errors occurred!
    See also &quot;/home/emi/rock-robotics/base/orogen/std/build/CMakeFiles/CMakeOutput.log&quot;.
</code></pre>
",44006.46042,63002782,322,1,0,0,,13805178,,44006.44306,28,63002782,"<p>Finaly I solved it following some advices of the rock-robotics team. I used another bootstrap.sh and also accepted the corba support, the complete process was:</p>
<pre><code>mkdir rock-workspace
cd rock-workspace
wget https://www.rock-robotics.org/bootstrap.sh
sh bootstrap.sh
</code></pre>
<p>Then the answers to the critical questions while bootstrapping:</p>
<pre><code>Do you need compatibility with OCL ? (yes or no)
New Rock users that don't need backward compatibility with legacy Orocos components
probably want to say 'no'. Otherwise, say 'yes'.
Saying 'yes' will significantly impact compilation time and the size of the resulting binaries
Please answer 'yes' or 'no' [no] no
the target operating system for Orocos/RTT (gnulinux, xenomai, or macosx) [gnulinux] gnulinux
which CORBA implementation should the RTT use ?
Answer &quot;none&quot; to disable CORBA, otherwise pick either tao or omniorb [omniorb] omniorb
</code></pre>
<p>so the answers were <strong>no</strong>, <strong>gnulinux</strong> and <strong>omniorb</strong></p>
",13805178,0,0,,,Error
424,3707,61838032,Coordinate axis transformation for robot manipulation,|python|linear-algebra|ros|robotics|coordinate-transformation|,"<p>This is a fairly generic robotics based Linear Algebra question, and I'm looking for an algorithm based mathematical approach to solve my problem and get an understanding, rather than a strictly ROS based answer.</p>

<p>I am trying out a custom picking of an object using a Robotic Arm. I have a separate perception module that detects objects and estimates the pose for grasping. It, however is in the camera frame and follows image processing convention of coordinate frames, i.e. with right: </p>

<blockquote>
  <p>+x-axis, forward: +z-axis, down: +y-axis</p>
</blockquote>

<p>From this perception module, I get two values - 3x3 Rotation matrix and 1x3 translation vector. 
As an example, say <code>T1</code></p>

<pre><code>Tra: [0.09014122 0.16243269 0.6211668 ]
Rot: [[ 0.          0.03210089 -0.99948463]
[ 0.          0.99948463  0.03210089]
[ 1.         -0.          0.        ]]
</code></pre>

<p>(i.e. I have to grasp at that location and in that orientation)
My robot base to camera transform is understandably in the right hand coordinate system. Here is an example of the same, say <code>T2</code></p>

<pre><code>translation: 
  x: 0.0564581200121
  y: 0.318823912978
  z: 0.452250135698
rotation: 
  x: -0.6954818376
  y: 0.693982204231
  z: -0.13156524004
  w: 0.13184954074
</code></pre>

<p>I am using <code>scipy.spatial.transform</code> to convert my poses from one format to another, so the actual implementation can handle any format of pose.</p>

<p>Now, to get the pose of the object from the robot, is a simple transformation T2 times T1. However, T1 follows a different convention from T2.</p>

<p>How would I go about this ? A detailed explanation using this example will be highly appreciated! I am trying to understand from scratch, hence I would prefer to arrive at a transformation matrix on my own to apply to the above ones to get the final pose.</p>

<p>This question probably belongs to Mathematics Stack Exchange and ROS as well, but as I mentioned, I am trying to approach it analytically.</p>
",43967.58542,61865294,475,1,0,1,,9926472,,43262.72431,124,61865294,"<p>If you want to get the pose of the object by multiplying T2*T1, both T1, T2 must be in the homogeneous coordinates, where T1, T2 contain rotation matrix and translation. And the size of T1, T2 should be 4x4 for 3D.</p>

<p><a href=""https://i.stack.imgur.com/ppa6R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ppa6R.png"" alt=""enter image description here""></a></p>

<p>In your case, the orientation of T2 is represented in Quaternion. Convert this into rotation matrix and create a homogeneous matrix using rotation matrix, translation.</p>
",1595504,0,3,,,Actuator
425,3646,60412326,What is the good choice for NodeJS Robotics Raspberry pi or Arduino?,|javascript|node.js|arduino|raspberry-pi|robotics|,"<p>Actually, I'm very new to NodeJS and Robotics. I want to know which device is good for Nodejs Robotics before buying it. </p>

<p>Thanks</p>
",43887.45486,,78,1,1,0,0,8101376,"Dhaka Division, Bangladesh",42888.2375,8,60412429,"<p>I've used both. For the Arduino I needed to use Firmata and drive with an RPi, so I'd say a RPi.</p>

<p>A Raspberry Pi Zero is close to the size of an Arduino.</p>
",1758461,2,0,106870231,You can compare them here http://johnny-five.io/platform-support/,Specifications
426,3934,65387147,Raspberry Pi connected to Motor Controller- Getting error in code,|python|gpio|robotics|,"<pre><code>*Hi, I am getting this code error:
*blink.py:25: RuntimeWarning: This channel is already in use, continuing anyway. Use GPIO.setwarnings(False) to disable warnings.*
GPIO.setup(ENA1,GPIO.OUT)
Traceback (most recent call last):
File &quot;/home/pi/my_python_programs/blink.py&quot;, line 27, in &lt;module&gt;
GPIO.output(IN1,GPIO.LOW)
RuntimeError: The GPIO channel has not been set up as an OUTPUT*
</code></pre>
<p><strong>It is in reference to this code I made to control a motor controller from the raspberry pi. Here is my code. I am wondering what the error is:</strong></p>
<pre><code>#ImportedLibraries
import RPi.GPIO as GPIO
from time import sleep
#Motor1
PWR= 17
ENA1 = 33
IN1 = 31
IN2 = 29
GND = 39
#Motor2
PWR = 1
ENA2 = 32
IN3 = 18
IN4 = 16
GND = 34
#SetMode
GPIO.setmode(GPIO.BOARD)
#Motor1
GPIO.setup(ENA1,GPIO.OUT)
PWM1=GPIO.PWM(ENA1,100)
GPIO.output(IN1,GPIO.LOW)
GPIO.output(IN2,GPIO.LOW)
#Motor2
GPIO.setup(ENA2,GPIO.OUT)
PWM2=GPIO.PWM(ENA2,100)
GPIO.output(IN3,GPIO.LOW)
GPIO.output(IN4,GPIO.LOW)
PWM1.start(10)
</code></pre>
",44186.13472,,201,1,1,0,,14862630,,44186.11319,1,65512574,"<p>It seems that the setup is not been doing very well, it fails in line 25.</p>
<p>If you want to control a motor you will need a H bridge, like L293D or L298, and then use the gpiozero library, look for it in the net.</p>
<p>Try to use this code:</p>
<pre><code>from gpiozero import Motor
from time import sleep

motor = Motor(forward=4, backward=14)

while True:
    motor.forward()
    sleep(5)
    motor.backward()
    sleep(5)
</code></pre>
",14402174,0,0,115601022,I would ask this at https://raspberrypi.stackexchange.com,Actuator
427,3909,64552488,OPC UA Robotics - Schema,|robotics|opc-ua|,"<p>The OPC UA Companion Spec for Robotics defines <code>new Enumeration DataTypes</code></p>
<ul>
<li>AxisMotionProfileEnumeration (used under AxisType)</li>
<li>ExecutionModeEnumeration (used under TaskControlType)</li>
<li>MotionDeviceCategoryEnumeration (used under MotionDeviceType)</li>
<li>OperationalModeEnumeration (used under SafetyStateType)</li>
</ul>
<p>However, <code>no official bsd file</code> was released as part of the OPC UA Robotics Schema. Any reason why?</p>
",44131.43611,,103,1,0,1,,10842865,,43462.49444,4,65253265,"<p>Just adding the <a href=""https://github.com/OPCFoundation/UA-Nodeset/issues/75#issuecomment-719587316"" rel=""nofollow noreferrer"">answer</a> of OPC Foundation, also here:</p>
<blockquote>
<p>The BSD is embedded in the NodeSet as Base64 data.
It can be extracted with a copy and paste.
We have requested that all specification writers provide the BSD when their spec is
released.
It will take a bit of time for current specifications to catch up.</p>
</blockquote>
",9474615,0,0,,,Programming
428,3859,63809575,Publish on topic for a certain period of time,|time|ros|robot|publisher|subscriber|,"<p>I have a ROS Node where i subscribe to a Topic and then publish to another topic on the following way :</p>
<pre><code>#include ...

//Ros Nodehandle + Publisher

//Callback Function definition

int main (int argc, char** argv){
   //Initialisation

   // Pub
   pub = nh-&gt;advertise&lt;Messagetype&gt;(&quot;Topic2&quot;, 1);
 
   //Sub
   ros::Subscriber sub = nh-&gt;subscribe(&quot;Topic1&quot;, 1, sub_cb);

   ros::spin();

   return 0;
}

void sub_cb(){
    //Write the msg i want to publish

    pub.publish(msg);

}
</code></pre>
<p>I wanted to publish the message for 15 seconds for example. I tried a solution with <strong>Ros::Time</strong> and <strong>Ros::Duration</strong> . But the fact that i have a publisher in my callback function didn't allow me to do that.</p>
<p>Is there a way to do it even is my <strong>publish event</strong> is in my callback function ? If not, any solution would work, the main thing that my subscriber and my publisher are on the same node.</p>
",44083.42917,,1273,1,5,0,,14247194,,44083.41944,1,63821535,"<p>Like I said in the comments, I think this is just a logic question, nothing really specific to ROS. Here is one of several possible solutions:</p>
<pre class=""lang-cpp prettyprint-override""><code>#include &quot;ros/ros.h&quot;
#include &quot;std_msgs/String.h&quot;

ros::Publisher pub;
ros::Time begin;

void sub_cb(const std_msgs::StringConstPtr&amp; str) {
  std_msgs::String msg;
  msg.data = &quot;hello world&quot;;
  ros::Time now = ros::Time::now();
  if (now.sec - begin.sec &lt; 15) { // stop publishing after 15 seconds
    std::cout &lt;&lt; &quot;.&quot; &lt;&lt; std::endl;
    pub.publish(msg);
  } else {
    std::cout &lt;&lt; &quot;stopped&quot; &lt;&lt; std::endl;  // just for debugging
  }
}

int main (int argc, char** argv){
  ros::init(argc, argv, &quot;test&quot;);
  ros::NodeHandle nh;
  pub = nh.advertise&lt;std_msgs::String&gt;(&quot;Topic2&quot;, 1);
  ros::Subscriber sub = nh.subscribe(&quot;Topic1&quot;, 1, sub_cb);
  begin = ros::Time::now();
  ros::spin();
  return 0;
}
</code></pre>
",1087119,1,1,112846729,Create a timer in the global scope; set it in main; check it in `sub_cb` before calling `publish`.,Other
429,3713,62043150,Robotics: What are the advantages of cartesian paths in MoveIt?,|ros|robotics|moveit|,"<p>I have some experience in C++ programming but I am a bit new to robotics. I have to create robot functionality for a customer, where the robot arm moves in between several objects to reach a given pose goal for its end-effector (gripper). </p>

<p>I have to use the MoveIt interface for motion planning. This interface can also compute Cartesian paths. If I understand correctly, cartesian paths are just a colletion of waypoint poses that the end-effector should reach sequentially.</p>

<p>My first question is, what is the advantage/disadvantage of executing a cartesian path, rather than moving to several waypoints manually one by one? </p>

<p>My second question is, would a motion planner first plan <strong>all</strong> the waypoints and then start executing them them one by one. Or will a motion planner repeatedly plan and execute for each waypoint in the cartesian path? </p>

<p>The reason that I would like to know this is because I think that moving the end-effector in steps (or waypoints) would improve the chances of finding a trajectory for a goal that is very difficult to reach due to many objects being in the environment. </p>

<p>Thank you in advance, </p>

<p>Dirk</p>
",43978.51944,,699,0,0,1,,13627136,,43978.51042,5,,,,,,,,Moving
430,3730,62662082,Receiving Data From Universal Robot and Decoding,|python|python-3.x|decode|robot|,"<p>I am working on a project where we are looking to get some data from a universal robot such as position and force data and then store that data in a text file for later reference. We can receive the data just fine, but turning it into readable coordinates is an issue. An example data string is below:</p>
<p>b'\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x80\xbf\x00\x00\x80\xbf\x00\x00\x80\xbf\x00\x00\x80\xbf\x00\x00\x80\xbf\x00\x00\x80\xbf\x00\x00\xc0?\x00\x00\x16C\x00\x00\xc0?\x00\x00\x16C\x00\x00\x00?\xcd\xcc\xcc&gt;\x00\x00\x96C\x00\x00\xc8A\x1e\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x88\xfb\x7f?\xd0M&gt;&lt;\xc0G\x9e:tNT?\r\x11\x07\xbc\xb9\xfd\x7f?~\xa0\xa1:\x03\x02+?\x16\xeb\x7f\xbf#\xce\xcc\xbc9\xdfl\xbbq\xc3\x8a&gt;i\x19T&lt;\xf3\xf9\x7f\xbf\xb4k\x87\xbb-&gt;\xc2&gt;\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x80?\xdb\x0f\xc9@\xa7\xdcU@\xa7\xdcU@\xa7\xdcU@\xa7\xdcU@\xa7\xdcU@\xa7\xdcU@\xfe\xff\xff\xff\xfe\xff\xff\xff\xfe\xff\xff\xff\xfe\xff\xff\xff\xfe\xff\xff\xff\xff\xff\xff\xff\xecb\xc7@\xecb\xc7@\xecb\xc7@\</p>
<p>*not entire string received</p>
<p>At first I thought it was hex so I tried the code:</p>
<pre><code>packet_12 = packet_12.encode('hex')
x = str(packet_12)
x = struct.unpack('!d', packet_12.decode('hex'))[0]
all_data.write(&quot;X=&quot;, x * 1000)
</code></pre>
<p>But to no avail. I tried several different decoding methods using codecs and .encode, but none worked. I found on a different post here the two code blocks below:</p>
<pre><code>y = codecs.decode(packet_12, 'utf-8', errors='ignore')


packet_12 = s.recv(8)
z = str(packet_12)
x = ''.join('%02x' % ord(c) for c in packet_12)
</code></pre>
<p>Neither worked for my application. Finally I tried saving the entire sting in a .txt file and opening it with python and decoding it with the code below, but again nothing seemed to happen.</p>
<pre><code>with io.open('C:\\Users\\myuser\\Desktop\\decode.txt', 'r', encoding='utf8') as f:
    text = f.read()

with io.open('C:\\Users\\myuser\\Desktop\\decode', 'w', encoding='utf8') as f:
    f.write(text)
</code></pre>
<p>I am aware I might be missing something incredibly simple such as using the wrong decoding type or I might even have jibberish as the robot output, but any help is appreciated.</p>
",44012.69028,,698,1,1,0,,13806555,,44006.60208,6,63545041,"<p>The easiest way to receive data from the robot with python is to use Universal Robots' <a href=""https://www.universal-robots.com/articles/ur/real-time-data-exchange-rtde-guide/"" rel=""nofollow noreferrer"">Real-Time-Data-Exchange Interface</a>. They offer some python examples for receiving and sending data.
Check out my GitHub repo for an example code which is based on the official code from UR:
<a href=""https://github.com/jonenfabian/Read_Data_From_Universal_Robots"" rel=""nofollow noreferrer"">https://github.com/jonenfabian/Read_Data_From_Universal_Robots</a></p>
",13318092,0,1,110813879,"It looks like binary data, so the struct module is probably the way to go.  But you need to know the format of the data being sent to unpack it properly; ideally this information should be found in the product's documentation.",Moving
431,3651,60465046,FRC problem with getting to run a motor for a certain amount of time,|java|robotics|,"<p>So I'm trying to have the code run the motor for a certain amount of time based on what the REV color sensor V3 senses. I've tried many things. but in the end it just gets stuck in an infinite loop.
I don't know how to fix this thing I don't know if there is a way to do this rather then what I'm trying to do. </p>

<pre><code>private void redDetect(){
  long t= System.currentTimeMillis();

  if (buttons[9]){
    long endg = t+15000; 
    while (System.currentTimeMillis() &lt; endg) {
      trenchMotor.set(0.6);
      break;
    }
  }
  if (buttons[10]){
    long endy = t+10000; 
    while (System.currentTimeMillis() &lt; endy) {
      trenchMotor.set(0.6);
      break;
    }
  }
    if (buttons[11]){
      long endr = t+7000; 
      while (System.currentTimeMillis() &lt; endr) {
        trenchMotor.set(0.6);
        break;
      }
    }
      if (buttons[12]){
        long endb = t+5000; 
        while (System.currentTimeMillis() &lt; endb) {
          trenchMotor.set(0.6);
          break;
        }
    } 
} 

private void blueDetect(){
  long t= System.currentTimeMillis();

  if (buttons[9]){
    long endg = t+15000; 
    while (System.currentTimeMillis() &lt; endg) {
      trenchMotor.set(0.6);
      break;
    }
  }
  if (buttons[10]){
    long endy = t+10000; 
    while (System.currentTimeMillis() &lt; endy) {
      trenchMotor.set(0.6);
      break;
    }
  }
    if (buttons[11]){
      long endr = t+7000; 
      while (System.currentTimeMillis() &lt; endr) {
        trenchMotor.set(0.6);
        break;
      }
    }
      if (buttons[12]){
        long endb = t+5000; 
        while (System.currentTimeMillis() &lt; endb) {
          trenchMotor.set(0.6);
          break;
        }
    } 
} 

private void greenDetect(){
int endg = 10;

  if (buttons[9]){
    while (endg &gt; 0) {
      trenchMotor.set(0.6);
      endg --; 

      break;
    }
  }
  if (buttons[10]){
    int endy = 1000;
    while (endy &gt; 0) {
      trenchMotor.set(0.6);
      endy--;
      break;
    }
  }
    if (buttons[11]){
      int endr = 7000; 
      while (endr &gt; 0) {
        trenchMotor.set(0.6);
        endr--;
        break;
      }
    }
      if (buttons[12]){
        int endb = 5000; 
        while (endb &gt; 0) {
          trenchMotor.set(0.6);
          endb--;
          break;
        }
    } 
} 

private void yellowDetect(){
  long t= System.currentTimeMillis();

  if (buttons[9]){
    long endg = t+15000; 
    while (System.currentTimeMillis() &lt; endg) {
      trenchMotor.set(0.6);
      break;
    }
  }
  if (buttons[10]){
    long endy = t+10000; 
    while (System.currentTimeMillis() &lt; endy) {
      trenchMotor.set(0.6);
      break;
    }
  }
    if (buttons[11]){
      long endr = t+7000; 
      while (System.currentTimeMillis() &lt; endr) {
        trenchMotor.set(0.6);
        break;
      }
    }
      if (buttons[12]){
        long endb = t+5000; 
        while (System.currentTimeMillis() &lt; endb) {
          trenchMotor.set(0.6);
          break;
        }
    } 
} 


</code></pre>

<pre><code>
if(detectedColor.red &gt; detectedColor.green &amp;&amp; detectedColor.red &gt; detectedColor.blue) {
      redDetect();
    }

    if(detectedColor.blue &gt; detectedColor.green &amp;&amp; detectedColor.blue &gt; detectedColor.red) {
      blueDetect();
    }

    if(detectedColor.green &gt; detectedColor.blue &amp;&amp; detectedColor.green &gt; detectedColor.red) {
      greenDetect();
    }
</code></pre>
",43890.51042,,89,0,3,0,0,12940002,,43882.70625,15,,,,,,106996714,"Hello Arda. Consider updating the code you posted to reflect the modifications you made. 
Also, could you explain what the ""buttons"" array used in your conditions is about?",Actuator
432,3698,61409469,How to make contour mapping of orientation angles plot,|matplotlib|plot|robotics|,"<p>I am developing a robotic system and found this interesting plot in a paper:</p>

<p><a href=""https://i.stack.imgur.com/xRNRj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xRNRj.png"" alt=""enter image description here""></a></p>

<p>The paper is ""A Comparison of Robot Wrist Implementations for the iCub Humanoid"".
The plot depicts the coupling of the two degrees of freedom of a robotic wrist. I wanted to do something similar for my application but I have no idea where to start and the paper doesn't explain how its done.
If anyone has done something similar in the past, I would be very grateful for any inputs.</p>
",43945.55903,61411294,50,1,0,0,,11267799,"Zürich, Schweiz",43551.71111,14,61411294,"<p>This looks like a demonstration of what <code>meshgrid</code> does.  Note here that what would normally be <code>Z</code> in the contour plot is now either <code>X</code> or <code>Y</code>.  </p>

<p><a href=""https://i.stack.imgur.com/p45vA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p45vA.png"" alt=""enter image description here""></a></p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt

x = np.arange(-90, 91, 15)
X, Y = np.meshgrid(x, x)

fig, ax = plt.subplots(figsize=(5,5))
cs = ax.contour(X, Y, X, colors=['blue'], levels=x) # Z = X
ax.clabel(cs, inline=1, fontsize=7)
cs = ax.contour(X, Y, Y, colors=['red'], levels=x) # Z = Y
ax.clabel(cs, inline=1, fontsize=7)
</code></pre>
",102302,1,0,,,Coordinates
433,3742,63202897,Arduino IDE is not showing any port macOS,|macos|arduino|arduino-uno|robotics|arduino-ide|,"<p>My <strong>Arduino Uno</strong> is plugged in with mac USB port. But Arduino IDE is not showing any port.
(macOS- Catalina)</p>
<p><a href=""https://i.stack.imgur.com/arPha.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/arPha.png"" alt=""enter image description here"" /></a></p>
",44044.31736,67665010,41213,11,7,17,,8400582,,42948.64028,69,64007180,"<p>This problem can occur due to two main reasons.<br></p>
<ol>
<li>Your computer does not recognize your Arduino board<br></li>
<li>Your Arduino Board is damaged</li>
</ol>
<p>Sometimes your computer does not recognize your arduino board. You can check it by opening <strong>Device Manager &gt; Other Devices</strong> If a device named Arduino Uno is there, probably it is due to the driver issue.<br>
You can solve it by manually configuring the driver <a href=""https://www.arduino.cc/en/Guide/DriverInstallation/"" rel=""nofollow noreferrer"">from here.</a><br>
<br>
If the Arduino Board is still not recognized, it can be due to a damaged cable or the Arduino board is damaged. Try using a different cable to connect it to your computer. If the board is still not recognized, the Arduino board might be damaged, You may have to repair the damaged board by giving it to a shop or may have to buy a new board which seems the easier way.</p>
",14301325,0,3,111812682,"No, it didn't work.",Connections
434,3899,64457784,Pointer gives abnormal values after deferencing,|c++|pointers|robotics|,"<p>I am trying to replicate a big C++ library. It has the following structure of code</p>
<p>RobotRunner.h</p>
<pre><code>class RobotRunner
{
public:
    VectorNavData* vectorNavData;
};
</code></pre>
<p>RobotRunner.cpp</p>
<pre><code>void RobotRunner::run()
{
printf(&quot;Quaternion[0]: %f \n&quot;, vectorNavData-&gt;quat[0]); //Output: 243235653487854 - Abnormal values
}
</code></pre>
<p>SimulationBridge.h</p>
<pre><code>class SimulationBridge
{
private:
    VectorNavData _vectorNavData; // Actual value of vectornavdata from the robot is stored here
    
    RobotRunner* _robotRunner = nullptr; //Pointer to the RobotRunner Object
}
</code></pre>
<p>SimulationBridge.cpp</p>
<pre><code>void SimulationBridge::init()
{
_robotRunner = new RobotRunner();

printf(&quot;Quaternion[0]: %f \n&quot;, _vectorNavData.quat[0]); // Output: 0.43 - Normal and expected
_robotRunner-&gt;vectorNavData = &amp;_vectorNavData;
}

void SimulationBridge::run()
{
_robotRunner-&gt;run();
}

//This function runs continuously and updates the _vectorNavData in a separate thread
void SimulationBridge::readIMU()
{
    while(true)
    { 
        //_lowState stores the values of different robot parameters at a given time

        _vectorNavData.accelerometer[0] = _lowState.imu.accelerometer[0];
        _vectorNavData.accelerometer[1] = _lowState.imu.accelerometer[1];
        _vectorNavData.accelerometer[2] = _lowState.imu.accelerometer[2];

        _vectorNavData.quat[0] = _lowState.imu.quaternion[1];
        _vectorNavData.quat[1] = _lowState.imu.quaternion[2];
        _vectorNavData.quat[2] = _lowState.imu.quaternion[3];
        _vectorNavData.quat[3] = _lowState.imu.quaternion[0];

        _vectorNavData.gyro[0] = _lowState.imu.gyroscope[0];
        _vectorNavData.gyro[1] = _lowState.imu.gyroscope[1];
        _vectorNavData.gyro[2] = _lowState.imu.gyroscope[2];
    }
}
</code></pre>
<p>VectorNavData is a struct which stores the details about the orientation of the robot. It has the following definition</p>
<pre><code>struct VectorNavData {
  Vec3&lt;float&gt; accelerometer;
  Vec3&lt;float&gt; gyro;
  Quat&lt;float&gt; quat;
};
</code></pre>
<p>I have included only the necessary part of the code here for brevity.</p>
<p><strong>Code Explanation:</strong></p>
<p>SimulationBridge class communicates with the robot in the simulation. It takes in vectorNavData and stores it in the member variable _vectorNavData. SimulationBridge also contains the pointer to the RobotRunner class as one of it's member. I am allocating the address of _vectorNavData object to the pointer _robotRunner-&gt;vectorNavData (check SimulationBridge.cpp). Inside the RobotRunner class I deference this pointer and use the values in other parts of the code.</p>
<p><strong>Problem:</strong></p>
<p>If I print the vectorNavData inside the SimulationBridge.cpp the values seems to be normal. But after assigning the pointer of the same object to the robot runner, if I print the values there the values seems to be abnormally high. My question is, is this way of using pointers for dynamic allocation recommended? If not what is the best alternative way I can use?</p>
<p>Another important point to note is, I am compiling the code with CMake and &quot;-O3&quot; optimization flag is set to the CMAKE_CXX_FLAGS. If I remove this flag, the code sorta works fine for the above object pointer but I am still getting similar error for another object pointer in another part of the code.  I have not included that here because it's pretty complex to describe the code structure and the problem essentially is the same.</p>
",44125.26806,,82,0,10,1,,9971625,,43272.31389,10,,,,,,113978439,Sorry! I have edited the post for better clarity. I hope this is clear now. @Someprogrammerdude,Programming
435,3886,64299715,Is there a way to simplify repetitive addition in x-drive?,|c++|robotics|,"<p>I wrote the C++ code below for controlling an x-drive, and it looks really repetitive, I feel like there should be a way to simplify, but I can't think of any that would keep it looking readable. Any ideas would be appreciated!</p>
<pre class=""lang-cpp prettyprint-override""><code>int lefty_analog = master.get_analog(ANALOG_LEFT_Y);
int leftx_analog = master.get_analog(ANALOG_LEFT_X);
int rightx_analog = master.get_analog(ANALOG_RIGHT_X);

rightf_motor = lefty_analog - leftx_analog - rightx_analog;
rightb_motor = lefty_analog + leftx_analog - rightx_analog;
leftf_motor = -lefty_analog - leftx_analog - rightx_analog;
leftb_motor = -lefty_analog + leftx_analog - rightx_analog;
</code></pre>
",44115.04097,,41,0,3,0,,14428374,,44115.03472,2,,,,,,113702661,You should only worry about this if this is in a tight loop. See how compilers optimize it: https://gcc.godbolt.org/z/KffP13,Other
436,3922,65099688,Need help resolving Arduino Code error [-Woverflow] in gimbal code. I am not sure how to resolve or if to disable yaw,|c++|c|arduino|robotics|gyroscope|,"<p>This code is not mine, but found on <a href=""https://howtomechatronics.com/projects/diy-arduino-gimbal-self-stabilizing-platform/"" rel=""nofollow noreferrer"">How To Mechatronics</a>.</p>
<p>I am working on an Arduino gimbal and am using this code. It brings up an error, which I will paste at the bottom.</p>
<p>I searched this sort of error and it seems it is because it has an output that is negative but is not defined to come out as negative or may be too large.</p>
<p>I am not quite sure what to change or how to change this in order to function. I also have a problem with the yaw motor, which I believe may be fried because my brother connected it to a 12 V battery and it is only supposed to be 5 V.</p>
<p>I am sure I can disable the yaw (although not sure if this would solve the other issue), but I don't know which lines to code out in order to do so.</p>
<pre><code>/*
  DIY Gimbal - MPU6050 Arduino Tutorial
  by Dejan, www.HowToMechatronics.com
  Code based on the MPU6050_DMP6 example from the i2cdevlib library by Jeff Rowberg:
  https://github.com/jrowberg/i2cdevlib
*/
// I2Cdev and MPU6050 must be installed as libraries, or else the .cpp/.h files
// for both classes must be in the include path of your project
#include &quot;I2Cdev.h&quot;

#include &quot;MPU6050_6Axis_MotionApps20.h&quot;
//#include &quot;MPU6050.h&quot; // not necessary if using MotionApps include file

// Arduino Wire library is required if I2Cdev I2CDEV_ARDUINO_WIRE implementation
// is used in I2Cdev.h
#if I2CDEV_IMPLEMENTATION == I2CDEV_ARDUINO_WIRE
#include &quot;Wire.h&quot;
#endif
#include &lt;Servo.h&gt;
// class default I2C address is 0x68
// specific I2C addresses may be passed as a parameter here
// AD0 low = 0x68 (default for SparkFun breakout and InvenSense evaluation board)
// AD0 high = 0x69
MPU6050 mpu;
//MPU6050 mpu(0x69); // &lt;-- use for AD0 high

// Define the 3 servo motors
Servo servo0;
Servo servo1;
Servo servo2;
float correct;
int j = 0;

#define OUTPUT_READABLE_YAWPITCHROLL

#define INTERRUPT_PIN 2  // use pin 2 on Arduino Uno &amp; most boards

bool blinkState = false;

// MPU control/status vars
bool dmpReady = false;  // set true if DMP init was successful
uint8_t mpuIntStatus;   // holds actual interrupt status byte from MPU
uint8_t devStatus;      // return status after each device operation (0 = success, !0 = error)
uint16_t packetSize;    // expected DMP packet size (default is 42 bytes)
uint16_t fifoCount;     // count of all bytes currently in FIFO
uint8_t fifoBuffer[64]; // FIFO storage buffer

// orientation/motion vars
Quaternion q;           // [w, x, y, z]         quaternion container
VectorInt16 aa;         // [x, y, z]            accel sensor measurements
VectorInt16 aaReal;     // [x, y, z]            gravity-free accel sensor measurements
VectorInt16 aaWorld;    // [x, y, z]            world-frame accel sensor measurements
VectorFloat gravity;    // [x, y, z]            gravity vector
float euler[3];         // [psi, theta, phi]    Euler angle container
float ypr[3];           // [yaw, pitch, roll]   yaw/pitch/roll container and gravity vector

// packet structure for InvenSense teapot demo
uint8_t teapotPacket[14] = { '$', 0x02, 0, 0, 0, 0, 0, 0, 0, 0, 0x00, 0x00, '\r', '\n' };



// ================================================================
// ===               INTERRUPT DETECTION ROUTINE                ===
// ================================================================

volatile bool mpuInterrupt = false;     // indicates whether MPU interrupt pin has gone high
void dmpDataReady() {
  mpuInterrupt = true;
}

// ================================================================
// ===                      INITIAL SETUP                       ===
// ================================================================

void setup() {
  // join I2C bus (I2Cdev library doesn't do this automatically)
#if I2CDEV_IMPLEMENTATION == I2CDEV_ARDUINO_WIRE
  Wire.begin();
  Wire.setClock(400000); // 400kHz I2C clock. Comment this line if having compilation difficulties
#elif I2CDEV_IMPLEMENTATION == I2CDEV_BUILTIN_FASTWIRE
  Fastwire::setup(400, true);
#endif

  // initialize serial communication
  // (115200 chosen because it is required for Teapot Demo output, but it's
  // really up to you depending on your project)
  Serial.begin(38400);
  while (!Serial); // wait for Leonardo enumeration, others continue immediately

  // initialize device
  //Serial.println(F(&quot;Initializing I2C devices...&quot;));
  mpu.initialize();
  pinMode(INTERRUPT_PIN, INPUT);
  devStatus = mpu.dmpInitialize();
  // supply your own gyro offsets here, scaled for min sensitivity
  mpu.setXGyroOffset(17);
  mpu.setYGyroOffset(-69);
  mpu.setZGyroOffset(27);
  mpu.setZAccelOffset(1551); // 1688 factory default for my test chip

  // make sure it worked (returns 0 if so)
  if (devStatus == 0) {
    // turn on the DMP, now that it's ready
    // Serial.println(F(&quot;Enabling DMP...&quot;));
    mpu.CalibrateAccel(6);
    mpu.CalibrateGyro(6);
    mpu.PrintActiveOffsets();
    mpu.setDMPEnabled(true);

    attachInterrupt(digitalPinToInterrupt(INTERRUPT_PIN), dmpDataReady, RISING);
    mpuIntStatus = mpu.getIntStatus();

    // set our DMP Ready flag so the main loop() function knows it's okay to use it
    //Serial.println(F(&quot;DMP ready! Waiting for first interrupt...&quot;));
    dmpReady = true;

    // get expected DMP packet size for later comparison
    packetSize = mpu.dmpGetFIFOPacketSize();
  } else {
    // ERROR!
    // 1 = initial memory load failed
    // 2 = DMP configuration updates failed
    // (if it's going to break, usually the code will be 1)
    // Serial.print(F(&quot;DMP Initialization failed (code &quot;));
    //Serial.print(devStatus);
    //Serial.println(F(&quot;)&quot;));
  }

  // Define the pins to which the 3 servo motors are connected
  servo0.attach(10);
  servo1.attach(9);
  servo2.attach(8);
}
// ================================================================
// ===                    MAIN PROGRAM LOOP                     ===
// ================================================================

void loop() {
  // if programming failed, don't try to do anything
  if (!dmpReady) return;

  // wait for MPU interrupt or extra packet(s) available
  while (!mpuInterrupt &amp;&amp; fifoCount &lt; packetSize) {
    if (mpuInterrupt &amp;&amp; fifoCount &lt; packetSize) {
      // try to get out of the infinite loop
      fifoCount = mpu.getFIFOCount();
    }
  }

  // reset interrupt flag and get INT_STATUS byte
  mpuInterrupt = false;
  mpuIntStatus = mpu.getIntStatus();

  // get current FIFO count
  fifoCount = mpu.getFIFOCount();

  // check for overflow (this should never happen unless our code is too inefficient)
  if ((mpuIntStatus &amp; _BV(MPU6050_INTERRUPT_FIFO_OFLOW_BIT)) || fifoCount &gt;= 1024) {
    // reset so we can continue cleanly
    mpu.resetFIFO();
    fifoCount = mpu.getFIFOCount();
    Serial.println(F(&quot;FIFO overflow!&quot;));

    // otherwise, check for DMP data ready interrupt (this should happen frequently)
  } else if (mpuIntStatus &amp; _BV(MPU6050_INTERRUPT_DMP_INT_BIT)) {
    // wait for correct available data length, should be a VERY short wait
    while (fifoCount &lt; packetSize) fifoCount = mpu.getFIFOCount();

    // read a packet from FIFO
    mpu.getFIFOBytes(fifoBuffer, packetSize);

    // track FIFO count here in case there is &gt; 1 packet available
    // (this lets us immediately read more without waiting for an interrupt)
    fifoCount -= packetSize;

    // Get Yaw, Pitch and Roll values
#ifdef OUTPUT_READABLE_YAWPITCHROLL
    mpu.dmpGetQuaternion(&amp;q, fifoBuffer);
    mpu.dmpGetGravity(&amp;gravity, &amp;q);
    mpu.dmpGetYawPitchRoll(ypr, &amp;q, &amp;gravity);

    // Yaw, Pitch, Roll values - Radians to degrees
    ypr[0] = ypr[0] * 180 / M_PI;
    ypr[1] = ypr[1] * 180 / M_PI;
    ypr[2] = ypr[2] * 180 / M_PI;
    
    // Skip 300 readings (self-calibration process)
    if (j &lt;= 300) {
      correct = ypr[0]; // Yaw starts at random value, so we capture last value after 300 readings
      j++;
    }
    // After 300 readings
    else {
      ypr[0] = ypr[0] - correct; // Set the Yaw to 0 deg - subtract  the last random Yaw value from the currrent value to make the Yaw 0 degrees
      // Map the values of the MPU6050 sensor from -90 to 90 to values suatable for the servo control from 0 to 180
      int servo0Value = map(ypr[0], -90, 90, 0, 180);
      int servo1Value = map(ypr[1], -90, 90, 0, 180);
      int servo2Value = map(ypr[2], -90, 90, 180, 0);
      
      // Control the servos according to the MPU6050 orientation
      servo0.write(servo0Value);
      servo1.write(servo1Value);
      servo2.write(servo2Value);
    }
#endif
  }
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/6Pnpf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6Pnpf.png"" alt=""enter image description here"" /></a></p>
<p>See image for error code. There is only one exit condition, so I believe this is the only issue.</p>
<p>the arrow in the error code (~~^~~~~~~) points at (2*16384);</p>
",44166.94792,65100114,200,1,2,1,,14745165,,44166.93542,3,65100114,"<p>This is a warning for an int overflow in the MPU6050 library code, not in your code.</p>
<p>On Github, an <a href=""https://github.com/jrowberg/i2cdevlib/issues/380"" rel=""nofollow noreferrer"">issue</a> was raised about this some time ago, which also has the fix in the same posting.</p>
<p>Another solution suggested in the <a href=""https://github.com/jrowberg/i2cdevlib/issues/380#issuecomment-408547936"" rel=""nofollow noreferrer"">comments there</a> to get rid of this warning is to simply change the &quot;16384&quot; to &quot;16384L&quot; in the library code.</p>
<p>Note that i2cdevlib has 247 open issues; I don't think the owner/maintainer will fix this particular problem any time soon.</p>
",12570891,0,2,115091302,"On Arduino, `int` is 16-bit. `2*16384` is too large to fit in a 16-bit signed int. I have no idea why authors of this library didn't use the constant 32768 there, perhaps you want to ask them?",Programming
437,3738,62913510,How to get Arduino Robot token?,|arduino|robotics|,"<p><a href=""https://i.stack.imgur.com/Ch7Fr.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ch7Fr.jpg"" alt=""enter image description here"" /></a>Yesterday I built a robot called <strong>Miro</strong>. I want to use Open Roberta but I have to connect the robot using a token, but I don't know how to get that token.
<strong>note</strong>: I'm using Windows 10 enterprise as my os.</p>
<p><a href=""https://i.stack.imgur.com/YH1Q7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YH1Q7.png"" alt=""enter image description here"" /></a></p>
<p>I really need some help with that.
Thank you!</p>
",44027.46319,,36,1,0,-1,,12097240,,43728.81597,18,62915953,"<p>When I go to <a href=""https://lab.open-roberta.org/"" rel=""nofollow noreferrer"">https://lab.open-roberta.org/</a> the very first thing I see is a pop-up that says:</p>
<blockquote>
<p>Would you like to get started, but do not know exactly how? We will
show you the first steps in an interactive tutorial.</p>
</blockquote>
<p>I've never used this service, but I would think that would be the place to start.</p>
",7994837,0,0,,,Remote
438,3927,65203283,improve remote control app command latency,|flutter|ros|latency|robotics|remote-control|,"<p>Is there a way to increase joystick command speed from my Flutter app to my robot? Right now I’m using wifi (which has other data streaming back to the tablet) and there is a lag between commands and robot motion. This is not an issue when I use a wireless 2.4GHz Logitech controller.</p>
<p><a href=""https://pub.dev/packages/control_pad"" rel=""nofollow noreferrer"">https://pub.dev/packages/control_pad</a></p>
",44173.70208,,107,0,1,1,,13132730,US,43917.12222,20,,,,,,115352406,"Normally, the latency of wifi is very low, because of the high throughput. You could write your own sockets for the commands which speeds the whole process or you can use bluetooth aswell.",Remote
439,3994,66827788,How to get homography matrix from gps information,|python|opencv|gps|robotics|drone.io|,"<p>I am working a gps-denied UAV localization project. I am reading this paper <strong>GPS-Denied UAV Localization using Pre-existing Satellite Imagery</strong>. In this paper, they try to align a UAV frame sequentially with a satellite map using an homography. The first homography is estimated using the GPS information stored in the first UAV frame. In the code, I don't see any information about this part. I am wondering if someone can explain this or point me to some reference that can help.</p>
",44282.15208,,189,0,3,0,,2130515,,41337.20625,385,,,,,,118146986,"@Micka, Yes. based on the paper `We extract the GPS-aligned satellite map from Google Earth Pro. The UAV imagery was captured in April, 2013, and the satellite imagery in May, 2012. High-accuracy RTK GPS (latitude, longitude, and altitude) is included in the metadata of each UAV frame in the dataset`",Coordinates
440,4000,66999588,"Given error of distance, and the error of angle, How would I set a percentage of motion attributed to angular motion, and the net speed",|math|robotics|,"<p>So I have a robot with a mecanum drive, meaning it can move in any direction. I am trying to program it to move to a point and reach that point at a given angle, all while moving in a straight line. So far I've gotten the robot to reach that point, I've gotten it to stay on the straight line, but I cant get it to reach the correct angle(without having it turn and then move). To simplify the movement, I have a function that I creatively called Move, Move takes in three floats:</p>
<p>The first is what angle the robot moves to relative to the robots angle, but ignore this one as I got it working</p>
<p>the second(angle%) determines the percentage of motion that is given to the angular motion, so when it is given 1, the robot will only be turning, and when it is given 0, it won't turn at all. At 0.5 it will be turning a bit, and moving towards the destination at the same time.</p>
<p>The last one is the speed the robot moves at ranging from 0-127. For this I will probably use two PID loops, one for the angle error, and one for the distance error, then add both up.</p>
<p>So my problem is finding an algorithm to find what I should set angle% to, in order to arrive at a point facing the right way. There are a few 'properties that angle% must have: if distance = 0, and angle error &gt; 0, angle% should = 1. And if distance &gt; 0 and angle error = 0. Also when the angle err is 180deg, angle % should = 1.</p>
<p>I know the solution is going to be really simple but I can't wrap my head around this one.</p>
",44294.33056,,207,0,1,1,,12996244,,43892.94097,2,,,,,,120535401,"how many joint your robot has ?  what are the minimum/maximum angle of rotation of each joint?   Of course, every single point in the work area can be reached in various ways (angles combinations). Perhaps a solution may be to calculate all possible rotations of all joints and save the achievable point for each combination of rotations. Once, forever. So with a database system ask for a particular point and get all the solutions for that point. In the end choose the most convenient solution for that point.",Coordinates
441,4056,68632492,Can ROS Gmaping be as good with big maps as Google's Cartographer?,|ros|robotics|slam|amr|,"<p>I am trying to determine, if ros gmaping (<a href=""http://wiki.ros.org/gmapping"" rel=""nofollow noreferrer"">http://wiki.ros.org/gmapping</a>) could be effectively and reliably used to map out large maps. So far I did not have any success. On larger maps (100x100 m) sometimes comes out curved.</p>
<p>To find other methods of mapping, I started dabbling in Google's Cartographer (<a href=""https://google-cartographer-ros.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">https://google-cartographer-ros.readthedocs.io/en/latest/</a>), and after a short while, it behaves much more reliably and precisely, but its much more time-consuming.</p>
<p>So in summary what calibration methods should I use to tune my gmaping procedure ?</p>
<p>My gmaping.launch :</p>
<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;launch&gt;

  &lt;!-- Gmapping --&gt;
  &lt;node pkg=&quot;gmapping&quot; type=&quot;slam_gmapping&quot; name=&quot;slam_gmapping&quot; output=&quot;screen&quot;&gt;
    &lt;remap from=&quot;scan&quot; to=&quot;/scan_front&quot;/&gt;
    &lt;param name=&quot;base_frame&quot; value=&quot;/base_footprint&quot;/&gt;
    &lt;param name=&quot;odom_frame&quot; value=&quot;/odom&quot;/&gt;
    &lt;param name=&quot;map_frame&quot; value=&quot;/map&quot;/&gt;

    &lt;param name=&quot;map_udpate_interval&quot; value=&quot;1.0&quot;/&gt;
    &lt;param name=&quot;maxUrange&quot; value=&quot;15.0&quot;/&gt;
    &lt;param name=&quot;sigma&quot; value=&quot;0.05&quot;/&gt;
    &lt;param name=&quot;kernelSize&quot; value=&quot;1&quot;/&gt;
    &lt;param name=&quot;lstep&quot; value=&quot;0.05&quot;/&gt;
    &lt;param name=&quot;astep&quot; value=&quot;0.05&quot;/&gt;
    &lt;param name=&quot;iterations&quot; value=&quot;10&quot;/&gt;
    &lt;param name=&quot;lsigma&quot; value=&quot;0.075&quot;/&gt;
    &lt;param name=&quot;ogain&quot; value=&quot;3.0&quot;/&gt;
    &lt;param name=&quot;lskip&quot; value=&quot;0&quot;/&gt;
    &lt;param name=&quot;srr&quot; value=&quot;0.01&quot;/&gt;
    &lt;param name=&quot;srt&quot; value=&quot;0.02&quot;/&gt;
    &lt;param name=&quot;str&quot; value=&quot;0.01&quot;/&gt;
    &lt;param name=&quot;stt&quot; value=&quot;0.02&quot;/&gt;
    &lt;param name=&quot;linearUpdate&quot; value=&quot;0.5&quot;/&gt;
    &lt;param name=&quot;angularUpdate&quot; value=&quot;0.5&quot;/&gt;
    &lt;param name=&quot;temporalUpdate&quot; value=&quot;-1.0&quot;/&gt;
    &lt;param name=&quot;resampleThreshold&quot; value=&quot;0.5&quot;/&gt;
    &lt;param name=&quot;particles&quot; value=&quot;100&quot;/&gt;
    &lt;param name=&quot;xmin&quot; value=&quot;-1.0&quot;/&gt;
    &lt;param name=&quot;ymin&quot; value=&quot;-1.0&quot;/&gt;
    &lt;param name=&quot;xmax&quot; value=&quot;1.0&quot;/&gt;
    &lt;param name=&quot;ymax&quot; value=&quot;1.0&quot;/&gt;
    &lt;param name=&quot;delta&quot; value=&quot;0.02&quot;/&gt;
    &lt;param name=&quot;llsamplerange&quot; value=&quot;0.01&quot;/&gt;
    &lt;param name=&quot;llsamplestep&quot; value=&quot;0.01&quot;/&gt;
    &lt;param name=&quot;lasamplerange&quot; value=&quot;0.005&quot;/&gt;
    &lt;param name=&quot;lasamplestep&quot; value=&quot;0.005&quot;/&gt;
    &lt;param name=&quot;inverted_laser&quot; value=&quot;true&quot;/&gt;
  &lt;/node&gt;

&lt;/launch&gt;
</code></pre>
",44411.34028,,487,2,0,1,,16583236,,44411.29583,1,68685350,"<p>a specific parameter that will help the output is <code>minimumScore</code> - this is the accepted match value between scans. the higher the value the higher the quality of the end map</p>
<p>or change the values associated with resampling and increase the number of scans taken i.e. using 1 scan every half a second will give better results than one scan every 10 seconds</p>
<p>otherwise you just need to <em>play</em> with the parameter values of the package until you see good results</p>
",9377091,0,0,,,Moving
442,4128,69587803,How to set SolverId when setting verbosity in IK solver options in Drake toolbox?,|robotics|drake|,"<pre><code> drake::solvers::SolverOptions options;
    options.SetOption(drake::solvers::**?**, &quot;verbose&quot;, {0, 1});   //{0,1} for verbose, {0,0} for no verbosity
    const auto result = Solve(ik.prog(), {}, options);
    const auto q_sol = result.GetSolution(ik.q());
</code></pre>
<p>What do I set the  SolverId to for solving the Inverse Kinematics nlp problem?</p>
",44484.68194,69588140,80,1,0,0,,15412613,,44272.23125,10,69588140,"<p>You have two options here:</p>
<ol>
<li>Set the option for the specific solver you use. You can know which solver is invoked by checking the <code>result</code>
<pre class=""lang-cc prettyprint-override""><code>std::cout &lt;&lt; result.get_solver_id().name() &lt;&lt; &quot;\n&quot;;
</code></pre>
if it prints &quot;IPOPT&quot;, then you can do <code>options.SetOption(drake::solvers::IpoptSolver::id(), ...)</code>.</li>
<li>Another (and better) solution is to set the common solver options
<pre class=""lang-cc prettyprint-override""><code>options.SetOption(CommonSolverOption::kPrintToConsole, 1);
</code></pre>
which will print the output information to the console for any solver that supports console printing. You can also do <code>options.SetOption(CommonSolverOption::kPrintFileName, &quot;output.txt&quot;)</code> which will print the output to <code>output.txt</code> file.</li>
</ol>
",1973861,0,3,,,Actuator
443,4011,67159164,denavit hartenberg 6dof moveo inverse kinematic robot arm,|arduino|raspberry-pi|robotics|inverse-kinematics|,"<p>I need your help.
I cant get the denavit hartenberg matrix right. (for this robot: <a href=""https://github.com/BCN3D/BCN3D-Moveo"" rel=""nofollow noreferrer"">https://github.com/BCN3D/BCN3D-Moveo</a>)
My robotic arm has 6 dof (normal one has only 5) but I dont get how to configure the theta and alpha variable for it.
Current matrix looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>joint</th>
<th>d</th>
<th>r</th>
<th>alpha</th>
<th>theta</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>232.0</td>
<td>0</td>
<td>?</td>
<td>?</td>
</tr>
<tr>
<td>2</td>
<td>0</td>
<td>223.0</td>
<td>?</td>
<td>?</td>
</tr>
<tr>
<td>3</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>?</td>
</tr>
<tr>
<td>4</td>
<td>224.0</td>
<td>0</td>
<td>?</td>
<td>?</td>
</tr>
<tr>
<td>5</td>
<td>0</td>
<td>0</td>
<td>?</td>
<td>?</td>
</tr>
<tr>
<td>6</td>
<td>175.0</td>
<td>0</td>
<td>?</td>
<td>?</td>
</tr>
</tbody>
</table>
</div>
<p>(If this table doesn't look right click <a href=""https://i.stack.imgur.com/9mUJN.png"" rel=""nofollow noreferrer"">here</a>)
The robotic arm is looking straight in the air while being in the home position.</p>
<p>How does the denavit-hartenberg matrix look like?</p>
<p>More pictures:
<a href=""https://www.bcn3d.com/bcn3d-moveo-the-future-of-learning/"" rel=""nofollow noreferrer"">https://www.bcn3d.com/bcn3d-moveo-the-future-of-learning/</a></p>
",44305.38403,67176874,1242,1,4,1,,15685971,,44304.99861,6,67176874,"<p>DH Parameters allow us to fill in the elements of our transformation matrices according to a schema. This schema has some limitations to it, which sometimes calls for clever tricks to get by any issues - but more on that in a minute.</p>
<p>First off, about the parameters themseleves.<br/></p>
<ul>
<li><code>d</code> is the distance between two frames <strong>i</strong> and <strong>(i-1)</strong> along the <strong>z</strong> axis of <strong>(i-1)</strong>.</li>
<li><code>a</code> - or <code>r</code> in your case - is the distance between two frames <strong>i</strong> and <strong>(i-1)</strong> along the <strong>x</strong> axis of <strong>i</strong>.</li>
<li><code>theta</code> is the angle between the <strong>x</strong> axes of <strong>i</strong> and <strong>(i-1)</strong> about the positive <strong>z</strong> axis of the <strong>(i-1)</strong> frame</li>
<li><code>alpha</code> is the angle between the <strong>z</strong> axes about the newly rotated x axis <strong>after</strong> the rotation of <code>theta</code> has been applied</li>
</ul>
<p>Furthermore, DH notation presupposes the following about the axes of the coordinate frames:</p>
<ul>
<li>the z-axis always points along the axis of actuation (that is, rotation in your case).</li>
<li>the x-axis of the frame <strong>i</strong> has to intersect the z-axis of the frame <strong>(i-1)</strong></li>
<li>the y-axis is set such that the frame forms a right-hand coordinate system</li>
</ul>
<p>Below is an image of your system in the home pose with coordinate frames applied according to DH notation.</p>
<p><a href=""https://i.stack.imgur.com/J2ts0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/J2ts0.png"" alt=""home pose kinematic chain"" /></a></p>
<p>As you might notice, DH Notation does not allow for a displacement between the rotation and torison joint frames. This is not a problem, since mathematically it does not make a difference on where the rotation occurrs. The curved lines denote that the relevant frames are placed in the same position for notation purposes.</p>
<p><a href=""https://i.stack.imgur.com/loXDv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/loXDv.png"" alt=""table snapshot"" /></a></p>
<p>Now the only thing you have to do is think about where the rotations of your joints might be inserted into the DH-table as well as the displacement beteween the rotational frames (l1 through 4).</p>
<p>You may then insert the DH Parameters into the DH Matrix for each frame and use these transformations for your kinematic calculations.</p>
<p>For future questions, you might want to think about posting them at the <a href=""https://robotics.stackexchange.com/"">Robotics Stack Exchange</a> site, it will probably be easier to get an answer there very quick.</p>
",10493834,2,2,118726837,I fixed your table.,Moving
444,3982,66732022,Almathswig error when trying to use almath to program NAO robot,|python|importerror|robotics|,"<p>Hi There Stackoverflow,</p>
<p>I am trying to program a NAO robot using Python. I would like to access some of the motion features of the NAO which require the &quot;almath&quot; module which I believe is installed as a part of the naoqi python sdk.</p>
<p>The naoqi python sdk is successfully installed on my machine and I have no problem importing &quot;naoqi&quot; into any of my scripts. However when trying to run any motion related scripts that require &quot;almath&quot; I run into the error seen in the second image. I am not sure why this module cannot be found when I have installed the naoqi library. The almathswig error is not well documented online so I thought I would put up a question to see if anyone can point my in the right track.</p>
<p>The error I am faced with is:</p>
<pre><code>Traceback (most recent call last):
  File &quot;c:/Users/Zachary Ringer/Desktop/Python/stackexchangeexample.py&quot;, line 4, in &lt;module&gt;
    import almath as m # python's wrapping of almath
  File &quot;C:\Python27\Lib\site-packages\pythonNaoqi\lib\almath.py&quot;, line 28, in &lt;module&gt;
    from almathswig import *
  File &quot;C:\Python27\Lib\site-packages\pythonNaoqi\lib\almathswig.py&quot;, line 26, in &lt;module&gt;
    _almathswig = swig_import_helper()
  File &quot;C:\Python27\Lib\site-packages\pythonNaoqi\lib\almathswig.py&quot;, line 18, in swig_import_helper
    import _almathswig
ImportError: No module named _almathswig
</code></pre>
<p>Within my naoqi library, the almath swig module is present
[1]: <a href=""https://i.stack.imgur.com/BdhFU.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/BdhFU.png</a></p>
<p>Please help, why can't python find it?</p>
",44276.49931,67379741,881,3,0,0,,15444742,,44276.49028,5,67379741,"<p>I have experienced the same while working with Almath. However, could you directly install the chorographe Version 2.8.6.X from <em><a href=""https://developer.softbankrobotics.com/nao6/downloads/nao6-downloads-windows"" rel=""nofollow noreferrer"">https://developer.softbankrobotics.com/nao6/downloads/nao6-downloads-windows</a></em>.</p>
<p>After installing it you can right click on the canvas and do select <strong>Create New Box</strong> then <em><strong>&quot;python&quot;</strong></em></p>
<p>In General Description write anything like Test then OK.</p>
<p>Double click on the Box and remove everything, then</p>
<p>import almath</p>
<p>After connecting the nodes run it,</p>
<p>Are you getting the same error.</p>
<p>If yes then you can 2.5 version.</p>
<p>There is some bug in the latest version. It is not working on my laptop as well. However Version 2.5 is working.</p>
",12195145,0,2,,,Error
445,3990,66786292,multiprocessing - blanket process termination,|python|python-multiprocessing|robotics|terminate|kill-process|,"<p>I'm building a GUI to control a robot in Python.
The GUI has a few buttons, each one executes a function that loops indefinitely.
It's like a roomba, where the &quot;clean kitchen&quot; function makes it continually clean until interrupted.</p>
<p>In order to keep the GUI interactive, I'm executing the function in a separate process using multiprocessing.</p>
<p>I've got a stop function that I call that returns the robot to home, and kills the child process (otherwise the child process would reach the next line, and the robot would start turning around when it's left the kitchen and gone home). The stop function runs in the main/parent process as it doesn't loop.</p>
<p>I've got a GUI button which calls Stop, and I'll also call it whenever I start a new process.</p>
<p>My processes are started like this:</p>
<pre><code>from file1 import kitchen
from file2 import bedroom

    if event == &quot;Clean kitchen&quot;:
       stoptour()
       p = multiprocessing.Process(target=kitchen,args=(s,),daemon=True)
       p.start()

    if event == &quot;Clean bedroom&quot;:
       stoptour()
       p = multiprocessing.Process(target=bedroom,args=(s,),daemon=True)
       p.start()
</code></pre>
<p>The argument being passed is just the socket that the script is using to connect to the robot.
My stop function is:</p>
<pre><code>def stoptour():
   p.terminate()
   p.kill()
   s.send(bytes.fromhex(XXXX)) #command to send the stop signal to the robot
   p.join()
</code></pre>
<p>This all runs without error and the robot stops, but then starts up again (because the child process is still running). I confirmed this by adding to the stop function:</p>
<pre><code>if p.is_alive:
        print('Error, still not dead')
    else:
        print('Success, its dead')
</code></pre>
<p>Every time it prints &quot;Error, still not dead&quot;...
Why are p.kill and p.terminate not working? Is something spawning more child processes?
Is there a way to write my <code>stoptour()</code> function so that it kills any and all child processes completely indiscriminately?</p>
<hr />
<p>Edited to show the code:</p>
<pre><code>import socket
import PySimpleGUI as sg
import multiprocessing
import time

from file1 import room1
from file2 import room2
from file3 import room3

#Define the GUI window
layout = [[sg.Button(&quot;Room 1&quot;)],
          [sg.Text(&quot;Start cleaning of room1&quot;)],
          [sg.Button(&quot;Room 2&quot;)],
          [sg.Text(&quot;Start cleaning of room2&quot;)],
          [sg.Button(&quot;Room 3&quot;)],
          [sg.Text(&quot;Start cleaning room3&quot;)],
          [sg.Button(&quot;Stop&quot;)],
          [sg.Text(&quot;Stop what you're doing&quot;)]]

# Create the window
window = sg.Window(&quot;Robot Utility&quot;, layout)

#Setup TCP Connection &amp; Connect
TCP_IP = '192.168.1.100' #IP
TCP_port = 2222 #Port
s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) #Setup TCP connection
s.connect((TCP_IP, TCP_port)) #Connect

#Define p so I can define stop function
if __name__ == '__main__':
    p = multiprocessing.Process(target=room1, args=(s,), daemon=True)
    p.start()
    p.terminate()
    p.kill()
    p.join()

#Define stop function
def stoptour():
    s.send(bytes.fromhex('longhexkey'))
    p.terminate()
    p.kill()
    p.join()
    s.send(bytes.fromhex('longhexkey')) #No harm stopping twice...
    if p.is_alive:
        print('Error, still not dead')
    else:
        print('Success, its dead')
stoptour()

#GUI event loop
while True:
    event, values = window.read()
    if event == &quot;Room 1&quot;:
        if __name__ == '__main__':
            stoptour()
            p = multiprocessing.Process(target=room1, args=(s,), daemon=True)
            p.start()

        
    if event == &quot;Room 2&quot;:
        if __name__ == '__main__':
            stoptour()
            p = multiprocessing.Process(target=room2, args=(s,), daemon=True)
            p.start()

    if event == &quot;Room 3&quot;:
        if __name__ == '__main__':
            stoptour()
            p = multiprocessing.Process(target=room3, args=(s,), daemon=True)
            p.start()
        
    if event == &quot;Stop&quot;:
        stoptour()
        sg.popup(&quot;Tour stopped&quot;)
    
    if event == sg.WIN_CLOSED:
        stoptour()
        s.close()
        print('Closing Program')
        break

window.close()
</code></pre>
",44279.72917,,93,0,4,0,,15460390,,44278.48264,2,,,,,,118058852,"Note `p.kill` doesn't call `kill`, you need to do `p.kill()`",Remote
446,3949,65703124,Convert Euler angle between camera & robot coordinate system,|python|opencv|robotics|euler-angles|aruco|,"<p>My problem is simple, but yet confusing as I personally have no experience in angles and angles conversion yet.</p>
<p>Basically, I need to locate the position of an object attached with single AruCo marker then send the 3d coordinate and pose of that object (the marker) to the robot. Note that the robot model I use is an industrial one manufactured by ABB, and the 3d coordinate I sent already been converted to Robot Coordinate System.</p>
<p>Put aside the problem of coordinate, I solved it using Stereo Cameras. However, I found the pose problem to be so difficult, especially when convert the pose of AruCo marker w.r.t camera to the robot coordinate system. The images below represent the two-coordinate system, one for camera and one for the robot.</p>
<p><a href=""https://i.stack.imgur.com/hwpG3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hwpG3.png"" alt=""Coordinate System"" /></a></p>
<p>The angle I collected from AruCo Marker were converted to Euler Angles, the methods were applied from OpenCV library here:</p>
<pre><code>def PoseCalculate(rvec, tvec, marker):
    rmat = cv2.Rodrigues(rvec)[0]
    P = np.concatenate((rmat, np.reshape(tvec, (rmat.shape[0], 1))), axis=1)
    euler = -cv2.decomposeProjectionMatrix(P)[6]
    eul = euler_angles_radians
    yaw = eul[1, 0] 
    pitch = eul[0, 0]
    roll = eul[2, 0]
    return (pitch, yaw, roll)
</code></pre>
<p>The result are three angles that represent pose of the marker. Pitch represents the rotation when the marker rotate around X axis (camera), Yaw for the Y axis (camera) and Roll for the Z axis (camera as well.)
So, how I can convert these three angles to the robot coordinate system?</p>
<p><a href=""https://i.stack.imgur.com/qqbi7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qqbi7.png"" alt=""Sample Angles Collected"" /></a></p>
<p>Thanks for reading this long question and wish all of you be healthy in new year 2021!</p>
",44209.56875,,418,0,0,2,0,11414471,,43581.31458,8,,,,,,,,Coordinates
447,3972,66222390,Communicating with MiR200 Robot using REST API,|python|json|robotics|rest|,"<p>I have been trying to control the Mir 200 robot using REST API in python. I have programmed the mission in Mir to work only if the PLC register value changes, so I give the following request in python but I get a 405 error. Can anybody help me with the syntax? I try to change the PLC register 41 value to 5.</p>
<pre class=""lang-py prettyprint-override""><code>register = {&quot;value&quot;: 5}
PLCregister = requests.post(host + 'registers/41', json = register, headers = headers)
print(PLCregister)
</code></pre>
",44243.42292,,1762,2,2,0,0,14643608,,44150.71597,12,66405743,"<p>I have never worked with the MiR200 or any of their robots (they look cool and I'm a little jealous) but it looks like instead of a <code>POST</code> request, you can try a <code>PUT</code> request to modify the value. I'm going off this pdf: <a href=""https://www.mobile-industrial-robots.com/media/2214/mir_robot_rest_api_200.pdf"" rel=""nofollow noreferrer"">https://www.mobile-industrial-robots.com/media/2214/mir_robot_rest_api_200.pdf</a></p>
<p>The 405 error means the request method (<code>POST</code> in this case) is not allowed, maybe it has been deprecated.</p>
<p>You can try this:</p>
<pre class=""lang-py prettyprint-override""><code>register = {'value': 5}
plc_register = requests.put(host + 'registers/41', json=register, headers=headers)
print(plc_register)
</code></pre>
<p><code>POST</code> requests are generally used to tell the server to create data, not modify it. You can see they kinda mention this in the MiR 2.0.X documentation:</p>
<p><code>POST /registers/{id}</code></p>
<blockquote>
<p>Modify the value of the PLC register with the specified ID. Registers
1 to 100 are integers and registers 101-200 are float. <code>Even though this is not a standard use of the POST call it has been included for compatibility purposes</code></p>
</blockquote>
<p>So, maybe the <code>POST</code> method is not in your robot's software, just speculating.</p>
",12442479,0,0,117398153,"`405` means not allowed, I'd take a look at the documentation for the MIR-200 (that's what anybody answering the question would have to do for you). You should update the question tags to make sure you get the right people viewing too.",Remote
448,4068,69113962,Wiiuse library - how to calculate the quaternion from the wm->exp.mp.angle_rate_gyro like the DSU Controller test,|quaternions|robotics|euler-angles|wiimote|wiiuse|,"<p>I currently have the wiiuse lib op and running with the motion plus output the <code>angle_rate</code> from the gyro. Now i want this to give me the output in angles either euler representation or best of with quaternions, and i am a little stuck here. any  solutions code examples that could point me in the way of how to calculate these?</p>
<p>I have an example of wiimoteHook running with a DSU controller test that gives output the quaternion output and is exactly what i want to give further to my program.</p>
<p>my program i am working on is that i am trying to make the wii remote hold by a person with a position system using ultra sound that gives me a coordinate(x,y,z) in a world frame then I want the wiimote to give me the rotation in that point to teach a 6 axis robot a tool center point that in the end would imitate movement of the remote.</p>
<p>I hope that somebody can guide me in the way of getting the rotations from the wiimote.</p>
<p>Thanks in advance.</p>
",44448.31042,,113,0,7,0,,16867345,,44448.29722,2,,,,,,122349677,"Yes your are right at some point, but i can calculate the orientation based upon the anglerate and the sample time. im am almost there with a big inspiration from this site. https://x-io.co.uk/open-source-imu-and-ahrs-algorithms/ and from this https://www.digikey.com/en/articles/apply-sensor-fusion-to-accelerometers-and-gyroscopes. now i have the quaternions and a rotation vector. next in my work i have to make my own calibration function since i have to calbrate over time becourse of the gyro is drifting",Remote
449,4125,69570518,OpenCV Detecting points along curved lines,|python|opencv|line|curve|robotics|,"<p>I am new to openCV programming and I would like to do something which seems fairly simple in my mind but I haven't found any useful solutions.</p>
<p>I am building a robot that needs to follow lines on an image drawn by a user. Let's use <a href=""https://i.stack.imgur.com/uyB8G.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uyB8G.png"" alt=""line drawing"" /></a> as a reference.</p>
<p>Obviously, the line will need to be converted into points in order to navigate to. I have attempted using Houghlines but that seems to only work for straight lines. Using contours seems promising to me but the result I get from findContours is a set of points on either side of the line such as: <a href=""https://i.stack.imgur.com/yNqKB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yNqKB.png"" alt=""drawing with contours"" /></a>I am not surprised as this is the definition of a contour.</p>
<p>My question is, is there a way to detect points along the line and not on either side of it?</p>
<p>Thanks!</p>
",44483.50972,,2263,0,3,3,,17078688,,44474.40972,10,,,,,,122968873,"For what it's worth, I had this same question a couple of years ago and I've not come across a solution since. There are some aspects that make this a difficult question in general. E.g., if you have a closed loop, where should the start point be; if you have a Y-fork, should your function return a path for each possibility; if you have a noisy/broken path, how do you reconstruct it, especially problematic if there are neighbouring paths.",Incoming
450,3959,65888385,How does a vision guided robot arm approach a target object to pick up and drop it off at a target location?,|python|opencv|computer-vision|robotics|,"<p>I have a work project currently where my company is using a 3D printer, a couple of servo motors, and a raspberry pi to produce a robotic arm. This robotic arm is meant to be visually-guided. I am task to research and code the software for the robot to be able to pick up a specific object and place it at a specific location. I need some help on how I could program the robot to pick up the object using visual guidance.</p>
<p>I've made a set-up where the top-down image will very likely look like the picture below. The objective is to pick up a die (those that you can find in a casino or in a game of monopoly) inside a clearly marked area of operation and place it on a piece of paper with an 'X' marked on it.
<a href=""https://i.stack.imgur.com/rXzt8.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rXzt8.jpg"" alt=""robotic-arm-sandbox"" /></a></p>
<p>This is what I've accomplished thus far:</p>
<ol>
<li>I am able to use OpenCV and Machine Learning to identify the die and the paper marked with 'X' and extract the coordinates in the image where the die is located (right now my camera is not located directly above the area of operations)</li>
<li>I am able to determine the orientation of the die (i.e. 30 degrees clockwise, 42 degrees anti-clockwise, etc) - this is important because the robotic arm uses a caliper-like clipper to pick up the die and I would not want it to hold the die insecurely by the corner</li>
<li>I am also able to code a program to move the robotic arm (something like servo1 to turn 30 degrees clockwise, servo2 to turn 40 degrees anti-clockwise, etc)</li>
</ol>
<p>However, I have no idea how I could code the robotic arm to pick up the die. The die and the X in the photo are for illustration purposes only, their location could change to any spot inside the area of operation.</p>
<p>What is really depressing is I can't come up with any strategy. Hopefully, someone can advise on some principles and recommend some strategies - I'm not asking for code because I am confident to code my own software. I also don't think the programming language is important at this point but if it helps, I am using Python for this project - are there any libraries for such a task already? I felt like I have searched the entire web but haven't found any helpful tutorials on this yet.</p>
<p>Also, if it's helpful - I come from a web-software developer background usually coding in HTML, CSS, and Javascript. Python was the first language I've managed to master to a competent degree before I started coding with web technologies. I have some experience in C during my high school but have not coded in C for more than 10 years already.</p>
<p>Thanks for any help.</p>
",44221.67431,,325,0,0,3,0,3910616,Singapore,41856.57431,76,,,,,,,,Moving
451,3964,66024645,I am getting error when i run this code to make raspberry pi face tracking robot with gui without any servo motor just to follow only people,|python|performance|raspberry-pi|computer-vision|robotics|,"<p>Please tell me what wrong here. It gives error like this</p>
<pre><code>if face == True:#i have used this to make if there is no face found then stop if face found go forward
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
</code></pre>
<p>The code is here:</p>
<pre><code>import RPi.GPIO as GPIO

import cv2

import carapp

import sys

vid = cv2.VideoCapture(0)

face_cascade = cv2.CascadeClassifier('/home/pi/harr cascade/haarcascade_frontalface_default.xml')


Motor1A = 21

Motor1B = 20

Motor2A = 16

Motor2B = 26

GPIO.setwarnings(False)

GPIO.setmode(GPIO.BCM)

GPIO.setup(Motor1A,GPIO.OUT)

GPIO.setup(Motor1B,GPIO.OUT)

GPIO.setup(Motor2A,GPIO.OUT)

GPIO.setup(Motor2B,GPIO.OUT)

def forward():

    print(&quot;GOING FORWARD&quot;)

    GPIO.output(Motor1A,GPIO.LOW)

    GPIO.output(Motor1B,GPIO.HIGH)

    GPIO.output(Motor2A,GPIO.LOW)

    GPIO.output(Motor2B,GPIO.HIGH)

def backward():

    print(&quot;GOING BACKWARD&quot;)

    GPIO.output(Motor1A,GPIO.HIGH)

    GPIO.output(Motor1B,GPIO.LOW)

    GPIO.output(Motor2A,GPIO.HIGH)

    GPIO.output(Motor2B,GPIO.LOW)

def Left():

    print(&quot;Going Left&quot;)

    GPIO.output(Motor1A,GPIO.HIGH)

    GPIO.output(Motor1B,GPIO.LOW)

    GPIO.output(Motor2A,GPIO.LOW)

    GPIO.output(Motor2B,GPIO.HIGH)

def Right():

    print(&quot;Going Right&quot;)

    GPIO.output(Motor1A,GPIO.LOW)

    GPIO.output(Motor1B,GPIO.HIGH)

    GPIO.output(Motor2A,GPIO.HIGH)

    GPIO.output(Motor2B,GPIO.LOW)

def stop():

    print(&quot;Stopping&quot;)

    GPIO.output(Motor1A,GPIO.LOW)

    GPIO.output(Motor1B,GPIO.LOW)

    GPIO.output(Motor2A,GPIO.LOW)

    GPIO.output(Motor2B,GPIO.LOW)
    

def cameo():
    while(True):
        _,img = vid.read()
        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
        face = face_cascade.detectMultiScale(gray,1.1,4)
        for (x,y,w,h) in face:
            cv2.rectangle(img,(x,y),(x+w,y+h),(50,20,70),3)
            
        if face == True:#i have used this to make if there is no face found then stop if face found go forward
            carapp.forward()
        else:
            carapp.stop()

        cv2.imshow('img',img)
        if cv2.waitKey(1) &amp; 0xff == ord('q'):
            break
            sys.exit()
    vid.release()
    cv2.destroyAllWindows()

import tkinter as tk

gui = tk.Tk()

gui.title(&quot;Car control&quot;)

gui.geometry(&quot;500x500&quot;)

lol = tk.Button(gui,text=&quot;Forward&quot;,bg=&quot;red&quot;,command=forward)

lol.grid(row=2,column=5)

bot = tk.Button(gui,text=&quot;Backward&quot;,bg=&quot;green&quot;,command=backward)

bot.grid(row=10,column=5)

ron = tk.Button(gui,text=&quot;Left&quot;,bg=&quot;orange&quot;,command=Left)

ron.grid(row=5,column=0)

bob = tk.Button(gui,text=&quot;Right&quot;,bg=&quot;yellow&quot;,command=Right)

bob.grid(row=5,column=10)

dol = tk.Button(gui,text=&quot;camera&quot;,bg=&quot;blue&quot;,command = cameo)

dol.grid(row=5,column=100)

sod = tk.Button(gui,text=&quot;stop&quot;,bg=&quot;cyan&quot;,command = stop)

sod.grid(row=5,column=5)

button = tk.Button(text = &quot;Click and Quit&quot;, command = sys.exit)

button.grid(row=15,column=10)

gui.mainloop()

#this product is copytright of shouryawadhwa aka @programmerShourya
</code></pre>
",44230.39514,,52,1,2,0,,15112207,,44226.58472,7,66025029,"<p>The return value from</p>
<pre><code>face_cascade = cv2.CascadeClassifier('/home/pi/harr cascade/haarcascade_frontalface_default.xml')
</code></pre>
<p>Is not a single True/False value - it's something that (from the error message) contains multiple values - perhaps that's something over which you can iterate, or alternately use the all, any functions to perform boolean tests over the collection.</p>
<p>Are the results even boolean?</p>
<p>Why don't you look inside of <code>face_cascade</code> and see what kind of data it holds? Once you know that, you'll be better equipped to deal with the result.</p>
<p>Also here:</p>
<pre><code>for (x,y,w,h) in face:
        cv2.rectangle(img,(x,y),(x+w,y+h),(50,20,70),3)
        
    if face == True:#i have used this to make if there is no face found then stop if face found go forward
        carapp.forward()
    else:
        carapp.stop() 
</code></pre>
<p>You start iterating over <code>face</code> in your for-loop, but then later you try to ask whether it's True or False.</p>
<p>Which is inconsistent.</p>
<p>What are the contents of x,y,w,h ? They look like they might describe bounding boxes, not what I'd imagine you'd sensibly be able to test with a True or False.</p>
<p>Are any of those boolean?</p>
<p>Also, some of this &quot;copyrighted code&quot; looks like you copy/pasted it from these docs:</p>
<p><a href=""https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html"" rel=""nofollow noreferrer"">https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html</a></p>
",4137061,0,1,116775053,Thnke for editing the code,Incoming
452,3970,66120048,Is it possible to change array size? How do you do it?,|robotics|kuka-krl|,"<p>Is it possible to change array size at runtime in KUKA KRL programming language? Is it possible to mimic the behavior of List from C#?</p>
",44236.56597,66177253,321,2,0,1,,11668410,"Kaunas, Lietuva",43635.26528,24,66131651,"<p>No.  It  is  not  possible  :(</p>
",5871947,0,0,,,Programming
453,4028,67449652,How to plot a right triangle path in MATLAB,|matlab|robotics|,"<p>The following code is used to plot a circular path:</p>
<pre><code>% Head velocity
v = 1.5;
% Angular velocity
w = 1;      
% Radius
R = v/w;
% Time
dt = 0.1;
t = 0:dt:(2*pi*R/v);
% Initial mobile robot position vector
x0 = 0; y0 = 0; th0 = 0;
P = [];
P(:,1) = [x0 y0 th0]; 
% Loop for all time steps
for k = 1 : length(t)
    P(:,k+1) = P(:,k) + dt*[v*cos(P(3,k));v*sin(P(3,k));w];
    plot(P(1,1:k+1),P(2,1:k+1))
    pause(0.01)
    axis square
    axis([-2 2 -0.5 3.5])
end                         
</code></pre>
<p>, how can I manipulate the position vector to plot a right triangle path.</p>
<p>Thanks in advance.</p>
",44324.67639,,203,0,3,0,0,15872705,,44324.66736,2,,,,,,119259361,"@MatteoV
The plot needs to be in terms of `v*cos(...),v*sin(...)` , I think of making 3 ""for"" loops for each segment.",Moving
454,4146,69841218,Line following challenge - robot moves forward but ignores the line,|c++|robotics|,"<p>I am trying to solve a line following challenge with a 3Pi+ robot, using 3 IR sensors (array in my code) and although I get my array working, receive the right data in the serial monitor and set a threshold value for my sensors, I cannot seem to make the robot follow the line : the robot moves forward but ignores the line.</p>
<p>I am adding my code here:</p>
<pre><code>const int left_sensor_pin = A0;
const int right_sensor_pin = A3;
const int centre_sensor_pin = A2;

// store values into array
#define NB_LS_PINS 3
int ls_pin[NB_LS_PINS] = { left_sensor_pin, centre_sensor_pin, right_sensor_pin };
int which;

// store time
unsigned long start_time;
unsigned long elapsed_time [3];
unsigned long end_time_ls [3];
unsigned long set_motors;

bool done = false;

# define L_PWM_PIN 10
# define L_DIR_PIN 16
# define R_PWM_PIN 9
# define R_DIR_PIN 15

void setup() {
  Serial.begin (9600);
  pinMode (left_sensor_pin, INPUT);
  pinMode (right_sensor_pin, INPUT);
  pinMode (centre_sensor_pin, INPUT);
  delay(5000);

  pinMode(L_PWM_PIN, OUTPUT);
  pinMode(L_DIR_PIN, OUTPUT);
  pinMode(R_PWM_PIN, OUTPUT);
  pinMode(R_DIR_PIN, OUTPUT);

  digitalWrite(L_DIR_PIN, LOW);
  digitalWrite(R_DIR_PIN, LOW);

  analogWrite(L_PWM_PIN, 50);
  analogWrite(R_PWM_PIN, 50);
}

void loop() {

  //charge capacitor
  pinMode (left_sensor_pin, OUTPUT);
  pinMode (right_sensor_pin, OUTPUT);
  pinMode (centre_sensor_pin, OUTPUT);
  digitalWrite(left_sensor_pin, HIGH);
  digitalWrite(right_sensor_pin, HIGH);
  digitalWrite(centre_sensor_pin, HIGH);

  // Delay for capacitor to charge.
  delayMicroseconds(10);
  start_time = micros();

  pinMode (left_sensor_pin, INPUT);
  pinMode (right_sensor_pin, INPUT);
  pinMode (centre_sensor_pin, INPUT);

  bool pins_read [3] = {false, false, false};
  done = false;

  while ( done == false ) {

    for ( int which = 0; which &lt; NB_LS_PINS; which++) {

      if ( digitalRead (ls_pin[ which ]) == LOW &amp;&amp; pins_read [which] == false) { // need also to track whether pins are read, a single = is setting to a value and == compares values

        end_time_ls [which] = micros ();
        elapsed_time [which] = micros() - start_time;
        pins_read [which] = true;
      }

      if (pins_read [0] == true &amp;&amp; pins_read [1] == true &amp;&amp; pins_read [2] == true) {

        done = true;
      }
    }
  }

  elapsed_time[0] = end_time_ls [0] - start_time;
  elapsed_time[1] = end_time_ls [1] - start_time;
  elapsed_time[2] = end_time_ls [2] - start_time;

  Serial.println (elapsed_time [0]);
  Serial.println (elapsed_time [1]);
  Serial.println (elapsed_time [2]);
  Serial.println(&quot;------------------------------&quot;);

  //Motors

  if (elapsed_time [0] &lt; 10000 &amp;&amp; elapsed_time[1] &gt; 10000 &amp;&amp; elapsed_time[2]  &lt; 10000)
  {
    //go forward
    analogWrite( L_PWM_PIN, 100 );
    analogWrite( R_PWM_PIN, 100 );
  }

  else if (elapsed_time [0] &gt; 10000 &amp;&amp; elapsed_time[1] &lt; 10000 &amp;&amp; elapsed_time[2]  &lt; 10000)
  {
    // Turn right.
    analogWrite( L_PWM_PIN, 100 );
    analogWrite( R_PWM_PIN, 0 );
  }
  else if (elapsed_time [0] &lt; 10000 &amp;&amp; elapsed_time[1] &lt; 10000 &amp;&amp; elapsed_time[2]  &gt; 10000)
  {
    // turn left.
    analogWrite( L_PWM_PIN, 0 );
    analogWrite( R_PWM_PIN, 100 );
  }
}
</code></pre>
",44504.62083,,284,0,4,1,,17328995,,44504.61042,4,,,,,,123456714,"True, I may need to use =><, I've been adjusting the threshold value and adding more conditions and my robot is starting to make jerky moves ! But do you know how to write intervals in Arduino ? I would like to write something like if  20000<sensorsX<30000 then ... but I cannot find this info on  C++ websites",Incoming
455,4370,73547951,Does WebRTC make sense for low-latency streaming over local network?,|python|stream|webrtc|real-time|robotics|,"<p>I am developing a python application where a drone and a computer communicate over local network (wifi). My need is to stream the drone's camera to OpenCV-python on the computer with the lowest possible latency at the highest possible resolution.</p>
<p>Thus far I have been trying rather naive approaches over TCP that give okay-ish results, I get something like 0.1s or 0.2s latency for VGA format. It has a point for some use cases as it enables lossless transmission, but since the most common scenario is to aggressively control the drone in real time from the stream, I am aiming for something of much lower latency and hopefully higher resolution.</p>
<p>My advisor has recommended using WebRTC. I have done some research on the matter, found the <code>aiortc</code> library that implements WebRTC in python, but I am unsure this is the way to go for my use case as it seems to be more geared toward web developers.</p>
<p>I am a bit lost I think. Could you highlight the advantages of WebRTC in my application if any, or point me toward solutions that are more relevant for my problem please?</p>
<p>Thanks in advance!</p>
",44803.83958,,595,1,0,3,,13224876,,43926.01944,48,73550430,"<p>[1]<a href=""https://i.stack.imgur.com/TCbTV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TCbTV.png"" alt=""Protocol comparison"" /></a></p>
<p>rtc communicate peer to peer, I think you knew that. And if u use local network U will not need STUN server or TURN to connect two devices. That is make more decrease latency and code shorter. I'm not work with drone but I think your method stream had latency &lt; 0,2 is good.
<a href=""https://restream.io/blog/streaming-protocols/"" rel=""nofollow noreferrer"">fyi protocol campare</a></p>
",16970888,1,0,,,Remote
456,4373,73792169,Why do I get wrong results in Eye to Hand calibration with TSAI method?,|opencv|camera-calibration|robotics|,"<p>My goal is to find the position of a fixed camera near an ABB robot. To achieve this I use OpenCV with the <code>calibrateHandEye()</code> method.</p>
<p>My setup :</p>
<ul>
<li>A fixed camera relative to the robot base.</li>
<li>An Aruco chessboard securely attached on the robot gripper.</li>
</ul>
<p>What are the datas I use :</p>
<ul>
<li>The robot flange position in the base robot frame. For each point of view I get X,Y,Z in mm and  Q0, Q1, Q2, Q3 (the quaternions for the orientation)</li>
<li>The chessboard pose using <code>Cv2.SolvePnP()</code> for each point of view.</li>
</ul>
<p>My problem :</p>
<p>In the <code>calibrateHandEye()</code> method when using the <code>HandEyeCalibrationMethod.PARK</code> parameter the results look fine but with the <code>HandEyeCalibrationMethod.TSAI</code> parameters the results are totally wrong.
What is weird is that with the <code>TSAI</code> method the results are wrong only because of a specific point of view. If I remove the problematic point I get good results. I double checked this point and nothing looks wrong about it. The only particularity of this point is that Q3 does not have the same sign compare to the others.
What do I miss? Is there a concept I do not get about the HandEye calibration? Do I need to &quot;convert&quot; my quaternion when Q3 has a different sign?</p>
<p>The robot poses (X Y Z Q0 Q1 Q2 Q3):</p>
<pre><code>752.38  -445.96 638.27  0.49314 0.82816 0.07749 -0.25488
695.07  -422.2  608.08  0.59008 0.55626 0.29763 -0.50378
676.35  -398.17 536.42  0.24081 0.79788 0.39938 -0.38195
564.8   -485.81 496.16  0.15692 0.67193 0.57801 -0.43565
636.3   -585.09 785.91  0.35118 0.70842 0.31395 -0.5256
674.15  -511.81 773.14  0.57508 0.58545 0.14986 -0.55143
571.73  -357.76 475.48  0.45984 0.76823 0.35026 -0.27512
570.54  -689.04 642.31  0.54275 0.73657 0.1737  -0.3643
522.64  -715.58 524.38  0.56756 0.69228 0.40363 -0.18898
959.89  -356.79 270.37  0.21973 0.84652 0.08542 -0.47732
797.93  -704.56 556.47  0.32647 0.79281 -0.20858    -0.47049
719.35  -976.8  364.49  0.30796 0.6863  -0.01742    -0.65867
906.75  -815.7  442.98  0.28293 0.61452 0.28696 -0.67821
817.77  60.02   154.15  0.39718 0.51765 0.5831  -0.48403
601.56  -569.27 480.03  0.73969 0.62158 0.24355 -0.08473
564.14 -408.19 612.04 0.43702 0.75943 0.42312 -0.23074
213.07 -606.02 670.48 0.43765 0.18263 0.87376 0.10798
</code></pre>
<p>The chessboard poses (X Y Z Rz Ry Rz):</p>
<pre><code>-173.77009900744957 -27.139489979884726 721.9859325597129 -2.334719719539493 -0.6062024477154826 -0.28966237372137954
-162.65501847055066 -26.85185566224528 554.2767630115629 -3.0903465310356175 -0.2708982558600907 0.3291807930683886
-212.2479141822481 51.766662118465504 800.8059566577332 2.84556559822533 0.6680923622745605 1.0186319821149945
-86.09262298219434 126.5189807416448 819.6499137042784 2.4464190779627226 0.3475444809955279 0.9796705709434745
-21.898455311634194 89.93494845557872 450.972207473501 2.9168894650869452 0.7851694907456874 0.36096988692878934
-125.26779119062992 46.70400960098488 387.12034740949827 -2.890237270353615 -0.6482179109246602 0.49523655100783626
-206.47692825640706 101.15492953993694 853.5432013448462 -2.7845901323880615 -0.22557851509655572 -0.6048715250043196
95.71225425941525 150.04326736083527 632.9246651210359 -2.667387096309625 -0.49559063361443423 -0.0800873810963933
185.27210452976593 75.05506502382198 785.5205288697724 -2.736330698283884 0.2238771996238751 -0.47211319818633085
-377.8399022540909 -154.0271857570537 1048.9407456307115 -2.5031939821571854 -1.4073251338872734 -0.564144060449975
-97.9813803618571 -2.8982716572417337 780.3826436567616 -2.0175024432723028 -1.52111122787833 0.07978790180677371
206.26218058582174 76.52368830230974 839.4288605610492 -2.5926961439942273 -1.6274800425133538 0.23606702098023685
168.593405541405 -172.2903924334086 728.3609797990065 2.5826463269087228 0.9741779880929422 0.06773704115655899
-599.5692170589801 -208.7756878651145 1082.1873391173815 2.643714241537085 -0.05662133314429313 0.32661484396025725
42.774011707655745 -13.244986693290826 777.4026418514148 -2.3533206372842397 0.38523715542666004 0.0006794851359404635
-146.04506073796117 71.8778078758312 735.3038919876881 -2.7999700612696636 -0.05617005954652475 -0.7930410515888396
-93.2419492180757 102.4051550208101 689.1444785949833 1.8674927518469928 -1.9023073605673702 0.8138748033587386

</code></pre>
<p>My results :
Translation X,Y,Z of my camera relative to the robot : 543.69 -551.87 282.19 (totally wrong results)</p>
<p>With same datas except the last one : 518.87 -769.82 1273.86 (coherent results)</p>
<p>My code if it can help : (I use OpenCV in C# with the nugget OpenCvSharp but you can provide me python or C++ code)</p>
<ul>
<li>To convert quaternion to rotation matrix :</li>
</ul>
<pre><code>    private double[,] Quaternion2RotationMatrix(float q0, float q1, float q2, float q3)
        {
            double[,] matrix = new double[3, 3];
            matrix[0, 0] = 2 * (q0 * q0 + q1 * q1) - 1;
            matrix[0, 1] = 2 * (q1 * q2 - q0 * q3);
            matrix[0, 2] = 2 * (q1 * q3 + q0 * q2);
            matrix[1, 0] = 2 * (q1 * q2 + q0 * q3);
            matrix[1, 1] = 2 * (q0 * q0 + q2 * q2) - 1;
            matrix[1, 2] = 2 * (q2 * q3 - q0 * q1);
            matrix[2, 0] = 2 * (q1 * q3 - q0 * q2);
            matrix[2, 1] = 2 * (q2 * q3 + q0 * q1);
            matrix[2, 2] = 2 * (q0 * q0 + q3 * q3) - 1;    
            return matrix;
        }
</code></pre>
<ul>
<li>To convert a Mat object to a Array</li>
</ul>
<pre><code>    private double[,] ConvertMat2Array(Mat _mat, bool _isFloat = true)
        {
            double[,] result = new double[_mat.Rows, _mat.Cols];
            for (int i = 0; i &lt; _mat.Rows; i++)
            {
                for (int j = 0; j &lt; _mat.Cols; j++)
                {
                    if (_isFloat)
                        result[i, j] = _mat.At&lt;float&gt;(i, j);
                    else
                        result[i, j] = _mat.At&lt;double&gt;(i, j);
                }
            }
            return result;
        }
</code></pre>
<ul>
<li>My Pose3D class to store the robot coordinates</li>
</ul>
<pre><code>    public class Pose3D
    {
        public float X { get; set; }

        public float Y { get; set; }

        public float Z { get; set; }

        public float Q0 { get; set; }

        public float Q1 { get; set; }

        public float Q2 { get; set; }

        public float Q3 { get; set; }

        public Pose3D()
        {

        }

        public Pose3D(float _x, float _y, float _z, float _q0, float _q1, float _q2, float _q3) : this()
        {
            X = _x;
            Y = _y;
            Z = _z;
            Q0 = _q0;
            Q1 = _q1;
            Q2 = _q2;
            Q3 = _q3;
        }

    }
</code></pre>
<ul>
<li>And finally my method to estimate the camera position in the robot base frame :</li>
</ul>
<pre><code>    private (double[,], double[,]) HandEyeCalibration(Pose3D[] _gripper2base, double[][] _rTarget2cam, double[][] _tTarget2Cam)
        {
            try
            {
                var l = _gripper2base.Length;
                Mat[] R_base2gripper = new Mat[l];
                Mat[] t_base2gripper = new Mat[l];

                Mat[] R_target2cam = new Mat[l];
                Mat[] t_target2cam = new Mat[l];
                for (int i = 0; i &lt; l; i++)
                {
                    // Convert quaternion to rotation matrix
                    var R_gripper2base = Quaternion2RotationMatrix(_gripper2base[i].Q0, _gripper2base[i].Q1, _gripper2base[i].Q2, _gripper2base[i].Q3);
                    double[,] T_gripper2base = new double[4, 4];
                    for (int row = 0; row &lt; 3; row++)
                        for (int col = 0; col &lt; 3; col++)
                            T_gripper2base[row, col] = R_gripper2base[row, col];

                    T_gripper2base[0, 3] = _gripper2base[i].X;
                    T_gripper2base[1, 3] = _gripper2base[i].Y;
                    T_gripper2base[2, 3] = _gripper2base[i].Z;
                    T_gripper2base[3, 3] = 1;

                    // Eye to Hand configuration. Need to invert gripper2base to get base2gripper to find cam2base
                    Mat mat = new Mat(4, 4, MatType.CV_64FC1, T_gripper2base);
                    var invExpr = mat.Inv();
                    var inv = invExpr.ToMat();

                    // Extract rotation matrix and translation vector
                    R_base2gripper[i] = inv.SubMat(0, 3, 0, 3);
                    t_base2gripper[i] = inv.SubMat(0, 3, 3, 4);
                    Mat tmp_R_t2c = new Mat(3, 1, MatType.CV_64FC1, _rTarget2cam[i]);
                    Mat tmp_t_t2c = new Mat(3, 1, MatType.CV_64FC1, _tTarget2Cam[i]);

                    R_target2cam[i] = tmp_R_t2c;
                    t_target2cam[i] = tmp_t_t2c;

                }

                using Mat R_cam2base = new Mat();
                using Mat t_cam2base = new Mat();
                // Replace HandEyeCalibrationMethod.TSAI by HandEyeCalibrationMethod.PARK to change the method used.
                Cv2.CalibrateHandEye(R_base2gripper, t_base2gripper, R_target2cam, t_target2cam, R_cam2base, t_cam2base, HandEyeCalibrationMethod.TSAI);
                var rotationMatrix = ConvertMat2Array(R_cam2base, false);
                var translationMatrix = ConvertMat2Array(t_cam2base, false);

                return (rotationMatrix, translationMatrix);
            }
            catch (Exception ex)
            {
                return (null, null);
            }
        }
</code></pre>
<p>Thank you !</p>
",44824.84306,,413,0,0,1,,13247582,"Montréal, CANADA",43928.43819,8,,,,,,,,Incoming
457,4275,71661263,What's consistency mapping in SLAM?,|c++|computer-vision|point-cloud-library|robotics|slam|,"<p>I always see &quot;consistent mapping&quot; or &quot;map consistency&quot; in SLAM papers and articles, but I have no idea about what consistent map is.</p>
<p>I have found <a href=""https://stackoverflow.com/questions/21086082/what-is-consistency-map-confidence-map"">enter link description here</a>, but it did not solve my problem.
Furthermore, what is local consistentcy and global consistency?</p>
",44649.48125,71688947,216,1,0,1,,17081429,,44474.64722,8,71688947,"<p>During the mapping, the robot sequentially tries to locate landmarks or objects around it with precise coordinates, both locally and globally. The local consistency of the landmarks in each sequential operation means that their positions relative to the robot and the positions among themselves correspond to reality.</p>
<p>The map fragments created at each step are combined to form a meaningful global map. In the meantime, the landmarks determined in the previous step can also take place in the next step. During this merge, if the locations of the landmarks are very different from each other on both local maps, local maps cannot be combined and mapping cannot be made because there will be no consistency.</p>
<p>Consistency of both the local and global map is critical, especially in loop closure studies. If the map you created is not consistent, you cannot detect loop closure.</p>
<p>In summary, <em>the consistency of your map</em> means that the locations of the same landmarks that you detect at different stages are consistent with each other.</p>
",4717084,0,1,,,Moving
458,4430,74131579,How can I get an accurate relative yaw angle from a 6DOF IMU like the LSM6DS3 with minimal gyro drift?,|accelerometer|robotics|gyroscope|kalman-filter|imu|,"<p>I currently have a platform that has a 6DOF IMU (the LSM6DS3) with no magnetometer. I want to get as accurate of a relative yaw angle reading as possible, avoiding or minimizing any gyro drift. I tried a simple approach of:</p>
<p>a) calculate the gyro angular rate zero-offset by computing the average gyro angular rate reading for a few seconds while my platform is known to be stationary.</p>
<p>b) read the angular rate while the LSM6DS3 reports a new reading is available (so, this should essentially be the output data rate I configured, i.e. 416Hz). I subtract this from the zero-offset calculated in (a) above to get the degrees/s angular rate and then integrate (multiply the time delta with the degrees/s angular rate and add to the current yaw angle).</p>
<p>Is this the best I can do to avoid gyro drift issues if I want to get as accurate of a relative yaw rating possible?</p>
<p>I looked a little bit at Kalman filters and Madgwick filters and Mahony filters but they don't seem to suggest the ability to improve yaw angle readings as it seems the accelerometer would not be useful to calculate the yaw angle? Is that correct?</p>
",44853.85694,,360,0,0,1,,20286248,,44853.84861,2,,,,,,,,Coordinates
459,4446,74284444,Hard Realtime C++ for Robot Control,|c++|embedded|real-time|robotics|,"<p><br />
I am trying to control a robot using a template-based controller class written in c++. Essentially I have a UDP connection setup with the robot to receive the state of the robot and send new torque commands to the robot. I receive new observations at a higher frequency (say 2000Hz) and my controller takes about 1ms (1000Hz) to calculate new torque commands to send to the robot. The problem I am facing is that I don't want my main code to wait to send the old torque commands while my controller is still calculating new commands to send. From what I understand I can use Ubuntu with RT-Linux kernel, multi-thread the code so that my getTorques() method runs in a different thread, set priorities for the process, and use mutexes and locks to avoid data race between the 2 threads, but I was hoping to learn what the best strategies to write hard-realtime code for such a problem are.</p>
<pre class=""lang-cpp prettyprint-override""><code>// main.cpp
#include &quot;CONTROLLER.h&quot;
#include &quot;llapi.h&quot;

void main{
    ...
    CONTROLLERclass obj;
    ...
    double new_observation;
    double u;
    ...
    while(communicating){
        get_newObs(new_observation); // Get new state of the robot (2000Hz)
        obj.getTorques(new_observation, u); // Takes about 1ms to calculate new torques
        send_newCommands(u); // Send the new torque commands to the robot
    }
    ...
}
</code></pre>
<p>Thanks in advance!</p>
",44867.19792,,319,1,5,0,,15412613,,44272.23125,10,74285362,"<p>Okay, so first of all, it sounds to me like you need to deal with the fact that you receive input at 2 KHz, but can only compute results at about 1 KHz.</p>
<p>Based on that, you're apparently going to have to discard roughly half the inputs, or else somehow (in a way that makes sense for your application) quickly combine the inputs that have arrived since the last time you processed the inputs.</p>
<p>But as the code is structured right now, you're going to fetch and process older and older inputs, so even though you're producing outputs at ~1 KHz, those outputs are constantly being based on older and older data.</p>
<p>For the moment, let's assume you want to receive inputs as fast as you can, and when you're ready to do so, you process the most recent input you've received, produce an output based on that input, and repeat.</p>
<p>In that case, you'd probably end up with something on this general order (using C++ threads and atomics for the moment):</p>
<pre class=""lang-cpp prettyprint-override""><code>std::atomic&lt;double&gt; new_observation;

std::thread receiver = [&amp;] { 
    double d;
    get_newObs(d);
    new_observation = d;
};

std::thread sender = [&amp;] {
    auto input = new_observation;
    auto u = get_torques(input);
    send_newCommands(u);
};
</code></pre>
<p>I've assumed that you'll always receive input faster than you can consume it, so the processing thread can always process whatever input is waiting, without receiving anything to indicate that the input has been updated since it was last processed. If that's wrong, things get a little more complex, but I'm not going to try to deal with that right now, since it sounds like it's unnecessary.</p>
<p>As far as the code itself goes, the only thing that may not be obvious is that instead of passing a reference to <code>new_input</code> to either of the existing functions, I've read new_input into variable local to the thread, then passed a reference to that.</p>
",179910,0,4,131183013,"@JeremyFriesner I see you mean something similar to Jerry's solution. But as my comment says, the object I am using, I am not sure if I can use it as an atomic.",Remote
460,4316,72090152,Optimizing algorithm for a robot picking order,|algorithm|optimization|robotics|plc|,"<p>I'm trying to optimize a pick and place problem based on the previous robot positions.
Let's assume that all the positions, which represents working stations, are numbered from <strong>0</strong> to <strong>5</strong> and there's a SCARA robot between them. Every position does need some <strong>fixed</strong> time to process that piece and the piece must go to all stations before we can call it done . A PLC controls all of this process so it knows when a piece is ready somewhere inside one of these stations and sends the robot there to pick it. <br> <br> It is also important to know that stations <strong>2,3,4</strong> do the same process so a part must go either to station <strong>2, 3</strong> or <strong>4</strong> and then to station <strong>5</strong>. So the first part comes to position <strong>0</strong> (position <strong>0</strong> generates parts), the robot picks that part and places it to position <strong>1</strong>. After a fixed time the robot takes that part and moves it to station <strong>2</strong>. Now the position <strong>1</strong> is empty so the robot takes a part from position <strong>0</strong> and puts it into position <strong>1</strong>. Every movement of the robot takes a small time but not 0, which affects the whole process cycle time. <br> I'm trying to include that robot movement time into the parts processing time so when a piece is ready inside a station, the robot should be right there ready to pick it and place it somewhere else.<br><br>
A real experiment based on 5 stations numbered 0 to 5 gives the following order of the positions:<br />
<br> <br> <code>0-1, 1-2, 0-1, 2-5, 1-2, 0-1, 1-3, 0-1, 2-5, 1-2, 0-1, 1-3</code> ...
<br>
<BR>
The positions can be grouped because once a part is picked (the digit before '-') i know where it will be placed (the digit after) .
<br>
How can I estimate the next picking point so that the robot can move itself there before the plc tells it to ?</p>
",44683.72153,72091541,84,1,0,0,,14814898,,44177.81736,4,72091541,"<p>I don't know if your robot has any resources for this... If it doesn't, you can probably do it on your PLC.</p>
<p>Since the time of your process in each station is fixed, I'm thinking of using 5 or 6 separate timers, one for each station. Once the robot leaves the part in place you could start the specific timer for that station. When the robot is idle, it consults the remaining time of each one and goes to the one with the shortest time to complete.</p>
<p>This could be improved if you have a way to calculate (or look up in a list) the travel time from the current point to the station that is about to end... for example, the robot is at station 0 and station 1 is at 1.0 s from finishing and station 5 is 0.99999s from finishing... it would probably be more efficient to go to position 1 (closer) instead of going to position 5 (far).</p>
<p>Obviously this won't work if you don't know how long the part will take to be available at one of the stations, but in that case if you use a buffer to calculate the average waiting time of a part at a station (in which case you would have a sensor or something to check), you could estimate that the part is about to be ready using timers as well.</p>
",9203951,1,0,,,Timing
461,4218,71122378,Reward Function for automated parking autonomous Robots,|python|reinforcement-learning|robotics|reward|,"<p>I'm implementing a reinforcement learning task, to solve a parking task for autonomous robots. So basically, the idea of the task is to start at a certain Point in front of the parking spot and drive to a pose without colliding with obstacles. The Agent has reached the goal if the a given Position and a Heading angle of the robot matches the goal pose.</p>
<p>I Have actually a lot of Problems shaping a Reward Function to solve this task. So I ask you guys, to help me with this. What I need a Reward Function depending on the following:</p>
<ul>
<li><strong>(Distance reward)</strong>
The closer the robot is to the target, the higher the reward</li>
<li><strong>(Orientation reward)</strong> The smaller the tolerance of the heading angle to the angle of the target position, the higher the reward</li>
<li><strong>(Speed reward)</strong> The slower the speed when approaching the target position, the higher the reward</li>
</ul>
<p>My current Reward function looks like this:</p>
<pre><code>    current_distance = self.get_euclidean_distance(current_position, desire_position)
    self.distance_reward = (-1)* current_distance/self.max_dist_to_targ

    # Heading Reward. 0: indicates vertical parking, -1: means reverse parking state
    heading_angle = abs(current_heading_angle - target_angle)
    if heading_angle &gt; 0 and heading_angle &lt; np.pi:
        self.heading_angle_reward = -1 * abs(heading_angle - np.pi/2) / np.pi
    else:
        self.heading_angle_reward = -2 * abs(heading_angle - (3*np.pi)/2) / (2*np.pi - 1)
        
    # Goal Reward
    self.goal_reward = 0
    if current_distance &lt;= cp.pose_tolerance and heading_angle &lt;= np.radians(cp.heading_angle_tolerance):
        self.goal_reward = 150

    # Collistion Penalty
    if not self.is_near_by_an_object2(beams_coords, current_trans_vel):
        self.collision_penalty = 0
    else:
        self.collision_penalty = -10

    reward = 10*((1 - w) * self.distance_reward + w * self.heading_angle_reward) +\
            10*self.goal_reward + 10* self.collision_penalty 
</code></pre>
<p>I would be very happy if someone can suggest me what is wrong with the function and how to implement the velocity reward!
Thank you guys.</p>
",44607.30069,,151,0,3,0,,18210401,,44607.28611,4,,,,,,125776886,Hi! Im really new on this topic.  Where do I find the MRE? the Reward is being return (I Edit it allreardy! I you tell me what do you need to better understandig I would provide you (I Guess) Thank you!,Other
462,4341,72675794,Balance 2-wheeled robot without making it drift forward/backward,|c|matlab|controls|robotics|control-theory|,"<p>I'm trying to design a controller to balance a 2-wheels robot (around 13kg) and making it robust against external forces (e.g. if someone kicks it, it should not fall and not drift indefinitely forward/backward). I'm pretty experienced with most control techniques (LQR, Sliding Mode Control, PID etc), but I've seen online that most people use LQR for balancing 2-wheels robots, hence I'm going with LQR.</p>
<p>My problem is that, despite I'm able to make the robot not fall down, it rapidly starts going forward/backward indefinitely, and I don't how to make it mantain a certain position on the ground. What I want to achieve is that, when the robot gets kicked by an external force, it must be able to stop moving forward/backward while mantaining the balance (it's not necessary to mantain a position on the ground, I just want the robot to stop moving).
The measurements to which I have access from the sensors are: position on both wheels (x), velocity of both wheels (x_dot), angular position of robot (theta), angular velocity of robot (theta_dot).
Since now I tried 2 approaches:</p>
<ol>
<li>set all reference signals to 0 and try to tune the LQR gain. With this (simple) approach I'm not sure if the coefficients of gain K relative to x and theta should have same or opposite sign, because, if for example the robot gets kicked away from its reference for x, the wheels should move in the direction that makes the robot return to the 0 point, but this would make theta go in the opposite direction. When the robot gets kicked, I would like that first theta gets adjusted in order to brake the movement given by the external force, and then x_dot should go in the same direction as theta in order to stop the robot.</li>
<li>use best LQR gains that I could find empirically/with MATLAB and use some &quot;heuristic&quot; in order to, given the current state of the robot (x, x_dot, theta, theta_dot), choose the reference signals for the state variables. I tried the heuristic &quot;if x_dot goes forward/backward, then make theta inclide backward/forward&quot;, which makes the robot avoid drifting forward/backward in case there're no disturbances, but if I kick the robot it starts oscillating really fast until it falls (I tried to adjust the K gain of LQR to solve this problem, but I couldn't find any that solved it).</li>
</ol>
<p>Which approach would you suggest me to use? Should I implement some more sophisticated heuristics (any suggestion?) or should I just tune the LQR gain until I find the perfect one? Should I consider using an integrator (for controlling which states?) together with the LQR?</p>
",44731.40486,72724753,1147,2,3,0,,10225193,"Ancona, Province of Ancona, Italy",43326.61389,33,72723724,"<p>The type of sensory system, the computation unit on board etc. would definitely define the approach you are taking. Since you did not give more details regarding the setup, let us assume you have a IMU aligned with the body frame and you have a method of computing roll, pitch and yaw of the robot at a given moment. Also let the speed at which u can compute RPY, is at least twice the speed of the main system loop.</p>
<p>You may want to start by designing three independent PID controllers, each for the three axis with 0 degrees being the target state that you want to maintain. Quite a while back, I was able to make my quadrotor self-balance by constraining two axes and tuning one at a time. In your case, you would first make the PID responsible for one of the axis be able to bring the robot to neutral position for a range of external disturbance that you expect the system to face during operation. A PID won't be able to respond fast enough if you had say tuned for say 5 - 10 N force kicks but later subjected to 100 N kick.</p>
<p>Give this a try and maybe furnish the question with details regarding the robot, the type of wheels u are using and so forth.</p>
<p>Good luck.</p>
",14860459,0,0,128708477,"Hi, unfortunately the code is not open source :\ The robot is also custom made",Actuator
463,4195,70602541,I2C issue on Raspi 4 running Raspbian,|robotics|raspberry-pi4|,"<p>I am running a Raspi4 with a HAT board to attach Grove attached components (Adeept's Mars Rover Picar-B kit).  I have had it succesfully up and running with remote access a couple of times.  I believe my SD card was corrupted due to improper shutdown, so I re-imaged and started over again.  Now I am getting this error message:</p>
<p>RuntimeError: Could not determine default I2C bus for platform.</p>
<p>I have never seen this before.  Is this a hardware issue, or can it be fixed on the software end?</p>
",44567.19306,,218,0,3,0,,17813826,,44563.32014,10,,,,,,125242661,"thank you Bill, I will try there",Remote
464,4335,72596431,IKPy Problem: TypeError: __init__() missing 1 required positional argument: 'origin_translation',|python|robotics|,"<p>I am trying to use IKPy Library(v3.3.3) with Python 3.8 to program some demos for my robot arm on the Mujoco platform. However, When I try to do the Kinematic demo, there is something wrong that happened as below.
<a href=""https://i.stack.imgur.com/RDQST.png"" rel=""nofollow noreferrer"">enter image description here</a>
Here is my project code below:</p>
<p><strong>utils.py</strong></p>
<pre><code>from mujoco_py import MjViewer
import glfw

import numpy as np
import ikpy
from ikpy.chain import Chain
from ikpy.link import OriginLink, URDFLink

open_viewers = []  # a static list to keep track of all viewers


class MjViewerExtended(MjViewer):
    &quot;&quot;&quot; An extension of mujoco-py's MjViewer. MjViewerExtended does not
        terminate all other viewers and the python interpreter when closeing.
    &quot;&quot;&quot;

    def __init__(self, sim):
        glfw.init()  # make sure glfw is initialized
        super().__init__(sim)
        open_viewers.append(self)

    def close(self):
        &quot;&quot;&quot; Closes the viewers glfw window. To open a new one, create a new
            instance of MjViewerExtended
        &quot;&quot;&quot;
        # MjViewer only calls glfw.terminate() here killing glfw entierly.
        if glfw.window_should_close(self.window):
            return
        try:
            glfw.set_window_should_close(self.window, 1)
            glfw.destroy_window(self.window)
        except Exception:
            pass

        open_viewers.remove(self)
        if len(open_viewers) == 0:
            glfw.terminate()

    def key_callback(self, window, key, scancode, action, mods):
        if action == glfw.RELEASE and key == glfw.KEY_ESCAPE:
            self.close()
        else:
            super().key_callback(window, key, scancode, action, mods)


class Wam4IK(Chain):
    &quot;&quot;&quot; A basic kinamatic model of the MjWAM4 &quot;&quot;&quot;

    def __init__(self, tool_orientation=None,
                 tool_translation=None,  # x, y, z
                 base_orientation=None,  # x, y, z
                 base_translation=None):

        if base_translation is None:
            base_translation = [0, 0, 0.84]
        if base_orientation is None:
            base_orientation = [0, 0, np.pi / 2]
        if tool_translation is None:
            tool_translation = [0, 0, 0]
        if tool_orientation is None:
            tool_orientation = [0, 0, 0]

        links = [OriginLink(),
                 URDFLink(name=&quot;wam/links/base&quot;,
                          translation=base_translation,  # translation of frame
                          origin_orientation=base_orientation,  # orientation of frame
                          rotation=[0, 0, 0]
                          ),  # joint axis [0, 0, 0] -&gt; no joint
                 URDFLink(name=&quot;wam/links/shoulder_yaw&quot;,
                          translation=[0, 0, 0.16],
                          origin_orientation=[0, 0, 0],
                          rotation=[0, 0, 1]
                          ),
                 URDFLink(name=&quot;wam/links/shoulder_pitch&quot;,
                          translation=[0, 0, 0.186],
                          origin_orientation=[0, 0, 0],
                          rotation=[1, 0, 0]
                          ),
                 URDFLink(name=&quot;wam/links/shoulder_roll&quot;,
                          translation=[0, 0, 0],
                          origin_orientation=[0, 0, 0],
                          rotation=[0, 0, 1]
                          ),
                 URDFLink(name=&quot;wam/links/upper_arm&quot;,
                          translation=[0, -0.045, 0.550],
                          origin_orientation=[0, 0, 0],
                          rotation=[1, 0, 0]
                          ),
                 URDFLink(name=&quot;wam/links/tool_base_wo_plate&quot;,
                          translation=[0, 0.045, 0.350],
                          origin_orientation=[0, 0, 0],
                          rotation=[0, 0, 0]
                          ),
                 URDFLink(name=&quot;wam/links/tool_base_w_plate&quot;,
                          translation=[0, 0, 0.008],
                          origin_orientation=[0, 0, 0],
                          rotation=[0, 0, 0]
                          ),
                 URDFLink(name=&quot;wam/links/tool&quot;,
                          translation=tool_translation,
                          origin_orientation=tool_orientation,
                          rotation=[0, 0, 0]
                          )
                 ]

        self.all_joints = [False, False, True, True, True, True, False, False, False]
        self.active_joints = list(map(lambda x: x == 1, active_joints))
        self.active_links = [False, False, *self.active_joints, False, False, False]
        Chain.__init__(self, name='wam4',
                       active_links_mask=self.active_links,
                       links=links)

    def fk(self, joints, full_kinematics=False):
        joints = np.array([0, 0, *joints, 0, 0, 0])
        return Chain.forward_kinematics(self, joints, full_kinematics)

    def ik(self, target_position=None, target_orientation=None, orientation_mode=None, **kwargs):
        full = Chain.inverse_kinematics(self, target_position, target_orientation, orientation_mode, **kwargs)
        active = self.joints_from_links(full)
        return active

    def joints_from_links(self, joints):
        return np.compress(self.all_joints, joints, axis=0)
</code></pre>
<p><strong>kinematics.py</strong></p>
<pre><code>import numpy as np
from mujoco_robots.utils import Wam4IK

import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d

# from juggling_wams.envs import SingleArmOneBall
from mujoco_robots.robots import MjWam4


def plot_joints(chain, qs):
    fig = plt.figure()
    ax = fig.add_subplot(1, 1, 1, projection='3d', facecolor=&quot;1.0&quot;)
    for pos in qs:
        chain.plot([0, 0] + pos + [0, 0, 0], ax)
    plt.xlabel('x')
    plt.ylabel('y')
    plt.show()


def main():
    chain = Wam4IK(base_translation=[0, 0, 0.84],
                   base_orientation=[0, 0, np.pi / 2])

    links = ['wam/links/base',
             'wam/links/shoulder_yaw',
             'wam/links/shoulder_pitch',
             'wam/links/upper_arm',
             'wam/links/forearm',
             'wam/links/tool_base_wo_plate',
             'wam/links/tool_base_w_plate']

    x0 = np.array([0, 0, 0.84])
    q_test = [[0, 0, 0, 0], [1, 1, 1, 1]]

    robot = MjWam4(render=True, g_comp=True)
    for q in q_test:
        print(f'\n\ntesting for q={q}')
        robot.reset(pos=q)
        cart = chain.forward_kinematics([0, 0] + q + [0, 0, 0], full_kinematics=True)

        for i in range(7):
            print(f'\n{links[i][10:]}')
            mj_pos = robot.get_body_full_mat(links[i])[:3, 3] - x0
            ikpy_pos = cart[i + 1][:3, 3] - x0
            print(f'mj:   {mj_pos}')
            print(f'ikpy: {ikpy_pos}')
            print(f'diff: {mj_pos - ikpy_pos}')

    plot_joints(chain, q_test)

    # inverse kinematics
    x_des = [0.15, 0.86, 1.45]
    q = chain.active_from_full(chain.inverse_kinematics(x_des))
    robot.set_mocap_pos('endeff_des', x_des)
    robot.step(des_pos=q, n_steps=5000)


if __name__ == '__main__':
    main()
</code></pre>
<p>I think there is no problem with my environment setting and other python files. The problem I think should be happened in these two files. If you would like to see other files, I would upload them soon. Thanks!</p>
",44724.95347,,149,1,1,1,,14809807,America,44176.79722,6,72597389,"<p>You made several calls like this:</p>
<pre><code>                 URDFLink(name=&quot;wam/links/shoulder_yaw&quot;,
                          translation=[0, 0, 0.16],
                          origin_orientation=[0, 0, 0],
                          rotation=[0, 0, 1]
                          ),
</code></pre>
<p>Please be sure to specify the mandatory <code>origin_translation</code> argument.</p>
",8431111,0,8,128239421,"Welcome back to Stack Overflow. [Please do not upload images of errors when asking a question.](//meta.stackoverflow.com/q/285551) Instead, copy and paste the error message, formatted like code.  Also, please start by trying to read and understand error messages. For example, do you understand what a `positional argument` is? Did you try to [read the documentation](https://ikpy.readthedocs.io/en/latest/link.html#ikpy.link.URDFLink) for `URDFLink`, in order to understand what needs to be passed?",Programming
465,4438,74230826,"How to get Raw mouse signal (x, y or displacement values) using python?",|python|mouseevent|mouse|robotics|,"<p>If i use libraries like pynput, I'm getting values which are mapped to screen, this value maxes out based on screen resolution.</p>
<p>I do not want that, I'm using my mouse for a robotic project, I want the raw values from my mouse to calculate displacement.</p>
<p>Kindly let me know if you have any answers. thanks.</p>
<p>I tried pynput etc. It does not help.</p>
",44862.20625,,68,0,2,0,,15304805,,44256.24306,2,,,,,,131104731,"Hi thank you for your reply, I tried that, This is the program in my repo.

https://github.com/SujithChristopher/armbo/blob/master/mouse_delta/test.py

It seems to be inactive for a particular direction. 

Because, for example, if my reference is (0, 0), if I move my mouse to (10, 10), then my mouse delta is (10, 10).

but I cannot move to (-10, -10) because that is not allowed, since it is mapped to the screen. 

I think ""Mouse Delta"" is the right term, if you have any ideas, do suggest. Thanks.",Remote
466,4326,72317047,Arduino only executing one set of if-else,|arduino|robotics|motordriver|,"<p>This code if for a simple robot car.
I'm trying to control the robot with 4 geared motors and L289 driver and standard RC Tx/Rx.</p>
<p>I have used some print statements to debug any errors.</p>
<p>When I try to move the robot forward/backward, I can see serial monitor printing froward/backward, but the robot doesn't move.</p>
<p>When I try to move if left/right it works fine. On commenting the left-right moving statements in code the robot does move forward and backward but fails to do so with all the if else statements uncommented.
Here's the code.</p>
<pre><code>//Receiver pin
byte CH1_PIN = 9;
byte CH2_PIN = 10;

//Motor driver pins
int left_motor_pin1 = 4;
int left_motor_pin2 = 5;
int right_motor_pin1 = 6;
int right_motor_pin2 = 7;
void setup() {
  
  // put your setup code here, to run once:
  pinMode(CH1_PIN, INPUT);
  pinMode(CH2_PIN, INPUT);
  pinMode(left_motor_pin1, OUTPUT);
  pinMode(left_motor_pin2, OUTPUT);
  pinMode(right_motor_pin1, OUTPUT);
  pinMode(right_motor_pin2, OUTPUT);
  digitalWrite(left_motor_pin1, LOW);
  digitalWrite(left_motor_pin2, LOW);
  digitalWrite(right_motor_pin1, LOW);
  digitalWrite(right_motor_pin2, LOW);
  Serial.begin(115200);
}

void loop() {
  // put your main code here, to run repeatedly:
  int ch_1 = pulseIn(CH1_PIN, HIGH);
  int ch_2 = pulseIn(CH2_PIN, HIGH);


  drive(ch_1, ch_2);
  delay(5);

}


void drive(int move_left_right, int move_fwd_back) {


  // Set direction for moving forward

  if ( move_fwd_back &gt; 1700 ) {
    digitalWrite(left_motor_pin1, HIGH);
    digitalWrite(left_motor_pin2, LOW);
    digitalWrite(right_motor_pin1, HIGH);
    digitalWrite(right_motor_pin2, LOW);
    Serial.println(&quot;forward&quot;);
  }
  // Set direction for moving backwards.
  else if (move_fwd_back &lt; 1300) {
    digitalWrite(left_motor_pin1, LOW);
    digitalWrite(left_motor_pin2, HIGH);
    digitalWrite(right_motor_pin1, LOW);
    digitalWrite(right_motor_pin2, HIGH);
    Serial.println(&quot;reverse&quot;);
  }
  else {
    digitalWrite(left_motor_pin1, LOW);
    digitalWrite(left_motor_pin2, LOW);
    digitalWrite(right_motor_pin1, LOW);
    digitalWrite(right_motor_pin2, LOW);
    Serial.println(&quot;NONE&quot;);
  }

  // Set direction for moving left
  if ( move_left_right &lt; 1300 ) {
    digitalWrite(left_motor_pin1, HIGH);
    digitalWrite(left_motor_pin2, LOW);
    digitalWrite(right_motor_pin1, LOW);
    digitalWrite(right_motor_pin2, HIGH);
    Serial.println(&quot;left&quot;);
  }
  
  //set directionfor moving right
  else if (move_left_right &gt; 1700) {
    digitalWrite(left_motor_pin1, LOW);
    digitalWrite(left_motor_pin2, HIGH);
    digitalWrite(right_motor_pin1, HIGH);
    digitalWrite(right_motor_pin2, LOW);
    Serial.println(&quot;right&quot;);
  }
  else {
    digitalWrite(left_motor_pin1, LOW);
    digitalWrite(left_motor_pin2, LOW);
    digitalWrite(right_motor_pin1, LOW);
    digitalWrite(right_motor_pin2, LOW);
    Serial.println(&quot;NONE&quot;);
  }


}
</code></pre>
",44701.41806,72318753,68,1,3,0,,14765788,India,44169.84514,18,72318753,"<p>The issue is that you have two <code>if-else</code> conditions - both changing the same outputs. So the 2nd <code>if-else</code> condition will always override what the 1st one has done.</p>
<p>eg. if you want the motor to just move forward, the code would set the motors to both move forward - however, immediately afterwards, the code decides there is no left/right input so sets the motors to stop. This is so fast you don't see any movement in the motors.</p>
<p>To start with, I would change the code so that the decision regarding the left/right input is inside the else condition of the forward/backward condition. This would give the forward/backward input priority over the left/right input.</p>
<p>i.e.</p>
<pre><code>if ( move_fwd_back &gt; 1700 ) {
    digitalWrite(left_motor_pin1, HIGH);
    digitalWrite(left_motor_pin2, LOW);
    digitalWrite(right_motor_pin1, HIGH);
    digitalWrite(right_motor_pin2, LOW);
    Serial.println(&quot;forward&quot;);
}
// Set direction for moving backwards.
else if (move_fwd_back &lt; 1300) {
    digitalWrite(left_motor_pin1, LOW);
    digitalWrite(left_motor_pin2, HIGH);
    digitalWrite(right_motor_pin1, LOW);
    digitalWrite(right_motor_pin2, HIGH);
    Serial.println(&quot;reverse&quot;);
}
else {
    // Set direction for moving left
    if ( move_left_right &lt; 1300 ) {
        digitalWrite(left_motor_pin1, HIGH);
        digitalWrite(left_motor_pin2, LOW);
        digitalWrite(right_motor_pin1, LOW);
        digitalWrite(right_motor_pin2, HIGH);
        Serial.println(&quot;left&quot;);
    }
    //set directionfor moving right
    else if (move_left_right &gt; 1700) {
        digitalWrite(left_motor_pin1, LOW);
        digitalWrite(left_motor_pin2, HIGH);
        digitalWrite(right_motor_pin1, HIGH);
        digitalWrite(right_motor_pin2, LOW);
        Serial.println(&quot;right&quot;);
    }
    else {
        digitalWrite(left_motor_pin1, LOW);
        digitalWrite(left_motor_pin2, LOW);
        digitalWrite(right_motor_pin1, LOW);
        digitalWrite(right_motor_pin2, LOW);
        Serial.println(&quot;NONE&quot;);
    }
}
</code></pre>
",18127542,0,3,127760556,I don't know. But I would start with a table (on paper or Excel) with inputs as header and first column and fill in the required outputs.,Remote
467,4211,71033078,module' object is not callable in python roboticstoolbox by Peter Corke,|python|plot|backend|robotics|,"<p>Good morning everybody, I'm currently using the python Roboticstoolbox written by Peter Corke but I have a problem when trying to make the noodlelike plot of my robotic arm. The same error appears when trying to run the provided example:</p>
<p><a href=""https://i.stack.imgur.com/LWRXA.png"" rel=""nofollow noreferrer"">Example code</a></p>
<p>The error is the following:</p>
<pre><code> pyplot = rtb.backends.PyPlot()  # create a PyPlot backend
</code></pre>
<p>TypeError: 'module' object is not callable</p>
<p>How can I make it work?</p>
",44600.475,,105,0,4,0,,18150939,Italy,44600.46528,2,,,,,,125570178,"Where is that distribution `rtb` available? If running one of their examples produces an errors, I think it is best to open an issue with the project.",Error
468,4440,74265400,"Project in Python for a robot moving in a 10 by 10 grid, with some extra requirements",|python|python-3.x|robotics|,"<p>I have a project to write code in Python that will control a robot's movements in a 10 by 10 grid. First i would like to point out that i am a beginner so it would be better for me if i can get simple lines of code that i can digest.</p>
<p>So, the project asks for:</p>
<p>A 10 by 10 grid, with the robot starting from the uppermost left position which is X(0,0).</p>
<p>Moving from X(0,0) down one tile will increase the value to (1,0) until (9,0) which is the downmost left corner, while moving from X(0,0) to the right will increase each time by (0,1) until (0,9) upper right corner. Moving from position (9,0) to the right will again be up to (9,9).</p>
<p>It will accept commands to move Up, down, left, right (u ,d ,l, r). Each command should be given together with and integer number that denotes the steps to the given direction (for ex. u5, or d2).</p>
<p>The user will give commands continuously until ENTER is pressed which will make the program exit.</p>
<p>After each command, the program must calculate the position of the robot and print out a message with it.</p>
<p>In the case that the user gives a command that cannot be executed or will make the robot go outside the grid, then an error message must appear. It must also give the error message in instances where X or Y =&gt; N.</p>
<p>Thanks to anyone who will take the time to help me!</p>
<p>I have not tried anything yet as i am in a loss of what to do and how.</p>
",44865.63542,,242,1,0,-1,,20380430,,44865.62639,3,74356725,"<pre><code>while True:
try:
    n = int(input(&quot;Δώσε διαστάσεις του grid:&quot;))
    n1 = n*n
    print(f&quot;Το ρομπότ κινείται σε χώρο {n1}. Η αρχική του θέση είναι:&quot;, x, &quot;,&quot;, y)
except ValueError:
    print(&quot;Παρακαλώ δώσε θετικό ακέραιο αριθμό&quot;)
    continue

while True:
    try:
        move = input(&quot;Δώσε κίνηση:&quot;)
        if move[0] == 'r':
            if int(move[1:])+y &gt;= n1:
                print(&quot;ΣΦΑΛΜΑ! Κίνηση έξω από τα όρια του χώρου.\nΗ θέση του ρομπότ παραμένει:&quot;, x, &quot;,&quot;, y)
            else:
                y = y+int(move[1:])
                print(&quot;Η νέα θέση του ρομπότ είναι:&quot;, x, &quot;,&quot;, y)

        if move[0] == 'l':
            if int(move[1:])-y &gt; 0:
                print(&quot;ΣΦΑΛΜΑ! Κίνηση έξω από τα όρια του χώρου.\nΗ θέση του ρομπότ παραμένει:&quot;, x, &quot;,&quot;, y)
            else:
                y = y-int(move[1:])
                print(&quot;Η νέα θέση του ρομπότ είναι:&quot;, x, &quot;,&quot;, y)

        if move[0] == 'u':
            if int(move[1:])-x &gt; 0:
                print(&quot;ΣΦΑΛΜΑ! Κίνηση έξω από τα όρια του χώρου.\nΗ θέση του ρομπότ παραμένει:&quot;, x, &quot;,&quot;, y)
            else:
                x = x-int(move[1:])
                print(&quot;Η νέα θέση του ρομπότ είναι:&quot;, x, &quot;,&quot;, y)

        if move[0] == 'd':
            if int(move[1:])+x &gt;= n1:
                print(&quot;ΣΦΑΛΜΑ! Κίνηση έξω από τα όρια του χώρου.\nΗ θέση του ρομπότ παραμένει:&quot;, x, &quot;,&quot;, y)
            else:
                x = x+int(move[1:])
                print(&quot;Η νέα θέση του ρομπότ είναι:&quot;, x, &quot;,&quot;, y)
    except ValueError:
        print(&quot;ΣΦΑΛΜΑ! Κίνηση έξω από τα όρια του χώρου.\nΗ θέση του ρομπότ παραμένει:&quot;, x, &quot;,&quot;, y)
        continue
while move == &quot;&quot;:
    print(&quot;Τερματισμός προγράμματος&quot;)
    exit()
</code></pre>
",20380430,0,1,,,Remote
469,4318,72200178,Arduino MQTT synchronicity,|arduino|wifi|mqtt|robotics|,"<p>I try to establish a MQTT-connection between Arduino WifiRev2  and
a python script on an ubuntu20-system.</p>
<p>For test purposes the arduino is connected with 3 ultrasonic sensors and a gyro sensor.</p>
<p>The mqtt-connection seems to work fine, arduino does publish the sensor data and
also receives commands from the python scripts.</p>
<p>But the sensor data are not sent synchronously.
The sensor sent first is received much more frequent (about faktor 4)
than the other sensor data.</p>
<p>The script is adopted from this example code:
<a href=""https://docs.arduino.cc/tutorials/uno-wifi-rev2/uno-wifi-r2-mqtt-device-to-device"" rel=""nofollow noreferrer"">https://docs.arduino.cc/tutorials/uno-wifi-rev2/uno-wifi-r2-mqtt-device-to-device</a></p>
<p>Here is the loop in my arduino script:</p>
<pre><code>const long interval = 30;
unsigned long previousMillis = 0;
void loop() {

  mqttClient.poll();

  unsigned long currentMillis = millis();

  if (currentMillis - previousMillis &gt;= interval) {
    // save the last time a message was sent
    previousMillis = currentMillis;

    //record random value from A0, A1 and A2
    //int Rvalue = analogRead(A0);
    int gyroSCL = digitalRead(gryoSCL_PIN);
    int gyroSDA = digitalRead(gryoSDA_PIN);
    int l_distance = getUSensDistance(USens_L_Trig_Pin,USens_L_Echo_Pin);
    int r_distance = getUSensDistance(USens_R_Trig_Pin,USens_R_Echo_Pin);
    int f_distance = getUSensDistance(USens_F_Trig_Pin,USens_F_Echo_Pin);
    int g_value = getGyro();

    mqttClient.beginMessage(topicL);
    mqttClient.print(l_distance);
    mqttClient.endMessage();

    mqttClient.beginMessage(topicR);
    mqttClient.print(r_distance);
    mqttClient.endMessage();

    mqttClient.beginMessage(topicF);
    mqttClient.print(f_distance);
    mqttClient.endMessage();

    mqttClient.beginMessage(topicG);
    mqttClient.print(g_value);
    mqttClient.endMessage();

    //Serial.println();
  }
}
</code></pre>
<p>The python script is based on this example:
<a href=""https://www.emqx.com/en/blog/how-to-use-mqtt-in-python"" rel=""nofollow noreferrer"">https://www.emqx.com/en/blog/how-to-use-mqtt-in-python</a></p>
<p>A print example from the python subscriber is like this:</p>
<pre><code>Connected to MQTT Broker!
Received `44` from `usens_l` topic (timeDiff: 678024
Received `25` from `usens_f` topic (timeDiff: 882899
Received `44` from `usens_l` topic (timeDiff: 87380
Received `49` from `usens_l` topic (timeDiff: 274183
Received `44` from `usens_l` topic (timeDiff: 501763
Received `44` from `usens_l` topic (timeDiff: 702241
Received `44` from `usens_l` topic (timeDiff: 911118
Received `44` from `usens_l` topic (timeDiff: 113206
Received `44` from `usens_l` topic (timeDiff: 316174
Received `45` from `usens_l` topic (timeDiff: 521477
Received `45` from `usens_l` topic (timeDiff: 725778
Received `45` from `usens_l` topic (timeDiff: 930363
Received `-1` from `gyro` topic (timeDiff: 135167
Received `45` from `usens_l` topic (timeDiff: 354054
Received `54` from `usens_l` topic (timeDiff: 647140
Received `42` from `usens_r` topic (timeDiff: 647306
Received `41` from `usens_r` topic (timeDiff: 852423
Received `55` from `usens_l` topic (timeDiff: 58828
Received `55` from `usens_l` topic (timeDiff: 261107
Received `55` from `usens_l` topic (timeDiff: 465823
Received `55` from `usens_l` topic (timeDiff: 671458
</code></pre>
<p>So obviously some sensor data are not sent (or not received).
I tried with different intervals, but in any case it seems
that usens_l is receveid much more frequent than the other data.</p>
<p>There could be a more simple way to send the data synchronously
but building a string or calculate to a single value to integrate
the data and send at once. Maybe this is the solution for
my current issue. But I would like to understand, why this
communication does not work as expected.</p>
<p>Thank you for some hints!</p>
",44692.47431,,77,1,0,0,,4074395,,41906.46875,52,72203580,"<p>O.k., I found the solution, it was far more easy than expected:</p>
<p>I just had to install a local mosquitto server (as ubuntu service), now the  log looks like this:</p>
<pre><code>Received `49` from `usens_l` topic (timeDiff: 305325
Received `82` from `usens_r` topic (timeDiff: 305449
Received `16` from `usens_f` topic (timeDiff: 307354
Received `-1` from `gyro` topic (timeDiff: 309461
Received `49` from `usens_l` topic (timeDiff: 334474
Received `82` from `usens_r` topic (timeDiff: 337135
Received `16` from `usens_f` topic (timeDiff: 337257
Received `-1` from `gyro` topic (timeDiff: 339283
</code></pre>
",4074395,0,0,,,Incoming
470,4414,73821190,What are the key differences between jogging the robot in world mode and joint mode for both ABB and Fanuc robots?,|automation|robotics|,"<p>ROBOTIC MOVEMENT</p>
<p>Jogging of robots</p>
<p>What are the key differences between jogging the robot in world mode and joint mode for both ABB and Fanuc robots?</p>
",44826.92778,,292,0,2,0,,15225039,"Thiruvalla, Kerala, India",44244.18125,5,,,,,,130568205,Maybe the Robotics Community is a better place to ask: https://robotics.stackexchange.com/,Actuator
471,4603,76351277,Trying to make a turtlebot circle another moving turtlebot in ROS,|python|geometry|ros|robotics|,"<p>I am trying to make one turtlebot named turtle2 to circle around a stationary turtlebot and then have another turtlebot turtle3 circle around turtle 2. Turtle2 circles around turtle1 with a radius of 2 and an angular velocity of 1 and turtle3 should circle around turtle2 with a radius of 0.5 and an angular velocity of 2. However I cannot figure out how to make turtle3 circle around turtle2 while also closing the distance caused by turtle2's movement. Here's my ROS code and a screenshot of what I get with my results to help you understand my goal. Any help would be appreciated.</p>
<pre><code>#!/usr/bin/env python  
import roslib
roslib.load_manifest('learning_tf')
import rospy
import math
import tf
import geometry_msgs.msg
import turtlesim.srv

if __name__ == '__main__':
    rospy.init_node('turtle_tf_listener')

    listener = tf.TransformListener()

    rospy.wait_for_service('spawn')
    spawner = rospy.ServiceProxy('spawn', turtlesim.srv.Spawn)

    # wait for the world to spawn
    rospy.sleep(0.3)

    (trans, rot) = listener.lookupTransform('/world', '/turtle1', rospy.Time(0))
    r2 = 2
    w2 = 1
    # spawn second turtle along the orbit of the first still turtle
    spawner(trans[0], trans[1] - r2, 0, 'turtle2')
    turtle_vel2 = rospy.Publisher('turtle2/cmd_vel', geometry_msgs.msg.Twist,queue_size=1)

    # wait for second turtle to spawn
    rospy.sleep(0.3)
    (trans, rot) = listener.lookupTransform('/world', '/turtle2', rospy.Time(0))
    r3 = 0.5
    w3 = 2
    # spawn third turtle along the orbit of the second turtle
    spawner(trans[0], trans[1] - r3, 0, 'turtle3')
    turtle_vel3 = rospy.Publisher('turtle3/cmd_vel', geometry_msgs.msg.Twist,queue_size=1)

    rate = rospy.Rate(10.0)
    while not rospy.is_shutdown():
        flag2 = True
        flag3 = True
        try:
            (trans, rot) = listener.lookupTransform('/world', '/turtle1', rospy.Time(0))
        except (tf.LookupException, tf.ConnectivityException, tf.ExtrapolationException):
            flag2 = False

        if(flag2):
            angular2 = w2
            linear2 = w2 * r2
            cmd2 = geometry_msgs.msg.Twist()
            cmd2.linear.x = linear2
            cmd2.angular.z = angular2
            turtle_vel2.publish(cmd2)
        
        try:
            (trans, rot) = listener.lookupTransform('/turtle3', '/turtle2', rospy.Time(0))
        except (tf.LookupException, tf.ConnectivityException, tf.ExtrapolationException):
            flag3 = False

        if(flag3):
            angular3 = w3
            linear3 = w3 * r3
            cmd3 = geometry_msgs.msg.Twist()
            cmd3.linear.x = linear3
            cmd3.angular.z = angular3
            turtle_vel3.publish(cmd3)

        rate.sleep()
</code></pre>
<p><a href=""https://i.stack.imgur.com/tccBV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tccBV.png"" alt=""enter image description here"" /></a></p>
",45074.48194,77151076,106,1,0,0,,12607149,Greece,43826.46597,16,77151076,"<p>That's how I would approach it:</p>
<ol>
<li>Get turtle2 position let's call it Pos2 which is vector (x2, y2)</li>
<li>Make a function which generates point which is circling around (0, 0) so it's something like SpinnerPos = (cx, cy) = DistanceToCenter * (cos(angle), sin(angle)) where angle is incremented every frame so the point (aka vector) would spin.
<a href=""https://i.stack.imgur.com/GQz3a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GQz3a.png"" alt=""enter image description here"" /></a></li>
<li>Combine both: Target = Pos2 + SpinnerPos. Now if drawn Target point would perfectly spin around turtle2</li>
<li>Now make turtle3 follow Target point by calculating distance to it (Pythagorean theorem) and adjusting it's orientation (trigonometry atan2()) and speed proportional to distance</li>
<li>After tuning parameters turtle3 should circle turtle2</li>
</ol>
",5031366,0,0,,,Remote
472,4700,77419699,B&R Automation Studio - Breakdown of cyclic network communication error,|automation|embedded|robotics|ansi-c|br-automation-studio|,"<p>I get this error on a B&amp;R controller computer that I use to control a 6-axis stewart platform, but we have not been able to figure out the exact cause with B&amp;R support. This problem comes on occasionally but goes away when I reboot the system and then comes back at random times. Can anyone help me if they have experienced this before?</p>
<p><img src=""https://i.stack.imgur.com/xKQ8f.jpg"" alt=""enter image description here"" /></p>
<p>Here it is controller link <a href=""https://www.br-automation.com/tr/ueruenler/enduestriyel-pcler/automation-pc-3100/system-units/5apc3100kbu0-000/"" rel=""nofollow noreferrer"">Controller</a></p>
<p>I thought it was related to the tasks I was using exceeding their time, so I monitored how much the controller was using the processor and it maxes out at 60% utilization. I also changed the connection cables thinking that there might be a bug with the powerlink communication but this did not solve the problem. This error does not happen after a certain process, so we cannot get the error by following certain steps in the system. Or we haven't discovered it yet, but for now it seems to us that it happens at random moments.</p>
",45233.87778,,116,0,2,1,,18771823,,44662.51875,12,,,,,,136515991,"The ethernet cable connected to the panel I use is connected to an ethernet switch port and different ethernet cables also enter this port, including the communication of the system connected to our stewart platform. I will check this, so I will try to arrange a cable that is more resistant to electromagnetic noise and protected.",Remote
473,4587,76247799,Path planning using an occupancy grid with specific constrains,|algorithm|graph-theory|robotics|,"<p>I have a binary occupancy grid representing an environment where zeros denote empty spaces and ones denote walls. I need to plan a path for a differential wheeled robot with a cylindrical shape. However, I have specific constraints that I am struggling to meet.</p>
<p>My objective is to generate a path that allows the robot to follow walls while maintaining a certain safe distance from the wall to avoid damaging it, additionally, I would like the robot to maintain a given maximum distance from the walls whenever possible. As an example in the distance trasform grid below I want a path that spans between ~15 and ~20, so when possible the path should go below the given maximum distance in order to cover certain areas of the map.</p>
<p>Could someone suggest an approach or algorithm that can help me achieve my desired path while considering these constraints? Any guidance or code examples would be greatly appreciated. Thank you!</p>
<p>Occupancy Grid:
<img src=""https://i.stack.imgur.com/bxKHP.png"" alt=""Occupancy Grid"" /></p>
<p>I have already attempted to generate a path by applying a distance transform to the grid and using it as a basis for planning. However, the resulting path is not perfect and exhibits some issues.</p>
<p>Distance Transform:
<img src=""https://i.stack.imgur.com/0kt3y.png"" alt=""Distance Transform"" /></p>
<p>Obtained Path:
<img src=""https://i.stack.imgur.com/RoKPr.png"" alt=""Obtained Path"" /></p>
<p>Desired path:
<img src=""https://i.stack.imgur.com/YlWYz.png"" alt=""Desired path"" /></p>
",45060.57292,,222,1,0,2,,21896451,,45060.50486,3,76248835,"<ul>
<li>Set R to the spacing you want between the robot and the wall.</li>
<li>Construct a rectangular grid of cells with spacing between the cells of R</li>
<li>Mark all cells the do not contain a wall and are less the 2 * R from a wall.</li>
<li>Create a graph with a vertex located at each marked cell and an edge between every adjacent vertex.</li>
<li>Run route planning code to generate a path that visits every vertex in graph.</li>
</ul>
<p>The last step is the tricky one!  For that, may I suggest my application called &quot;Obstacle&quot; which produces this sort of result:</p>
<p><a href=""https://i.stack.imgur.com/U5ZnW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/U5ZnW.png"" alt=""enter image description here"" /></a></p>
<p>Obstacle documentation and code:  <a href=""https://github.com/JamesBremner/PathFinder/wiki/Obstacles"" rel=""nofollow noreferrer"">https://github.com/JamesBremner/PathFinder/wiki/Obstacles</a></p>
",16582,0,0,,,Moving
474,4564,75961498,Creating a Ros2 subscriber within a Nav2 Costmap Layer,|c++|robotics|ros2|,"<p>I am currently writing my own custom costmap plugin for use with the navigation2 stack. I'm trying to have the costmap layer take the navigation goal into account when updating costs near it. I've been trying to do this by adding a subscriber to the custom costmap layer that will pull the position of the navigation goal when it is set.</p>
<p>However, I haven't found a good way to do this.</p>
<p>I've tried to use the existing Lifecycle node within the nav2_costmap_2d::Layer class, but no matter what I do, I simply cannot get the data types to match. When I first tried to create the subscriber within the <code>onInitialize</code> method I wasn't able to build the plugin due to colcon complaining that it could not find the subscriber (error looked like this):</p>
<pre><code>error: no matching function for call to ‘std::function&lt;void(const geometry_msgs::msg::Pose2D_&lt;std::allocator&lt;void&gt; &gt;&amp;)&gt;::function(std::_Bind&lt;void (jackstand_objective_costmap_plugin::JSLayer::*(std::shared_ptr&lt;rclcpp_lifecycle::LifecycleNode&gt;, std::_Placeholder&lt;1&gt;))(const geometry_msgs::msg::Pose2D_&lt;std::allocator&lt;void&gt; &gt;&amp;)&gt;&amp;)’
  394 |       callback_variant_ = static_cast&lt;typename scbth::callback_type&gt;(callback);
</code></pre>
<p>I figured this was because the Lifecycle node needed to be in the process of being configured to create a subscriber. So the next thing I attempted to do was define an <code>on_configure</code> callback to the node within the Layer class. I attempted to do this via the <code>register_on_configure</code> method. I could not get my types to match though. When I made a <code>func</code> function the method always complained that it needed a type <code>std::function&lt;func&gt;</code>.</p>
<p>I tried making a separate regular node that I could set as a shared pointer within my costmap layer which also didn't work. This was due to the plugin not even recognizing the node I had made within my code.</p>
<p>Is there a way to add a subscriber to a custom costmap layer plugin? If so, would anyone please tell me how?</p>
",45023.82153,,263,0,0,0,,21591458,,45023.80347,8,,,,,,,,Moving
475,4509,75446976,How to measure the distance for robot center to obstacles?,|matlab|orientation|robotics|,"<p>I want to orient my robot based on the distance from the obstacle, but I do not know how to measure the distance from robot center to the obstacle surrounding.</p>
<p><a href=""https://i.stack.imgur.com/QGnPx.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QGnPx.jpg"" alt=""enter image description here"" /></a></p>
",44971.46597,75532145,115,2,3,-1,,18055837,,44589.34236,8,75448750,"<p>I think it might depend on how your robot moves. For instance, if your robot navigation algorithm guides the robot to move one direction at a time, (x then y, or y then x, along axes in XoY space), the <a href=""https://www.wikiwand.com/en/Taxicab_geometry"" rel=""nofollow noreferrer"">Manhattan distance</a> would be more appropriate, as shown in the picture below. <a href=""https://i.stack.imgur.com/vAaBh.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vAaBh.jpg"" alt=""Manhattan distance"" /></a> The <strong>Yellow</strong> and <strong>Blue</strong> distances are the same, while the <strong>Green</strong> distance is the Euclidean distance that will be described below.</p>
<p>However, most modern robots may move in any direction rather than one at a time. But from point A to point B the straight line is the obvious best solution to save a great effort. Then, the <a href=""https://www.wikiwand.com/en/Euclidean_distance"" rel=""nofollow noreferrer"">Euclidean distance</a> should be chosen, as shown below.<a href=""https://i.stack.imgur.com/FUX5a.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FUX5a.jpg"" alt=""enter image description here"" /></a></p>
",1300650,-1,0,133120731,"what do you mean ""best method"". Its a distance, its an euclidean norm. The distance between a point and e.g. a point or line is basic math, not a ""method"" among others.",Moving
476,4713,77522536,"I was learning ROS2NAV2, following the documentations, when I launched the file, The ODOM TF started moving itself in the space",|navigation|ros|robotics|slam|rviz|,"<p>I am learning ROS2 NAV2, created a URDF file, it runs perfectly in Gazebo...But after 20-25 seconds the ODOM Tf automatically starts moving upwards or sometimes goes down it the grid, For the code, I have literally copy pasted from the documentations twice, getting the same issue. Please do help if you have the solution.
<a href=""https://i.stack.imgur.com/odvix.png"" rel=""nofollow noreferrer"">ODOM TF moves away from the robot automatically </a>
<a href=""https://i.stack.imgur.com/zPamx.png"" rel=""nofollow noreferrer"">Rviz shows no error</a></p>
<p>I tried to go through the documentations and copy paste my code from it. But tbh I feel there is some issue in the code itself...</p>
",45251.48819,,16,0,1,0,,21399590,,44999.90556,1,,,,,,136687839,Please provide enough code so others can better understand or reproduce the problem.,Moving
477,4633,76765373,"If I run on Raspberry Pi some Python commands from the shell they are executed, but not from scripts",|python|shell|raspberry-pi|robot|gpiozero|,"<p>I'm trying to control a robot via a Raspberry Pi, using Python language.
If I run the commands from the shell they work fine. If I call them from a *.py script the robot doesn't move.
If in the script I put the command
print('hello') the word 'hello' is displayed but the motors still do not move.
How can I solve the problem?
Thank you</p>
<p><a href=""https://i.stack.imgur.com/sBL6A.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sBL6A.jpg"" alt=""enter image description here"" /></a></p>
<pre><code>from gpiozero import Motor

motor1 = Motor(4, 14)
motor2 = Motor(17, 27)

motor1.forward()
motor2.forward()
</code></pre>
<blockquote>
<p>UPDATE: I solved the problem with sleep(5) after the command for move motors. Thanks!!!</p>
</blockquote>
",45132.74583,,65,1,7,1,,2203619,ITALY,41357.05694,12,76766288,"<p>did you check this one, maybe the problem is related to the library <a href=""https://stackoverflow.com/a/72207592/11594679"">https://stackoverflow.com/a/72207592/11594679</a></p>
<p>alternatively, did you check, if the path of the library is a problem. Do you call the script with the same python version, that you execute as shell? Maybe the gpio library is not installed for the one you call the script with.</p>
",11594679,0,3,135334409,"If I run the ""python"" command and then manually run (example: >>> motor1.forward()) every single command works fine.",Actuator
478,4557,75773727,How can I use HAL3/Camera2 API alongwith OpenCV to capture the video from two MIPI cameras?,|python|opencv|computer-vision|gstreamer|robotics|,"<p>I am using Qualcomm rb5 development kit along with two MIPI camera's OV9282. Somehow I am not able to use Gstreamer with OpenCV to access these stereo cameras. Does anyone knows how to use HAL3 +OpenCV? There aren't basic tutorials on that. I am stuck with this problem. Please help me.</p>
<p>I have tried using Gstreamer pipeline to access those cameras using below code.</p>
<pre class=""lang-py prettyprint-override""><code>import cv2
from threading import Thread
from time import sleep
import gi

gi.require_version(&quot;Gst&quot;, &quot;1.0&quot;)
from gi.repository import Gst, GLib

Gst.init(None)

main_loop = GLib.MainLoop()
thread = Thread(target=main_loop.run)
thread.start()

pipeline_str = &quot;&quot;&quot;
    qtiqmmfsrc camera=1 ! video/x-raw, format=NV12, width=1280, height=720, framerate=15/1 ! videoconvert ! waylandsink 
&quot;&quot;&quot;
pipeline = Gst.parse_launch(pipeline_str)
pipeline.set_state(Gst.State.PLAYING)

pipeline_str2 = &quot;&quot;&quot;
    qtiqmmfsrc camera=3 ! video/x-raw, format=NV12, width=1280, height=720, framerate=15/1 ! videoconvert ! waylandsink
&quot;&quot;&quot;
pipeline2 = Gst.parse_launch(pipeline_str2)
pipeline2.set_state(Gst.State.PLAYING)

cap = cv2.VideoCapture(pipeline_str, cv2.CAP_GSTREAMER)
cap2 = cv2.VideoCapture(pipeline_str2, cv2.CAP_GSTREAMER)

num = 0

while True:
    succes1, img = cap.read()
    succes2, img2 = cap2.read()

    if not succes1 or not succes2:
        break

    cv2.imshow('Img 1', img)
    cv2.imshow('Img 2', img2)

    k = cv2.waitKey(5)

    if k == 27:
        break
    elif k == ord('s'):
        cv2.imwrite('images/stereoLeft/imageL{}.png'.format(num), img)
        cv2.imwrite('images/stereoRight/imageR{}.png'.format(num), img2)
        print('images saved!')
        num += 1

cap.release()
cap2.release()
cv2.destroyAllWindows()

pipeline.set_state(Gst.State.NULL)
pipeline2.set_state(Gst.State.NULL)
main_loop.quit()
</code></pre>
<p>It is only displaying one camera and that camera stream is not getting fed into the rest of the code with VideoCapture function. I don't know what is wrong with it. Hence, I am trying to see if there is other ways to access camera's using HAL3/Camera2 APIs.</p>
",45003.14861,,208,1,0,0,,21416083,,45002.08958,11,75775196,"<p>There are 2 problems with your code:</p>
<ul>
<li>For using a gstreamer pipeline as capture from opencv, the pipeline would have to end with appsink (the opencv application).</li>
<li>Opencv videocapture will manage the pipeline creation and change states.</li>
</ul>
<p>First try:</p>
<pre><code>import cv2
from time import sleep

pipeline_str = &quot;&quot;&quot;
    videotestsrc ! video/x-raw, format=NV12, width=1280, height=720, framerate=15/1 ! videoconvert ! video/x-raw,format=BGR ! appsink drop=1
&quot;&quot;&quot;
cap = cv2.VideoCapture(pipeline_str, cv2.CAP_GSTREAMER)
if not cap.isOpened():
    print('Failed to open capture')
    exit(-1)
    
    
pipeline_str2 = &quot;&quot;&quot;
    videotestsrc pattern=ball ! video/x-raw, format=NV12, width=1280, height=720, framerate=15/1 ! videoconvert ! video/x-raw,format=BGR ! appsink drop=1
&quot;&quot;&quot; 
cap2 = cv2.VideoCapture(pipeline_str2, cv2.CAP_GSTREAMER)
if not cap2.isOpened():
    print('Failed to open capture2')
    exit(-2)

num = 0

while True:
    succes1, img = cap.read()
    succes2, img2 = cap2.read()

    if not succes1 or not succes2:
        break

    cv2.imshow('Img 1', img)
    cv2.imshow('Img 2', img2)

    k = cv2.waitKey(5)

    if k == 27:
        break
    elif k == ord('s'):
        cv2.imwrite('images/stereoLeft/imageL{}.png'.format(num), img)
        cv2.imwrite('images/stereoRight/imageR{}.png'.format(num), img2)
        print('images saved!')
        num += 1

cap.release()
cap2.release()
cv2.destroyAllWindows()
</code></pre>
<p>If it works, change the sources with your cameras.</p>
<p><strong>EDIT:</strong>
It is unclear what is your gstreamer stack with only camera source and waylandsink. If you don't have gstreamer appsink, you may not be able to use opencv VideoCapture that relies on that.</p>
<p>You may however try in such case to add a src pad probe to camera plugin. From the callback, you may be able to get buffer data and put it into an opencv mat or numpy array for further processing. You may push these into fifos or circular buffers to be picked up by the application, checking PTS for synchronization between cameras:</p>
<p><strong>Callback function:</strong></p>
<pre><code>def probe_callback(pad,info): 
    gst_buffer = info.get_buffer()
    print(' PTS:', gst_buffer.pts)
    ret, mapinfo = gst_buffer.map(Gst.MapFlags.READ)
    if ret:        
        #read the mapped buffer into np frame
        frameNV12 = np.frombuffer(mapinfo.data, dtype=np.uint8)
        frameNV12.shape = ((int)(720*3/2), 1280, 1)
        
        # Convert NV12 into BGR 
        frameBGR = cv2.cvtColor(frameNV12, cv2.COLOR_YUV2BGR_NV12)
                
        # Here you would push a copy of the frame into a queue or circular buffer for application
        # You may attach the pts for checking synchronization between various cameras 
        
        # Or write to disk for checking last image...Note that it may be slow depending on your hw
        #cv2.imwrite(&quot;test.png&quot;, frameBGR)
        
        del frameBGR
        del frameNV12
        gst_buffer.unmap(mapinfo)
    else:
        print('ERROR: Failed to map buffer')
        
    return Gst.PadProbeReturn.OK
</code></pre>
<p><strong>Before starting pipeline, add the probe to src pad of camera source plugin:</strong></p>
<pre><code>srcpad = source.get_static_pad('src')
probeID = srcpad.add_probe(Gst.PadProbeType.BUFFER, probe_callback)
</code></pre>
",5025035,1,6,,,Incoming
479,4581,76164592,DJI RS 2: Calculate Attitude from Joint Angles,|python|numpy|robotics|,"<p>At the moment I'm working on the DJI Ronin RS 2 gimbal. The configuration is shown in the image. (<a href=""https://i.stack.imgur.com/iwrme.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/iwrme.png</a>) There is a camera mounted on the gimbal. The red arrows mark the axes of the gimbal. The joint angles alpha, beta and gamma are measured and therefore known. The green coordinate system is the fixed reference system.</p>
<p>So much for the setup. Now to the complicated part: How can I combine the angles alpha, beta and gamma to calculate one yaw angle (around green z-axis), one roll angle (around green x-axis) and one pitch angle (around green z-axis) in the reference system, to let me know where the camera is pointing to?</p>
<p>I know that I could also get the attitude-information directly from the gimbal by sending another &quot;obtain&quot;-command. However, I do need the joint angles and the attitude angles. Therefore I thought it would be easier to calculate the attitude angles from the joint angles.</p>
<p>Any help is highly appreciated! Thank you.</p>
<p>So far, I set up different rotation matrices (basic rotations around x-, y-, z-axis combined by multiplication) and tried to calculate the attitude with the following formulas:</p>
<pre><code>def Rx(a):
    # Transformation to radiant
    a = np.radians(a)
    Rx = [[1, 0, 0],
          [0, np.cos(a), np.sin(a)],
          [0, -np.sin(a), np.cos(a)]]
    Rx = np.round(Rx, decimals = 3)
    return Rx

def Ry(a):
    # Transformation to radiant
    a = np.radians(a)
    Ry = [[np.cos(a), 0, -np.sin(a)],
          [0, 1, 0],
          [np.sin(a), 0, np.cos(a)]]
    Ry = np.round(Ry, decimals = 3)
    return Ry

def Rz(a):
    # Transformation to radiant
    a = np.radians(a)
    Rz = [[np.cos(a), np.sin(a), 0],
          [-np.sin(a), np.cos(a), 0],
          [0, 0, 1]]
    Rz = np.round(Rz, decimals = 3)
    return Rz

# Rotation around z-axis
R10 = Rz(alpha)

# Rotation around y-axis
R21 = Ry(55)

# Rotation around x-axis
R32 = Rx(beta)

# Rotation around y-axis
R43 = Ry(gamma)

R20 = np.matmul(R21, R10)
R30 = np.matmul(R32, R20)
R = np.matmul (R43, R30)

pitch = -np.arcsin(R[2,0])
roll = np.arctan2(R[2,1]/np.cos(pitch), R[2,2]/np.cos(pitch))
yaw = np.arctan2(R[1,0]/np.cos(pitch), R[0,0]/np.cos(pitch))`
</code></pre>
<p>Unfortunately, this hasn't led to the right result yet. I guess that these formulas only apply to rotation around x-, y-, and z-axis in a certain order. This gimbal uses another rotation axis due to the joint in the middle.</p>
",45049.57153,,35,0,1,1,,21805961,,45049.53264,4,,,,,,134318799,"How do you define pitch, roll and yaw? I would expect pitch to be about the X axis, roll about the Y and yaw about the Z, but you don't seem to do this. So can you specify what you're using? Also, you mention alpha, beta and gamma but don't actually use them in your code. I assume you used them to calculate R. How?",Coordinates
480,4703,77446977,Stop Python function running on a Raspberry Pi from a different Pi using TCP/IP connection,|python|tcp|raspberry-pi|robotics|,"<p>I am programming a robot that has a Raspberry Pi as the microcontroller using Python. The robot has multiple motors and sensors. The robot is controlled using a different Raspberry Pi that sends commands through TCP/IP connection using an ethernet cable (I will refer to this Raspberry Pi as the controller and the robot Raspberry Pi as the Robot). The controller is running a Tkinter UI that has buttons programed to send a message to the robot once they are pressed.</p>
<p>When the robot receives a message from the controller, it processes the message to see what is contained in it, and depending on the content of it, it runs a specific function on the robot program.</p>
<p>The comunications between the robot and controller work fine but the problem I am having now is that once the robot has received a message from the controller, and its running a function, there is no way of stopping the function running on the robot from the controller until it has finished or the power is turned off.</p>
<p>The code running on the robot that receives and processes the mesagges is the following:</p>
<pre><code>SERVER = '192.168.0.60'     # This is the server, CPU1 - this one!
PORT = 22001
ADDR = (SERVER, PORT)
    
# Create socket for the Control Client, Pi4(2) client and Server to use...
s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)  # Using TCP.
s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)

# Manage any binding errors...
try:
    s.bind(ADDR)
except socket.error:
    robbie.system_message = &quot;Socket bind failed.&quot;

s.listen(5)
    
robbie.system_message = &quot;Awaiting connection request from TT robot controller...&quot;

while True:
    (conn, addr) = s.accept() # Blocking call; this loop runs for each new client connection.
    connected = True
    robbie.system_message = f&quot;[NEW CONNECTION] {addr} connected. {threading.activeCount()-1}&quot;
        
     # This IP address is hard coded - very fixed indeed as any deviation will render the robot                         useless.

     if '192.168.0.70' in str(conn):
         robbie.cpu_connected = True
            
        filtered_msg = ''    
        
        msg_route_thread = threading.Thread(target = route_messages, args=(conn, addr))
        msg_route_thread.start()
         
        
def route_messages(conn, addr):
    global robbie, connected
    # This method provides message routing
    
    connected = True
    try:
        while connected:
            # Waiting for messages 
            raw_data = conn.recv(256) # custom command messages can be very long
            data = raw_data.decode()
            raw_data = &quot;&quot;
            x = len(raw_data)
            data = data.strip(' ,')
            
            msg_prefix = data[0:4]
            filtered_msg = data[5:]
            
           
        
            # Each of these procedure calls proceses one message. 
            if msg_prefix == &quot;CTRL&quot;:
                robbie.ctrl_latest_message = f&quot;{msg_prefix}~ {filtered_msg}&quot;
                process_ctrl_messages(conn, filtered_msg)
               

def process_ctrl_messages(conn, msg):
    global connected, robbie
    
    robbie.ctrl_latest_message = msg
    
    print(f&quot;CTRL~ ---&gt; {msg}&quot;)
    robbie.current_command = msg
    robbie.ctrl_connected = True
    
    # Few exapmles of messages that the robot receives from the controller
        if 'S-HOME' in msg:
            slide_home()
        elif 'P-HOME' in msg:
            pitch_home()
        elif 'R-HOME' in msg:
            roll_home()            
</code></pre>
<p>The code running on the controller that sends messages to the robot is:</p>
<pre><code># ------------------ Communications setup -------------------------------------------------------------------------------------
HOST = '192.168.0.60'
PORT = 22001

try:
    time.sleep(10)
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    s.connect((HOST, PORT))
    
except socket.error as msg:
    
    print('Check that the robot is powered, is running its own control program, has flashed its lights and is beeping.')
    print(msg)
    exit()
    
print('Controller connected to Tubetech rover.')

# ------------------ Set the various initial states ---------------------------------------------------------------------------
command = ''

# ------------------ Event functions ------------------------------------------------------------------------------------------ (Some examples of the functions that send the mesagges to the robot.)
def lightsToggle():
    command = 'CTRL~L-TOGGLE'
    s.send(command.encode())
    print(command)
    
def forward():
    # Request the rover to move forwards at the speed currently set.
    command = 'CTRL~FORWARD'
    s.send(command.encode())
    #reply = s.recv(32)
    print(command)
</code></pre>
<p>As I mentioned before, once the robot has received a message from the controller and it's running a function, the robot does not receive any message until the function has finished. When I get this problem, I have been able to verify that the messages are being sent from the controller but not received in the robot.</p>
<p>I have tried running a different function just for the stop messages on a thread and changing the route messages function but nothing has worked.</p>
<p>Could anyone please give me some guidance on any way of doing it or help me find if I have done something wrong? Even though the robot works fine, I think being able to stop it at anytime using the controller is something essential but I have been stuck on it for a few weeks.</p>
<p>If you feel like I haven't explained something properly or there is something missing please let me know.</p>
<p>Thank you :)</p>
",45238.65069,,37,0,3,0,,22880720,,45238.59722,0,,,,,,136544759,"I don't get a traceback on the program, it is just that the messages don't get processed on the robot when a function is being run until it has finished. My thought was that as I have the function to read the messages as a thread, I would be able to send messages from the controller to the robot constantly and process them. Also,  I have tried removing the index on the s.listen() but I still have the same problem.",Remote
481,4569,76039877,Reading sensors in real time NAO robot,|c++|robotics|nao-robot|,"<p>I'm working with a real time application with the robot NAO and i need to read the joints sensors fast, the function getData from ALMemory is too slow. I have seen the example with the modue almemoryfastaccess but i have an error when i run the .exe.</p>
<p>I have read about the almemoryfastaccess module but when i try to connect to the sensors with the function ConnectToVariables(getParentBroker(), fSensorKeys, false) i recieved the following error in the function getDataPtr: &quot;uncaught error pointer serialization not implemented&quot;. Does anyone have work with this?. Thanks.</p>
<p>I can't you getData from ALMemory because is too slow for my application.</p>
<p>The code is something like this. It compiles well but when I run the .exe the error appears:</p>
<p><a href=""https://i.stack.imgur.com/lbQ83.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lbQ83.png"" alt=""enter image description here"" /></a></p>
<pre class=""lang-cpp prettyprint-override""><code>#include &lt;almemoryfastaccess/almemoryfastaccess.h&gt;
#include &lt;boost/shared_ptr.hpp&gt;
#include &lt;string&gt;
#include &lt;vector&gt;
#include &lt;alcommon/albroker.h&gt;
#include &lt;qi/log.hpp&gt;

using namespace AL;
using namespace std;

boost::shared_ptr&lt;AL::ALMemoryFastAccess&gt; fMemoryFastAccess;
vector&lt;string&gt; fSensorKeys;
fSensorKeys.clear();
fSensorKeys.resize(3);
fSensorKeys[0] = &quot;Device/SubDeviceList/HeadPitch/Position/Sensor/Value&quot;;
fSensorKeys[1] = &quot;Device/SubDeviceList/HeadYaw/Position/Sensor/Value&quot;;
fSensorKeys[2] = &quot;Device/SubDeviceList/LAnklePitch/Position/Sensor/Value&quot;;
fMemoryFastAccess-&gt;ConnectToVariables(getParentBroker(), fSensorKeys, false); //in this line the error appears.
</code></pre>
",45033.95833,76385774,94,1,5,-1,,14238185,,44081.99028,7,76385774,"<p>This &quot;fast access&quot; API only works when your module runs in the same process as NAOqi.</p>
<p>When running your code from a remote client (your Windows PC here), it is not possible to achieve real-time data collection.</p>
<p>There are alternatives:</p>
<ul>
<li>cross-compile your module and have it loaded by NAOqi. You'll create a library instead of an executable (see <a href=""http://doc.aldebaran.com/2-5/dev/cpp/examples/core/helloworld/example.html#module-creation"" rel=""nofollow noreferrer"">the use of <code>qi_create_lib</code> here</a>). Sadly I do not know how to find the cross-toolchain. You might need to contact the support.</li>
<li>you can poll the data less frequently (say every 100ms) by using <a href=""http://doc.aldebaran.com/2-5/naoqi/core/almemory-api.html#ALMemoryProxy::getListData__AL::ALValueCR"" rel=""nofollow noreferrer""><code>ALMemory::getListData</code></a></li>
<li>you can poke for the module called <code>ALMemoryWatcher</code> by running <code>qicli info --show-doc ALMemoryWatcher</code> on the robot. It captures real-time data, but you are meant to collect the data by batches at a lower frequency.</li>
</ul>
",3836562,0,1,134115141,"Have a look at this answer, as it show how to get one sensor fast: https://stackoverflow.com/questions/72627941/nao-robot-imu-data-rates/72642451#72642451",Incoming
482,4681,77266977,Check if UR10 (Universal Robot) Finished job successfully,|c#|events|communication|robot|universal|,"<p>I am communicating with UR10 with C# under automation NuGet package.</p>
<p>How can I know if robot finished given job successfully?</p>
<p>we are giving UR10 jobs with scripts like this:</p>
<pre><code>def FullCommand():
    global curent_j_pose = get_actual_joint_positions ()
    global current_b_value = curent_j_pose[0]
     movep(p[.556075771156, -.493695531905, .571111521277, 1.426415630227, .726027077406, .605252480495], a = 1.5, v = 0.6, r = 0.05) ....
end
</code></pre>
<p>For now we are getting this information from OnKeyMessageReceived event from PrimaryInterface</p>
<pre><code>void OnKeyMessageReceived(int? DeviceID, KeyMessageEventArgs E)
{
    if (E.RobotMessageTitle.Contains(&quot;STOPPED&quot;))    
    {
                
    }
}       
</code></pre>
<p>Is this correct way to achieve this ?
Because I think sometimes this event is raised even when job was not finished successfully.</p>
<p>help.</p>
",45209.63194,,22,0,0,0,,9404754,Georgia,43155.28819,5,,,,,,,,Coordinates
483,4697,77416468,Detect whether a vehicle touched a white line drawn on ground in realtime,|python|image-processing|deep-learning|computer-vision|robotics|,"<p>I am currently working on a project where I take real time frames from an IP camera using RTSP protocol and do some processing on it.</p>
<p>The footage will be of vehicles captured from 3 different angles including its top view. I want to find whether the vehicle touches white lines drawn on the ground. I did it with OpenCV using background masking method. It worked for me. But then the shadow of the vehicle started causing problems as long shadows (during morning and evening hours) are detected as foreground crossing the white lines. So then I applied a shadow cancellation algorithm on it. but its too heavy and is causing too much delay so it wont work I real time.</p>
<p>I am planning to use deep learning to do the same now. I trained a resnet18 based model to do the task. but it is not generalizing and is getting overfit. So it is only able to detect those images I trained it. when I implement it, it started to show false results.</p>
<p>I would  like to have help in solving this project. What could the the best way I should have done this. Sorry if it is not the correct place to ask. Also do ask if any more information is needed.</p>
<p>sample image :</p>
<p><a href=""https://i.stack.imgur.com/RytU3.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RytU3.jpg"" alt=""enter image description here"" /></a></p>
<p>Regards.</p>
",45233.49583,,38,0,3,0,,1493259,Trivandrum,41090.6875,24,,,,,,136489037,@ChristophRackwitz I have  added a sample image. I need to fin whether a vehicle crossed the white line.,Incoming
484,4498,75279981,cartographer: Why does the position of radar not change during real-time mapping？,|lua|robotics|slam|,"<p><a href=""https://i.stack.imgur.com/Xa1LC.jpg"" rel=""nofollow noreferrer"">result Map</a></p>
<p>No matter how I move the radar, the position of the radar that rviz shows don't change.</p>
<p>Why is this happening? Do I need to add odom information to it?</p>
<p>I want the radar position on the map to change as the radar position changes</p>
<p><strong>.lua file:</strong></p>
<pre><code>options = {
  map_builder = MAP_BUILDER,
  trajectory_builder = TRAJECTORY_BUILDER,
  map_frame = &quot;map&quot;,
  tracking_frame = &quot;base_link&quot;,
  published_frame = &quot;base_link&quot;,
  odom_frame = &quot;odom&quot;,
  provide_odom_frame = true,
  publish_frame_projected_to_2d = false,
  use_pose_extrapolator = true,
  use_odometry = false,
  use_nav_sat = false,
  use_landmarks = false,
  num_laser_scans = 1,
  num_multi_echo_laser_scans = 0,
  num_subdivisions_per_laser_scan = 1,
  num_point_clouds = 0,
  lookup_transform_timeout_sec = 0.2,
  submap_publish_period_sec = 0.3,
  pose_publish_period_sec = 5e-3,
  trajectory_publish_period_sec = 30e-3,
  rangefinder_sampling_ratio = 1.,
  odometry_sampling_ratio = 1.,
  fixed_frame_pose_sampling_ratio = 1.,
  imu_sampling_ratio = 1.,
  landmarks_sampling_ratio = 1.,
}

MAP_BUILDER.use_trajectory_builder_2d = true
TRAJECTORY_BUILDER_2D.submaps.num_range_data = 30
TRAJECTORY_BUILDER_2D.use_imu_data = false
TRAJECTORY_BUILDER_2D.num_accumulated_range_data = 1
POSE_GRAPH.optimization_problem.huber_scale = 1e2
POSE_GRAPH.constraint_builder.min_score = 0.65
</code></pre>
<p><strong>.launch file:</strong></p>
<pre><code>&lt;launch&gt;
&lt;param name=&quot;robot_description&quot;
  textfile=&quot;$(find cartographer_ros)/urdf/car.urdf&quot; /&gt;
&lt;param name=&quot;/use_sim_time&quot; value=&quot;false&quot; /&gt;
&lt;node name=&quot;robot_state_publisher&quot; pkg=&quot;robot_state_publisher&quot;
  type=&quot;robot_state_publisher&quot; /&gt;
&lt;node name=&quot;cartographer_node&quot; pkg=&quot;cartographer_ros&quot;
  type=&quot;cartographer_node&quot; args=&quot;
  -configuration_directory $(find cartographer_ros)/configuration_files
  -configuration_basename myLds.lua&quot;
  output=&quot;screen&quot;&gt;
&lt;remap from=&quot;scan&quot; to=&quot;/scangkw&quot; /&gt;
&lt;/node&gt;
&lt;node name=&quot;cartographer_occupancy_grid_node&quot; pkg=&quot;cartographer_ros&quot;
  type=&quot;cartographer_occupancy_grid_node&quot; args=&quot;-resolution 0.05&quot; /&gt;
&lt;/launch&gt;
</code></pre>
",44956.13819,75385722,105,1,2,-2,,17206746,,44490.07222,5,75385722,"<p>Check the frame that your radar is publishing data on. Is the frame the same as base_link? If not, set the frame name that the radar is providing to be equal to the tracking_frame in the .lua configuration file.</p>
<p>If you do not want to do this and keep the original configuration, provide a static_transform_publisher which provides the transform between the radar frame and the base_link.</p>
<p>What may be happening here is that your radar is providing data on a different frame, while your cartographer is set up to accept data with position at the base_link frame.</p>
",16782709,0,2,132841853,"In addition, remove non-related flag such as C and python please. You should put the lua flag instead.",Moving
485,4577,76085799,Pepper emulator not start because INSTALL_FAILED_INSUFFICIENT_STORAGE,|android-studio|kotlin|robotics|pepper|,"<p>I can't run my app by Pepper's emulator because from the log it tells me that there is not sufficient space in the emulator.</p>
<p><code>Installation did not succeed. The application could not be installed: INSTALL_FAILED_INSUFFICIENT_STORAGE  List of apks: \[0\] 'C:\\Users\\aless\\AndroidStudioProjects\\TestPepper\\app\\build\\intermediates\\apk\\debug\\app-debug.apk' The device needs more free storage to install the application (extra space is needed in addition to APK size). Retry Failed to launch an application on all devices</code></p>
<p>I have already reset the emulator to factory data but it keeps telling me there is no space.</p>
<p>How can i increase the space since in the Device Manager it doesn't show me the emulator?</p>
<p>Thank you very much</p>
<p>(<a href=""https://i.stack.imgur.com/XLuro.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/XLuro.png</a>)</p>
<p>(<a href=""https://i.stack.imgur.com/8MmCH.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/8MmCH.png</a>)</p>
<p>(<a href=""https://i.stack.imgur.com/r1k2V.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/r1k2V.png</a>)</p>
<p>(<a href=""https://i.stack.imgur.com/tifsB.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/tifsB.png</a>)</p>
",45039.65486,,67,0,1,0,,21714826,,45039.62778,2,,,,,,134184926,"Setup another device and see if it still does it.. I never use emulator, I always use a physical device. For the exact same reason as the issue you are having",Other
486,4733,77678698,Unique Detections from a Camera video feed using Pytorch Faster-RCNN Model,|pytorch|conv-neural-network|robotics|faster-rcnn|,"<p>I have made a custom dataset then used that to fine tune Pytorch faster RCNN model. Now I have a conveyor, on top of which I have placed a camera. I am using the image from that camera and passing it into the model for inference.The model outputs a result dictionary which contains labels, bbox coordinates, score. Since I have a continuous camera feed and objects are coming on the conveyor, I am getting multiple entries for the same object. Say if I have a can on the conveyor then I am getting atleast 10-15 entries till the can goes out of camera’s frame of view.
I want to save the unique detections in an array as I have to use that data further up in the pipeline.</p>
<p>I am at my wits end, I have tried using Bounding Box similarity as a factor to store unique detections, I have tried assigning unique ids to the detections, I have also tried using Bounding Box corner coordinates as a way to filter out unique detections. But all of that fails if I have say 2 cans simultaneously behind each other coming in on the conveyor, then the array only saves that as 1 detection and this messes up the further pipeline.
I would like to know if there is a better way to do this task or if I am doing something wrong. Any help would be much appreciated.</p>
",45278.46806,,10,0,0,0,,23116043,,45277.44028,1,,,,,,,,Incoming
487,4490,75160864,How to project LiDAR points into camera according to literature,|matlab|computer-vision|robotics|matlab-cvst|extrinsic-parameters|,"<p>For my project I was trying 3D LiDAR to Camera projection. I was recommended MATLAB LiDAR-Camera modules for calibration and then use the results for the projection on a live stream of the data.</p>
<p>In MATLAB, I got the rotation matrix ( R ), the translation matrix ( T ) and the camera intrinsic matrix ( M ). When using the MATLAB tools I am getting the results as follows :
<a href=""https://i.stack.imgur.com/Y8npG.png"" rel=""nofollow noreferrer"">MATLAB generated projection for the same values of R, T and M</a></p>
<p>But using the literature in which the projection matrix is given as</p>
<p>( M 0 )x( [[ R T ], [ 0 1]] ) to get from a [x y z 1] to [u v w], I am getting the following result :</p>
<p><a href=""https://i.stack.imgur.com/Uvs5b.png"" rel=""nofollow noreferrer"">Projection of M, R and T as per the literature </a></p>
<p>M, R,T as calculated from MATLAB</p>
<pre><code>
M = array([[904.4679,   0.    , 596.9176],
       [  0.    , 814.7088, 349.8212],
       [  0.    ,   0.    ,   1.    ]])

R = array([[ 0.124 , -0.0038,  0.9923],
       [-0.9912,  0.0474,  0.124 ],
       [-0.0475, -0.9989,  0.0021]])

T = array([[-0.56  ,  0.241 , -0.4454]])

</code></pre>
<p>A code snippet looks like this</p>
<pre><code>rotation = np.array([[0.1240,-0.0038,0.9923],                                                       [-0.9912,0.0474,0.1240],[-0.0475,-0.9989,0.0021]])
traslation = np.array([[-0.5600,0.2410,-0.4454]])
traslation_1 = np.array([[-0.4454,0.2410,-0.5600]])
intrinsic = np.array([[904.4679,0,596.9176],[0,814.7088,349.8212],[0,0,1]])

a = np.concatenate((rotation,np.array([[0,0,0]])), axis =0)
b = np.concatenate((traslation_1.T, np.array([[1]])), axis =0)
c = np.concatenate((a,b), axis =1)

print('\n Extrinsic:\n \n',c)
d = np.concatenate((intrinsic, np.array([[0,0,0]]).T), axis =1)
print('\n Intrinsic:\n \n',d)
e = np.matmul(d,c)
print(&quot;\n Final:\n \n&quot;, e)
df = pd.read_csv('out_file.csv')

img = cv2.imread('images/0001.png')
v1_max = 0
v2_max = 0
uv = []
import matplotlib.pyplot as plt 
 
for i in range(df.shape[0]):
    point = np.array([[df['x'].iloc[i],df['y'].iloc[i],df['z'].iloc[i],1]]).T
    v = np.matmul(e, point)
    v = v/v[2]
    if v[0]&lt;=720 and v[0] &gt; 0  and v[1] &lt; 1280 and v[1] &gt; 0:
        img[int(np.floor(v[0])),int(np.floor(v[1]))] = [0,255,255] 
            
                      
        
cv2.imwrite('file.png', img) 

</code></pre>
<p>Hardware configuration :</p>
<p>Velodyne 64 Channel LiDAR @ 10 HZ</p>
<p>Camera : 1280*720 monocular camera</p>
<p>Checkerboard : 10*7 with 10 cm of pattern and a padding .</p>
<p>I need some help as to how I cam take the parameters from the MATLAB and do the same in the scripts and get the near results .</p>
<p>I am expecting as to where I am going wrong when projecting the LiDAR points onto the camera, as to when I am using MATLAB functions I am getting a correct projection. Same is not there with the literature.</p>
",44944.60764,,174,0,3,1,,21035048,,44944.44375,6,,,,,,132679767,"Thank you for your mention, this is my first question on the form. I have updated the code.",Incoming
488,4580,76121679,Pybullet - Gripper is not able to Grasp Object,|python|robotics|pybullet|grasp|urdf|,"<p>I am using the Kinova Jaco robotic arm with the j2n6s300.urdf file and I simply want to grasp a mug from the side and live it up.
However, the gripper is not able to pick up the mug. It seems that there is an &quot;invisible&quot; barrier between gripper fingers and mugs. If I lift the gripper the mug stays where it was.</p>
<p>However, if I run the same code using the Franka Panda robot (slight adaptions with joints) I can lift the mug.
What is my problem? Is it related to the urdf file?</p>
<p>This is my code:</p>
<pre><code>import os 
import pybullet as pb
import pybullet_data
import math
import time
import pybullet_data as pd 
jaco_path = 'jaco_reza/j2n6s300.urdf'
pb.connect(pb.GUI)
path = pd.getDataPath()

jaco_orientation_euler = [0,math.pi/2,0]
jaco_orientation_quaternion = pb.getQuaternionFromEuler(jaco_orientation_euler)
jacoUid = pb.loadURDF(os.path.join(jaco_path,), useFixedBase=True, basePosition=[-0.63,0,0.46], baseOrientation=jaco_orientation_quaternion)

pos, ori = pb.getBasePositionAndOrientation(jacoUid)
tableUid = pb.loadURDF(os.path.join(pybullet_data.getDataPath(), &quot;table/table.urdf&quot;),basePosition=[0.5,0,-0.66])

mug_orientation_euler = [0,0,-math.pi/2]
mug_orientation_quaternion = pb.getQuaternionFromEuler(mug_orientation_euler)

mugUid = pb.loadURDF(os.path.join(pybullet_data.getDataPath(), &quot;objects/mug.urdf&quot;),basePosition=[0.148,0,-0.035], baseOrientation=mug_orientation_quaternion)

pb.setGravity(0,0,-10)

pb.resetDebugVisualizerCamera(
    cameraDistance=1.5, 
    cameraYaw=0, 
    cameraPitch=-20, 
    cameraTargetPosition=[-0.15,0,0.1])

# #init Jaco Position
rest_poses = [0,0,0,1.5,1.4,1.5,0,1.5,3,0,0.5,0,0.5,0,0.5]
for i in range(15):
     pb.resetJointState(jacoUid,i, rest_poses[i])


fingerAngle = 0.7 

for i in range(180):
    pb.setJointMotorControl2(jacoUid,9,pb.POSITION_CONTROL,targetPosition=fingerAngle,force=2000)
    pb.setJointMotorControl2(jacoUid,11,pb.POSITION_CONTROL,targetPosition=fingerAngle,force=1000)
    pb.setJointMotorControl2(jacoUid,13,pb.POSITION_CONTROL,targetPosition=fingerAngle,force=1000)
    pb.stepSimulation()
    fingerAngle += 0.1 / 100.
    if fingerAngle &gt; 2:
        fingerAngle = 2 #upper limit
    time.sleep(0.02)

for i in range(20):
    print(pb.getJointState(jacoUid,4)[0])
    pb.setJointMotorControl2(jacoUid, 4,
                                        pb.POSITION_CONTROL,
                                        targetPosition=1)
    pb.stepSimulation()

while True:
    pb.configureDebugVisualizer(pb.COV_ENABLE_SINGLE_STEP_RENDERING) 
    pb.stepSimulation()

</code></pre>
<p>I tried to change the .urdf file of the Kinova robotic but it was not working and I have the feeling that I am missing an important step.</p>
",45043.61111,,116,0,0,1,,21755651,,45043.59583,3,,,,,,,,Actuator
489,4562,75920792,How to do robot lokalization while robot moves between walls and using Lidar and Vusal SLAM?,|localization|ros|robotics|,"<p>have a mobile robot equipped with 2DLidar, Stereo RGBD Camera , IMU Unit and wheels encoders. The robot is driving in a corridors area (means between walls-obstacles). So i Have ROS node that gives me distance from the robot to the obstacles-walls that are detected. ALso in the same node have the angle from the robot to those obstacles -walls. So also I have visual SLAM such as ORB_SLAM2 that gives me robot pose but with the time is not accurate. So I would like to use information from ROS LIDAR node that detect the walls and the distance and angle to the robot to get accurate pose (to say correct the VISUAL SLAM pose. ) I dont wonna do any ICP-scan matching using AMCL or Hector SLAM .</p>
<p>As said i like to use the LIDAR node with walls and distance to get accurate localization .I think the way is to use ROS transformation faction (tf) to correct the pose. But not sure is that right way. Any help?</p>
<p>So I have the following topics that can help me for the lokalization.</p>
<ul>
<li><p>/robot/data/vehicle_state : Current speed and yawrate calculated from
wheel speed.</p>
<p>/robot/data/twist :Twist data created from vehicle speed and IMU
data.</p>
<p>/robot/data/vslam_localization/pose :Output pose from ORB_SLAM</p>
<p>/camera/left/image_raw and /camera/right/image_raw : Images from
camera</p>
<p>/scan : YDLidar scan</p>
<p>/imu/sensor_msgs/Imu : IMU data</p>
</li>
</ul>
<p>And when use EKF then</p>
<ul>
<li><p>/robot/localization/ekf_base/set_pose External command to reset pose
of EKF for base_link</p>
<p>/robot/localization/ekf_odom/set_pose External command to reset pose
of EKF for odom</p>
</li>
</ul>
<p>And the following tf:</p>
<p>tf
map- map frame
odom- odom frame
base_link- vehicle pose (center of driving wheel axis)
slam_base -a frame where ORB_SLAM is working</p>
<p>Any help?</p>
<p>Thanks</p>
",45019.61806,,46,0,1,0,,10319366,Singapore,43348.39375,109,,,,,,133961129,Looks like this is the question I was hinting at in my [last comment here](https://stackoverflow.com/a/75886118/6411540). Looking at your question history it would really help if you could separate all the issues you are having into small concise questions with good formatting. And hint at the new questions instead of [moving the goalpost](https://stackoverflow.com/a/75886118). I'll later write an answer for this as well.,Moving
490,4715,77570751,L298n motor control with Raspberry pi,|python|python-3.x|raspberry-pi3|robotics|motordriver|,"<p>I was trying to implement a single L298n motor driver with a 4WD robot using 2 motors connected to a Raspberry Pi. Now, I can turn on and off the L298n driver (the speed of both the motors remains the same) but cannot set a variable PWM for each of the motors, even if I try to put different speed values they remain the same.</p>
<p>NOTE:
What I am trying to accomplish after getting proper motor control is that I want to control the driver from a virtual joystick.</p>
<p>My code follows:</p>
<pre class=""lang-py prettyprint-override""><code>import RPi.GPIO as GPIO
import time
import math
import smbus
import firebase_admin
from firebase_admin import credentials, db

# Initialize Firebase with the private key
cred = credentials.Certificate(&quot;/home/pi/Desktop/GCC/firebase_credentials.json&quot;)
firebase_admin.initialize_app(cred, {
    'databaseURL': '#################################'
})

# Reference to the Firebase Realtime Database for commands and sensor data
command_ref = db.reference('/command')
sensor_ref = db.reference('/Mobile')

#Board connect
GPIO.setmode(GPIO.BCM)

#forward
in1 = 20
in3 = 5
en1 = 12

#backward
in2 = 21
in4 = 19
en2 = 13

GPIO.setup(in1, GPIO.OUT)
GPIO.setup(in2, GPIO.OUT)
GPIO.setup(in3, GPIO.OUT)
GPIO.setup(in4, GPIO.OUT)
GPIO.setup(en1, GPIO.OUT)
GPIO.setup(en2, GPIO.OUT)

#duty cycles for l298n motor
p1 = GPIO.PWM(en1, 1000)
p2 = GPIO.PWM(en2, 1000)
p1.start(0)
p2.start(0)

motorSpeed1 = 0;
motorSpeed2 = 100;


def main():
    while True:
        data = command_ref.get()
        if data is not None:
            joystick_mode = data.get('joystick_mode', False)
            if joystick_mode:
                joystick_data = db.reference('/Joystick movement').get()
                x = joystick_data.get('xPercent')
                y = joystick_data.get('yPercent')
#                print(&quot;x {:.3f}, y {:.3f}&quot;.format(x,y))
                GPIO.output(in1, GPIO.LOW)
                GPIO.output(in2, GPIO.HIGH)
                GPIO.output(in3, GPIO.LOW)
                GPIO.output(in4, GPIO.HIGH)
                
                p1.ChangeDutyCycle(motorSpeed1)
                p2.ChangeDutyCycle(motorSpeed2)

                
                time.sleep(5)
                GPIO.output(in1, GPIO.LOW)
                GPIO.output(in2, GPIO.LOW)
                GPIO.output(in3, GPIO.LOW)
                GPIO.output(in4, GPIO.LOW)
                
                return

if __name__ == &quot;__main__&quot;:
    try:
        main()
    except KeyboardInterrupt:
        print(&quot;Program stopped&quot;)
    finally:
        GPIO.cleanup()
</code></pre>
<p>I have tried checking on the hardware connections, which seems correct; how do I set the correct PWM?</p>
",45259.47778,,103,0,0,0,,23006805,,45259.45972,1,,,,,,,,Actuator
491,4515,75451788,What are the differences between exec and name in ros2 launch file?,|launch|robotics|ros2|,"<p>I'am a ros2 noob and I would like to understand differences between exec and name in launch file.</p>
<p>I'll give an example to be precise:</p>
<pre><code>&lt;launch&gt;
    &lt;node pkg=&quot;camera&quot; exec=&quot;&quot; name=&quot;&quot;&gt;
&lt;/launch&gt;
</code></pre>
<p>Thanks!</p>
",44971.76319,75541108,233,1,0,0,,15496345,,44282.84931,4,75541108,"<ul>
<li>exec: The filename of the executable</li>
<li>name: The name that <code>ros2 node list</code> shows</li>
</ul>
<p>For example, from the <a href=""https://docs.ros.org/en/foxy/How-To-Guides/Launch-file-different-formats.html"" rel=""nofollow noreferrer"">tutorial comparison in launch files</a>:</p>
<p>There is a line that sets both <code>name</code> and <code>exec</code></p>
<pre class=""lang-xml prettyprint-override""><code>&lt;node pkg=&quot;turtlesim&quot; exec=&quot;turtlesim_node&quot; name=&quot;sim&quot; namespace=&quot;turtlesim2&quot;&gt;
</code></pre>
<p>When I created a package to run that, here's what happens.</p>
<pre class=""lang-bash prettyprint-override""><code>$ ros2 node list
/listener
/mimic
/my/chatter/ns/listener
/my/chatter/ns/talker
/talker
/turtlesim1/sim
/turtlesim2/sim
</code></pre>
<p>You can see how the node name shows up, with an extra caveat - because namespace was also added, it appears as <code>&lt;namespace&gt;/&lt;name&gt;</code>.</p>
<p>Now, for the executable, you can just go see that in your ROS installation directory. For example, on my Linux computer running <code>humble</code>, there's the executable. I added <code>-l</code> to show it's executable.</p>
<pre><code>$ ls -l /opt/ros/humble/lib/turtlesim/turtlesim_node 
-rwxr-xr-x 1 root root 798384 Jan 17 18:00 /opt/ros/humble/lib/turtlesim/turtlesim_node
</code></pre>
<p>An easy way to find out the executables in your package is through tab complete with <code>ros2 pkg executables &lt;package_name&gt;</code></p>
<pre><code>$ ros2 pkg executables turtlesim
turtlesim draw_square
turtlesim mimic
turtlesim turtle_teleop_key
turtlesim turtlesim_node
</code></pre>
<p>The reason you have both is because you may re-use the same executable multiple times in a DDS domain, so the <code>name</code> , or <code>namespace</code>, can be used to differentiate them.</p>
",11032285,0,0,,,Other
492,4668,77153709,Executable in a non-ROS workspace links to an executable in a ROS workspace,|ubuntu|cmake|linker|ros|robotics|,"<p>I'm working in ubuntu 20.04 and have ROS noetic. I have a catkin workspace (catkin_ws), and it has DBow2 in it at <code>/home/glenn/catkin_ws/devel/lib/libDBoW2.so</code>. I also have the library <a href=""https://github.com/UZ-SLAMLab/ORB_SLAM3"" rel=""nofollow noreferrer"">ORB-SLAM3</a> installed on my system. It uses DBoW2 and has it installed in its <code>Thirdparty</code> folder, so after building this package, its location is at <code>/home/glenn/ORB_SLAM3/Thirdparty/DBoW2/lib/libDBoW2.so</code>.</p>
<p>When I try to run any executable from ORB-SLAM3, for example, <code>./Monocular/mono_euroc ../Vocabulary/ORBvoc.txt ./Monocular/EuRoC.yaml ~/Documents/Trial_Data/euroc/V1_01 ./Monocular/EuRoC_TimeStamps/V101.txt dataset-V101_mono</code>, it gives the following error:</p>
<p><code>./Monocular/mono_euroc: symbol lookup error: /home/glenn/ORB_SLAM3/lib/libORB_SLAM3.so: undefined symbol: _ZN5DBoW24FORB1LE</code></p>
<p>When I typed <code>ldd -r /home/glenn/ORB_SLAM3/lib/libORB_SLAM3.so</code>, I get a large output, but the relevant one to my issue is the following line:</p>
<p><code> libDBoW2.so =&gt; /home/glenn/catkin_ws/devel/lib/libDBoW2.so (0x00007fd840758000)</code></p>
<p>which shows it's pointing to the DBoW library from the catkin workspace and not the ORB_SLAM3 workspace. To get around this, I simply comment out <code>source ~/catkin_ws/devel/setup.bash</code> line in my bashrc and source it again, the same line from the same command gets the right output:</p>
<p><code>libDBoW2.so =&gt; /home/glenn/ORB_SLAM3/Thirdparty/DBoW2/lib/libDBoW2.so (0x00007f9136949000)</code></p>
<p>and everything in ORB_SLAM3 works fine again.</p>
<p>I just wanted to know if there was a better way to manage a shared library looking in the wrong place at runtime without having to constantly comment out the line to source my catkin workspace? From ORB_SLAM3's CMakeLists.txt <a href=""https://github.com/UZ-SLAMLab/ORB_SLAM3/blob/master/CMakeLists.txt#L118-L126"" rel=""nofollow noreferrer"">lines 118-126</a>, it seems they do link to the appropriate DBoW library under the Thirdparty folder. I was thinking of possibly setting the RPATH by adding the following lines to the top-level CMakeLists.txt:</p>
<pre><code>set(CMAKE_SKIP_BUILD_RPATH FALSE)

set(CMAKE_BUILD_WITH_INSTALL_RPATH TRUE)
</code></pre>
<p>but that gave significantly more errors for libORB_SLAM3.so. Any advice would be appreciated. Thank you.</p>
",45190.88889,,52,0,0,0,,16484468,,44397.06528,7,,,,,,,,Error
493,4586,76231952,Inverse Rotation and Translation for Pinhole Projection,|python|camera|projection|robotics|perspectivecamera|,"<p>So, I feel really dumb to try to solve this problem for so long. I really need help.</p>
<p>I am trying to project the camera pixel coordinate to the world coordinate using Pinhole perspective projection. But somehow the rays I generate has weird rotation (direction). So the function is:</p>
<pre><code>def pixel_coords_to_world_coords_ray(pixel_coords, camera_matrix, distortion_coeffs, R_world_to_camera, t_world_to_camera, plane_normal_world, plane_distance_world):
    
    pixel_coords_undistorted, newcameramtx = undistort_contour_pixel(pixel_coords, camera_matrix, distortion_coeffs)
    pixel_coords_undistorted = np.squeeze(pixel_coords_undistorted, axis=1)
    pixel_coords_homogeneous = np.hstack((pixel_coords_undistorted, np.ones((len(pixel_coords_undistorted), 1))))
    
    # Invert the camera matrix to get pixel to ray conversion matrix
    camera_matrix_inv = np.linalg.inv(newcameramtx)
    
    # Compute the light ray direction in the camera frame.
    ray_directions_camera = (camera_matrix_inv @ pixel_coords_homogeneous.T)[:3, :]

    # Transform the light ray direction to the world frame
    R_camera_to_world = R_world_to_camera.T
    t_camera_to_world = -R_camera_to_world @ t_world_to_camera
    ray_directions_world = R_camera_to_world @ ray_directions_camera
    
    # Normalize the rays to have unit length
    ray_directions_world /= np.linalg.norm(ray_directions_world, axis=0)
    
    # Compute the intersection between the light rays and the plane
    ray_origin_world = t_camera_to_world.reshape(3)
    world_coords = []
    for ray_direction_world in ray_directions_world.T:
        intersection = ray_plane_intersection(ray_origin_world, ray_direction_world, plane_normal_world, plane_distance_world)
        if intersection is not None:
            world_coords.append(intersection)
    world_coords = np.array(world_coords)

    return world_coords
</code></pre>
<ul>
<li>So I've tried to debug it by setting the input pixel to be (w/2, h/2). It results to the expected value of ray_directions_camera = near [0,0,1]</li>
</ul>
<p>But somehow the rotation I get in the ray's direction in world frame is wrong.</p>
<ul>
<li>I also plot the result, the transformation and rotation of the camera. So the translation and rotation must be right.</li>
</ul>
<p><a href=""https://i.stack.imgur.com/s4hY6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s4hY6.png"" alt=""enter image description here"" /></a></p>
<p>the red line pointing downward at the middle is the resulting ray for the pixel (w/2, h/2)</p>
<p>Am I missing something here? It's been a week I am trying to solve this with many different method. I really need to finish this. Thank you all.</p>
",45057.92917,,43,0,0,0,,7053355,braunschweog,42664.55694,5,,,,,,,,Incoming
494,4750,77923548,Multi layer abstraction design for physical liquid handlers,|architecture|robotics|,"<p>I am attempting to implement a solution for our robotic liquid handlers. The problem is that we have many different vendor liquid handlers in our company and even liquid handlers from the same company can differ. So I want to create a multi layer abstraction to simplify the programming. We cannot use open source projects hence why I need to do this internally. We have also not found any good open source options anyway.</p>
<p>As a high level overview:
You have a liquid handler that picks up tips and performs pipetting operations, this robot also has a plate arm that pickup and move plates around the pipetting deck. Nothing in the physical world is perfect so of course the liquid handler will occasionally run into errors. Also, even if the liquid handler is working perfectly user may incorrectly load something on the deck that could also cause an error. What I want to do is create a multi layer abstraction to help with programming but also to simplify error handling.</p>
<p>Per programming design I want to compose the layers in a way which is as decoupled as possible, and therein lies my problem:</p>
<ol>
<li>My driver layer is for raw command execution. Lots of low level errors can occur here.</li>
<li>My HAL layer is for simplified execution and the for the majority of error handling. Some errors will require user input (a UI dialogue or similar) and a retry (repeat the action or cleanup and then repeat).</li>
<li>My API layer will group HAL devices and commands together to create very complex behaviors.</li>
</ol>
<p>So my question is this: <strong>How can I design a system in such a way that I can handle the UI portion at such a low level without implementing UI components in the HAL or API layer?</strong> According to everything I have read the UI should be implemented in a UI layer or even dependency injection for even more flexibility.</p>
<p>I can add additional information as requested!</p>
<p>Thanks!</p>
",45323.90417,,11,0,0,0,,6231022,,42480.62361,19,,,,,,,,Other
495,4797,78231595,Linear trinagulation yields poor results,|computer-vision|robotics|visual-odometry|,"<p>I am working ona  project regarding visual odometry and I have trouble extracting the 3D coordinates of matched features using the method of linear triangulation. I a using the KITTI dataset and specifically I am using the synced and rectified data. For simplification purposes i have chosen a feature that is detected only in the first and second camera frame. Since the rotation and translation of the camera poses are not given directly i have chosen to use the GPS/IMU rotation and translation matrices and use the relevant transformations to calculate the camera rotation and translation. acoording to equation (8) of the relevant papaer (<a href=""https://www.cvlibs.net/publications/Geiger2013IJRR.pdf"" rel=""nofollow noreferrer"">https://www.cvlibs.net/publications/Geiger2013IJRR.pdf</a>) a 3D point in GPS IMU coordinates (current coordinate system of the GPS/IMU) is projected to a 2D point in the cameras frame as follows :
y = P_rect<em>R_rect</em>Tvelo_cam*Timu_velo, where P_rect is the projection matrix of the camera after rectification. R_rect is the rectifying rotation matrix , Tvelo_cam is the transformation from the velodyne's coordinate frame to the cameras frame and Timu_velo is the transformation from the IMU's coordinate frame to the velodyne's coordinate frame. Note that I have chosen the first pose of the IMU as the origin of my world coordinate frame, meaning at point (0,0,0) the IMU has not rotated or moved.</p>
<p>Assuming that X is computed with respect to the real world coordinate frame, the results dont make sense : Specifically X,Y,Z = array([-10.55149986,   4.61478588,   2.40134485]). By taking into account the configuration of the worlds coordinate frame (x axis forward, y axis left and z axis up) this means that the point was detected behind the IMU (X&lt;0), which is impossible because the IMU is behind the camera. I would appreciate any kind of help, since I am stuck a long time in this.</p>
<pre class=""lang-py prettyprint-override""><code>gray1 = images[0,:,:]

gray2 = images[1,:,:]

kp1 , ds1 = sift.detectAndCompute(gray1, None)

kp2 , ds2 = sift.detectAndCompute(gray2, None)


bf = cv2.BFMatcher()
matches12 = bf.knnMatch(ds1, ds2, k=2)

       
        

ratio_thresh = 0.4
good_matches = []
for m, n in matches12:
    if m.distance &lt; ratio_thresh * n.distance:
        good_matches.append(m)

coordinates_kp1 = []
coordinates_kp2 = []

for match in good_matches:
    # Get the coordinates in the first image
    x1, y1 = kp1[match.queryIdx].pt
    coordinates_kp1.append((x1, y1))
    
    # Get the coordinates in the second image
    x2, y2 = kp2[match.trainIdx].pt
    coordinates_kp2.append((x2, y2))

    
XY1 = coordinates_kp1[440]
XY2 = coordinates_kp2[440]

#%% Slides

K2 = P0@R0@T_VelotoCam@T_IMUtoVelo
Rt1 = Rt_IMU[:2]

tracks = np.vstack((XY1,XY2)).T



A = np.zeros((2 * tracks.shape[1], 4))




P1 = K2@Rt1[0]
P2 = K2@Rt1[1]


A[0,:] = P1[2,:]*tracks[0,0] - P1[0,:]
A[1,:] = P1[2,:]*tracks[1,0] - P1[1,:]
A[2,:] = P2[2,:]*tracks[0,1] - P2[0,:]
A[3,:] = P2[2,:]*tracks[1,1] - P2[1,:]

U, S, Vt = np.linalg.svd(A)

V = np.transpose(Vt)

Xtilde = V[:, -1]

X = Xtilde[0:3] / Xtilde[3]
</code></pre>
",45378.50903,,11,0,0,0,,23839991,,45378.47986,2,,,,,,,,Incoming
496,4773,78127251,trouble with Python package import command in Spyder5,|python|math|anaconda|robotics|,"<p>having the need to use a package not included in the Anaconda repository (<a href=""https://github.com/NxRLab/ModernRobotics"" rel=""nofollow noreferrer"">https://github.com/NxRLab/ModernRobotics</a>), I followed the instructions in the official Anaconda documentation and installed, running the Anaconda command terminal as administrator, pip and then followed the package of interest with the instruction <em>pip install modern-robotics</em>, receiving the positive result of the installation as feedback.
Afterwards I checked the presence of the package with <em>conda list</em> and then, for greater security, from Anaconda Navigator Environment, seeing it regularly installed.</p>
<p>Now, in a python script in Spider5, I entered <em>import modern-robotics as mr</em>, but I get the red error incorrect syntax.</p>
<blockquote>
<p>Invalid sintax pyfLakes E</p>
</blockquote>
<p>From the Code Analyzer report in Spyder IDE, I've</p>
<blockquote>
<p><em>************* Module robotics_test</em>
<em>E0001:syntax-error: 11,14: Parsing failed: 'invalid syntax (&lt;unknown&gt;, line 11)'</em></p>
</blockquote>
<p>But the package is there !
Having suspected that there might be inconsistencies in the updating of the packages as seen by Spider IDE, I closed the entire environment (including Anaconda Navigator) and restarted it, but the problem remains. Needless to say, I also trivially turned the PC off and on again.</p>
<p>I am not a guru of Python or other languages, I use them for scientific needs, but now I don't understand the origin of the error.</p>
",45359.4625,,22,0,0,0,,23559725,,45359.44514,2,,,,,,,,Error
497,4741,77800615,Servo Controller Fail State,|stm32|robotics|encoder|servo|,"<blockquote>
<p>Hi, I am relatively new to programming and the robotics world, so I apologize if this is trivial. I am trying to control a DXW90 Servo using a KY-040 standard Rotary Encoder. The microcontroller I am using is NUCLEO-L476RG.</p>
<p>Below is the current code I have scrapped together from a few tutorials I have watched. While the code returns no errors my Putty won't read anything past &quot;Starting Up&quot; and the encoder dose not control the servo at all. I feel like I need to write a fail statement, but am unsure where to begin.</p>
<p>I appreciate any time or advice anyone can provide me on my code, or on how to get into robotics in general.</p>
</blockquote>
<pre><code>int counter=0;
uint16_t PWMVal =0;
uint8_t dir;
uint8_t MSG[50] = {'\0'};
/* USER CODE END 0 */

/**
* @brief  The application entry point.
* @retval int
*/
int main(void)
{
/* USER CODE BEGIN 1 */

/* USER CODE END 1 */

/* MCU Configuration--------------------------------------------------------*/

/* Reset of all peripherals, Initializes the Flash interface and the Systick. */
HAL_Init();

/* USER CODE BEGIN Init */

/* USER CODE END Init */

/* Configure the system clock */
SystemClock_Config();

/* USER CODE BEGIN SysInit */

/* USER CODE END SysInit */

/* Initialize all configured peripherals */
MX_GPIO_Init();
MX_USART1_UART_Init();
MX_TIM1_Init();
/* USER CODE BEGIN 2 */
sprintf(MSG, &quot;Starting Up&quot;);

HAL_UART_Transmit(&amp;huart1, MSG, sizeof(MSG), 100);
HAL_TIM_PWM_Start(&amp;htim1, TIM_CHANNEL_1);

/* USER CODE END 2 */

/* Infinite loop */
/* USER CODE BEGIN WHILE */
while (1)
{

/* USER CODE END WHILE */

/* USER CODE BEGIN 3 */
  if(HAL_GPIO_ReadPin(GPIOA, GPIO_PIN_1) == GPIO_PIN_RESET) //If the OUTA is RESET
  {
        if(HAL_GPIO_ReadPin(GPIOA, GPIO_PIN_2) == GPIO_PIN_RESET)// If the OUTB is also RESET
      {
while(HAL_GPIO_ReadPin(GPIOA, GPIO_PIN_2) == GPIO_PIN_RESET){}; // Wait for OUTB to go high
        counter++;
        dir = &quot;CW&quot;;
while(HAL_GPIO_ReadPin(GPIOA, GPIO_PIN_1) == GPIO_PIN_RESET){}; // Wait for OUTA to go high
        HAL_Delay(10); //Wait for some time
      }

      if(HAL_GPIO_ReadPin(GPIOA, GPIO_PIN_2) == GPIO_PIN_SET)// If the OUTB is also SET
      {
while(HAL_GPIO_ReadPin(GPIOA, GPIO_PIN_2) == GPIO_PIN_SET){}; // Wait for OUTB to go low
        counter--;
        dir = &quot;CCW&quot;;
while(HAL_GPIO_ReadPin(GPIOA, GPIO_PIN_1) == GPIO_PIN_RESET){}; // Wait for OUTA to go high
while(HAL_GPIO_ReadPin(GPIOA, GPIO_PIN_2) == GPIO_PIN_RESET){}; // Wait for OUTB to go high
        HAL_Delay(10); //Wait for some time
      }
 if (counter&lt;0) counter = 0;
 if (counter&gt;180) counter =180;
  }
  PWMVal = counter*55/10;
  htim1.Instance-&gt;CCR1 = 250 + PWMVal;
// NEED TO ADD A FAILED STATE SOMEWHERE EITHER AN ELSE STATEMENT OR SOMETHING OF THAT NATURE
 if(HAL_GPIO_ReadPin (GPIOA, GPIO_PIN_2))
 {
     sprintf(MSG, &quot;Counter= %d\n\r&quot;, counter);
     HAL_UART_Transmit(&amp;huart1, MSG, sizeof(MSG), 100);
 }
 HAL_Delay(100);
</code></pre>
<p>` }</p>
",45302.58333,,14,0,1,0,,18486691,,44636.84097,1,,,,,,137183558,"You have to start debugging your code more deeply.  ```HAL_UART_Transmit(&huart1, MSG, sizeof(MSG), 100);``` is a very good way of debugging. Use this to understand where your code stops working or crashes or know what is the state of your variables.",Actuator
498,4757,78027853,"Robot that moves with putting in X,Y coordinates of a U shaped field",|python|robotics|dji-sdk|,"<p>So i just started a project for an employer. One of the things they want is a robot that runs on Python and can be controlled from anywhere with a internet connection and a laptop. They hired another person before me who convinced the company to buy a DJI Robomaster EP core<a href=""https://www.dji.com/nl/robomaster-ep"" rel=""nofollow noreferrer"">this</a>.
This robot is programmed in the robomasters app you can download from their website. Now one of the things i need to do is to make it possible to use pycharm to code the robot. So far i havent been able to do that.
Secondly i need to get the robot to move by giving it an X and Y coordinate (which must work from distance). I have been able to get a sort of code for the moving to X,Y and another function which returns it to the starting position(X=0,Y=0), but I haven't tested that yet as I need to figure something out to use pycharm to program the robot. The second problem with the code is that the area it needs to drive in is U shaped, and it cant hit the object in the middle as it is very expensive.</p>
<p>The code atm:</p>
<pre><code>import time
from dji_robomaster_sdk import RoboMaster
from dji_robomaster_sdk import robot
from dji_robomaster_sdk import armor
from dji_robomaster_sdk import chassis

# Initialize the robot
robo = RoboMaster()
robo.initialize()

# Define the starting position
starting_position = (0, 0)

# Define the U-shaped area
area_width = 100
area_height = 50

# Function to make the robot go back to the starting position
def go_to_starting_position():
    global starting_position
    chassis.move(0, 0, 0, 0).wait_for_completed()
    chassis.move(-starting_position[0], 0, 0, 0).wait_for_completed()
    chassis.move(0, 0, 0, 0).wait_for_completed()

# Function to check if the given coordinates are within the U-shaped area
def is_within_area(x, y):
    global area_width, area_height
    return -area_width / 2 &lt;= x &lt;= area_width / 2 and -area_height / 2 &lt;= y &lt;= area_height / 2

# Function to make the robot go to the given coordinates
def go_to_coordinates(x, y):
    global starting_position
    if not is_within_area(x, y):
        print(&quot;The given coordinates are outside the U-shaped area.&quot;)
        return
    if x &lt; 0:
        x = 0
    if y &lt; 0:
        y = 0
    if x &gt; area_width:
        x = area_width
    if y &gt; area_height:
        y = area_height
    chassis.move(x - starting_position[0], y - starting_position[1], 0, 0).wait_for_completed()
    starting_position = (x, y)

# Example usage
go_to_coordinates(50, 0)
time.sleep(2)
go_to_coordinates(0, 25)
time.sleep(2)
go_to_coordinates(-50, 0)
time.sleep(2)
go_to_coordinates(0, -25)
time.sleep(2)
go_to_starting_position()

# Shut down the robot
robo.close()
</code></pre>
<p>I'm not a programmer so i dont even know if the above code is correct.
All help is apreciated!</p>
",45342.57431,,37,1,0,1,,23448729,,45342.55903,2,78096812,"<p>This sounds like a nice robot path-planning problem. A few options:</p>
<ul>
<li>Discretize the space (break it up into a grid) and make some cells &quot;untraversable&quot; and use something like <a href=""https://en.wikipedia.org/wiki/A*_search_algorithm"" rel=""nofollow noreferrer"">A* (A Star)</a> to plan a shortest path through the grid cells. Then iteratively feed each grid cell location to your <code>go_to_coordinates</code> function.</li>
<li>Use a random sampling planner like RRT which generates a tree of randomly sampled points in a space to eventually reach a goal location without passing through obstacles (<a href=""https://github.com/AtsushiSakai/PythonRobotics/tree/master/PathPlanning/RRT"" rel=""nofollow noreferrer"">open source RRT code</a>).</li>
<li>Use a <a href=""https://github.com/mbpeterson70/tomma/blob/main/examples/dubins_trajectory_opt.ipynb"" rel=""nofollow noreferrer"">trajectory optimization approach</a> to create a sequence of timestamped trajectory points to reach the goal while avoiding the obstacle in the middle.</li>
</ul>
",14045755,0,0,,,Remote
499,4744,77819704,ESP32-CAM Arduino Upload Error: Invalid head of packet (0x61),|esp32|arduino-uno|robot|,"<p>i am trying to build a robot car with arduino and esp32-cam but when i'm trying to upload the code through the arduino i get this:</p>
<pre><code>A fatal error occurred: Failed to connect to ESP32: Invalid head of packet (0x61): Possible serial noise or corruption.
</code></pre>
<p>the compile goes smoothly and the problem happens at the upload.
i am connected to ESP32 WROVER MUDOLE, upload speed: 115200.
i checked connections, tried reset but nothing worked.</p>
<pre><code>#include &quot;esp_camera.h&quot;
#include &lt;Arduino.h&gt;
#include &lt;WiFi.h&gt;
#include &lt;AsyncTCP.h&gt;
#include &lt;ESPAsyncWebServer.h&gt;
#include &lt;iostream&gt;
#include &lt;sstream&gt;

struct MOTOR_PINS
{
  int pinEn;  
  int pinIN1;
  int pinIN2;    
};

std::vector&lt;MOTOR_PINS&gt; motorPins = 
{
  {12, 13, 15},  //RIGHT_MOTOR Pins (EnA, IN1, IN2)
  {12, 14, 2},  //LEFT_MOTOR  Pins (EnB, IN3, IN4)
};
#define LIGHT_PIN 4

#define UP 1
#define DOWN 2
#define LEFT 3
#define RIGHT 4
#define STOP 0

#define RIGHT_MOTOR 0
#define LEFT_MOTOR 1

#define FORWARD 1
#define BACKWARD -1

const int PWMFreq = 1000; /* 1 KHz */
const int PWMResolution = 8;
const int PWMSpeedChannel = 2;
const int PWMLightChannel = 3;

//Camera related constants
#define PWDN_GPIO_NUM     32
#define RESET_GPIO_NUM    -1
#define XCLK_GPIO_NUM      0
#define SIOD_GPIO_NUM     26
#define SIOC_GPIO_NUM     27
#define Y9_GPIO_NUM       35
#define Y8_GPIO_NUM       34
#define Y7_GPIO_NUM       39
#define Y6_GPIO_NUM       36
#define Y5_GPIO_NUM       21
#define Y4_GPIO_NUM       19
#define Y3_GPIO_NUM       18
#define Y2_GPIO_NUM        5
#define VSYNC_GPIO_NUM    25
#define HREF_GPIO_NUM     23
#define PCLK_GPIO_NUM     22

const char* ssid     = &quot;MyWiFiCar&quot;;
const char* password = &quot;12345678&quot;;

AsyncWebServer server(80);
AsyncWebSocket wsCamera(&quot;/Camera&quot;);
AsyncWebSocket wsCarInput(&quot;/CarInput&quot;);
uint32_t cameraClientId = 0;

const char* htmlHomePage PROGMEM = R&quot;HTMLHOMEPAGE(
&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
  &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no&quot;&gt;
    &lt;style&gt;
    .arrows {
      font-size:40px;
      color:red;
    }
    td.button {
      background-color:black;
      border-radius:25%;
      box-shadow: 5px 5px #888888;
    }
    td.button:active {
      transform: translate(5px,5px);
      box-shadow: none; 
    }

    .noselect {
      -webkit-touch-callout: none; /* iOS Safari */
        -webkit-user-select: none; /* Safari */
         -khtml-user-select: none; /* Konqueror HTML */
           -moz-user-select: none; /* Firefox */
            -ms-user-select: none; /* Internet Explorer/Edge */
                user-select: none; /* Non-prefixed version, currently
                                      supported by Chrome and Opera */
    }

    .slidecontainer {
      width: 100%;
    }

    .slider {
      -webkit-appearance: none;
      width: 100%;
      height: 15px;
      border-radius: 5px;
      background: #d3d3d3;
      outline: none;
      opacity: 0.7;
      -webkit-transition: .2s;
      transition: opacity .2s;
    }

    .slider:hover {
      opacity: 1;
    }
  
    .slider::-webkit-slider-thumb {
      -webkit-appearance: none;
      appearance: none;
      width: 25px;
      height: 25px;
      border-radius: 50%;
      background: red;
      cursor: pointer;
    }

    .slider::-moz-range-thumb {
      width: 25px;
      height: 25px;
      border-radius: 50%;
      background: red;
      cursor: pointer;
    }

    &lt;/style&gt;
  
  &lt;/head&gt;
  &lt;body class=&quot;noselect&quot; align=&quot;center&quot; style=&quot;background-color:white&quot;&gt;
     
    &lt;!--h2 style=&quot;color: teal;text-align:center;&quot;&gt;Wi-Fi Camera &amp;#128663; Control&lt;/h2--&gt;
    
    &lt;table id=&quot;mainTable&quot; style=&quot;width:400px;margin:auto;table-layout:fixed&quot; CELLSPACING=10&gt;
      &lt;tr&gt;
        &lt;img id=&quot;cameraImage&quot; src=&quot;&quot; style=&quot;width:400px;height:250px&quot;&gt;&lt;/td&gt;
      &lt;/tr&gt; 
      &lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td class=&quot;button&quot; ontouchstart='sendButtonInput(&quot;MoveCar&quot;,&quot;1&quot;)' ontouchend='sendButtonInput(&quot;MoveCar&quot;,&quot;0&quot;)'&gt;&lt;span class=&quot;arrows&quot; &gt;&amp;#8679;&lt;/span&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td class=&quot;button&quot; ontouchstart='sendButtonInput(&quot;MoveCar&quot;,&quot;3&quot;)' ontouchend='sendButtonInput(&quot;MoveCar&quot;,&quot;0&quot;)'&gt;&lt;span class=&quot;arrows&quot; &gt;&amp;#8678;&lt;/span&gt;&lt;/td&gt;
        &lt;td class=&quot;button&quot;&gt;&lt;/td&gt;    
        &lt;td class=&quot;button&quot; ontouchstart='sendButtonInput(&quot;MoveCar&quot;,&quot;4&quot;)' ontouchend='sendButtonInput(&quot;MoveCar&quot;,&quot;0&quot;)'&gt;&lt;span class=&quot;arrows&quot; &gt;&amp;#8680;&lt;/span&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td class=&quot;button&quot; ontouchstart='sendButtonInput(&quot;MoveCar&quot;,&quot;2&quot;)' ontouchend='sendButtonInput(&quot;MoveCar&quot;,&quot;0&quot;)'&gt;&lt;span class=&quot;arrows&quot; &gt;&amp;#8681;&lt;/span&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr/&gt;&lt;tr/&gt;
      &lt;tr&gt;
        &lt;td style=&quot;text-align:left&quot;&gt;&lt;b&gt;Speed:&lt;/b&gt;&lt;/td&gt;
        &lt;td colspan=2&gt;
         &lt;div class=&quot;slidecontainer&quot;&gt;
            &lt;input type=&quot;range&quot; min=&quot;0&quot; max=&quot;255&quot; value=&quot;150&quot; class=&quot;slider&quot; id=&quot;Speed&quot; oninput='sendButtonInput(&quot;Speed&quot;,value)'&gt;
          &lt;/div&gt;
        &lt;/td&gt;
      &lt;/tr&gt;        
      &lt;tr&gt;
        &lt;td style=&quot;text-align:left&quot;&gt;&lt;b&gt;Light:&lt;/b&gt;&lt;/td&gt;
        &lt;td colspan=2&gt;
          &lt;div class=&quot;slidecontainer&quot;&gt;
            &lt;input type=&quot;range&quot; min=&quot;0&quot; max=&quot;255&quot; value=&quot;0&quot; class=&quot;slider&quot; id=&quot;Light&quot; oninput='sendButtonInput(&quot;Light&quot;,value)'&gt;
          &lt;/div&gt;
        &lt;/td&gt;   
      &lt;/tr&gt;
    &lt;/table&gt;
  
    &lt;script&gt;
      var webSocketCameraUrl = &quot;ws:\/\/&quot; + window.location.hostname + &quot;/Camera&quot;;
      var webSocketCarInputUrl = &quot;ws:\/\/&quot; + window.location.hostname + &quot;/CarInput&quot;;      
      var websocketCamera;
      var websocketCarInput;
      
      function initCameraWebSocket() 
      {
        websocketCamera = new WebSocket(webSocketCameraUrl);
        websocketCamera.binaryType = 'blob';
        websocketCamera.onopen    = function(event){};
        websocketCamera.onclose   = function(event){setTimeout(initCameraWebSocket, 2000);};
        websocketCamera.onmessage = function(event)
        {
          var imageId = document.getElementById(&quot;cameraImage&quot;);
          imageId.src = URL.createObjectURL(event.data);
        };
      }
      
      function initCarInputWebSocket() 
      {
        websocketCarInput = new WebSocket(webSocketCarInputUrl);
        websocketCarInput.onopen    = function(event)
        {
          var speedButton = document.getElementById(&quot;Speed&quot;);
          sendButtonInput(&quot;Speed&quot;, speedButton.value);
          var lightButton = document.getElementById(&quot;Light&quot;);
          sendButtonInput(&quot;Light&quot;, lightButton.value);
        };
        websocketCarInput.onclose   = function(event){setTimeout(initCarInputWebSocket, 2000);};
        websocketCarInput.onmessage = function(event){};        
      }
      
      function initWebSocket() 
      {
        initCameraWebSocket ();
        initCarInputWebSocket();
      }

      function sendButtonInput(key, value) 
      {
        var data = key + &quot;,&quot; + value;
        websocketCarInput.send(data);
      }
    
      window.onload = initWebSocket;
      document.getElementById(&quot;mainTable&quot;).addEventListener(&quot;touchend&quot;, function(event){
        event.preventDefault()
      });      
    &lt;/script&gt;
  &lt;/body&gt;    
&lt;/html&gt;
)HTMLHOMEPAGE&quot;;


void rotateMotor(int motorNumber, int motorDirection)
{
  if (motorDirection == FORWARD)
  {
    digitalWrite(motorPins[motorNumber].pinIN1, HIGH);
    digitalWrite(motorPins[motorNumber].pinIN2, LOW);    
  }
  else if (motorDirection == BACKWARD)
  {
    digitalWrite(motorPins[motorNumber].pinIN1, LOW);
    digitalWrite(motorPins[motorNumber].pinIN2, HIGH);     
  }
  else
  {
    digitalWrite(motorPins[motorNumber].pinIN1, LOW);
    digitalWrite(motorPins[motorNumber].pinIN2, LOW);       
  }
}

void moveCar(int inputValue)
{
  Serial.printf(&quot;Got value as %d\n&quot;, inputValue);  
  switch(inputValue)
  {

    case UP:
      rotateMotor(RIGHT_MOTOR, FORWARD);
      rotateMotor(LEFT_MOTOR, FORWARD);                  
      break;
  
    case DOWN:
      rotateMotor(RIGHT_MOTOR, BACKWARD);
      rotateMotor(LEFT_MOTOR, BACKWARD);  
      break;
  
    case LEFT:
      rotateMotor(RIGHT_MOTOR, FORWARD);
      rotateMotor(LEFT_MOTOR, BACKWARD);  
      break;
  
    case RIGHT:
      rotateMotor(RIGHT_MOTOR, BACKWARD);
      rotateMotor(LEFT_MOTOR, FORWARD); 
      break;
 
    case STOP:
      rotateMotor(RIGHT_MOTOR, STOP);
      rotateMotor(LEFT_MOTOR, STOP);    
      break;
  
    default:
      rotateMotor(RIGHT_MOTOR, STOP);
      rotateMotor(LEFT_MOTOR, STOP);    
      break;
  }
}

void handleRoot(AsyncWebServerRequest *request) 
{
  request-&gt;send_P(200, &quot;text/html&quot;, htmlHomePage);
}

void handleNotFound(AsyncWebServerRequest *request) 
{
    request-&gt;send(404, &quot;text/plain&quot;, &quot;File Not Found&quot;);
}

void onCarInputWebSocketEvent(AsyncWebSocket *server, 
                      AsyncWebSocketClient *client, 
                      AwsEventType type,
                      void *arg, 
                      uint8_t *data, 
                      size_t len) 
{                      
  switch (type) 
  {
    case WS_EVT_CONNECT:
      Serial.printf(&quot;WebSocket client #%u connected from %s\n&quot;, client-&gt;id(), client-&gt;remoteIP().toString().c_str());
      break;
    case WS_EVT_DISCONNECT:
      Serial.printf(&quot;WebSocket client #%u disconnected\n&quot;, client-&gt;id());
      moveCar(0);
      ledcWrite(PWMLightChannel, 0);  
      break;
    case WS_EVT_DATA:
      AwsFrameInfo *info;
      info = (AwsFrameInfo*)arg;
      if (info-&gt;final &amp;&amp; info-&gt;index == 0 &amp;&amp; info-&gt;len == len &amp;&amp; info-&gt;opcode == WS_TEXT) 
      {
        std::string myData = &quot;&quot;;
        myData.assign((char *)data, len);
        std::istringstream ss(myData);
        std::string key, value;
        std::getline(ss, key, ',');
        std::getline(ss, value, ',');
        Serial.printf(&quot;Key [%s] Value[%s]\n&quot;, key.c_str(), value.c_str()); 
        int valueInt = atoi(value.c_str());     
        if (key == &quot;MoveCar&quot;)
        {
          moveCar(valueInt);        
        }
        else if (key == &quot;Speed&quot;)
        {
          ledcWrite(PWMSpeedChannel, valueInt);
        }
        else if (key == &quot;Light&quot;)
        {
          ledcWrite(PWMLightChannel, valueInt);         
        }     
      }
      break;
    case WS_EVT_PONG:
    case WS_EVT_ERROR:
      break;
    default:
      break;  
  }
}

void onCameraWebSocketEvent(AsyncWebSocket *server, 
                      AsyncWebSocketClient *client, 
                      AwsEventType type,
                      void *arg, 
                      uint8_t *data, 
                      size_t len) 
{                      
  switch (type) 
  {
    case WS_EVT_CONNECT:
      Serial.printf(&quot;WebSocket client #%u connected from %s\n&quot;, client-&gt;id(), client-&gt;remoteIP().toString().c_str());
      cameraClientId = client-&gt;id();
      break;
    case WS_EVT_DISCONNECT:
      Serial.printf(&quot;WebSocket client #%u disconnected\n&quot;, client-&gt;id());
      cameraClientId = 0;
      break;
    case WS_EVT_DATA:
      break;
    case WS_EVT_PONG:
    case WS_EVT_ERROR:
      break;
    default:
      break;  
  }
}

void setupCamera()
{
  camera_config_t config;
  config.ledc_channel = LEDC_CHANNEL_0;
  config.ledc_timer = LEDC_TIMER_0;
  config.pin_d0 = Y2_GPIO_NUM;
  config.pin_d1 = Y3_GPIO_NUM;
  config.pin_d2 = Y4_GPIO_NUM;
  config.pin_d3 = Y5_GPIO_NUM;
  config.pin_d4 = Y6_GPIO_NUM;
  config.pin_d5 = Y7_GPIO_NUM;
  config.pin_d6 = Y8_GPIO_NUM;
  config.pin_d7 = Y9_GPIO_NUM;
  config.pin_xclk = XCLK_GPIO_NUM;
  config.pin_pclk = PCLK_GPIO_NUM;
  config.pin_vsync = VSYNC_GPIO_NUM;
  config.pin_href = HREF_GPIO_NUM;
  config.pin_sscb_sda = SIOD_GPIO_NUM;
  config.pin_sscb_scl = SIOC_GPIO_NUM;
  config.pin_pwdn = PWDN_GPIO_NUM;
  config.pin_reset = RESET_GPIO_NUM;
  config.xclk_freq_hz = 20000000;
  config.pixel_format = PIXFORMAT_JPEG;
  
  config.frame_size = FRAMESIZE_VGA;
  config.jpeg_quality = 10;
  config.fb_count = 1;

  // camera init
  esp_err_t err = esp_camera_init(&amp;config);
  if (err != ESP_OK) 
  {
    Serial.printf(&quot;Camera init failed with error 0x%x&quot;, err);
    return;
  }  

  if (psramFound())
  {
    heap_caps_malloc_extmem_enable(20000);  
    Serial.printf(&quot;PSRAM initialized. malloc to take memory from psram above this size&quot;);    
  }  
}

void sendCameraPicture()
{
  if (cameraClientId == 0)
  {
    return;
  }
  unsigned long  startTime1 = millis();
  //capture a frame
  camera_fb_t * fb = esp_camera_fb_get();
  if (!fb) 
  {
      Serial.println(&quot;Frame buffer could not be acquired&quot;);
      return;
  }

  unsigned long  startTime2 = millis();
  wsCamera.binary(cameraClientId, fb-&gt;buf, fb-&gt;len);
  esp_camera_fb_return(fb);
    
  //Wait for message to be delivered
  while (true)
  {
    AsyncWebSocketClient * clientPointer = wsCamera.client(cameraClientId);
    if (!clientPointer || !(clientPointer-&gt;queueIsFull()))
    {
      break;
    }
    delay(1);
  }
  
  unsigned long  startTime3 = millis();  
  Serial.printf(&quot;Time taken Total: %d|%d|%d\n&quot;,startTime3 - startTime1, startTime2 - startTime1, startTime3-startTime2 );
}

void setUpPinModes()
{
  //Set up PWM
  ledcSetup(PWMSpeedChannel, PWMFreq, PWMResolution);
  ledcSetup(PWMLightChannel, PWMFreq, PWMResolution);
      
  for (int i = 0; i &lt; motorPins.size(); i++)
  {
    pinMode(motorPins[i].pinEn, OUTPUT);    
    pinMode(motorPins[i].pinIN1, OUTPUT);
    pinMode(motorPins[i].pinIN2, OUTPUT);  

    /* Attach the PWM Channel to the motor enb Pin */
    ledcAttachPin(motorPins[i].pinEn, PWMSpeedChannel);
  }
  moveCar(STOP);

  pinMode(LIGHT_PIN, OUTPUT);    
  ledcAttachPin(LIGHT_PIN, PWMLightChannel);
}


void setup(void) 
{
  setUpPinModes();
  Serial.begin(115200);

  WiFi.softAP(ssid, password);
  IPAddress IP = WiFi.softAPIP();
  Serial.print(&quot;AP IP address: &quot;);
  Serial.println(IP);

  server.on(&quot;/&quot;, HTTP_GET, handleRoot);
  server.onNotFound(handleNotFound);
      
  wsCamera.onEvent(onCameraWebSocketEvent);
  server.addHandler(&amp;wsCamera);

  wsCarInput.onEvent(onCarInputWebSocketEvent);
  server.addHandler(&amp;wsCarInput);

  server.begin();
  Serial.println(&quot;HTTP server started&quot;);

  setupCamera();
}


void loop() 
{
  wsCamera.cleanupClients(); 
  wsCarInput.cleanupClients(); 
  sendCameraPicture(); 
  Serial.printf(&quot;SPIRam Total heap %d, SPIRam Free Heap %d\n&quot;, ESP.getPsramSize(), ESP.getFreePsram());
}
</code></pre>
",45306.50833,,36,0,1,0,,14888644,,44190.69167,14,,,,,,137194280,is the esp32 in bootlotader mode? how is it connected to the computer? upload of Blink works?,Connections
